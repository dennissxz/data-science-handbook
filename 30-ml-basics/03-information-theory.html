
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Information Theory &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Kernels" href="05-kernels.html" />
    <link rel="prev" title="Taxonomy" href="02-taxonomy.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-extension.html">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-t-SNE.html">
     SNE and $t$-SNE
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     K-means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/30-ml-basics/03-information-theory.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F30-ml-basics/03-information-theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shannon-entropy">
     Shannon Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differential-entropy">
     Differential Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-entropy">
     Joint Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-entropy">
     Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-entropy">
     Cross Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kullback-leibler-divergence">
     Kullback-Leibler Divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mutual-information">
     Mutual Information
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#identities">
   Identities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chain-rule-for-conditional-entropy">
     Chain Rule for Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-rule-for-conditional-entropy">
     Bayes’ Rule for Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#others">
     Others
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inequalities">
   Inequalities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-processing-inequality">
     Data Processing Inequality
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="information-theory">
<h1>Information Theory<a class="headerlink" href="#information-theory" title="Permalink to this headline">¶</a></h1>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<p>First we introduce the definitions of some fundamental concepts in informaition theory summarized in the table below.</p>
<p>$$\begin{align}
&amp; \text{Entropy} &amp; \operatorname{H}(Y)&amp;=-\sum_{\mathcal{Y}}p(y)\log p(y) \
&amp; \text{Differntial Entropy} &amp; h(Y)&amp;=-\int_{\mathcal{Y}}f(y)\log f(y)dy \
&amp; \text{Joint Entropy} &amp; \operatorname{H}(X,Y)&amp;=-\operatorname{E}<em>{X,Y\sim p(x,y)}\log\operatorname{P}(X,Y) \
&amp; \text{Conditional Entropy} &amp; \operatorname{H}(Y\vert X)&amp;=-\operatorname{E}</em>{X,Y\sim p(x,y)}\log \operatorname{P}(Y\mid X) \
&amp; \text{Cross Entropy} &amp; \operatorname{H}(P,Q)&amp;=-\operatorname{E}<em>{Y\sim P}\left[\ln\left(Q(Y)\right)\right] \
&amp; \text{KL Divergence} &amp; \operatorname{KL}(P,Q)&amp;=\operatorname{E}</em>{Y\sim P}\left[\ln\frac{P(Y)}{Q(Y)}\right] \
&amp; \text{Mutual Information} &amp; \operatorname{I}(X,Y)&amp;=\operatorname{KL}\left(P_{X,Y},P_{X}P_{Y}\right)
\end{align}$$</p>
<div class="section" id="shannon-entropy">
<h3>Shannon Entropy<a class="headerlink" href="#shannon-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The Shannon Entropy of a discrete distribution $p\left( y \right)$ is defined by</p>
</dd>
</dl>
<p>$$\begin{align}
\operatorname{H}\left( Y \right)
&amp;= \operatorname{E}<em>{Y\sim p(y)}\left[ - \log \operatorname{P}\left( Y \right) \right]\
&amp;= -\sum</em>{i=1}^{n}p(y_i)\log p(y_i)
\end{align}$$</p>
<p>This quantity measures the average level of “information”, or “uncertainty” inherent in the variables’ possible outcomes.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Entropy has similarity with variance. Both are non-negative, measure uncertainty/information. But variance depends on the observed value $y$ of the random variable $Y$, while entropy only depends on the probability $p(y)$.</p>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><p>$\operatorname{H}\left( Y \right) &gt; 0$</p>
</dd>
</dl>
</div>
<div class="section" id="differential-entropy">
<h3>Differential Entropy<a class="headerlink" href="#differential-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Let $Y$ be a random variable with probability density function $f$ whose support is $\mathcal{Y}$. The differential entropy of $Y$, denoted $h(Y)$, is defined as</p>
<p>$$
h(Y)=-\int_{\mathcal{Y}}f(y)\log f(y)dy
$$</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Differential Entropy began as an attempt by Shannon to extend the idea of Shannon entropy to continuous PDF. Actually, the correct extension of Shannon entropy to continuous distributions is called limiting density of discrete points (LDDP), while differential entropy, aka continuous entropy, is a limiting case of the LDDP.</p>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul>
<li><p>Can be negative</p>
<p>Differential entropy does not have all properties of discrete entropy. For instance, discrete entropy is always greater than 0, while differential entropy can be less than 0, or diverges to $-\infty$.</p>
</li>
<li><p>Sensitive to units</p>
<p>The value of differential entropy is sensitive to the choice of units.</p>
</li>
<li><p>Transformation</p>
<p>The entropy of processed information $h\left(z(Y)\right)$ can be larger <strong>or</strong> smaller than the original entropy $h(Y)$. Just consider $z=ay$ for $a&gt;1$ and $a&lt;1$.</p>
</li>
</ul>
</dd>
<dt>Example</dt><dd><ul>
<li><p>Consider a uniform distribution over an interval on the real line of width $\Delta$, then $p(y) = \frac{1}{\Delta}$, and hence the entropy is</p>
<p>$$\begin{align}
h(X) &amp;= \operatorname{E}_{x\sim p(\cdot)}\left( - \ln \frac{1}{\Delta}  \right)\
&amp;= \ln \Delta \
\end{align}$$</p>
<p>As $\Delta \rightarrow 0$, we have $h\rightarrow -\infty$.</p>
</li>
<li><p>The entropy for a Gaussian variable with density $N(0, \sigma)$ is</p>
<p>$$
h \left( N(0, \sigma) \right) = C + \ln \sigma
$$</p>
<p>As $\sigma \rightarrow 0$, we have $h\rightarrow -\infty$.</p>
</li>
<li><p>Consider the uniform distribution above. Suppose the unit is meter and the interval is $(0, 1000)$. Note if we change the unit to kilometer, then the interval changes to $(0, 1)$, and the interval decreases from $\ln 1000$ to $\ln 1$.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The right way to think about differential entropy is that, it is actually <strong>infinite</strong>. Recall entropy measures uncertainty. An actual real number carries an infinite number of bits, i.e. infinite amount of information. A meaningful convention is the $h(f)=+\infty$ for any continuous density $p(y)$.</p>
</div>
</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="joint-entropy">
<h3>Joint Entropy<a class="headerlink" href="#joint-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The joint Shannon entropy of two discrete random variables $X$ and $Y$ with joint distribution $p(x,y)$ is defined as</p>
<p>$$
\begin{align}
\operatorname{H}(X,Y)
&amp; = \operatorname{E}<em>{X,Y\sim p(x, y)}\left[ - \log \operatorname{P}(X,Y) \right]\
&amp; = - \sum</em>{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log_{2}[p(x,y)]
\end{align}
$$</p>
<p>More generally, for multiple random variables, we can define</p>
<p>$$
\operatorname{H}\left(X_{1},\ldots,X_{n}\right)=-\sum_{x_{1}\in\mathcal{X}<em>{1}}\ldots\sum</em>{x_{n}\in\mathcal{X}<em>{n}}p\left(x</em>{1},\ldots,x_{n}\right)\log_{2}\left[p\left(x_{1},\ldots,x_{n}\right)\right]
$$</p>
<p>Likewise, we can define joint differential entropy for two continuous random variables with joint density $f(x,y)$ as</p>
<p>$$
h(X,Y)=-\int_{\mathcal{X},\mathcal{Y}}f(x,y)\log f(x,y)dxdy
$$</p>
<p>and</p>
<p>$$
h\left(X_{1},\ldots,X_{n}\right)=-\int f\left(x_{1},\ldots,x_{n}\right)\log f\left(x_{1},\ldots,x_{n}\right)dx_{1}\ldots dx_{n}
$$</p>
</dd>
<dt>Properties of Discrete Joint Entropy</dt><dd><ul class="simple">
<li><p>Nonnegativeity: $\operatorname{H}(X,Y)\ge0$</p></li>
<li><p>Greater than or equal to individual entropies: $\operatorname{H}(X,Y)\ge\max\left(\operatorname{H}(X),\operatorname{H}(Y)\right)$</p></li>
<li><p>Less than or equal to the sum of individual entropies $\operatorname{H}(X,Y)\le \operatorname{H}(X)+\operatorname{H}(Y)$</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="conditional-entropy">
<h3>Conditional Entropy<a class="headerlink" href="#conditional-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>For two random variables $X, Y$ with density function $p(x,y)$, the conditional entropy of $Y$ given $X$ is defined as</p>
<p>$$\begin{align}
\operatorname{H}(Y\mid X)	
&amp; = \operatorname{E}<em>{X,Y\sim p(x, y)}\left[ - \log \operatorname{P}(Y \mid X) \right]\
&amp;=-\sum</em>{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(y\vert x) \
h(Y\mid X)	&amp;=-\int_{\mathcal{X},\mathcal{Y}}f(x,y)\log f(y\vert x)dxdy
\end{align}$$</p>
<p>It also quantifies the amount of information needed to describe the outcome of a random variable $Y$ given the value of another random variable $X$.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One can compare entropy vs. conditional entropy, and probability vs conditional probability. Both are defined by the expectation of negative log probability.</p>
<ul class="simple">
<li><p>the entropy of $Y$ is the expectation of negative unconditional log probability $\log p(y)$</p></li>
<li><p>the conditional entropy of $Y$ given $X$ is the expectation of the negative conditional log probability $\log p(y \mid x)$</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Conditional entropy samples $X,Y$ from the joint distribution $p(x,y)$. This is different from conditional expectation, which samples $Y$ from the conditional distribution $Y \sim p_{Y\mid X}(y\mid x)$.</p>
</div>
<p>To understand it, first we consider the entropy of $Y$ given $X$ takes a certain value $x$. Recall the unconditional entropy of $Y$ is</p>
<p>$$
\operatorname{H}(Y)=-\sum_{y\in\mathcal{Y}}\operatorname{P}(Y=y)\log_{2}\operatorname{P}(Y=y)
$$</p>
<p>Now $X$ is known to be $x$, then the distribution of $Y$ becomes conditional on $X=x$. We just replace $\operatorname{P}(Y=y)$ by $\operatorname{P}(Y=y\mid X=x)$. Hence,</p>
<p>$$
\operatorname{H}(Y\vert X=x)=-\sum_{y\in\mathcal{Y}}\operatorname{P}(Y=y\mid X=x)\log_{2}\operatorname{P}(Y=y\mid X=x)
$$</p>
<p>Finally, the conditional entropy $\operatorname{H}(Y\vert X)$ is defined as the sum of $\operatorname{H}(Y\vert X=x)$ weighted on $\operatorname{P}(X=x)$, i.e.,</p>
<p>$$
\begin{aligned}\operatorname{H}(Y\vert X) &amp; \equiv\sum_{x\in\mathcal{X}}p(x)\operatorname{H}(Y\vert X=x)\
&amp; =-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y\vert x)\log p(y\vert x)\
&amp; =-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(y\vert x)\
&amp; =-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)}
\end{aligned}
$$</p>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p>$\operatorname{H}(Y\vert X)=0$ iff $Y$ is completely determined by the value of $X$</p></li>
<li><p>$\operatorname{H}(Y\vert X)=\operatorname{H}(Y)$ iff $Y$ and $X$ are independent</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="cross-entropy">
<h3>Cross Entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The cross entropy of two distributions $P$ and $Q$ on the same
support $\mathcal{Y}$ is defined as</p>
</dd>
</dl>
<p>$$
\begin{align}
\operatorname{H}\left( P, Q \right) &amp; =\operatorname{E}<em>{Y\sim p(y)}\left[-\ln\left(Q(Y)\right)\right]\
&amp; =\int</em>{\mathcal{Y}}f_{P}(y)\left[-\ln\left(f_{Q}(y)\right)\right]\text{d}y\quad\text{for continuous}\ P,Q\
&amp; =-\sum_{y\in\mathcal{Y}}P(y)\log Q(y)\qquad\qquad\text{for discrete}\ P,Q
\end{align}
$$</p>
<p>Cross entropy can be interpreted as the <strong>expected message-length</strong> per datum when a wrong distribution $Q$ is assumed while the data actually follows a distribution $P$. If we use $\log_{2}$, the quantity $\operatorname{H}\left( P, Q \right)$ can also be interpreted
as 1.44 times the number of bits used to code draws from $P$ when
using the imperfect code defined by $Q$.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The cross entropy $\operatorname{H}(P,Q)$ of <strong>two</strong> distributions $P(y), Q(y)$ is different from the joint entropy $\operatorname{H}(X,Y)$ of <strong>one</strong> joint distribution $p(x,y)$ of <strong>two</strong> random variables $X, Y$.</p></li>
<li><p>“Cross” means we sample $y$ from one distribution $P(y)$, and then compute the negative log probability with the other distribution $-\ln(Q(y))$.</p></li>
</ul>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p>asymmetric: $\operatorname{H}\left( P, Q \right)\ne \operatorname{H}(Q,P)$</p></li>
<li><p>larger than entropy: $\operatorname{H}\left( P, Q \right)\ge \operatorname{H}(P)$ with equality iff $Q=P$</p></li>
</ul>
</dd>
</dl>
<p>Cross entropy is tightly related to KL divergence.</p>
</div>
<div class="section" id="kullback-leibler-divergence">
<h3>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this headline">¶</a></h3>
<p>: The KL Divergence of two distributions $P$ and $Q$ on the \textbf{same}
support $\mathcal{Y}$ is defined as</p>
<p>$$
\begin{aligned}
\operatorname{KL}\left( P, Q \right) &amp; =\operatorname{E}_{Y\sim p(y)}\left[\ln\frac{P(Y)}{Q(Y)}\right]
\end{aligned}
$$</p>
<p>Kullback–Leibler divergence (also called relative entropy) is a  measure of how one probability distribution is different from a second,  reference probability distribution, i.e. the <strong>distance</strong> between  two distributions on the same support. It is a distribution-wise <strong>asymmetric</strong> measure and thus does not qualify as a statistical <strong>metric</strong> of spread - it also does not satisfy the triangle inequality.</p>
<dl class="simple myst">
<dt>Properties</dt><dd><ul>
<li><p>$\operatorname{KL}\left( P, Q \right) \ge 0$, with equality iff the two distributions are identical.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Proof<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">KL divergence is non-negative by Jensen’s inequality of convex functions</p>
<p class="card-text">$$
\begin{aligned}\operatorname{KL}\left( P, Q \right) &amp; =\operatorname{E}<em>{y\sim p(\cdot)}\left[-\ln\frac{Q(y)}{P(y)}\right]\
&amp; \geq-\ln \operatorname{E}</em>{y\sim p(\cdot)}\frac{Q(y)}{P(y)}\
&amp; =-\ln\sum_{y}P(y)\frac{Q(y)}{P(y)}\
&amp; =-\ln\sum_{y}Q(y)\
&amp; =0
\end{aligned}
$$</p>
<p class="card-text">For two continuous distributions,</p>
<p class="card-text">$$\begin{align}
\operatorname{KL}\left( P, Q \right) &amp; =\int_{-\infty}^{\infty}p(y)\log\left(\frac{p(y)}{q(y)}\right)dy\
&amp; \ge0
\end{align}$$</p>
<p class="card-text">Equality holds iff $\forall y\ P(y)=Q(y)$, i.e. two distributions
are identical.</p>
</div>
</details></li>
<li><p>Relation to cross entropy:</p>
<p>$$
\operatorname{KL}\left( P, Q \right)=\operatorname{H}\left( P, Q \right)-\operatorname{H}(P)
$$</p>
</li>
<li><p>If $P$ is some fixed distribution, then $\operatorname{H}(P)$ is a constant. Hence,</p>
<p>$$
\operatorname{argmax}<em>{Q}\ \operatorname{KL}\left( P, Q \right)=\operatorname{argmax}</em>{Q}\operatorname{H}\left( P, Q \right)
$$</p>
<p>i.e. minimizing/maximizing the KL divergence $\operatorname{KL}\left( P, Q \right)$ of two distributions is equivalent to minimizing/maximizing their cross entropy $\operatorname{H}\left( P, Q \right)$, given $P$ is a fixed distribution.</p>
</li>
</ul>
</dd>
<dt>Example</dt><dd><p>The KL-Divergence between two multivariate Gaussian $\mathcal{N}(\mu_{p},\Sigma_{p})$
and $\mathcal{N}(\mu_{q},\Sigma_{q})$ is</p>
<p>$$
\operatorname{KL}\left( P, Q \right)=\frac{1}{2}\left[\log\frac{\left|\Sigma_{q}\right|}{\left|\Sigma_{p}\right|}-k+\left(\mu_{p}-\mu_{q}\right)^{T}\Sigma_{q}^{-1}\left(\mu_{p}-\mu_{q}\right)+\operatorname{tr}\left{ \Sigma_{q}^{-1}\Sigma_{p}\right} \right]
$$</p>
<p>In particular, if the reference distribution $Q$ is $\mathcal{N}(0,I)$ then we get</p>
<p>$$
\operatorname{KL}\left( P, Q \right)=\frac{1}{2}\left[\Vert\mu_{p}\Vert^{2}+\operatorname{tr}\left{ \Sigma_{p}\right} -k-\log\left|\Sigma_{p}\right|\right]
$$</p>
</dd>
</dl>
</div>
<div class="section" id="mutual-information">
<h3>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this headline">¶</a></h3>
<p>Aka information gain.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Let $\left(X,Y\right)$ be a pair of random variables with values over the space $\mathcal{X}\times\mathcal{Y}$. Suppose their joint distribution is $\operatorname{P}<em>{X,Y}$ and the marginal distributions are $\operatorname{P}</em>{X}$ and $\operatorname{P}_{Y}$. The mutual information of $X, Y$ is defined by a KL divergence</p>
<p>$$\begin{align}
\operatorname{I}\left(X, Y \right) &amp; = \operatorname{KL}\left(\operatorname{P}<em>{X,Y},\operatorname{P}</em>{X}\operatorname{P}<em>{Y}\right)\
&amp; = \operatorname{E}</em>{X,Y}\left[ \ln\frac{\operatorname{P}_{X,Y}(X,Y)}{\operatorname{P}_X(X)\operatorname{P}_Y(Y)} \right]\
\end{align}$$</p>
<p>For discrete case,</p>
<p>$$
\operatorname{I}\left(X, Y \right)=\sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}p_{X,Y}(x,y)\log\left(\frac{p_{(X,Y)}(x,y)}{p_{X}(x)p_{Y}(y)}\right)
$$</p>
<p>and for continuous case,</p>
<p>$$
\operatorname{I}\left(X, Y \right)=\int_{y}\int_{\mathcal{X}}p_{X,Y}(x,y)\log\left(\frac{p_{(X,Y)}(x,y)}{p_{X}(x)p_{Y}(y)}\right)
$$</p>
</dd>
<dt>Properties</dt><dd><p>$\operatorname{I}(X,Y) \ge 0$, with equality holds iff $P_{X,Y}=P_{X}P_{Y}$, i.e. when $X$ and $Y$ are independent, and hence there is no mutual dependence.</p>
</dd>
</dl>
<p>Mutual information is a measure of the mutual <strong>dependence</strong> between the two variables. More specifically, it quantifies the amount of information (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Similar to absolute correlation \left\vert \rho \right\vert? Both are non-negative, larger when
$X$ and $Y$ are dependent, and equal 0 when independent. But $\vert\rho\vert=0\ \not{\Rightarrow}\ X\perp Y$ while $\operatorname{I}\left(X, Y \right)=0\ \Leftrightarrow\ X\perp Y$.</p>
</div>
</div>
</div>
<div class="section" id="identities">
<h2>Identities<a class="headerlink" href="#identities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="chain-rule-for-conditional-entropy">
<h3>Chain Rule for Conditional Entropy<a class="headerlink" href="#chain-rule-for-conditional-entropy" title="Permalink to this headline">¶</a></h3>
<p>Analogous to $p(y\vert x)=\frac{p(x,y)}{p(x)}$ in probability, we have an equation that connects entropy, joint entropy and conditional entropy. Instead of division, we use subtraction,</p>
<p>$$
\operatorname{H}(Y\vert X)=\operatorname{H}(X,Y)-\operatorname{H}(X)
$$</p>
<p>where we have the following interpretation</p>
<ul class="simple">
<li><p>$\operatorname{H}(X,Y)$ measures the bits of information on average to describe the state of the combined system $(X,Y)$</p></li>
<li><p>$\operatorname{H}(X)$ measures the bits of information we have about $\operatorname{H}(X)$</p></li>
<li><p>$\operatorname{H}(Y\vert X)$ measures the <strong>additional</strong> information required to describe $(X,Y)$, given $X$. Note that $\operatorname{H}(Y\vert X)\le \operatorname{H}(Y)$ with equality iff $X\bot Y$.</p></li>
</ul>
<p>The general form for multiple random variables is</p>
<p>$$ \operatorname{H}\left(X_{1},X_{2},\ldots,X_{n}\right)=\sum_{i=1}^{n}\operatorname{H}\left(X_{i}\vert X_{1},\ldots,X_{i-1}\right)
$$</p>
<p>which has a similar form to chain rule in probability theory, except that here is addition $\sum_{i=1}^{n}$ instead of multiplication $\Pi_{i=1}^{n}$.</p>
<p><em><strong>Proof</strong></em></p>
<p>By definition,</p>
<p>$$
\begin{aligned}\operatorname{H}(Y\vert X)
&amp; = -\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x)}{p(x,y)}\
&amp; =- \sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)(\log p(x)-\log p(x,y))\
&amp; =-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(x,y)+\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(x)\
&amp; =\operatorname{H}(X,Y)+\sum_{x\in\mathcal{X}}p(x)\log p(x)\
&amp; =\operatorname{H}(X,Y)-\operatorname{H}(X)
\end{aligned}
$$</p>
</div>
<div class="section" id="bayes-rule-for-conditional-entropy">
<h3>Bayes’ Rule for Conditional Entropy<a class="headerlink" href="#bayes-rule-for-conditional-entropy" title="Permalink to this headline">¶</a></h3>
<p>Analogous to the Bayes rule $p(y\vert x)=\frac{p(x\vert y)p(y)}{p(x)}$ in probability, we have an equation to link $\operatorname{H}(Y\vert X)$ and $\operatorname{H}(X\vert Y)$, which is simply a result from the chain rule. Instead of division for the Bayes’ rule in probability, we use subtraction</p>
<p>$$
\operatorname{H}(Y\vert X)=\operatorname{H}(X\vert Y)-\operatorname{H}(X)+\operatorname{H}(Y)
$$</p>
</div>
<div class="section" id="others">
<h3>Others<a class="headerlink" href="#others" title="Permalink to this headline">¶</a></h3>
<p>$$
\begin{aligned}\operatorname{H}(X,Y) &amp; =\operatorname{H}(X\vert Y)+\operatorname{H}(Y\vert X)+\operatorname{I}\left(X, Y \right)\
\operatorname{H}(X,Y) &amp; =\operatorname{H}(X)+\operatorname{H}(Y)-\operatorname{I}\left(X, Y \right)\
\operatorname{I}\left(X, Y \right) &amp; \leq \operatorname{H}(X)
\end{aligned}
$$</p>
</div>
</div>
<div class="section" id="inequalities">
<h2>Inequalities<a class="headerlink" href="#inequalities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-processing-inequality">
<h3>Data Processing Inequality<a class="headerlink" href="#data-processing-inequality" title="Permalink to this headline">¶</a></h3>
<p>The data processing inequality says post-processing cannot increase information.</p>
<p>Let three random variables form the Markov Chain $X\rightarrow Y\rightarrow Z$, implying that the conditional distribution of $Z$ depends only on $Y$ and is conditionally independent of $X$. The joint PMF can be written as</p>
<p>$$p(x,y,z)=p(x)p(y\mid x)p(z\mid y)$$</p>
<p>In this setting, no processing $Z(Y)$ of $Y$, deterministic or random, can increase the information that $Y$ contains about $X$.</p>
<p>$$\operatorname{I}\left(X, Y \right)\ge I(X,Z)$$</p>
<p>with the quality iff $Z$ and $Y$ contain the same information about $X$.</p>
<p>Note that $\operatorname{H}(Z)\le \operatorname{H}(Y)$ but $h(Z)$ can be larger or smaller than $h(Y)$.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./30-ml-basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="02-taxonomy.html" title="previous page">Taxonomy</a>
    <a class='right-next' id="next-link" href="05-kernels.html" title="next page">Kernels</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>