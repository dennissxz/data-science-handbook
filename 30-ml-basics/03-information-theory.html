
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Information Theory &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Kernels" href="05-kernels.html" />
    <link rel="prev" title="Taxonomy" href="02-taxonomy.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05-model-selection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/41-penalized-regression.html">
     Penalized Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/12-pca-variants.html">
     PCA Variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/61-indep-component-analysis.html">
     Independent Component Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/81-density-fitting.html">
     Application to Density Fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/51-embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/53-graph-neural-networks.html">
     Graphical Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/30-ml-basics/03-information-theory.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F30-ml-basics/03-information-theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shannon-entropy">
     Shannon Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differential-entropy">
     Differential Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-entropy">
     Joint Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-entropy">
     Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-entropy">
     Cross Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kullback-leibler-divergence">
     Kullback-Leibler Divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mutual-information">
     Mutual Information
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jensen-shannon-divergence">
     Jensen-Shannon Divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wasserstein-distance">
     Wasserstein Distance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#definition">
       Definition
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dual-form">
       Dual Form
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples">
       Examples
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#identities">
   Identities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chain-rule-for-conditional-entropy">
     Chain Rule for Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-rule-for-conditional-entropy">
     Bayes’ Rule for Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#others">
     Others
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inequalities">
   Inequalities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-processing-inequality">
     Data Processing Inequality
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="information-theory">
<h1>Information Theory<a class="headerlink" href="#information-theory" title="Permalink to this headline">¶</a></h1>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<p>First we introduce the definitions of some fundamental concepts in informaition theory summarized in the table below.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp; \text{Name} &amp; &amp; \text{Definition}  &amp; &amp; \text{Remark}  \\
&amp; \text{Entropy} &amp; \operatorname{H}(Y)&amp;=-\sum_{\mathcal{Y}}p(y)\log p(y) &amp; &amp; \text{Non-negative} \\
&amp; \text{Differntial Entropy} &amp; h(Y)&amp;=-\int_{\mathcal{Y}}f(y)\log f(y)\mathrm{~d}y &amp; &amp; \text{Can be negative} \\
&amp; \text{Joint Entropy} &amp; \operatorname{H}(X,Y)&amp;=-\mathbb{E}_{X,Y\sim p(x,y)}\log\operatorname{P}(X,Y) &amp; &amp; \\
&amp; \text{Conditional Entropy} &amp; \operatorname{H}(Y\vert X)&amp;=-\mathbb{E}_{X,Y\sim p(x,y)}\log \operatorname{P}(Y\mid X) &amp; &amp; \text{Sampling from } p(x,y) \\
&amp; \text{Cross Entropy} &amp; \operatorname{H}(P,Q)&amp;=-\mathbb{E}_{Y\sim P}\log Q(Y) &amp; &amp; \text{Asymmetric} \\
&amp; \text{KL Divergence} &amp; \operatorname{KL}(P,Q)&amp;=\mathbb{E}_{Y\sim P}\log\frac{P(Y)}{Q(Y)} &amp; &amp; \text{Non-negative. Asymmetric} \\
&amp; \text{Mutual Information} &amp; \operatorname{I}(X,Y)&amp;=\operatorname{KL}\left(P_{X,Y},P_{X}P_{Y}\right) &amp;&amp; \text{Non-negative. Symmetric}
\end{align}\end{split}\]</div>
<div class="section" id="shannon-entropy">
<h3>Shannon Entropy<a class="headerlink" href="#shannon-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The Shannon Entropy of a discrete distribution <span class="math notranslate nohighlight">\(p\left( y \right)\)</span> is defined by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{H}\left( Y \right)
  &amp;= \mathbb{E}_{Y\sim p(y)}\left[ - \log \operatorname{P}\left( Y \right) \right]\\
  &amp;= -\sum_{i=1}^{n}p(y_i)\log p(y_i)
  \end{align}\end{split}\]</div>
</dd>
</dl>
<p>This quantity measures the average level of “information”, or “uncertainty” inherent in the variables’ possible outcomes.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Entropy has similarity with variance. Both are non-negative, measure uncertainty/information. But variance depends on the observed value <span class="math notranslate nohighlight">\(y\)</span> of the random variable <span class="math notranslate nohighlight">\(Y\)</span>, while entropy only depends on the probability <span class="math notranslate nohighlight">\(p(y)\)</span>.</p>
</div>
<p>Examples</p>
<ul>
<li><p>For a binary varible <span class="math notranslate nohighlight">\(Y\sim \operatorname{Ber}(p)\)</span>, its entropy is</p>
<div class="math notranslate nohighlight">
\[
  H(Y) = -p\log p  - (1-p) \log(1-p)
  \]</div>
<p>which is maximized when <span class="math notranslate nohighlight">\(p=1/2\)</span>. The maximum is 1 if we use <span class="math notranslate nohighlight">\(\log_2\)</span>.</p>
</li>
</ul>
<dl class="simple myst">
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\operatorname{H}\left( Y \right) \ge 0\)</span> with equality iff <span class="math notranslate nohighlight">\(p(y)=1\)</span> for some <span class="math notranslate nohighlight">\(y\)</span>, i.e. no uncertainty.</p>
</dd>
</dl>
</div>
<div class="section" id="differential-entropy">
<span id="id1"></span><h3>Differential Entropy<a class="headerlink" href="#differential-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Let <span class="math notranslate nohighlight">\(Y\)</span> be a random variable with probability density function <span class="math notranslate nohighlight">\(f\)</span> whose support is <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. The differential entropy of <span class="math notranslate nohighlight">\(Y\)</span>, denoted <span class="math notranslate nohighlight">\(h(Y)\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[
  h(Y)=-\int_{\mathcal{Y}}f(y)\log f(y)\mathrm{~d}y
  \]</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Differential Entropy began as an attempt by Shannon to extend the idea of Shannon entropy to continuous PDF. Actually, the correct extension of Shannon entropy to continuous distributions is called limiting density of discrete points (LDDP), while differential entropy, aka continuous entropy, is a limiting case of the LDDP.</p>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul>
<li><p>Can be negative</p>
<p>Differential entropy does not have all properties of discrete entropy. For instance, discrete entropy is always greater than 0, while differential entropy can be less than 0, or diverges to <span class="math notranslate nohighlight">\(-\infty\)</span>.</p>
</li>
<li><p>Sensitive to units</p>
<p>The value of differential entropy is sensitive to the choice of units.</p>
</li>
<li><p>Transformation</p>
<p>The entropy of processed information <span class="math notranslate nohighlight">\(h\left(z(Y)\right)\)</span> can be larger <strong>or</strong> smaller than the original entropy <span class="math notranslate nohighlight">\(h(Y)\)</span>. Just consider <span class="math notranslate nohighlight">\(z=ay\)</span> for <span class="math notranslate nohighlight">\(a&gt;1\)</span> and <span class="math notranslate nohighlight">\(a&lt;1\)</span>.</p>
</li>
</ul>
</dd>
<dt>Examples</dt><dd><ul>
<li><p>Consider a uniform distribution over an interval on the real line of width <span class="math notranslate nohighlight">\(\Delta\)</span>, then <span class="math notranslate nohighlight">\(p(y) = \frac{1}{\Delta}\)</span>, and hence the entropy is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    h(Y) &amp;= \mathbb{E}_{y\sim p(\cdot)} \left[ - \ln \frac{1}{\Delta} \right]\\
    &amp;= \ln \Delta \\
    \end{align}\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(\Delta &lt; 1\)</span>, we have <span class="math notranslate nohighlight">\(h\rightarrow &lt; 0\)</span>.</p>
</li>
<li><p>The entropy for a Gaussian variable with density <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
    h \left( \mathcal{N}(\mu, \sigma^2) \right) = \ln(\sigma \sqrt{2 \pi e})
    \]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(\phi(y)\)</span> be the density of <span class="math notranslate nohighlight">\(\mathcal{N} (\mu, \sigma^2)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    h(Y)
    &amp;= -\int_{\mathcal{Y}}\phi(y)\ln \phi(y)\mathrm{~d}y\\
    &amp;= - \int \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( -\frac{(y-\mu)^2}{\sigma^2} \right) \left[ - \ln(\sqrt{2 \pi} \sigma)  -\frac{(y-\mu)^2}{2\sigma^2} \right] \mathrm{~d} y\\
    &amp;=  \ln(\sqrt{2 \pi} \sigma) + \int \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( -\frac{(y-\mu)^2}{\sigma^2} \right) \left[\frac{(y-\mu)^2}{2\sigma^2} \right] \mathrm{~d} y\\
    &amp;= \ln(\sqrt{2 \pi} \sigma) +  \mathbb{E} \left[\frac{(y-\mu)^2}{2\sigma^2} \right] \\
    &amp;= \ln(\sqrt{2 \pi} \sigma) +  \frac{\mathbb{E} [X^2] - 2\mu \mathbb{E} [X] + \mu^2  } {2\sigma^2} \\
    &amp;= \ln(\sqrt{2 \pi} \sigma) +  \frac{1}{2}  \\
    &amp;= \ln(\sqrt{2 \pi e} \sigma)\\
    \end{aligned}\end{split}\]</div>
</div>
<ul>
<li><p>Obviously, the entropy does not depend on location <span class="math notranslate nohighlight">\(\mu\)</span> but only depends on scale <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(\sigma &lt; \frac{1}{\sqrt{2 \pi e}}\)</span>, we have <span class="math notranslate nohighlight">\(h&lt; 0\)</span>.</p></li>
<li><p>Among all continuous distribution <span class="math notranslate nohighlight">\(f(y)\)</span> with mean <span class="math notranslate nohighlight">\(\mu\)</span>, variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, normal distribution has the largest differential entropy, i.e. most randomness.</p>
<div class="math notranslate nohighlight">
\[h(Y) \le \ln(\sigma \sqrt{2 \pi e})\quad \forall f(y): \mathbb{E} [Y] = \mu, \operatorname{Var}\left( Y \right) = \sigma^2\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Lemma: Let <span class="math notranslate nohighlight">\(\phi(y)\)</span> be the density of <span class="math notranslate nohighlight">\(\mathcal{N} (\mu, \sigma^2)\)</span>, then note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
      -\int_{\mathcal{Y}}f(y)\ln \phi(y)\mathrm{~d}y
      &amp;= - \int f(y) \left[ - \ln(\sqrt{2 \pi} \sigma)  -\frac{(y-\mu)^2}{2\sigma^2} \right] \mathrm{~d} y\\
      &amp;=  \ln(\sqrt{2 \pi} \sigma) + \int f(y) \left[\frac{(y-\mu)^2}{2\sigma^2} \right] \mathrm{~d} y\\
      &amp;= \ln(\sqrt{2 \pi} \sigma) +  \mathbb{E} \left[\frac{(y-\mu)^2}{2\sigma^2} \right] \\
      &amp;= \ln(\sqrt{2 \pi} \sigma) +  \frac{\mathbb{E} [Y^2] - 2\mu \mathbb{E} [Y] + \mu^2  } {2\sigma^2} \\
      &amp;= \ln(\sqrt{2 \pi} \sigma) +  \frac{1}{2}  \\
      &amp;= \ln(\sqrt{2 \pi e} \sigma)\\
      \end{aligned}\end{split}\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
      h(Y)
      &amp;= - \int f(y) \ln f(y) \mathrm{~d} y\\
      &amp;= - \int f(y) \ln \left( \frac{f(y)}{\phi(y)} \phi(y) \right)  \mathrm{~d} y\\
      &amp;= - \int f(y) \ln \frac{f(y)}{\phi(y)}  \mathrm{~d} y - \int f(y) \ln \phi (y)  \mathrm{~d} y\\
      &amp;= - \mathbb{E} _f\left[- \ln \frac{\phi(y)}{f (y)}  \right] +  \ln(\sqrt{2 \pi e} \sigma) \quad \because \text{Lemma} \\
      &amp;\le \ln \mathbb{E} _f\left[ \frac{\phi(y)}{f (y)}  \right] +  \ln(\sqrt{2 \pi e} \sigma) \quad \because \text{Jensen's inequality}  \\
      &amp;= \ln \int f(y) \frac{\phi(y)}{f(y)}  \mathrm{~d} y+  \ln(\sqrt{2 \pi e} \sigma) \\
      &amp;= \ln(\sqrt{2 \pi e} \sigma)\\
      \end{aligned}\end{split}\]</div>
<p>The equality hold if and only if <span class="math notranslate nohighlight">\(\frac{\phi(y)}{f(y)}\)</span> is a constant for all <span class="math notranslate nohighlight">\(y\)</span>, i.e. <span class="math notranslate nohighlight">\(Y\)</span> is of normal distribution.</p>
</div>
</li>
</ul>
</li>
<li><p>Consider the uniform distribution above. Suppose the unit is meter and the interval is <span class="math notranslate nohighlight">\((0, 1000)\)</span>. Note if we change the unit to kilometer, then the interval changes to <span class="math notranslate nohighlight">\((0, 1)\)</span>, and the entropy decreases from <span class="math notranslate nohighlight">\(\ln 1000\)</span> to <span class="math notranslate nohighlight">\(\ln 1\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The right way to think about differential entropy is that, it is actually <strong>infinite</strong>. Recall entropy measures uncertainty. An actual real number carries an infinite number of bits, i.e. infinite amount of information. A meaningful convention is the <span class="math notranslate nohighlight">\(h(f)=+\infty\)</span> for any continuous density <span class="math notranslate nohighlight">\(p(y)\)</span>.</p>
</div>
</li>
</ul>
</dd>
<dt>Definition (NegEntropy)</dt><dd><p>Short for Negative Entropy, it is a non-Gaussian-ness measure, a measure of distance to normality. The negEntropy for a random variable <span class="math notranslate nohighlight">\(X\)</span> is</p>
<div class="math notranslate nohighlight">
\[
  J(X) = \operatorname{H} (Z) - \operatorname{H} (X)
  \]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{H} (Z)\)</span> is the differential entropy of the Gaussian density with the same mean and variance as <span class="math notranslate nohighlight">\(X\)</span>.</p>
</dd>
</dl>
<p>NegEntropy is used for its convenience in computation and approximation. A common approximation (supposedly from Jones 1987)</p>
<div class="math notranslate nohighlight">
\[
J(X) \approx \frac{1}{12} \mathbb{E} [X^{3}]^{2}+\frac{1}{48} \kappa(X)^{2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa(X)\)</span> is the excess kurtosis of the distribution of <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
<div class="section" id="joint-entropy">
<h3>Joint Entropy<a class="headerlink" href="#joint-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The joint Shannon entropy of two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with joint distribution <span class="math notranslate nohighlight">\(p(x,y)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align}
  \operatorname{H}(X,Y)
  &amp; = \mathbb{E}_{X,Y\sim p(x, y)}\left[ - \log \operatorname{P}(X,Y) \right]\\
  &amp; = - \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log_{2}[p(x,y)]
  \end{align}
  \end{split}\]</div>
<p>More generally, for multiple random variables, we can define</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{H}\left(X_{1},\ldots,X_{n}\right)=-\sum_{x_{1}\in\mathcal{X}_{1}}\ldots\sum_{x_{n}\in\mathcal{X}_{n}}p\left(x_{1},\ldots,x_{n}\right)\log_{2}\left[p\left(x_{1},\ldots,x_{n}\right)\right]
  \]</div>
<p>Likewise, we can define joint differential entropy for two continuous random variables with joint density <span class="math notranslate nohighlight">\(f(x,y)\)</span> as</p>
<div class="math notranslate nohighlight">
\[
  h(X,Y)=-\int_{\mathcal{X},\mathcal{Y}}f(x,y)\log f(x,y) \mathrm{~d}x\mathrm{~d}y
  \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
  h\left(X_{1},\ldots,X_{n}\right)=-\int f\left(x_{1},\ldots,x_{n}\right)\log f\left(x_{1},\ldots,x_{n}\right)\mathrm{~d}x_{1}\ldots \mathrm{~d}x_{n}
  \]</div>
</dd>
<dt>Properties of Discrete Joint Entropy</dt><dd><ul class="simple">
<li><p>Nonnegativeity: <span class="math notranslate nohighlight">\(\operatorname{H}(X,Y)\ge0\)</span></p></li>
<li><p>Greater than or equal to individual entropies: <span class="math notranslate nohighlight">\(\operatorname{H}(X,Y)\ge\max\left(\operatorname{H}(X),\operatorname{H}(Y)\right)\)</span></p></li>
<li><p>Less than or equal to the sum of individual entropies <span class="math notranslate nohighlight">\(\operatorname{H}(X,Y)\le \operatorname{H}(X)+\operatorname{H}(Y)\)</span>. Equality holds iff <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="conditional-entropy">
<h3>Conditional Entropy<a class="headerlink" href="#conditional-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>For two random variables <span class="math notranslate nohighlight">\(X, Y\)</span> with density function <span class="math notranslate nohighlight">\(p(x,y)\)</span>, the conditional entropy of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{H}(Y\mid X)
  &amp; = \mathbb{E}_{X,Y\sim p(x, y)}\left[ - \log \operatorname{P}(Y \mid X) \right]\\
  &amp;=-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(y\vert x) \\
  h(Y\mid X)	&amp;=-\int_{\mathcal{X},\mathcal{Y}}f(x,y)\log f(y\vert x)\mathrm{~d}x\mathrm{~d}y
  \end{align}\end{split}\]</div>
<p>It also quantifies the amount of information needed to describe the outcome of a random variable <span class="math notranslate nohighlight">\(Y\)</span> given the value of another random variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One can compare entropy vs. conditional entropy, and probability vs conditional probability. Both are defined by the expectation of negative log probability.</p>
<ul class="simple">
<li><p>the entropy of <span class="math notranslate nohighlight">\(Y\)</span> is the expectation of negative unconditional log probability <span class="math notranslate nohighlight">\(\log p(y)\)</span></p></li>
<li><p>the conditional entropy of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X\)</span> is the expectation of the negative conditional log probability <span class="math notranslate nohighlight">\(\log p(y \mid x)\)</span></p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Conditional entropy samples <span class="math notranslate nohighlight">\(X,Y\)</span> from the joint distribution <span class="math notranslate nohighlight">\(p(x,y)\)</span>. This is different from conditional expectation, which samples <span class="math notranslate nohighlight">\(Y\)</span> from the conditional distribution <span class="math notranslate nohighlight">\(Y \sim p_{Y\mid X}(y\mid x)\)</span>.</p>
</div>
<p>To understand it, first we consider the entropy of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X\)</span> takes a certain value <span class="math notranslate nohighlight">\(x\)</span>. Recall the unconditional entropy of <span class="math notranslate nohighlight">\(Y\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\operatorname{H}(Y)=-\sum_{y\in\mathcal{Y}}\operatorname{P}(Y=y)\log_{2}\operatorname{P}(Y=y)
\]</div>
<p>Now <span class="math notranslate nohighlight">\(X\)</span> is known to be <span class="math notranslate nohighlight">\(x\)</span>, then the distribution of <span class="math notranslate nohighlight">\(Y\)</span> becomes conditional on <span class="math notranslate nohighlight">\(X=x\)</span>. We just replace <span class="math notranslate nohighlight">\(\operatorname{P}(Y=y)\)</span> by <span class="math notranslate nohighlight">\(\operatorname{P}(Y=y\mid X=x)\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{H}(Y\vert X=x)=-\sum_{y\in\mathcal{Y}}\operatorname{P}(Y=y\mid X=x)\log_{2}\operatorname{P}(Y=y\mid X=x)
\]</div>
<p>Finally, the conditional entropy <span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)\)</span> is defined as the sum of <span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X=x)\)</span> weighted on <span class="math notranslate nohighlight">\(\operatorname{P}(X=x)\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}\operatorname{H}(Y\vert X) &amp; \equiv\sum_{x\in\mathcal{X}}p(x)\operatorname{H}(Y\vert X=x)\\
 &amp; =-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y\vert x)\log p(y\vert x)\\
 &amp; =-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(y\vert x)\\
 &amp; =-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)}
\end{aligned}
\end{split}\]</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)=0\)</span> iff <span class="math notranslate nohighlight">\(Y\)</span> is completely determined by the value of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)=\operatorname{H}(Y)\)</span> iff <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are independent</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="cross-entropy">
<h3>Cross Entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The cross entropy of two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> on the same
support <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is defined as</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\operatorname{H}\left( P, Q \right) &amp; =\mathbb{E}_{Y\sim p(y)}\left[-\ln\left(Q(Y)\right)\right]\\
 &amp; =\int_{\mathcal{Y}}f_{P}(y)\left[-\ln\left(f_{Q}(y)\right)\right]\text{d}y\quad\text{for continuous}\ P,Q\\
 &amp; =-\sum_{y\in\mathcal{Y}}P(y)\log Q(y)\qquad\qquad\text{for discrete}\ P,Q
\end{align}
\end{split}\]</div>
<p>Cross entropy can be interpreted as the <strong>expected message-length</strong> per datum when a wrong distribution <span class="math notranslate nohighlight">\(Q\)</span> is assumed while the data actually follows a distribution <span class="math notranslate nohighlight">\(P\)</span>. If we use <span class="math notranslate nohighlight">\(\log_{2}\)</span>, the quantity <span class="math notranslate nohighlight">\(\operatorname{H}\left( P, Q \right)\)</span> can also be interpreted
as 1.44 times the number of bits used to code draws from <span class="math notranslate nohighlight">\(P\)</span> when
using the imperfect code defined by <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The cross entropy <span class="math notranslate nohighlight">\(\operatorname{H}(P,Q)\)</span> of <strong>two</strong> distributions <span class="math notranslate nohighlight">\(P(y), Q(y)\)</span> is different from the joint entropy <span class="math notranslate nohighlight">\(\operatorname{H}(X,Y)\)</span> of <strong>one</strong> joint distribution <span class="math notranslate nohighlight">\(p(x,y)\)</span> of <strong>two</strong> random variables <span class="math notranslate nohighlight">\(X, Y\)</span>.</p></li>
<li><p>“Cross” means we sample <span class="math notranslate nohighlight">\(y\)</span> from one distribution <span class="math notranslate nohighlight">\(P(y)\)</span>, and then compute the negative log probability with the other distribution <span class="math notranslate nohighlight">\(-\ln(Q(y))\)</span>.</p></li>
</ul>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p>asymmetric: <span class="math notranslate nohighlight">\(\operatorname{H}\left( P, Q \right)\ne \operatorname{H}(Q,P)\)</span></p></li>
<li><p>larger than entropy: <span class="math notranslate nohighlight">\(\operatorname{H}\left( P, Q \right)\ge \operatorname{H}(P)\)</span> with equality iff <span class="math notranslate nohighlight">\(Q=P\)</span></p></li>
</ul>
</dd>
</dl>
<p>Cross entropy is tightly related to KL divergence.</p>
</div>
<div class="section" id="kullback-leibler-divergence">
<span id="kl-divergence"></span><h3>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The KL Divergence of two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> on the \textbf{same}
support <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
  \begin{aligned}
  \operatorname{KL}\left( P, Q \right) &amp; =\mathbb{E}_{Y\sim p(y)}\left[\ln\frac{P(Y)}{Q(Y)}\right]
  \end{aligned}
  \]</div>
</dd>
</dl>
<p>Kullback–Leibler divergence (also called relative entropy) is a  measure of how one probability distribution is different from a second, reference probability distribution, i.e. the <strong>distance</strong> between two distributions on the same support.</p>
<dl class="simple myst">
<dt>Properties</dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(\operatorname{KL}\left( P, Q \right) \ge 0\)</span>, with equality iff the two distributions are identical.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>KL divergence is non-negative by Jensen’s inequality of convex functions</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}\operatorname{KL}\left( P, Q \right) &amp; =\mathbb{E}_{y\sim p(\cdot)}\left[-\ln\frac{Q(y)}{P(y)}\right]\\
     &amp; \geq-\ln \mathbb{E}_{y\sim p(\cdot)}\frac{Q(y)}{P(y)}\\
     &amp; =-\ln\sum_{y}P(y)\frac{Q(y)}{P(y)}\\
     &amp; =-\ln\sum_{y}Q(y)\\
     &amp; =0
    \end{aligned}
    \end{split}\]</div>
<p>For two continuous distributions,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \operatorname{KL}\left( P, Q \right) &amp; =\int_{-\infty}^{\infty}p(y)\log\left(\frac{p(y)}{q(y)}\right)\mathrm{~d}y\\
     &amp; \ge0
    \end{align}\end{split}\]</div>
<p>Equality holds iff <span class="math notranslate nohighlight">\(\forall y\ P(y)=Q(y)\)</span>, i.e. two distributions
are identical.</p>
</div>
</li>
<li><p>It is a distribution-wise <strong>asymmetric</strong> measure and thus does not qualify as a statistical <strong>metric</strong> of spread - it also does not satisfy the triangle inequality.</p></li>
<li><p>Relation to cross entropy:</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{KL}\left( P, Q \right)=\operatorname{H}\left( P, Q \right)-\operatorname{H}(P)
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(P\)</span> is some fixed distribution, then <span class="math notranslate nohighlight">\(\operatorname{H}(P)\)</span> is a constant. Hence,</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{argmax}_{Q}\ \operatorname{KL}\left( P, Q \right)=\operatorname{argmax}_{Q}\operatorname{H}\left( P, Q \right)
    \]</div>
<p>i.e. minimizing/maximizing the KL divergence <span class="math notranslate nohighlight">\(\operatorname{KL}\left( P, Q \right)\)</span> of two distributions is equivalent to minimizing/maximizing their cross entropy <span class="math notranslate nohighlight">\(\operatorname{H}\left( P, Q \right)\)</span>, given <span class="math notranslate nohighlight">\(P\)</span> is a fixed distribution.</p>
</li>
</ul>
</dd>
<dt>Example</dt><dd><p>The KL-Divergence between two multivariate Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{\mu}_{p},\boldsymbol{\Sigma}_{p})\)</span>
and <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{\mu}_{q},\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{q})\)</span> is</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{KL}\left( P, Q \right)=\frac{1}{2}\left[\log\frac{\left|\boldsymbol{\Sigma}_{q}\right|}{\left|\boldsymbol{\Sigma}_{p}\right|}-k+\left(\boldsymbol{\mu}_{p}-\boldsymbol{\mu}_{q}\right)^{\top}\boldsymbol{\Sigma}_{q}^{-1}\left(\boldsymbol{\mu}_{p}-\boldsymbol{\mu}_{q}\right)+\operatorname{tr} (\boldsymbol{\Sigma}_{q}^{-1}\boldsymbol{\Sigma}_{p}) \right]
  \]</div>
<p>In particular, if the reference distribution <span class="math notranslate nohighlight">\(Q\)</span> is <span class="math notranslate nohighlight">\(\mathcal{N}(0,I)\)</span> then we get</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{KL}\left( P, Q \right)=\frac{1}{2}\left[\Vert\boldsymbol{\mu}_{p}\Vert^{2}+\operatorname{tr}(\boldsymbol{\Sigma}_{p}) -k-\log\left|\boldsymbol{\Sigma}_{p}\right|\right]
  \]</div>
</dd>
</dl>
</div>
<div class="section" id="mutual-information">
<span id="id2"></span><h3>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this headline">¶</a></h3>
<p>Aka information gain.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Let <span class="math notranslate nohighlight">\(\left(X,Y\right)\)</span> be a pair of random variables with values over the space <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span>. Suppose their joint distribution is <span class="math notranslate nohighlight">\(\operatorname{P}_{X,Y}\)</span> and the marginal distributions are <span class="math notranslate nohighlight">\(\operatorname{P}_{X}\)</span> and <span class="math notranslate nohighlight">\(\operatorname{P}_{Y}\)</span>. The mutual information of <span class="math notranslate nohighlight">\(X, Y\)</span> is defined by a KL divergence</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{I}\left(X, Y \right) &amp; = \operatorname{KL}\left(\operatorname{P}_{X,Y},\operatorname{P}_{X}\operatorname{P}_{Y}\right)\\
   &amp; = \mathbb{E}_{X,Y}\left[ \ln\frac{\operatorname{P}_{X,Y}(X,Y)}{\operatorname{P}_X(X)\operatorname{P}_Y(Y)} \right]\\
  \end{align}\end{split}\]</div>
<p>For discrete case,</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{I}\left(X, Y \right)=\sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}p_{X,Y}(x,y)\log\left(\frac{p_{(X,Y)}(x,y)}{p_{X}(x)p_{Y}(y)}\right)
  \]</div>
<p>and for continuous case,</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{I}\left(X, Y \right)=\int_{\mathcal{Y} }\int_{\mathcal{X}}p_{X,Y}(x,y)\log\left(\frac{p_{(X,Y)}(x,y)}{p_{X}(x)p_{Y}(y)}\right)
  \]</div>
</dd>
<dt>Properties</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{I}(X,Y) =\operatorname{H}(X)+\operatorname{H}(Y)-\operatorname{H}\left(X, Y \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{I}(X,Y) \ge 0\)</span>, with equality iff <span class="math notranslate nohighlight">\(P_{X,Y}=P_{X}P_{Y}\)</span>, i.e. when <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, and hence there is no mutual dependence.</p></li>
</ul>
</dd>
</dl>
<p>Mutual information is a measure of the mutual <strong>dependence</strong> between the two variables. More specifically, it quantifies the amount of information (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Similar to absolute correlation <span class="math notranslate nohighlight">\(\left\vert \rho \right\vert\)</span>,  both are</p>
<ul class="simple">
<li><p>non-negative,</p></li>
<li><p>larger when <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent,</p></li>
<li><p>0 when independent.</p></li>
</ul>
<p>But <span class="math notranslate nohighlight">\(\vert\rho\vert=0\ \not{\Rightarrow}\ X\perp Y\)</span> while <span class="math notranslate nohighlight">\(\operatorname{I}\left(X, Y \right)=0\ \Leftrightarrow\ X\perp Y\)</span>.</p>
</div>
</div>
<div class="section" id="jensen-shannon-divergence">
<h3>Jensen-Shannon Divergence<a class="headerlink" href="#jensen-shannon-divergence" title="Permalink to this headline">¶</a></h3>
<p>KL divergence can measure the ‘distance’ between two distributions, but it is asymmetric. Jensen-Shannon distance of two distributions, is symmetric, and it always has a finite value.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Given two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, the Jenson-Shannon divergence (JSD) is defined as</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{JSD}(P, Q) = \frac{1}{2} \operatorname{KL}(P, M) + \frac{1}{2} \operatorname{KL}(Q, M)
  \]</div>
<p>where <span class="math notranslate nohighlight">\(M = \frac{1}{2}(P + Q)\)</span>.</p>
</dd>
<dt>Properties</dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(0 \le \operatorname{JSD}(P, Q) \le 1\)</span> if we use 2 logarithm.</p></li>
<li><p>For (discrete) distributions <span class="math notranslate nohighlight">\(P_1, \ldots, P_n\)</span> with weights <span class="math notranslate nohighlight">\(\pi_1, \ldots \pi_n\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{JSD}_{\pi_{1}, \ldots, \pi_{n}}\left(P_{1}, P_{2}, \ldots, P_{n}\right)=H\left(\sum_{i=1}^{n} \pi_{i} P_{i}\right)-\sum_{i=1}^{n} \pi_{i} H\left(P_{i}\right)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(H(P)\)</span> is the Shannon entropy for distribution <span class="math notranslate nohighlight">\(P\)</span>.</p>
</li>
<li><p>The square root of the Jensen–Shannon divergence is a metric often referred to as <strong>Jensen-Shannon distance</strong>.</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="wasserstein-distance">
<span id="id3"></span><h3>Wasserstein Distance<a class="headerlink" href="#wasserstein-distance" title="Permalink to this headline">¶</a></h3>
<div class="section" id="definition">
<h4>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h4>
<p>KL divergence can measure the ‘distance’ between two distributions, but it is asymmetric. Wasserstein distance of two distributions, is symmetric, and it is actually a metric.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Given two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> over a region <span class="math notranslate nohighlight">\(D\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R} ^{d}\)</span>, let <span class="math notranslate nohighlight">\(\mathcal{\Pi}\)</span> be all joint distributions <span class="math notranslate nohighlight">\(\pi(X, Y)\)</span> over <span class="math notranslate nohighlight">\(D \times D\)</span> that have marginals <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. Then the <span class="math notranslate nohighlight">\(p\)</span>-Wasserstein distance between <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> is defined as</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
W_{p}(P, Q)
&amp;=\left(\inf _{\pi \in \mathcal{\Pi} (P, Q)} \mathbb{E}_{x, y \sim \pi} [\left\| x - y \right\|^p ]  \right)^{1 / p} \\
&amp;=\left(\inf _{\pi \in \mathcal{\Pi} (P, Q)} \int\|x-y\|^{p} \pi(x, y) \mathrm{~d}x \mathrm{~d} y \right)^{1 / p}
\end{aligned}\end{split}\]</div>
<p>Specifically, <span class="math notranslate nohighlight">\(\mathcal{\Pi} (P, Q)\)</span> represents the collection of all binary functions <span class="math notranslate nohighlight">\(\pi\)</span> satisfying</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\int \pi(x,y) \mathrm{~d}y = P(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\int \pi(x,y) \mathrm{~d}x = Q(y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\pi(x,y) \ge 0\)</span></p></li>
</ul>
<p>It can be shown that <span class="math notranslate nohighlight">\(W_p\)</span> satisfies all the axioms of a metric. The optimal <span class="math notranslate nohighlight">\(\pi^*\)</span> is called the optimal <strong>coupling</strong> of <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>Usually, <span class="math notranslate nohighlight">\(p=1\)</span> is used. <span class="math notranslate nohighlight">\(1\)</span>-Wasserstein distance is also known as Earth mover’s distance (EMD) in computer science. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region <span class="math notranslate nohighlight">\(D\)</span>, the EMD is the minimum cost of turning one pile into the other.</p>
</div>
<div class="section" id="dual-form">
<span id="wasserstein-dual"></span><h4>Dual Form<a class="headerlink" href="#dual-form" title="Permalink to this headline">¶</a></h4>
<p>It can be shown that the dual formulation is</p>
<div class="math notranslate nohighlight">
\[
W_p ^p(P, Q) =\sup _{\psi, \phi} \int \psi(y)  Q(y) \mathrm{~d}y-\int \phi(x)  P(x) \mathrm{~d}x
\]</div>
<p>where <span class="math notranslate nohighlight">\(\psi(y) - \phi(x) \le \left\| x - y \right\| ^p\)</span>.</p>
<p>In special case where <span class="math notranslate nohighlight">\(p=1\)</span> we have the very simple representation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sup_{f \text{ is 1-Lipschitz} } &amp;&amp; \int f (x) P(x) \mathrm{~d} x -  \int f (y) Q(y) \mathrm{~d} y\\
\end{aligned}\end{split}\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span>, consider the above three constarints, the Lagragean is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L} (\pi, f_1, f_2, M)
&amp;= \int \left\| x - y \right\| \pi(x, y) \mathrm{~d}x \mathrm{~d} y   \\
&amp;- \langle f_1, \int \pi(\cdot,y) \mathrm{~d}y - p(\cdot) \rangle \\
&amp;- \langle f_2, \int \pi(x,\cdot) \mathrm{~d}y - p_T(\cdot) \rangle \\
&amp;- \langle M, \pi \rangle \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_1 (\cdot), f_2(\cdot)\)</span> and <span class="math notranslate nohighlight">\(M(\cdot, \cdot)\)</span> are dual functions, and <span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle\)</span> is the inner product of two functions: <span class="math notranslate nohighlight">\(\langle f, g \rangle = \int f(x)g(x) \mathrm{~d}x\)</span>.</p>
<p>Setting the first order derivative to 0 gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \pi} = 0  \Leftrightarrow \left\| x - y \right\| - f_1 (x) - f_2 (y) = M \ge 0
\]</div>
<p>Substituting this equality, the dual problem is then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sup_{f_1, f_2} &amp;&amp; \int f_1 (x) P(x) \mathrm{~d} x +  \int f_2 (y) Q(y) \mathrm{~d} y\\
\mathrm{s.t.}
&amp;&amp; \left\| x- y \right\|  \ge f_1(x) + f_2(y)\\
\end{aligned}\end{split}\]</div>
<p>Lemma: For <span class="math notranslate nohighlight">\(W_1\)</span>, we have a relation of two maximizers <span class="math notranslate nohighlight">\(f_2 = - f_1\)</span>.</p>
<p>Hence, the problem becomes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sup_{f_1} &amp;&amp; \int f_1 (x) P(x) \mathrm{~d} x -  \int f_1 (y) Q(y) \mathrm{~d} y\\
\mathrm{s.t.}
&amp;&amp; \left\| x- y \right\|  \ge f_1(x) - f_1(y)\\
\end{aligned}\end{split}\]</div>
<p>Equivalently,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sup_{f \text{ is 1-Lipshitz} } &amp;&amp; \int f (x) P(x) \mathrm{~d} x -  \int f (y) Q(y) \mathrm{~d} y\\
\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p>In particular, when <span class="math notranslate nohighlight">\(d=1\)</span>, the distance has a closed form</p>
<div class="math notranslate nohighlight">
\[
  W_{p}(P, Q)=\left(\int_{0}^{1}\left|F^{-1}(z)-G^{-1}(z)\right|^{p}\right)^{1 / p}
  \]</div>
<p>where <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(G\)</span> are the CDF’s of <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(P\)</span> is the empirical distribution of a dataset <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> is the empirical distribution of another dataset <span class="math notranslate nohighlight">\(y_1, y_2, \ldots, y_n\)</span> of the same size, then the distance takes a very simple function of the order statistics:</p>
<div class="math notranslate nohighlight">
\[
  W_{p}(P, Q)=\left(\sum_{i=1}^{n}\left\|x_{(i)}-y_{(i)}\right\|^{p}\right)^{1 / p}
  \]</div>
</li>
<li><p>An interesting special case occurs for normal distributions. If <span class="math notranslate nohighlight">\(P=\mathcal{N}\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right)\)</span> and <span class="math notranslate nohighlight">\(Q=\mathcal{N}\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right)\)</span> then</p>
<div class="math notranslate nohighlight">
\[
  W^{2}(P, Q)=\left\|\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\right\|^{2}+B^{2}\left(\boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma} _2\right)
  \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
  B^{2}\left(\boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma} _2\right)=\operatorname{tr}\left(\boldsymbol{\Sigma}_{1}\right)+\operatorname{tr}\left(\boldsymbol{\Sigma}_{2}\right)-2 \operatorname{tr}\left[\left(\boldsymbol{\Sigma}_{1}^{1 / 2} \boldsymbol{\Sigma}_{2} \boldsymbol{\Sigma}_{1}^{1 / 2}\right)^{1 / 2}\right]
  \]</div>
</li>
</ul>
<p>Reference</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.stat.cmu.edu/~larry/=sml/Opt.pdf">Larry’s notes</a></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="identities">
<h2>Identities<a class="headerlink" href="#identities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="chain-rule-for-conditional-entropy">
<h3>Chain Rule for Conditional Entropy<a class="headerlink" href="#chain-rule-for-conditional-entropy" title="Permalink to this headline">¶</a></h3>
<p>Analogous to <span class="math notranslate nohighlight">\(p(y\vert x)=\frac{p(x,y)}{p(x)}\)</span> in probability, we have an equation that connects entropy, joint entropy and conditional entropy. Instead of division, we use subtraction,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{H}(Y\vert X)=\operatorname{H}(X,Y)-\operatorname{H}(X)
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}\operatorname{H}(Y\vert X)
&amp; = -\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x)}{p(x,y)}\\
 &amp; =- \sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)(\log p(x)-\log p(x,y))\\
 &amp; =-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(x,y)+\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(x)\\
 &amp; =\operatorname{H}(X,Y)+\sum_{x\in\mathcal{X}}p(x)\log p(x)\\
 &amp; =\operatorname{H}(X,Y)-\operatorname{H}(X)
\end{aligned}
\end{split}\]</div>
</div>
<p>Interpretation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{H}(X,Y)\)</span> measures the bits of information on average to describe the state of the combined system <span class="math notranslate nohighlight">\((X,Y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{H}(X)\)</span> measures the bits of information we have about <span class="math notranslate nohighlight">\(\operatorname{H}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)\)</span> measures the <strong>additional</strong> information required to describe <span class="math notranslate nohighlight">\((X,Y)\)</span>, given <span class="math notranslate nohighlight">\(X\)</span>. Note that <span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)\le \operatorname{H}(Y)\)</span> with equality iff <span class="math notranslate nohighlight">\(X\bot Y\)</span>.</p></li>
</ul>
<p>The general form for multiple random variables is</p>
<div class="math notranslate nohighlight">
\[ \operatorname{H}\left(X_{1},X_{2},\ldots,X_{n}\right)=\sum_{i=1}^{n}\operatorname{H}\left(X_{i}\vert X_{1},\ldots,X_{i-1}\right)
\]</div>
<p>which has a similar form to chain rule in probability theory, except that here is addition <span class="math notranslate nohighlight">\(\sum_{i=1}^{n}\)</span> instead of multiplication <span class="math notranslate nohighlight">\(\Pi_{i=1}^{n}\)</span>.</p>
</div>
<div class="section" id="bayes-rule-for-conditional-entropy">
<h3>Bayes’ Rule for Conditional Entropy<a class="headerlink" href="#bayes-rule-for-conditional-entropy" title="Permalink to this headline">¶</a></h3>
<p>Analogous to the Bayes rule <span class="math notranslate nohighlight">\(p(y\vert x)=\frac{p(x\vert y)p(y)}{p(x)}\)</span> in probability, we have an equation to link <span class="math notranslate nohighlight">\(\operatorname{H}(Y\vert X)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{H}(X\vert Y)\)</span>, which is simply a result from the chain rule. Instead of division for the Bayes’ rule in probability, we use subtraction</p>
<div class="math notranslate nohighlight">
\[
\operatorname{H}(Y\vert X)=\operatorname{H}(X\vert Y)-\operatorname{H}(X)+\operatorname{H}(Y)
\]</div>
</div>
<div class="section" id="others">
<h3>Others<a class="headerlink" href="#others" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}\operatorname{H}(X,Y) &amp; =\operatorname{H}(X\vert Y)+\operatorname{H}(Y\vert X)+\operatorname{I}\left(X, Y \right)\\
\operatorname{H}(X,Y) &amp; =\operatorname{H}(X)+\operatorname{H}(Y)-\operatorname{I}\left(X, Y \right)\\
\operatorname{I}\left(X, Y \right) &amp; \leq \operatorname{H}(X)
\end{aligned}
\end{split}\]</div>
</div>
</div>
<div class="section" id="inequalities">
<h2>Inequalities<a class="headerlink" href="#inequalities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-processing-inequality">
<h3>Data Processing Inequality<a class="headerlink" href="#data-processing-inequality" title="Permalink to this headline">¶</a></h3>
<p>The data processing inequality says post-processing cannot increase information.</p>
<p>Let three random variables form the Markov Chain <span class="math notranslate nohighlight">\(X\rightarrow Y\rightarrow Z\)</span>, implying that the conditional distribution of <span class="math notranslate nohighlight">\(Z\)</span> depends only on <span class="math notranslate nohighlight">\(Y\)</span> and is conditionally independent of <span class="math notranslate nohighlight">\(X\)</span>. The joint PMF can be written as</p>
<div class="math notranslate nohighlight">
\[p(x,y,z)=p(x)p(y\mid x)p(z\mid y)\]</div>
<p>In this setting, no processing <span class="math notranslate nohighlight">\(Z(Y)\)</span> of <span class="math notranslate nohighlight">\(Y\)</span>, deterministic or random, can increase the information that <span class="math notranslate nohighlight">\(Y\)</span> contains about <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="math notranslate nohighlight">
\[\operatorname{I}\left(X, Y \right)\ge \operatorname{I} (X,Z)\]</div>
<p>with the quality iff <span class="math notranslate nohighlight">\(Z\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> contain the same information about <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\operatorname{H}(Z)\le \operatorname{H}(Y)\)</span> but <span class="math notranslate nohighlight">\(h(Z)\)</span> can be larger or smaller than <span class="math notranslate nohighlight">\(h(Y)\)</span>.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./30-ml-basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="02-taxonomy.html" title="previous page">Taxonomy</a>
    <a class='right-next' id="next-link" href="05-kernels.html" title="next page">Kernels</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>