
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Fourier Transform-based Representations &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Regression" href="../31-regression/00-regression.html" />
    <link rel="prev" title="Self-supervised Learning" href="53-self-supervised.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-fields-and-vector-spaces.html">
     Fields and Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/61-graphs.html">
     Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   Machine Learning for Graph Data
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/30-ml-basics/61-fourier-transform.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F30-ml-basics/61-fourier-transform.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics">
   Basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#main-idea">
     Main idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-and-applications">
     Examples and Applications
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discrete-fourier-transform-dft">
   Discrete Fourier Transform (DFT)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformation">
     Transformation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relation-between-x-k-and-a-k">
     Relation between
     <span class="math notranslate nohighlight">
      \(X_k\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(a_k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples">
     Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-time-fourier-transform">
   Continuous-time Fourier Transform
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-discrete-time-fourier-series-transforms">
   2-D Discrete-“time” Fourier series/transforms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-discrete-time-convolution">
   2-D Discrete-“time” convolution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-to-machine-learning">
   Applications to Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#faster-cnn-training">
     Faster CNN Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-fourier-features">
     Random Fourier Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularizing-neural-networks">
     Regularizing Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="fourier-transform-based-representations">
<h1>Fourier Transform-based Representations<a class="headerlink" href="#fourier-transform-based-representations" title="Permalink to this headline">¶</a></h1>
<p>Why study Fourier transform in ML?</p>
<ul class="simple">
<li><p>Staple of feature representation for many types of data (audio, images, others) for many decades, still useful in many settings</p></li>
<li><p>Also useful for deriving new algorithms in machine learning</p></li>
</ul>
<p>Before we do any learning, we start with some data representation. Typically, this involves converting raw data into feature vectors. These days, much of feature engineering is replaced by neural network-based feature learning. But not all…choice of initial representation can still be important for</p>
<ul class="simple">
<li><p>Visualization of raw data</p></li>
<li><p>Qualitative understanding of the data</p></li>
<li><p>Better input to a learning algorithm</p></li>
<li><p>When number of observations is small, handcraft features are also useful than complicated models, e.g. neural networks</p></li>
</ul>
<p>Fourier methods include</p>
<p>Fourier methods include:</p>
<ul class="simple">
<li><p>Discrete-time and continuous-time Fourier series</p></li>
<li><p>Discrete-time and continuous-time Fourier transforms</p></li>
<li><p>Discrete Fourier transform (most common for digital signals)</p></li>
</ul>
<div class="section" id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="main-idea">
<h3>Main idea<a class="headerlink" href="#main-idea" title="Permalink to this headline">¶</a></h3>
<p>Decompose a signal <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x} _t \right\}=x_0, x_1, \ldots, x_{N-1}\)</span> as a sum of multiple sinusoids at different frequencies</p>
<div class="math notranslate nohighlight">
\[x_t=\sum_{k} a_{k} f_{k}(t)\]</div>
<p>where</p>
<ul class="simple">
<li><p>Signal = function of a discrete “time” variable <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>Subscript notation reminds us that the time variable is discrete</p></li>
<li><p><span class="math notranslate nohighlight">\(f_k(t)\)</span> sinusoid (sine/cosine function) at frequency indexed by <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_k =\)</span> “amount” of frequency <span class="math notranslate nohighlight">\(k\)</span> present in <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x} _t \right\}\)</span>, aka “<strong>spectrum</strong>” of the signal (but this word has other meanings).</p></li>
<li><p>Fourier transform algorithms: ways of finding <span class="math notranslate nohighlight">\(a_k\)</span> given <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x} _t \right\}\)</span></p></li>
</ul>
<p>Demo:</p>
<ul class="simple">
<li><p>http://www.falstad.com/dfilter/</p></li>
<li><p>https://www.youtube.com/watch?v=spUNpyF58BY&amp;ab_channel=3Blue1Brown</p></li>
</ul>
</div>
<div class="section" id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h3>
<p>Physiology of hearing suggest that the structure of human ears are doing Fourier transform.</p>
<div class="figure align-default" id="fourier-ear">
<a class="reference internal image-reference" href="../_images/fourier-ear.png"><img alt="" src="../_images/fourier-ear.png" style="width: 30%;" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Hairs in the cochlea (3) have different frequency responses [<a class="reference external" href="http://texasearcenter.com">image link</a>]</span><a class="headerlink" href="#fourier-ear" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="examples-and-applications">
<h3>Examples and Applications<a class="headerlink" href="#examples-and-applications" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Speech</p>
<div class="figure align-default" id="fourier-speech">
<a class="reference internal image-reference" href="../_images/spectral-clustering-speech-sep.png"><img alt="" src="../_images/spectral-clustering-speech-sep.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Separate components from a speech (apply FT to rolling windows)</span><a class="headerlink" href="#fourier-speech" title="Permalink to this image">¶</a></p>
</div>
</li>
<li><p>Financial market data</p></li>
<li><p>Weather data</p></li>
<li><p>Medical imaging, other scientific imaging</p></li>
<li><p>Image compression (e.g. JPEG)</p></li>
</ul>
<p>Applications to machine learning</p>
<ul class="simple">
<li><p>Feature extraction, compression and de-noising of speech/images: Can be an important precursor to unsupervised (or supervised) learning</p></li>
<li><p>Approximating kernels [Rahimi &amp; Recht 2007]</p></li>
<li><p>Speeding up convolutional neural networks [Mathieu et al. 2013]</p></li>
<li><p>Analyzing and regularizating neural networks [Aghazadeh et al. 2020]</p></li>
</ul>
</div>
</div>
<div class="section" id="discrete-fourier-transform-dft">
<h2>Discrete Fourier Transform (DFT)<a class="headerlink" href="#discrete-fourier-transform-dft" title="Permalink to this headline">¶</a></h2>
<p>We often start with a very long signal and compute its spectrum over sub-sequences (“windows”) of fixed length <span class="math notranslate nohighlight">\(N\)</span> starting at sample <span class="math notranslate nohighlight">\(t\)</span></p>
<div class="math notranslate nohighlight">
\[
\left\{ \boldsymbol{x}_t \right\} = x_t, x_{t+1}, \ldots x_{t+N-1}
\]</div>
<div class="section" id="transformation">
<h3>Transformation<a class="headerlink" href="#transformation" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Euler’s relation <span class="math notranslate nohighlight">\(e^{j a}=\cos (a)+j \sin (a)\)</span> is used in derivation</p>
</div>
<p>The <em>discrete Fourier transform</em> (DFT) transforms <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x}_t \right\}\)</span> into another sequence <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{X} _k \right\} = X_0, X_1, \ldots, X_{N-1}\)</span> where</p>
<div class="math notranslate nohighlight">
\[
X_k=\sum_{n=t}^{t+N-1} x_n e^{-j 2 \pi k n / N}, \quad k=0, \ldots, N-1
\]</div>
<ul class="simple">
<li><p>The DFT <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{X} _k \right\}\)</span> is also called the <strong>spectrum</strong>. <span class="math notranslate nohighlight">\(X_k\)</span> is the value of the spectrum at the <span class="math notranslate nohighlight">\(k\)</span>-th frequency</p></li>
<li><p>Equivalently, we can consider <span class="math notranslate nohighlight">\(k = −N/2,...,0,...,N/2\)</span>, i.e. the sequence is <span class="math notranslate nohighlight">\(N\)</span>-periodic. Sometimes people write <span class="math notranslate nohighlight">\(\sum_{n=&lt;N&gt;}\)</span> for simplicity</p></li>
<li><p>The fast Fourier transform (FFT) is an algorithm used to compute DFT for window length <span class="math notranslate nohighlight">\(M = 2m\)</span> for some <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p>After doing this for all frames (windows) of a signal, the result is a
<strong>spectrogram</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(X_k\)</span> is in general complex-valued</p>
<ul>
<li><p>often only the real part of it is used. (Why complex numbers? Real-world signals are real… but complex signals are often much easier to analyze)</p></li>
<li><p>we often use only its magnitude or phase</p></li>
</ul>
</li>
<li><p>units:</p>
<ul>
<li><p>Each time sample n corresponds to a time in seconds</p></li>
<li><p>Each frequency sample k corresponds to a frequency <span class="math notranslate nohighlight">\(f(k) = \frac{k}{N} R\)</span> in Hz, where R is the sampling rate.</p></li>
</ul>
</li>
</ul>
<p>Important property: Spectra of real signals are conjugate-symmetric</p>
<ul class="simple">
<li><p>Magnitude is symmetric about <span class="math notranslate nohighlight">\(k = 0\)</span> (equivalently about <span class="math notranslate nohighlight">\(N/2\)</span>)</p></li>
<li><p>Phase is anti-symmetric about <span class="math notranslate nohighlight">\(k = 0\)</span> (equivalently about <span class="math notranslate nohighlight">\(N/2\)</span>)</p></li>
<li><p>So we need only think about positive frequencies</p></li>
</ul>
</div>
<div class="section" id="relation-between-x-k-and-a-k">
<h3>Relation between <span class="math notranslate nohighlight">\(X_k\)</span> and <span class="math notranslate nohighlight">\(a_k\)</span><a class="headerlink" href="#relation-between-x-k-and-a-k" title="Permalink to this headline">¶</a></h3>
<p>For historical reasons we will define <span class="math notranslate nohighlight">\(X_k=N a_k\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[
a_{k}=\frac{1}{N} \sum_{n=&lt;N&gt;} x_n e^{-j k \omega_{0} n}
\]</div>
<p>which is called the <strong>analysis equation</strong>.</p>
<p>The decomposition is called the <strong>synthesis equation</strong></p>
<div class="math notranslate nohighlight">
\[
x_n=\sum_{k=&lt;N&gt;} a_{k} e^{j k \omega_{0} n}
\]</div>
<p>Note that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(ω0 = 2π/N\)</span></p></li>
<li><p>Notation: Sometimes written <span class="math notranslate nohighlight">\(x_n \leftrightarrow a_k\)</span></p></li>
<li><p>Convenient to think of <span class="math notranslate nohighlight">\(a_k\)</span> as being defined for all <span class="math notranslate nohighlight">\(k\)</span>, although we only need a subset of <span class="math notranslate nohighlight">\(N\)</span> of them: <span class="math notranslate nohighlight">\(a_{k+N} = a_k\)</span></p></li>
<li><p>Since <span class="math notranslate nohighlight">\(x_n\)</span> is periodic, it is specified uniquely by only <span class="math notranslate nohighlight">\(N\)</span> numbers, either in time or in frequency domain</p></li>
</ul>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>Cosine function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
x_n &amp;=\cos \left(\frac{\pi}{4} n\right) \\
&amp;=\frac{1}{2}\left(e^{j \pi n / 4}+e^{-j \pi n / 4}\right) \\
\Longrightarrow \omega_{0} &amp;=\pi / 4, N=8, a_{1}=a_{-1}=1 / 2, X[1]=X[-1]=8 \frac{1}{2}=4
\end{aligned}
\end{split}\]</div>
<p>Sine function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
x_n &amp;=\sin \left(\frac{\pi}{4} n\right) \\
&amp;=\frac{1}{2 j}\left(e^{j \pi n / 4}-e^{-j \pi n / 4}\right) \\
\Longrightarrow \omega_{0} &amp;=\pi / 4, N=8, a_{1}=\frac{1}{2 j}, a_{-1}=-\frac{1}{2 j}
\end{aligned}
\end{split}\]</div>
</div>
</div>
<div class="section" id="continuous-time-fourier-transform">
<h2>Continuous-time Fourier Transform<a class="headerlink" href="#continuous-time-fourier-transform" title="Permalink to this headline">¶</a></h2>
<p>Even though we mainly deal with digitized data, we sometimes wish to reason about continuous-time functions. Continuous-time Fourier transform describes signals as continuous “sums” (integrals) of sinusoids at arbitrary frequency <span class="math notranslate nohighlight">\(\omega\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
x(t) &amp;=\frac{1}{2 \pi} \int_{-\infty}^{\infty} X(j \omega) e^{j \omega t} \mathrm{~d} \omega &amp;\text { Synthesis equation } \\
X(j \omega) &amp;=\int_{-\infty}^{\infty} x(t) e^{-j \omega t}  \mathrm{~d} t &amp; \text { Analysis equation }
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="d-discrete-time-fourier-series-transforms">
<h2>2-D Discrete-“time” Fourier series/transforms<a class="headerlink" href="#d-discrete-time-fourier-series-transforms" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X_{k l}=&amp; \frac{1}{M N} \sum_{&lt;N&gt;} \sum_{&lt;M&gt;} x[m, n] e^{-j 2 \pi(n k / N+m l / M)} \\
&amp; \text{where }  0 \leq k \leq N-1,0 \leq l \leq M-1 \\
x[m, n]=&amp; \sum_{k=0}^{N-1} \sum_{l=0}^{M-1} X_{k l} e^{j 2 \pi(n k / N+m l / M)}
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>Equivalent to 1-D transforms when one frequency dim is <strong>fixed</strong>.</p></li>
<li><p>2-D fast Fourier transform requires <span class="math notranslate nohighlight">\(M N (\log _{2} M) (\log _{2} N)\)</span> operations.</p></li>
</ul>
</div>
<div class="section" id="d-discrete-time-convolution">
<h2>2-D Discrete-“time” convolution<a class="headerlink" href="#d-discrete-time-convolution" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
y\left[n_{1}, n_{2}\right]=x\left[n_{1}, n_{2}\right] * h\left[n_{1}, n_{2}\right]=\sum_{k_{1}=-\infty}^{\infty} \sum_{k_{2}=-\infty}^{\infty} x\left[k_{1}, k_{2}\right] h\left[n_{1}-k_{1}, n_{2}-k_{2}\right]
\]</div>
<ul class="simple">
<li><p>This is the operation being done in convolutional neural networks, on the image <span class="math notranslate nohighlight">\(x\)</span> and the filter <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p>But we typically don’t bother with flipping the filter and state it as a dot product</p></li>
<li><p>The properties of convolution tell us <span class="math notranslate nohighlight">\(Y_{k l}=X_{k l} H_{k l}\)</span></p></li>
</ul>
</div>
<div class="section" id="applications-to-machine-learning">
<h2>Applications to Machine Learning<a class="headerlink" href="#applications-to-machine-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="faster-cnn-training">
<h3>Faster CNN Training<a class="headerlink" href="#faster-cnn-training" title="Permalink to this headline">¶</a></h3>
<p>Today there are other methods that speed up CNN training.</p>
</div>
<div class="section" id="random-fourier-features">
<h3>Random Fourier Features<a class="headerlink" href="#random-fourier-features" title="Permalink to this headline">¶</a></h3>
<p>We introduced that computing kernel matrix is computationally expensive. We can approximate <span class="math notranslate nohighlight">\(\phi(x)\)</span> corresponding to a given kernel.</p>
<dl class="simple myst">
<dt>Theorem (Bochner’s)</dt><dd><p>Shift-invariant kernels (only depends on the difference <span class="math notranslate nohighlight">\(x-y\)</span>) are n-dimensional continuous Fourier transforms of some probability distribution <span class="math notranslate nohighlight">\(p(w)\)</span>,</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
k(x, y)=k(x-y)=\int p(w) e^{j w ^{\top} (x-y)} \mathrm{~d} w=\mathbb{E}_{w}\left[\xi_{w}(x) \xi_{w}(y)^{*}\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi_{w}(x)=e^{j w ^{\top}  x}\)</span>.</p>
<p>So, we can estimate the kernel by <strong>averaging</strong> a bunch of such products for various random drawn values of <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>Idea: Use a feature map <span class="math notranslate nohighlight">\(φ(x)\)</span> that is a concatenation of a bunch of <span class="math notranslate nohighlight">\(\xi\)</span>’s. Then we have an explicit nonlinear map, and no need to do large kernel computations!</p>
<p>For Gaussian, Laplacian and Cauchy kernel, <span class="math notranslate nohighlight">\(p(w)\)</span> is easy to find.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{lll}
\text { Kernel Name } &amp; k(\Delta) &amp; p(\omega) \\
\hline \text { Gaussian } &amp; e^{-\frac{\|\Delta\|_{2}^{2}}{2}} &amp; (2 \pi)^{-\frac{D}{2}} e^{-\frac{\|\omega\|_{2}^{2}}{2}} \\
\text { Laplacian } &amp; e^{-\|\Delta\|_{1}} &amp; \prod_{d} \frac{1}{\pi\left(1+\omega_{d}^{2}\right)} \\
\text { Cauchy } &amp; \prod_{d} \frac{2}{1+\Delta_{d}^{2}} &amp; e^{-\|\Delta\|_{1}}
\end{array}
\end{split}\]</div>
<div class="figure align-default" id="fourier-kernel-algo">
<a class="reference internal image-reference" href="../_images/fourier-kernel-algo.png"><img alt="" src="../_images/fourier-kernel-algo.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">fourier-kernel-algo</span><a class="headerlink" href="#fourier-kernel-algo" title="Permalink to this image">¶</a></p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l}
\text { Algorithm 1 Random Fourier Features. } \\
\hline \text { Require: A positive definite shift-invariant kernel } k(\mathbf{x}, \mathbf{y})=k(\mathbf{x}-\mathbf{y}) . \\
\text { Ensure: A randomized feature map } \mathbf{z}(\mathbf{x}): \mathcal{R}^{d} \rightarrow \mathcal{R}^{2 D} \text { so that } \mathbf{z}(\mathbf{x})^{\prime} \mathbf{z}(\mathbf{y}) \approx k(\mathbf{x}-\mathbf{y}) . \\
\text { Compute the Fourier transform } p \text { of the kernel } k: p(\omega)=\frac{1}{2 \pi} \int e^{-j \omega^{\prime} \Delta} k(\Delta) d \Delta . \\
\text { Draw } D \text { iid samples } \omega_{1}, \cdots, \omega_{D} \in \mathcal{R}^{d} \text { from } p . \\
\text { Let } \mathbf{z}(\mathbf{x}) \equiv \sqrt{\frac{1}{D}}\left[\cos \left(\omega_{1}^{\prime} \mathbf{x}\right) \cdots \cos \left(\omega_{D}^{\prime} \mathbf{x}\right) \sin \left(\omega_{1}^{\prime} \mathbf{x}\right) \cdots \sin \left(\omega_{D}^{\prime} \mathbf{x}\right)\right]^{\prime} \\
\hline
\end{array}
\end{split}\]</div>
<p>…</p>
</div>
<div class="section" id="regularizing-neural-networks">
<h3>Regularizing Neural Networks<a class="headerlink" href="#regularizing-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>where is FT used?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./30-ml-basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="53-self-supervised.html" title="previous page">Self-supervised Learning</a>
    <a class='right-next' id="next-link" href="../31-regression/00-regression.html" title="next page">Regression</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>