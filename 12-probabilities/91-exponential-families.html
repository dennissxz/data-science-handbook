
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exponential Families &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Large Sample Theory" href="91-large-sample-theory.html" />
    <link rel="prev" title="Multivariate Notations" href="90-multivariate-notations.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-probabilities.html">
   Probabilities
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-model-selection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/41-penalized-regression.html">
     Penalized Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/12-pca-variants.html">
     PCA Variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/61-indep-component-analysis.html">
     Independent Component Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/81-density-fitting.html">
     Application to Density Fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/51-embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/53-graph-neural-networks.html">
     Graphical Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/12-probabilities/91-exponential-families.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/12-probabilities/91-exponential-families.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F12-probabilities/91-exponential-families.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/12-probabilities/91-exponential-families.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#one-parameter-exponential-family">
   One-parameter Exponential Family
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#moments-relations">
     Moments Relations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood">
     Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multivariate-gaussian">
   Multivariate Gaussian
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualization">
     Visualization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#properties">
     Properties
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation">
     Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mle">
       MLE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#wishart-distribution">
       Wishart Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-and-cons">
     Pros and Cons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#univariate-gaussian">
   Univariate Gaussian
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-normality">
     Check Normality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformation-to-near-normality">
     Transformation to Near Normality
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="exponential-families">
<h1>Exponential Families<a class="headerlink" href="#exponential-families" title="Permalink to this headline">¶</a></h1>
<div class="section" id="one-parameter-exponential-family">
<span id="one-dim-exponential"></span><h2>One-parameter Exponential Family<a class="headerlink" href="#one-parameter-exponential-family" title="Permalink to this headline">¶</a></h2>
<div class="section" id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h3>
<p>Consider a random variable <span class="math notranslate nohighlight">\(Y\)</span> with probability density function parameterized by <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}\)</span>. If its PDF can be written in the form</p>
<div class="math notranslate nohighlight">
\[f(x;\theta) = f_0 (x) \exp \left( x\theta - b(\theta) \right) \]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b(\theta)\)</span> is some function of <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_0(x)\)</span> involves only <span class="math notranslate nohighlight">\(x\)</span>, no <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
</ul>
<p>then we call there PDF from one-parameter exponential family, where “one” means <span class="math notranslate nohighlight">\(\theta \in \mathbb{R} ^1\)</span>.</p>
<p>Some examples include</p>
<ul>
<li><p>Normal with known variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, parameterized by <span class="math notranslate nohighlight">\(\mu\)</span></p>
<div class="math notranslate nohighlight">
\[
  f(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp(-\frac{1}{2\sigma^{2}}(x^{2}-2\mu x+\mu^{2}))=\underbrace{\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right)}_{\theta}{f_{0}(x)}\exp\left(x\underbrace{\frac{\mu}{\sigma^{2}}}_{\theta}-\frac{\mu^{2}}{2\sigma^{2}}\right)
  \]</div>
</li>
<li><p>Bernoulli parameterized by <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P} (X=x)=p^{x}(1-p)^{1-x}=\exp(x\underbrace{\ln\frac{p}{1-p}}_{\theta}+\ln(1-p))
  \]</div>
</li>
<li><p>Binomial parameterized by <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P} (X=x)=\binom{n}{x} p^{x}(1-p)^{n-x}= \binom{n}{x}
  \exp(x\underbrace{\ln\frac{p}{1-p}}_{\theta}+n\ln(1-p))
  \]</div>
</li>
<li><p>Poisson parameterized by <span class="math notranslate nohighlight">\(\mu\)</span></p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P}  (X=x)=\frac{e^{-\mu}\mu^{x}}{x!}=\frac{1}{x!}\exp(x\underbrace{\ln\mu}_{\theta}-\mu)
  \]</div>
</li>
</ul>
<p>Moreover, we call</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span>: sufficient statistics</p></li>
<li><p><span class="math notranslate nohighlight">\(b(\theta)\)</span>: normalizing or cumulant function</p></li>
</ul>
</div>
<div class="section" id="moments-relations">
<h3>Moments Relations<a class="headerlink" href="#moments-relations" title="Permalink to this headline">¶</a></h3>
<p>Distributions in one-parameter exponential family has some nice properties</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mu = \mathbb{E}\left( X \right) = b ^\prime (\theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{Var}\left( X \right) = b ^{\prime\prime}  (\theta) = v(\mu)\)</span></p>
<p>This variance-mean relation uniquely characterize a distribution class (normal/binomial//Poisson) from exponential family.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial \mu}{\partial \theta} = b ^{\prime\prime}  (\theta) = \operatorname{Var}\left(X \right) &gt; 0\)</span>.</p></li>
</ul>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
1 &amp;= \int f(x; \theta) \mathrm{~d}x \\
&amp;= e ^{-b(\theta)} \int e^{x\theta} f_0(x) \mathrm{~d} x\\
\Rightarrow \quad e ^{b(\theta)}&amp;=  \int e^{x\theta} f_0(x) \mathrm{~d} x \\
\end{aligned}\end{split}\]</div>
<p>Taking derivative w.r.t. <span class="math notranslate nohighlight">\(\theta\)</span> on both sides, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
b ^\prime (\theta) e ^{b(\theta)}
&amp;=  \int x e^{x\theta} f_0(x) \mathrm{~d} x \\
&amp;= e ^{b(\theta)} \int x e^{x\theta - b(\theta)} f_0(x) \mathrm{~d} x \\
&amp;= e ^{b(\theta)} \int x f(x;\theta) \mathrm{~d} x \\
&amp;= e ^{b(\theta)} \mathbb{E}\left( X \right)\\
\Rightarrow \quad b ^\prime (\theta) &amp;= \mathbb{E}\left( X \right) \\
\end{aligned}\end{split}\]</div>
<p>With a similar approach we can find <span class="math notranslate nohighlight">\(b ^{\prime \prime }(\theta) = \operatorname{Var}\left( X \right)\)</span></p>
</div>
</div>
<div class="section" id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline">¶</a></h3>
<p>Consider observations <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>, each from a one-parameter exponential distribution parameterized bx <span class="math notranslate nohighlight">\(\theta_i\)</span>. The log-likelihood of <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \ldots \theta_n\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\ell(\theta)
&amp;= \log \prod_{i=1}^n f(x_i ;\theta)\\
&amp;= \sum_{i=1}^n \left\{ x_i \theta_i - b(\theta_i) + \ln f_0 (x_i) \right\}\\
\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="multivariate-gaussian">
<span id="multi-gaussian"></span><h2>Multivariate Gaussian<a class="headerlink" href="#multivariate-gaussian" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Definition<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Multivariate normal)</dt><dd><p>A <span class="math notranslate nohighlight">\(p\)</span>-variate random vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is said to have a multivariate normal distribution (multinormal distribution) if <strong>every</strong> linear combination of its components has a univariate normal distribution. That is, <span class="math notranslate nohighlight">\(\boldsymbol{c} ^{\top} \boldsymbol{x} \sim \mathcal{N} (\boldsymbol{c} ^{\top} \boldsymbol{\mu} _x,  \boldsymbol{c} ^{\top}  \boldsymbol{\Sigma}_x \boldsymbol{c})\)</span> for any <span class="math notranslate nohighlight">\(\boldsymbol{c} \in \mathbb{R} ^p\)</span>.</p>
</dd>
</dl>
<p>For a multivariate normal distribution <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{\mu} , \boldsymbol{\Sigma} )\)</span>, the probability density function is</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
f(\boldsymbol{x} ;\boldsymbol{\mu}, \boldsymbol{\Sigma})=\frac{1}{(2 \pi)^{p / 2}|\boldsymbol{\Sigma}|^{1 / 2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)
\end{equation}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> is the mean vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix, with <span class="math notranslate nohighlight">\(\sigma_{ij} = \operatorname{Cov}\left( x_i, x_j \right)\)</span></p></li>
<li><p>The fraction <span class="math notranslate nohighlight">\(\frac{1}{(2 \pi)^{p / 2}|\boldsymbol{\Sigma}|^{1 / 2}}\)</span> is a normalizing constant.</p></li>
<li><p>The determinant <span class="math notranslate nohighlight">\(\left\vert \boldsymbol{\Sigma}  \right\vert = \operatorname{det}(\boldsymbol{\Sigma})\)</span> is called <a class="reference internal" href="90-multivariate-notations.html#id1"><span class="std std-ref">generalized variance</span></a>. It can be viewed as a higher dimension generalization of the scalar-valued <span class="math notranslate nohighlight">\(\sigma^2\)</span> in univariate case that measures uncertainty of the distribution. Hence we have the square root <span class="math notranslate nohighlight">\(\left\vert \cdot \right\vert ^{1/2}\)</span>, as <span class="math notranslate nohighlight">\(\sigma\)</span> in the univariate Gaussian density.</p></li>
</ul>
</div>
<div class="section" id="visualization">
<h3>Visualization<a class="headerlink" href="#visualization" title="Permalink to this headline">¶</a></h3>
<p>All <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> satisfy the equality below lies on a ellipsoid contour, with equal Mahalanobis distance to its center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
c &amp;= f(\boldsymbol{x} ;\boldsymbol{\mu}, \boldsymbol{\Sigma}) \\
\Leftrightarrow \qquad c ^\prime &amp;= (\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})
\end{aligned}\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> determines the center of the ellipsoid.</p></li>
<li><p>Let EVD <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U}\)</span>, then</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> determines the rotation angle of the ellipsoid. The vectors <span class="math notranslate nohighlight">\(\boldsymbol{u} _i\)</span> are the directions of the axes of the ellipsoid.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> determines the lengths of the axes. The length should be proportional to <span class="math notranslate nohighlight">\(\sqrt{\lambda_i}\)</span>. If all eigenvalues are the same, the ellipsoid reduces to a ball.</p></li>
</ul>
</li>
<li><p>(To be shown below) the rotation transformation <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\prime  = \boldsymbol{U} ^\top \boldsymbol{x}, \boldsymbol{\mu} ^\prime  = \boldsymbol{U} ^\top \boldsymbol{\mu}, \boldsymbol{\Sigma} ^\prime  = \boldsymbol{\Lambda}\)</span> change the center, align the ellipsoid axes to the coordinate axes (so the variables becomes independent and the joint PDF factorizes to univariate PDF), while keep the axes lengths intact (rotation preserve lengths and angles).</p></li>
</ul>
<p>In the 2-d case, an ellipsoid reduces to an ellipse.</p>
<div class="myclass figure align-default" id="gaussian-density-2d">
<a class="reference internal image-reference" href="../_images/gaussian-density-2d.png"><img alt="" src="../_images/gaussian-density-2d.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">Bivariate Gaussian density and ellipse</span><a class="headerlink" href="#gaussian-density-2d" title="Permalink to this image">¶</a></p>
</div>
<p>A numerical example to illustrate how <span class="math notranslate nohighlight">\(\boldsymbol{\mu}, \boldsymbol{\Lambda} , \boldsymbol{U}\)</span> jointly decide the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, and the effect of rotation transformation (rotation) <span class="math notranslate nohighlight">\(\boldsymbol{U} ^{\top} \boldsymbol{x}\)</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span><span class="o">,</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">Lam</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Lam</span> <span class="o">@</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">S</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">bottom</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">right</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">top</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

<span class="c1"># print(f&#39;new center: {mu @ U.T}&#39;)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">bottom</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">right</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">top</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/91-exponential-families_1_0.png" src="../_images/91-exponential-families_1_0.png" />
</div>
</div>
</div>
<div class="section" id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Sometimes <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim \mathcal{N} (\boldsymbol{0} , \boldsymbol{I} _p)\)</span> is called <strong>white noise</strong>. If the off-diagonal entires are non-zero, we say there is ‘color’.</p></li>
<li><p><strong>MGF</strong>: <span class="math notranslate nohighlight">\(M_{\boldsymbol{x}}(\boldsymbol{t})=\exp \left(\boldsymbol{t}^{\top} \boldsymbol{\mu}+\frac{1}{2} \boldsymbol{t}^{\top} \boldsymbol{\Sigma} \boldsymbol{t}\right)\)</span>.</p></li>
<li><p><strong>Sum</strong>: if <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N} _p (\boldsymbol{\mu} _1, \boldsymbol{\Sigma} _1)\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim \mathcal{N} _p (\boldsymbol{\mu} _2, \boldsymbol{\Sigma} _2)\)</span> are independent, then <span class="math notranslate nohighlight">\(\boldsymbol{x} + \boldsymbol{y}  \sim \mathcal{N} _p (\boldsymbol{\mu} _1 + \boldsymbol{\mu} _2, \boldsymbol{\Sigma} _1 + \boldsymbol{\Sigma} _2)\)</span>.</p></li>
<li><p>What is the condition for a valid covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>? Given a positive definite matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N} _p (\boldsymbol{\mu} , \boldsymbol{\Sigma})\)</span> iff there exists a non-singular matrix <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z} \sim \mathcal{N} _p (\boldsymbol{0} , \boldsymbol{I})\)</span> such that <span class="math notranslate nohighlight">\(\boldsymbol{x} = \boldsymbol{\mu} + \boldsymbol{B} \boldsymbol{z}\)</span>. In this case <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \boldsymbol{B} \boldsymbol{B} ^{\top}\)</span>.</p></li>
<li><p><strong>Sub-vectors</strong>: if <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathcal{N} _{p + q} (\boldsymbol{\mu} , \boldsymbol{\Sigma})\)</span>, let <span class="math notranslate nohighlight">\(\boldsymbol{x}_1\)</span> be its first <span class="math notranslate nohighlight">\(p\)</span> components and <span class="math notranslate nohighlight">\(\boldsymbol{x} _2\)</span> be the rest <span class="math notranslate nohighlight">\(q\)</span> components, then</p>
<ul>
<li><p>still normal: <span class="math notranslate nohighlight">\(\boldsymbol{x}_{1} \sim \mathcal{N}_{p}\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{11}\right),  \boldsymbol{x}_{2} \sim \mathcal{N}_{q}\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma} _{22}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x} _1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x} _2\)</span> are independent iff <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \boldsymbol{x} _1, \boldsymbol{x} _2 \right) = \boldsymbol{\Sigma} _{12} = \boldsymbol{0}_{p \times q}\)</span> (hint for <span class="math notranslate nohighlight">\(\Leftarrow\)</span>: use definition <span class="math notranslate nohighlight">\(f_{12} = f_1 f_2\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\((\boldsymbol{x} _1 - \boldsymbol{\Sigma} _{12} \boldsymbol{\Sigma} _{22} ^{-1} \boldsymbol{x} _2)\)</span> is independent of <span class="math notranslate nohighlight">\(\boldsymbol{x} _2\)</span> and is distributed as <span class="math notranslate nohighlight">\(\mathcal{N} _p (\boldsymbol{\mu} _1 -  \boldsymbol{\Sigma} _{12} \boldsymbol{\Sigma} _{22} ^{-1} \boldsymbol{\mu} _2, \boldsymbol{\Sigma} _{11}-  \boldsymbol{\Sigma} _{12} \boldsymbol{\Sigma} _{22} ^{-1} \boldsymbol{\Sigma} _{21})\)</span>.</p></li>
<li><p>conditional distribution is normal:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{x}_{1} \mid \boldsymbol{x}_{2} \sim  N_{p}\left(\boldsymbol{\mu}_{1}+\boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1}\left(\boldsymbol{x}_{2}-\boldsymbol{\mu}_{2}\right), \boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1} \boldsymbol{\Sigma}_{21}\right)
    \]</div>
<p>Note the variance does not change with <span class="math notranslate nohighlight">\(\boldsymbol{x} _2\)</span>. In particular,</p>
<ul>
<li><p>for <span class="math notranslate nohighlight">\(p=q=1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
      \begin{aligned}
      X_{1} \mid X_{2}=x_{2} \sim &amp; N\left(\mu_{1}+\frac{\sigma_{12}}{\sigma_{22}}\left(x_{2}-\mu_{2}\right), \sigma_{11}-\frac{\sigma_{12}^{2}}{\sigma_{22}}\right) \\
      &amp;=N\left(\mu_{1}+\rho \frac{\sigma_{1}}{\sigma_{2}}\left(x_{2}-\mu_{2}\right), \sigma_{1}^{2}\left(1-\rho^{2}\right)\right)
      \end{aligned}
      \end{split}\]</div>
</li>
<li><p>if <span class="math notranslate nohighlight">\(p=1\)</span>, then <span class="math notranslate nohighlight">\(x_1 = \mu_1 + \boldsymbol{\beta} ^{\top} (\boldsymbol{x} _2 - \boldsymbol{\mu} _2)\)</span>, i.e. a ‘regression’ model, where <span class="math notranslate nohighlight">\(\boldsymbol{\beta} ^{\top} = \boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1}\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{\Omega} = \boldsymbol{\Sigma} ^{-1}\)</span>, using block matrix inverse <a class="reference internal" href="../11-math/21-linear-algebra.html#matrix-inverse"><span class="std std-ref">formula</span></a>, it is easy to show that <span class="math notranslate nohighlight">\(\beta_j = - \omega_{1j}/\omega_{11}\)</span>.</p></li>
</ul>
<div class="myclass figure align-default" id="gaussian-marginal-conditional">
<a class="reference internal image-reference" href="../_images/gaussian-marginal-conditional.png"><img alt="" src="../_images/gaussian-marginal-conditional.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">Marginal Gaussian and conditional Gaussian are also Gaussians [Shi 2021]</span><a class="headerlink" href="#gaussian-marginal-conditional" title="Permalink to this image">¶</a></p>
</div>
</li>
</ul>
</li>
<li><p><strong>Transformation</strong>: If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is <span class="math notranslate nohighlight">\(p\)</span>-variate normal <span class="math notranslate nohighlight">\(\mathcal{N} (\boldsymbol{\mu} , \boldsymbol{\Sigma})\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{c}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is a <span class="math notranslate nohighlight">\(k \times p\)</span> matrix and <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-vector of constants, then <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> is of <span class="math notranslate nohighlight">\(k\)</span>-variate normal, with mean <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{\mu} + \boldsymbol{c}\)</span> and variance <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A} ^{\top}\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(k=1\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{a} ^{\top} \boldsymbol{x} \sim \mathcal{N} ( \boldsymbol{a} ^{\top} \boldsymbol{\mu} , \boldsymbol{a} ^{\top} \boldsymbol{\Sigma} \boldsymbol{a})\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is standard normal <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim \mathcal{N} (\boldsymbol{c} , \boldsymbol{A} \boldsymbol{A} ^{\top})\)</span></p></li>
</ul>
</li>
<li><p><strong>Independency</strong> after transformation: for any <span class="math notranslate nohighlight">\(m\times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(n\times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span>,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{x}  \perp \boldsymbol{B} \boldsymbol{x} \Leftrightarrow \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{B} ^{\top} = \boldsymbol{0}\)</span></p>
<ul>
<li><p>corollary: <span class="math notranslate nohighlight">\(X_i \perp X_j \Leftrightarrow \sigma_{ij} =0\)</span>.</p></li>
</ul>
</li>
<li><p>when <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is symmetric, <span class="math notranslate nohighlight">\(\boldsymbol{x} ^{\top}\boldsymbol{A} \boldsymbol{x}  \perp \boldsymbol{B} \boldsymbol{x} \Leftrightarrow \boldsymbol{B} \boldsymbol{\Sigma} \boldsymbol{A} = \boldsymbol{0}\)</span></p></li>
<li><p>when both <span class="math notranslate nohighlight">\(\boldsymbol{A}, \boldsymbol{B}\)</span> are symmetric, <span class="math notranslate nohighlight">\(\boldsymbol{x} ^{\top}\boldsymbol{A} \boldsymbol{x}  \perp \boldsymbol{x} ^{\top} \boldsymbol{B} \boldsymbol{x} \Leftrightarrow \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{B} = \boldsymbol{0}\)</span></p></li>
</ul>
</li>
<li><p>Since EVD <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top\)</span>, then <span class="math notranslate nohighlight">\(\left\vert \boldsymbol{\Sigma}  \right\vert = \left\vert \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top  \right\vert = \left\vert \boldsymbol{\Lambda}  \right\vert\)</span></p></li>
<li><p>For every multivariate Gaussian <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim N(\boldsymbol{\mu} , \boldsymbol{\Sigma} )\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}
= \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top\)</span>, there exists a transformation <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\prime  = \boldsymbol{U} ^\top \boldsymbol{x}, \boldsymbol{\mu} ^\prime = \boldsymbol{U} ^\top \boldsymbol{\mu}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
  f(\boldsymbol{x} ; \boldsymbol{\mu}, \boldsymbol{\Sigma} ) = f(\boldsymbol{x} ^\prime ; \boldsymbol{\mu} ^\prime , \boldsymbol{\Lambda})
  \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  f(\boldsymbol{x} ^\prime ; \boldsymbol{\mu} ^\prime , \boldsymbol{\Lambda})
  &amp;= \frac{1}{(2 \pi)^{p / 2}|\boldsymbol{\Lambda}|^{1 / 2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}^\prime -\boldsymbol{\mu} ^\prime )^{\top} \boldsymbol{\Lambda} ^{-1}(
    \boldsymbol{x} ^\prime  -\boldsymbol{\mu} ^\prime )\right) \\
  &amp;=  \frac{1}{(2 \pi)^{p / 2}\Pi_{i=1}^p \lambda_i} \exp \left(-\sum_{i=1}^p\frac{1}{2\sigma_{ii}}(x_i^\prime -\mu_i ^\prime )^2\right) \\
  &amp;= \prod_{i=1}^{p} \frac{1}{(2 \pi)^{1 / 2} \lambda_{i}} \exp \left(-\frac{1}{2 \lambda_{i}}\left(x_{i}^\prime -\mu_{i}^\prime \right)^{2}\right)
  \end{align}\end{split}\]</div>
<p>which is a product of PDFs of univariate Gaussians, i.e. dependency is dropped. Geometrically, <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> rotate the axes of the distribution but keep the function value intact.</p>
</li>
<li><p><strong>Quadratic form</strong>: if <span class="math notranslate nohighlight">\(\boldsymbol{x} \sim \mathcal{N} _p (\boldsymbol{\mu} , \boldsymbol{\Sigma})\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is p.d., then <span class="math notranslate nohighlight">\((\boldsymbol{x} - \boldsymbol{\mu} ) ^{\top} \boldsymbol{\Sigma} ^{-1} (\boldsymbol{x} - \boldsymbol{\mu} ) \sim \chi ^2 _p\)</span>.</p>
<ul class="simple">
<li><p>For standard Gaussian <span class="math notranslate nohighlight">\(\boldsymbol{z} \sim \mathcal{N} _p (\boldsymbol{0} , \boldsymbol{I})\)</span>: $<span class="math notranslate nohighlight">\(\boldsymbol{z} ^{\top} \boldsymbol{z} = \sum_{j=1}^p Z_i^2 \sim \chi ^2 _p\)</span>$</p></li>
<li><p>In sample data, as <span class="math notranslate nohighlight">\(n\)</span> is large, <span class="math notranslate nohighlight">\(n(\bar{\boldsymbol{x}}-\boldsymbol{\mu})^{\prime} \boldsymbol{S}^{-1}(\bar{\boldsymbol{x}}-\boldsymbol{\mu}) \sim \chi_{p}^{2}\)</span></p></li>
</ul>
</li>
<li><p>Related to unit surface: if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> follow spherical Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}_p (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _p)\)</span>, let the norm be <span class="math notranslate nohighlight">\(R = \left\| \boldsymbol{x}\right\|\)</span>, then its density is</p>
<div class="math notranslate nohighlight">
\[
    f_p(r) = \frac{S_p}{  (2 \pi \sigma^2)^{p / 2}} r^{p -1} \exp \left( -\frac{r^2}{2 \sigma^2 }  \right), \quad r \ge 0
    \]</div>
<p>where <span class="math notranslate nohighlight">\(S_p=\frac{2 \pi^{p / 2}}{\Gamma(p / 2)}\)</span> is the surface area of the unit sphere in <span class="math notranslate nohighlight">\(p\)</span>-dimensions.</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(r= \sqrt{\sigma^2 (p-1)}\)</span>, <span class="math notranslate nohighlight">\(f_p(r)\)</span> achieves its maximum. That is, it characterizes the unit spherical shell that has the ‘most’ points on it among all unit spherical shells in <span class="math notranslate nohighlight">\(\mathbb{R} ^p\)</span>.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\sigma^2\)</span> is fixed, then the optimal radius scales sub-linearly in <span class="math notranslate nohighlight">\(d\)</span>, i.e. <span class="math notranslate nohighlight">\(r^* = \mathcal{O}(\sqrt{p})\)</span></p></li>
<li><p>if we set <span class="math notranslate nohighlight">\(\sigma^2 = 1/d\)</span>, then <span class="math notranslate nohighlight">\(r = \sqrt{(p-1)/p} \approx 1\)</span>. See below.</p></li>
</ul>
</li>
<li><p>In high-dimensional case, as <span class="math notranslate nohighlight">\(p \rightarrow \infty\)</span>,</p>
<ul>
<li><p>independent Gaussian points with variance <span class="math notranslate nohighlight">\(\frac{1}{p}\)</span> concentrate on the surface of sphere <span class="math notranslate nohighlight">\(S^{p-1}\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[\mathcal{N} _p (\boldsymbol{0} , p ^{-1}  \boldsymbol{I} _p) \approx \operatorname{Unif} (S^{p-1})\]</div>
</li>
<li><p>for <span class="math notranslate nohighlight">\(\boldsymbol{x} , \boldsymbol{y} \sim \mathcal{N} _p(\boldsymbol{0} , \frac{1}{p} \boldsymbol{I} _p)\)</span>, we have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{x} ^{\top} \boldsymbol{x} ] = \sum_{i=1}^p \mathbb{E} [X_j^2] = \sum_{j=1}^p \frac{1}{p} = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{x} ^{\top} \boldsymbol{y} ] = \sum_{j=1}^p \mathbb{E} [X_j Y_j]  = \sum_{j=1}^p \mathbb{E} [X_j] \mathbb{E} [Y_j] = 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E} [\left\| \boldsymbol{a} ^{\top} \boldsymbol{x} \right\| ^2 ] = \sum_{j=1}^p a_j ^2 \mathbb{E} [X_j^2] + \sum_{j\ne k}^p a_j a_k \mathbb{E} [X_j X_k]  = \sum_{j=1}^p \frac{a_j^2}{p}  = \frac{1}{p} \left\| \boldsymbol{a}  \right\|^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\left\| \boldsymbol{x}  \right\| \rightarrow 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x} ^{\top} \boldsymbol{y} \rightarrow 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{a} ^{\top} \boldsymbol{x} \rightarrow 0\)</span> for <span class="math notranslate nohighlight">\(\left\| \boldsymbol{a}  \right\| =1\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="estimation">
<h3>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="mle">
<h4>MLE<a class="headerlink" href="#mle" title="Permalink to this headline">¶</a></h4>
<p>The MLE of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> are respectively</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>For derivation see <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html#mle-gaussian-derivation"><span class="std std-ref">here</span></a>.</p>
</div>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\mu}}=\overline{\boldsymbol{x}} \text { and } \hat{\mathbf{\Sigma}}=\frac{\boldsymbol{W}}{n}=\frac{(n-1) \boldsymbol{S}}{n}
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\mu}}\)</span> is unbiased but <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma} }\)</span> is unbiased. However, <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> is unbiased for <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
</div>
<div class="section" id="wishart-distribution">
<span id="id2"></span><h4>Wishart Distribution<a class="headerlink" href="#wishart-distribution" title="Permalink to this headline">¶</a></h4>
<p>Recall that for univariate i.i.d. standard Gaussians we have <span class="math notranslate nohighlight">\(\sum_{i=1}^k Z_i^2 \sim \sigma^2 \chi ^2 _n\)</span>. The multivariate extension of this chi-squared distribution is Wishart distribution.</p>
<dl class="simple myst">
<dt>Definition (Wishart distribution)</dt><dd><p>Suppose <span class="math notranslate nohighlight">\(\boldsymbol{x}_1, \ldots, \boldsymbol{x} _k\)</span> and independent <span class="math notranslate nohighlight">\(\mathcal{N} _p (\boldsymbol{\mu} _i, \boldsymbol{\Sigma})\)</span>. Define a symmetric <span class="math notranslate nohighlight">\(p \times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{V} = \sum_{i=1}^k \boldsymbol{x} _i \boldsymbol{x}_i ^{\top} = \boldsymbol{X} ^{\top} \boldsymbol{X}
  \]</div>
<p>Then <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> is said to follow a Wishart distribution <span class="math notranslate nohighlight">\(W_p(k, \boldsymbol{\Sigma} , \boldsymbol{\Psi})\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the degree of freedom</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is called the <strong>scaling matrix</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Psi} = \sum_{i=1}^k \boldsymbol{\mu} _i \boldsymbol{\mu} _i ^{\top}\)</span> is the <span class="math notranslate nohighlight">\(p \times p\)</span> symmetric <strong>non-centrality matrix</strong></p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{\Psi} = \boldsymbol{0}\)</span>, it is called the <strong>central</strong> Wishart distribution, denoted by <span class="math notranslate nohighlight">\(W_p(k, \boldsymbol{\Sigma})\)</span></p></li>
</ul>
</li>
</ul>
</dd>
</dl>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \sigma^2\)</span>, it reduces to a non-central chi-sqaured distribution <span class="math notranslate nohighlight">\(\sigma^2 \chi ^2 (k, \sum_{i=1}^k  \boldsymbol{\mu} _i^2)\)</span>.</p>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p><strong>sum of independent</strong>: if <span class="math notranslate nohighlight">\(\boldsymbol{V} _1 \sim W_p(k, \boldsymbol{\Sigma} , \boldsymbol{\Psi} _1)\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V} _2 \sim W_p(k, \boldsymbol{\Sigma} , \boldsymbol{\Psi} _2)\)</span> are independent Wisharts, then <span class="math notranslate nohighlight">\(\boldsymbol{V} _1 + \boldsymbol{V} _2 \sim W_p (k_1 + k_2, \boldsymbol{\Sigma} , \boldsymbol{\Psi} _1 + \boldsymbol{\Psi} _2)\)</span>.</p></li>
<li><p><strong>transformation</strong>: if <span class="math notranslate nohighlight">\(\boldsymbol{V} \sim W_p (k, \boldsymbol{\Sigma} , \boldsymbol{\Psi})\)</span>, then</p>
<ul>
<li><p>for any constant <span class="math notranslate nohighlight">\(q \times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{V} \boldsymbol{A} ^{\top} \sim W_q (k, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A} ^{\top} , \boldsymbol{A} \boldsymbol{\Psi} \boldsymbol{A} ^{\top})\)</span>.</p></li>
<li><p>for any constant <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{a}^{\top} \boldsymbol{V} \boldsymbol{a} / \boldsymbol{a}^{\top} \boldsymbol{\Sigma} \boldsymbol{a} \sim \chi^{2}\left(k, \boldsymbol{a}^{\top} \boldsymbol{\Psi} \boldsymbol{a}\right)\)</span>. In particular, <span class="math notranslate nohighlight">\(v_{ii}/\sigma_{ii} \sim \chi ^2 (k, \Phi_{ii})\)</span>.</p></li>
<li><p>for any random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> independent of <span class="math notranslate nohighlight">\(\boldsymbol{V} \sim W_p(k, \boldsymbol{\Sigma})\)</span>,</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}^{\top} \boldsymbol{V} \boldsymbol{y} / \boldsymbol{y}^{\top} \boldsymbol{\Sigma} \boldsymbol{y} \sim \chi^{2}(k)\)</span> and is independent of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}^{\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{y} / \boldsymbol{y}^{\top} \boldsymbol{V}^{-1} \boldsymbol{y} \sim \chi^{2}(k-p+1)\)</span> if <span class="math notranslate nohighlight">\(k &gt; p\)</span>, and is independent of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>for random <span class="math notranslate nohighlight">\(k \times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>,</p>
<ul>
<li><p>for a symmetric <span class="math notranslate nohighlight">\(\boldsymbol{A}: \boldsymbol{X}^{\top} \boldsymbol{A} \boldsymbol{X} \sim W_{p}(r, \boldsymbol{\Sigma}, \boldsymbol{\Psi}) \Leftrightarrow \boldsymbol{A} ^2 = \boldsymbol{A}\)</span> (idempotent), in which case <span class="math notranslate nohighlight">\(r=\operatorname{rank}(\boldsymbol{A})=\operatorname{tr}(\boldsymbol{A})\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Psi}=\boldsymbol{M}^{\top} \boldsymbol{A} \boldsymbol{M}\)</span></p></li>
<li><p>for symmetric idempotent <span class="math notranslate nohighlight">\(\boldsymbol{A}, \boldsymbol{B}\)</span>, we have <span class="math notranslate nohighlight">\(\boldsymbol{X} ^{\top} \boldsymbol{A} \boldsymbol{X}  \perp \boldsymbol{X} ^{\top} \boldsymbol{B} \boldsymbol{X} \Leftrightarrow \boldsymbol{A} \boldsymbol{B} = \boldsymbol{0}\)</span>.</p></li>
<li><p>for symmetric idempotent <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, we have <span class="math notranslate nohighlight">\(\boldsymbol{X} ^{\top} \boldsymbol{A} \boldsymbol{X}  \perp \boldsymbol{X} ^{\top} \boldsymbol{B} \Leftrightarrow \boldsymbol{A} \boldsymbol{B} = \boldsymbol{0}\)</span>.</p></li>
</ul>
</li>
<li><p>Gaussian MLE</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\bar{\boldsymbol{x}} \sim \mathcal{N} _p (\boldsymbol{\mu} , \frac{1}{n} \boldsymbol{\Sigma} )\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((n-1) \boldsymbol{S} = \boldsymbol{W} \sim W_p(n-1, \boldsymbol{\Sigma})\)</span>, extension of <span class="math notranslate nohighlight">\(\sum_{i=1}^n (x_i - \bar{x})^2 \sim \sigma^2\chi ^2 _{n-1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{\boldsymbol{x} } \perp \boldsymbol{S}\)</span></p></li>
</ul>
</li>
<li><p>In the Cholesky decomposition <span class="math notranslate nohighlight">\(\boldsymbol{V} = \boldsymbol{T} \boldsymbol{T}  ^{\top}\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{V} \sim W_p (k, \boldsymbol{\Sigma})\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{T}\)</span> is lower triangular,</p>
<ul>
<li><p>off-diagonal <span class="math notranslate nohighlight">\(t_{ij} \sim \mathcal{N} (0,1)\)</span> for <span class="math notranslate nohighlight">\(i &gt;j\)</span></p></li>
<li><p>diagonal <span class="math notranslate nohighlight">\(t_{ii} \sim \chi ^2 _{k-i+1}\)</span></p></li>
<li><p>all non-zero elements are mutually independent</p></li>
</ul>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \sim \mathcal{N} (\boldsymbol{0} , \boldsymbol{I} _p)\)</span>, then the eigenvalues of a <span class="math notranslate nohighlight">\(\frac{1}{n} \boldsymbol{X} ^{\top} \boldsymbol{X}\)</span>, as <span class="math notranslate nohighlight">\(p, n \rightarrow \infty\)</span>, follows <a class="reference internal" href="../11-math/21-linear-algebra.html#marchenko-pastur-distribution"><span class="std std-ref">Marchenko–Pastur distribution</span></a>.</p></li>
</ul>
</dd>
</dl>
</div>
</div>
<div class="section" id="pros-and-cons">
<h3>Pros and Cons<a class="headerlink" href="#pros-and-cons" title="Permalink to this headline">¶</a></h3>
<p>Pros</p>
<ul>
<li><p>Related to CLM</p></li>
<li><p>Evaluation is convenient</p></li>
<li><p>Can be convert to product of univariate Gaussians in some rotated space</p></li>
<li><p>Mixtures of Gaussian are sufficient to approximate a wide range of distributions</p></li>
<li><p>Arbitrary linear combinations of jointly Gaussian variables are also Gaussian</p></li>
<li><p>Marginals and conditionals of multivariate Gaussians are also Gaussian</p></li>
<li><p>Sums of many i.i.d. random variables converge to Gaussian variables (Central Limit Theorem)</p></li>
<li><p>Log of Gaussian looks like a weighted Euclidean distance, related to squared loss</p>
<div class="math notranslate nohighlight">
\[
    -\ln \sqrt{(2 \pi \sigma)}-\frac{(x-\mu)^{2}}{\left(2 \sigma^{2}\right)}
    \]</div>
</li>
</ul>
<p>Cons</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is hard and expensive to estimate in high dimensions</p>
<ul>
<li><p>sol: assume special structure: diagonal, spherical</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="univariate-gaussian">
<h2>Univariate Gaussian<a class="headerlink" href="#univariate-gaussian" title="Permalink to this headline">¶</a></h2>
<div class="section" id="check-normality">
<h3>Check Normality<a class="headerlink" href="#check-normality" title="Permalink to this headline">¶</a></h3>
<p>For most multivariate analyses, it is important that the data indeed follow the multivariate normal, at least approximately if not exactly.</p>
<ul>
<li><p>QQ-plots (quantile-quantile plot)</p>
<ul class="simple">
<li><p>Sample quantiles are plotted against the expected sample quantiles of a standard normal distribution</p></li>
<li><p>If the points lie on a straight line, then it indicates univariate normality. In addition to measure this linearity by eye, we can also use correlation coefficient to quantify it.</p></li>
<li><p>non-linearity may indicate a need to transform the variable.</p></li>
</ul>
<div class="figure align-default" id="gaussian-qq-plot">
<a class="reference internal image-reference" href="../_images/gaussian-qq-plot.png"><img alt="" src="../_images/gaussian-qq-plot.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">QQ-plots for Gaussian (left) and non-Gaussian (right)</span><a class="headerlink" href="#gaussian-qq-plot" title="Permalink to this image">¶</a></p>
</div>
</li>
<li><p>P-P plot (probability against probability plot) that plots the sample CDF against the theoretical CDF.</p></li>
<li><p>Among all continuous random variable with mean <span class="math notranslate nohighlight">\(0\)</span>, variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, normal random variable has the largest <a class="reference internal" href="../30-ml-basics/03-information-theory.html#differential-entropy"><span class="std std-ref">differential entropy</span></a> with value <span class="math notranslate nohighlight">\(\operatorname{H}(\mathcal{N}(0, \sigma^2)) = \ln(\sigma \sqrt{2 \pi e})\)</span>. Hence, the distance of an entropy from the normal entropy (termed <strong>neg-entropy</strong>), can be used to develop measures of non-Gaussianity, or non-normality.</p></li>
</ul>
<p>Rigorous Tests</p>
<ul class="simple">
<li><p>Shapiro–Wilk <span class="math notranslate nohighlight">\(W\)</span> test</p>
<ul>
<li><p>test statistic: a modified version of the squared sample correlation between the sample quantiles and the expected quantiles.</p></li>
</ul>
</li>
<li><p>Kolmogorov–Smirnov test</p>
<ul>
<li><p>require: large sample, at least 100s</p></li>
<li><p>test statistic: maximum difference between the empirical CDF and the normal CDF.</p></li>
</ul>
</li>
<li><p>Jarque–Bera test</p></li>
<li><p>Test for zero skewness, zero excess kurtosis.</p>
<ul>
<li><p>Normal distribution has kurtosis <span class="math notranslate nohighlight">\(\frac{\mu_{4}}{\mu_{2}^{2}}=3\)</span> and excess kurtosis <span class="math notranslate nohighlight">\(\frac{\mu_{4}}{\mu_{2}^{2}}-3 = 0\)</span>. Large magnitude of excess kurtosis indicates deviation from normal distribution.</p></li>
</ul>
</li>
</ul>
<p>Multivariate case:</p>
<ul>
<li><p>Check whether the squared generalized distance as defined below follows a chi-squared distribution by a Q-Q plot (necessary and sufficient conditions for very large sample size))</p>
<div class="math notranslate nohighlight">
\[
  d_{i}^{2}=\left(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}}\right)^{\prime} \boldsymbol{S}^{-1}\left(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}}\right)
  \]</div>
<p>Again, correlation coefficient can be used to quantity the linearity of the points.</p>
</li>
<li><p>Check each Principal Component (PC) for univariate normality (necessary condition; and if the sample size n is large enough, a sufficient condition)</p></li>
</ul>
</div>
<div class="section" id="transformation-to-near-normality">
<span id="transform-normality"></span><h3>Transformation to Near Normality<a class="headerlink" href="#transformation-to-near-normality" title="Permalink to this headline">¶</a></h3>
<p>To achieve the multinormality of the data, univariate transformation is applied to each variable individually. After then, the multinormality of transformed variables is checked again (Notice that each of the <span class="math notranslate nohighlight">\(X_p\)</span> variables after transformation is normally distributed does not imply that they jointly follow a multivariate normal distribution).</p>
<p>Assume <span class="math notranslate nohighlight">\(x &gt; 0\)</span>.</p>
<ul>
<li><p>common method <span class="math notranslate nohighlight">\(\log x\)</span></p></li>
<li><p>based on theory:</p>
<ul class="simple">
<li><p>Poisson count: <span class="math notranslate nohighlight">\(\sqrt{x}\)</span></p></li>
<li><p>Binomial proportion: <span class="math notranslate nohighlight">\(\sin ^{-1} \sqrt{x}\)</span></p></li>
</ul>
</li>
<li><p>Box-Cox transformation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  x_{i}^{[\lambda]}=\left\{\begin{array}{lll}
  \frac{x_{i}^{\lambda}-1}{\lambda} &amp; \text { for } \lambda \neq 0 &amp; , i=1, \ldots, n \\
  \log x_{i} &amp; \text { for } \lambda=0 &amp; , i=1, \ldots, n
  \end{array}\right.
  \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is unknown. It can be chosen by priori information of <span class="math notranslate nohighlight">\(\lambda\)</span>, or search among some values <span class="math notranslate nohighlight">\(3,2,1,\frac{1}{2}, \frac{1}{3}, 0\)</span> and their negatives.</p>
<p>Cons:</p>
<ul class="simple">
<li><p>Box-Cox transformation <strong>cannot</strong> guarantee that the transformed variable is close to the normal distribution.</p></li>
<li><p>Choice of <span class="math notranslate nohighlight">\(\lambda\)</span> depends on scale of <span class="math notranslate nohighlight">\(x_i\)</span>. Sol: power transform</p></li>
</ul>
</li>
<li><p>Power transform, where <span class="math notranslate nohighlight">\(G M(x)=\left(x_{1} x_{2} \cdots x_{n}\right)^{1 / n}\)</span> is the geometric mean of observations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  x_{i}^{[\lambda]}=\left\{\begin{array}{ll}
  \frac{x_{i}^{\lambda}-1}{\lambda[G M(x)]^{\lambda-1}}, &amp; \text { if } \lambda \neq 0 \quad, i=1, \ldots, n \\
  G M(x) \log x_{i}, &amp; \text { if } \lambda=0 \quad, i=1, \ldots, n
  \end{array}\right.
  \end{split}\]</div>
<p>The optimal <span class="math notranslate nohighlight">\(\lambda\)</span> minimizes sample variance after transformation.</p>
</li>
</ul>
<!-- ## Weibull Distribution --></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./12-probabilities"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="90-multivariate-notations.html" title="previous page">Multivariate Notations</a>
    <a class='right-next' id="next-link" href="91-large-sample-theory.html" title="next page">Large Sample Theory</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>