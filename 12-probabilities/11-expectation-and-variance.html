
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Expectation and Variance &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Correlation and Dependence" href="13-correlation-and-dependence.html" />
    <link rel="prev" title="Probabilities" href="00-probabilities.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-probabilities.html">
   Probabilities
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-model-selection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/41-penalized-regression.html">
     Penalized Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/12-pca-variants.html">
     PCA Variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/61-indep-component-analysis.html">
     Independent Component Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/81-density-fitting.html">
     Application to Density Fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/51-embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/53-graph-neural-networks.html">
     Graphical Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/12-probabilities/11-expectation-and-variance.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F12-probabilities/11-expectation-and-variance.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#identities">
   Identities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basics">
     Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-combinations">
     Linear Combinations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-of-nonnegative-random-variables">
     Expectation of Nonnegative Random Variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#law-of-total-expectation">
     Law of Total Expectation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#law-of-total-variance">
     Law of Total Variance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inequalities">
   Inequalities
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-s-inequality">
     Markov’s Inequality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chebyshev-s-inequality">
     Chebyshev’s Inequality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cauchy-schewarz-inequality-in-probability">
     Cauchy-Schewarz Inequality in Probability
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coin-flips">
     Coin Flips
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#incremental-update-of-mean-and-variance">
     Incremental Update of Mean and Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#miscellaneous">
     Miscellaneous
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="expectation-and-variance">
<h1>Expectation and Variance<a class="headerlink" href="#expectation-and-variance" title="Permalink to this headline">¶</a></h1>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<p>We quickly review the definitions of expectation, variance and covariance.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;&amp;&amp;\text{Population} &amp;&amp; \text{Sample} \\
&amp; \text{Mean} &amp; \mu &amp;= \sum_{i=1}^n x_i p(x_i) \text{ or } \int_{\mathcal{X}} x f(x) \mathrm{~d}x &amp;  \bar x &amp;= \frac{1}{n}\sum_i x_i  \\
&amp; \text{Variance} &amp; \sigma^2 &amp;= \operatorname{\mathbb{E}}\left[ \left( X-\mu \right)^2 \right]  &amp; s^2 &amp;= \frac{1}{n}\sum_i(x_i - \bar x)^2\\
&amp; \text{Standard deviation}  &amp; \sigma &amp;= \sqrt{\operatorname{\mathbb{E}}\left[ \left( X-\mu \right)^2 \right]}  &amp; s &amp;= \sqrt{\frac{1}{n}\sum_i(x_i - \bar x)^2} \\
&amp; \text{Covariance}  &amp; \sigma_{X,Y} &amp;= \operatorname{\mathbb{E}}\left[ (X-\mu_X)(Y-\mu_Y) \right] &amp; s_{X,Y} &amp;= \frac{1}{n}\sum_i \left[ (x_i - \bar x)(y_i - \bar y) \right]
\end{align}\end{split}\]</div>
<p>Also recall the definitions of conditional expectation and conditional variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \operatorname{\mathbb{E}}(X \mid Y=y)
 &amp;= \sum_{x} x P(X=x \mid Y=y) \\
 &amp;\text{or} \int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid y) \mathrm{~d} x \\
\operatorname{Var}\left( X \mid Y=y \right)
 &amp;= \operatorname{\mathbb{E}}\left[ (X-\mu_{X\mid Y=y})^{2} \mid Y=y \right] \\
\end{align}\end{split}\]</div>
<div class="note admonition">
<p class="admonition-title"> Notations</p>
<ul class="simple">
<li><p>The notation <span class="math notranslate nohighlight">\(X \mid Y=y\)</span> means that <span class="math notranslate nohighlight">\(Y=y\)</span> is observed. In this case, the conditional expectation (variance) is a function of the observed value <span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}(X \mid Y=y) = g(y)\)</span>, which itself is a constant.</p></li>
<li><p>The notation <span class="math notranslate nohighlight">\(X \mid Y\)</span> means that <span class="math notranslate nohighlight">\(Y\)</span> is a random variable and has not been observed yet. In this case, the conditional expectation (variance) is a function of the random variable <span class="math notranslate nohighlight">\(Y\)</span>, i.e., <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}(X \mid Y) = g(Y)\)</span>, which itself is a random variable.</p></li>
</ul>
</div>
<p>More:</p>
<ul>
<li><p>Kurtosis is a measure of peakedness of the probability density function of a random variable X, defined by</p>
<div class="math notranslate nohighlight">
\[
  \frac{\mu_{4}}{\mu_{2}^{2}}=\frac{\mathbb{E} [(X-\mu)^4]}{\mathbb{E} [(X-\mu)^{2}] ^{2}} \quad \in[0, \infty)
  \]</div>
<p>Normal distribution has kurtosis 3.</p>
</li>
<li><p>Excess Kurtosis is defined as the deviation from the normal kurtosis:</p>
<div class="math notranslate nohighlight">
\[
  \frac{\mu_{4}}{\mu_{2}^{2}}-3 \quad \in[-3, \infty)
  \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(q\)</span>-quantiles are values that partition a finite set of values into <span class="math notranslate nohighlight">\(q\)</span> subsets of (nearly) equal sizes. There are <span class="math notranslate nohighlight">\(q-1\)</span> of the <span class="math notranslate nohighlight">\(q\)</span>-quantiles, one for each integer <span class="math notranslate nohighlight">\(k\)</span> satisfying <span class="math notranslate nohighlight">\(0 &lt; k &lt;q\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-th <span class="math notranslate nohighlight">\(q\)</span>-quantile for a variable <span class="math notranslate nohighlight">\(X\)</span> if</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{P} (X &lt; x)\leq k / q
    \]</div>
</li>
<li><p>If, instead of using integers <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, the “<span class="math notranslate nohighlight">\(\tau\)</span>-quantile” is based on a real number <span class="math notranslate nohighlight">\(p\)</span> with <span class="math notranslate nohighlight">\(0 &lt; \tau &lt; 1\)</span> then <span class="math notranslate nohighlight">\(\tau\)</span> replaces <span class="math notranslate nohighlight">\(k/q\)</span> in the above formulas. This broader terminology is used when quantiles are used to parameterize continuous probability distributions.</p></li>
<li><p>computing sample <span class="math notranslate nohighlight">\(\tau\)</span>-quantile can be formulated as an optimization problem</p>
<div class="math notranslate nohighlight">
\[
    \tau\text{-th sample quantile} = \arg\min_\xi \sum_{i=1}^n \rho_\tau (x_i - \xi)
    \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \rho_\tau(u)
    &amp;= u(\tau - \mathbb{I} \left\{ u &lt; 0 \right\})\\
    &amp;= \mathbb{I} \left(u&gt;0\right) \tau\left|u\right|+\mathbb{I} \left(u\le 0\right)(1-\tau)\left|u\right| \\
    \end{aligned}\end{split}\]</div>
<p>In particular, <span class="math notranslate nohighlight">\(\tau = 0.5\)</span> we obtain median = <span class="math notranslate nohighlight">\(\arg\min_\xi \sum_{i=1}^n \left\vert x_i - \xi \right\vert\)</span>.</p>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="identities">
<h2>Identities<a class="headerlink" href="#identities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="basics">
<h3>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h3>
<p>In general, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{\mathbb{E}}\left( aX + bY \right) &amp;= a \operatorname{\mathbb{E}}\left( X \right) + b \operatorname{\mathbb{E}}\left( Y \right) \\
\operatorname{Var}\left( aX + bY \right) &amp;= a^2\operatorname{Var}\left( X \right) + b^2\operatorname{Var}\left( Y \right) + 2ab\operatorname{Cov}\left( X, Y \right) \\
\operatorname{Var}\left( X \right) &amp;= \operatorname{\mathbb{E}}\left( X^2 \right) - \left[ \operatorname{\mathbb{E}}\left( X \right) \right]^2\\
\operatorname{\mathbb{E}}\left( X^2 \right) &amp;= \mu^2 + \sigma^2 \\
\operatorname{Cov}\left( X, X \right) &amp;= \operatorname{Var}\left( X \right) \\
\operatorname{Cov}\left( X,Y \right) &amp;= \operatorname{\mathbb{E}}\left( XY \right) - \operatorname{\mathbb{E}}\left( X \right)\operatorname{\mathbb{E}}\left( Y \right) \\
\operatorname{Cov}\left( X, a \right) &amp;= 0 \\
\operatorname{Cov}\left( X, Y+Z \right) &amp;= \operatorname{Cov}\left( X, Y \right) + \operatorname{Cov}\left( X, Z \right) \\
\operatorname{Cov}\left( aX, bY \right) &amp;= ab \operatorname{Cov}\left( X, Y \right)
\end{align}\end{split}\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Be careful about the notations <span class="math notranslate nohighlight">\(\sigma_X ^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_{X,X}\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_X^2 = \operatorname{Var}\left( X \right) = \operatorname{Cov}\left( X, X \right) = \sigma_{X,X}
\]</div>
</div>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{\mathbb{E}}\left( XY \right) &amp;= \operatorname{\mathbb{E}}\left( X \right)\operatorname{\mathbb{E}}\left( Y \right) \\
\operatorname{Cov}\left( X, Y \right) &amp;= 0 \\
\operatorname{Var}\left( aX + bY \right) &amp;= a^2\operatorname{Var}\left( X \right) + b^2\operatorname{Var}\left( Y \right) \\
\end{align}\end{split}\]</div>
</div>
<div class="section" id="linear-combinations">
<h3>Linear Combinations<a class="headerlink" href="#linear-combinations" title="Permalink to this headline">¶</a></h3>
<p>For <span class="math notranslate nohighlight">\(n\)</span> random variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>, consider a linear combination <span class="math notranslate nohighlight">\(\sum_i^n a_i X_i\)</span>. Though we have no information about the dependence between <span class="math notranslate nohighlight">\(X_i\)</span>’s, the expectation of the sum equals to the sum of the expectations</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}\left( \sum_i^n a_i X_i \right)
 = \sum_i^n \operatorname{\mathbb{E}}\left(a_i X_i \right)
 = \sum_i^n a_i\operatorname{\mathbb{E}}\left( X_i \right)
\]</div>
<p>In this sense, expectation is a linear operator.</p>
<p>In particular, for independently and identically distributed <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> with common mean <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X_i \right)=\mu\)</span>, the expectation of the average value is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{\mathbb{E}}\left( \bar X \right) &amp;= \operatorname{\mathbb{E}}\left( \frac{1}{n}\sum_{i} X_i \right)\\
&amp;= \frac{1}{n}\sum_i \operatorname{\mathbb{E}}\left( X_i \right)\\
&amp;= \mu \\
\end{align}\end{split}\]</div>
<p>In general, the variance of a sum of a linear combination is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{Var}\left(\sum_{i=1}^{n} a_{i} X_{i}\right)
&amp;= \operatorname{Cov}\left( \sum_i a_i X_i, \sum_i a_i X_i \right)\\
&amp;=\sum_{i, j=1}^{n}   \operatorname{Cov}\left(a_{i}X_{i}, a_{j}X_{j}\right) \\
&amp;=\sum_{i=1}^{n}  \operatorname{Var}\left(a_{i}X_{i}\right)+\sum_{i \neq j}   \operatorname{Cov}\left(a_{i}X_{i}, a_{j}X_{j}\right) \\
&amp;=\sum_{i=1}^{n} a_{i}^{2} \operatorname{Var}\left(X_{i}\right)+2 \sum_{1 \leq i&lt;j \leq n} a_{i} a_{j} \operatorname{Cov}\left(X_{i}, X_{j}\right)
\end{aligned}
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title"> Tip</p>
<p>One can imagine that there is a <span class="math notranslate nohighlight">\(n \times n\)</span> covariance table with the <span class="math notranslate nohighlight">\(i,j\)</span>-th entry being <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( a_i X_i, a_j X_j \right)\)</span>, and the required variance is the sum of all the entries, which consists of</p>
<ul class="simple">
<li><p>the sum of the diagonal entries as <span class="math notranslate nohighlight">\(\sum_i\operatorname{Var}\left(a_{i}X_{i}\right)\)</span></p></li>
<li><p>the sum of the off-diagonal entries as <span class="math notranslate nohighlight">\(\sum_{i\ne j}\operatorname{Cov}\left(a_{i}X_{i}, a_{j}X_{j}\right)\)</span></p></li>
</ul>
</div>
<p>In particular, the variance of the average value of the IID sum is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \bar X \right) &amp;= \operatorname{Var}\left( \frac{1}{n}\sum_{i} X_i \right)\\
&amp;= \frac{1}{n^2}\sum_i \operatorname{Var}\left( X_i \right)\\
&amp;= \frac{1}{n} \sigma^2 \\
\end{align}\end{split}\]</div>
</div>
<div class="section" id="expectation-of-nonnegative-random-variables">
<h3>Expectation of Nonnegative Random Variables<a class="headerlink" href="#expectation-of-nonnegative-random-variables" title="Permalink to this headline">¶</a></h3>
<p>For nonnegative random variables, the expectation can be computed from the complementary cumulative distribution function <span class="math notranslate nohighlight">\(1 - F(x) = \operatorname{\mathbb{P}}\left( X &gt; x \right)\)</span>.</p>
<ul class="simple">
<li><p>discrete case</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}\left( X \right)=\sum_{n=0}^{\infty} \operatorname{\mathbb{P}}\left( X&gt;n \right)
\]</div>
<ul class="simple">
<li><p>continuous case</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}\left( X \right) = \int_{0}^{\infty} \operatorname{\mathbb{P}}(X \geq x) \mathrm{~d} x
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof by changing the order of summation/integral</em></p>
<ul>
<li><p>discrete case</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \sum_{n=0}^{\infty} \operatorname{\mathbb{P}}\left( X&gt;n \right)
  &amp;= \sum_{n=0}^{\infty} \sum_{k=n+1}^{\infty} \operatorname{\mathbb{P}}\left( X=k \right) \\
  &amp;= \sum_{k=1}^{\infty} \sum_{n=1}^k \operatorname{\mathbb{P}}\left( X=k \right) \\
  &amp;= \sum_{k=1}^{\infty} k \operatorname{\mathbb{P}}\left( X=k \right) \\
  &amp;= \sum_{k=0}^{\infty} k \operatorname{\mathbb{P}}\left( X=k \right) \\
  &amp;= \operatorname{\mathbb{E}}\left( X \right)
  \end{align}\end{split}\]</div>
</li>
<li><p>continuous case</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \int_{0}^{\infty} \operatorname{\mathbb{P}}(X \ge x) \mathrm{~d} x
  &amp;=\int_{0}^{\infty} \int_{x}^{\infty} f_{X}(y) \mathrm{~d} y \mathrm{~d} x \\
  &amp;=\int_{0}^{\infty} \int_{0}^{y} f_{X}(y) \mathrm{~d} x \mathrm{~d} y \\
  &amp;=\int_{0}^{\infty} f_{X}(y) \int_{0}^{y} 1 \mathrm{~d} x \mathrm{~d} y \\
  &amp;=\int_{0}^{\infty} y f_{X}(y) \mathrm{~d} y \\
  &amp;=\operatorname{\mathbb{E}}\left( X \right)
  \end{aligned}
  \end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</li>
</ul>
</div>
</div>
<div class="section" id="law-of-total-expectation">
<h3>Law of Total Expectation<a class="headerlink" href="#law-of-total-expectation" title="Permalink to this headline">¶</a></h3>
<p>Aka law of iterated expectations, tower rule, smoothing theorem.</p>
<p>Given the conditional expectation <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid Y \right)\)</span>, the law of total expectation states that we can obtained the unconditional expectation <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \right)\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}\left( X \right)=\operatorname{\mathbb{E}}\left[ \operatorname{\mathbb{E}}(X \mid Y) \right]
\]</div>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<p>The inside expectation is taken w.r.t. <span class="math notranslate nohighlight">\(X\)</span> and the outside expectation is taken w.r.t. <span class="math notranslate nohighlight">\(Y\)</span>, since the conditional expectation <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left(X \mid Y \right)\)</span> is a function <span class="math notranslate nohighlight">\(g(Y)\)</span> that depends on the random variables <span class="math notranslate nohighlight">\(Y\)</span>. To emphasize this we can write</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}_X\left( X \right)=\operatorname{\mathbb{E}}_Y\left[ \operatorname{\mathbb{E}}_X(X \mid Y) \right]
\]</div>
</div>
<p>In general, we can partition the sample space into finite or countably infinite sets <span class="math notranslate nohighlight">\(A_i\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}(X)=\sum_{i} \operatorname{\mathbb{E}}\left(X \mid A_{i}\right) \operatorname{\mathbb{P}}\left(A_{i}\right)
\]</div>
<p>For instance, we can compute the expectation as a weighted sum of the expectation of the positive part and the expectation of the negative part on respective probabilities.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}(X)=\operatorname{\mathbb{E}}\left(X \mid X&gt;0\right) \operatorname{\mathbb{P}}\left(X&gt;0\right) + \operatorname{\mathbb{E}}\left(X \mid X&lt;0\right) \operatorname{\mathbb{P}}\left(X&lt;0\right)
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>By definition</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{\mathbb{E}}\left( \operatorname{\mathbb{E}}\left( X \mid Y \right) \right) &amp;=\operatorname{\mathbb{E}}\left[\sum_{x} x \cdot \operatorname{\mathbb{P}}(X=x \mid Y)\right] \\
&amp;=\sum_{y}\left[\sum_{x} x \cdot \operatorname{\mathbb{P}}(X=x \mid Y=y)\right] \cdot \operatorname{\mathbb{P}}(Y=y) \\
&amp;=\sum_{y} \sum_{x} x \cdot \operatorname{\mathbb{P}}(X=x, Y=y) \\
&amp;=\sum_{x} x \sum_{y} \operatorname{\mathbb{P}}(X=x, Y=y) \\
&amp;=\sum_{x} x \cdot \operatorname{\mathbb{P}}(X=x) \\
&amp;=\operatorname{\mathbb{E}}(X)
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
<div class="section" id="law-of-total-variance">
<h3>Law of Total Variance<a class="headerlink" href="#law-of-total-variance" title="Permalink to this headline">¶</a></h3>
<p>Aka law of iterated variances, variance decomposition formula, Eve’s law.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}(X)=\operatorname{\mathbb{E}}[\operatorname{Var}(X \mid Y)]+\operatorname{Var}(\operatorname{\mathbb{E}}[X \mid Y])
\]</div>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<p>Here both <span class="math notranslate nohighlight">\(\operatorname{Var}\left( X \mid Y \right)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid Y \right)\)</span> are random. The outside expectation and variance are taken w.r.t. the conditioned variable, <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</div>
<p>The first and the second term can be interpreted as the unexplained and the explained components of the variance of <span class="math notranslate nohighlight">\(X\)</span> by knowing <span class="math notranslate nohighlight">\(Y\)</span>. Imagine that there is a deterministic relation <span class="math notranslate nohighlight">\(X=f(Y)\)</span>, then <span class="math notranslate nohighlight">\(\operatorname{Var}\left( X \mid Y \right) = 0\)</span> so that the first term is 0, and the second term becomes <span class="math notranslate nohighlight">\(\operatorname{Var}\left(  f(Y) \right) = \operatorname{Var}\left( X \right)\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Note that the relation <span class="math notranslate nohighlight">\(\operatorname{Var}\left( X \right) = \operatorname{\mathbb{E}}\left( X^2 \right) - \left[ \operatorname{\mathbb{E}}\left( X \right) \right]^2\)</span> holds in a similar fashion when conditioning on <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}\left( X \mid Y \right) = \operatorname{\mathbb{E}}\left( X^2 \mid Y \right) - \left[ \operatorname{\mathbb{E}}\left( X \mid Y\right) \right]^2
\]</div>
<p>By the law of total expectation, we can compute <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X^2 \right)\)</span> by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\operatorname{\mathbb{E}}\left( X^2 \right)
&amp;= \operatorname{\mathbb{E}}\left[ \operatorname{\mathbb{E}}\left( X^2 \mid Y \right) \right] \\
&amp;=  \operatorname{\mathbb{E}}\left\{ \operatorname{Var}\left( X \mid Y \right) + \left[ \operatorname{\mathbb{E}}\left( X \mid Y\right) \right]^2  \right\}
\end{align}
\end{split}\]</div>
<p>and compute <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \right)\)</span> by <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left[ \operatorname{\mathbb{E}}\left( X\mid Y \right) \right]\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( X \right) &amp;= \operatorname{\mathbb{E}}\left( X^2 \right) - \left[ \operatorname{\mathbb{E}}\left( X \right) \right]^2\\
&amp;=  \operatorname{\mathbb{E}}\left\{ \operatorname{Var}\left( X \mid Y \right) + \left[ \operatorname{\mathbb{E}}\left( X \mid Y\right) \right]^2  \right\} - \left\{ \operatorname{\mathbb{E}}\left[ \operatorname{\mathbb{E}}\left( X\mid Y \right) \right] \right\}^2\\
&amp;= \operatorname{\mathbb{E}}[\operatorname{Var}(X \mid Y)]+\operatorname{Var}(\operatorname{\mathbb{E}}[X \mid Y])
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>From above we see that the identity that holds for expectation</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{E}}(X)=\sum_{i} \operatorname{\mathbb{E}}\left(X \mid A_{i}\right) \operatorname{\mathbb{P}}\left(A_{i}\right)
\]</div>
<p>does <strong>not</strong> hold for variance</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}(X) \ne \sum_{i} \operatorname{Var}\left(X \mid A_{i}\right) \operatorname{\mathbb{P}}\left(A_{i}\right)
\]</div>
<p>unless <span class="math notranslate nohighlight">\(\operatorname{Var}(\operatorname{\mathbb{E}}[X \mid A]) = 0\)</span>, which implies that <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A \right) = \text{constant}\)</span>, i.e., <span class="math notranslate nohighlight">\(X\)</span> and the partitioning <span class="math notranslate nohighlight">\(A_i\)</span> are independent.</p>
</div>
</div>
</div>
<div class="section" id="inequalities">
<h2>Inequalities<a class="headerlink" href="#inequalities" title="Permalink to this headline">¶</a></h2>
<p>There are important inequalities that connect probability, expectation and variance. For more inequalities that also relates the sum/average of multiple variables, see <a class="reference internal" href="91-large-sample-theory.html#large-sample-inequalities"><span class="std std-ref">here</span></a>.</p>
<div class="section" id="markov-s-inequality">
<h3>Markov’s Inequality<a class="headerlink" href="#markov-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>Markov’s inequality upper bounds right-tail probability <span class="math notranslate nohighlight">\(\operatorname{\mathbb{P}}\left( X\ge \lambda \right)\)</span> by <span class="math notranslate nohighlight">\(\frac{\mu}{\lambda}\)</span>. For a <strong>nonnegative</strong> random variable <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{P}}(X \geq \lambda) \leq \frac{\mu}{\lambda}
\]</div>
<p>A more useful form is to substitute <span class="math notranslate nohighlight">\(\lambda \leftarrow \lambda \mu\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{P}}(X \geq \lambda \mu) \leq \frac{1}{\lambda}
\]</div>
<p>That is, the probability of exceeding expectation by more than a factor  of <span class="math notranslate nohighlight">\(\lambda\)</span> is at most <span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<ul>
<li><p>By the law of total expectation, and since <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}(X \mid X&lt;\lambda)\ge 0\)</span> and <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}(X \mid X \geq \lambda)\ge \lambda\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \operatorname{\mathbb{E}}(X) &amp; =  \operatorname{\mathbb{E}}(X \mid X&lt;\lambda) \cdot \operatorname{\mathbb{P}}(X&lt;\lambda) +  \operatorname{\mathbb{E}}(X \mid X \geq \lambda) \cdot \operatorname{\mathbb{P}}(X \geq \lambda)\\
    &amp; \ge 0 \cdot \operatorname{\mathbb{P}}(X&lt;\lambda) +\operatorname{\mathbb{E}}(X \mid X \geq \lambda) \cdot \operatorname{\mathbb{P}}(X \geq \lambda) \\
    &amp; \geq \lambda \cdot \operatorname{\mathbb{P}}(X \geq \lambda)
    \end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</li>
<li><p>By the definition of expectation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \operatorname{\mathbb{E}}(X) &amp;= \int_{0}^{\lambda} x f(x) \mathrm{~d} x+\int_{\lambda}^{\infty} x f(x) \mathrm{~d} x \\
    &amp; \geq \int_{\lambda}^{\infty} x f(x) \mathrm{~d} x \\
    &amp; \geq \int_{\lambda}^{\infty} \lambda f(x) \mathrm{~d} x \\
    &amp; =\lambda \int_{\lambda}^{\infty} f(x) \mathrm{~d} x \\
    &amp;=\lambda \operatorname{\mathbb{P}}(X \geq \lambda)
    \end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</li>
</ul>
</div>
</div>
<div class="section" id="chebyshev-s-inequality">
<h3>Chebyshev’s Inequality<a class="headerlink" href="#chebyshev-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>The probability of deviating from <span class="math notranslate nohighlight">\(\mu\)</span> by more than <span class="math notranslate nohighlight">\(\lambda \sigma\)</span> is at most <span class="math notranslate nohighlight">\(\frac{1}{\lambda^2}\)</span>. For any <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{P}}(|X-\mu| \geq \lambda \sigma) \leq \frac{1}{\lambda^{2}}
\]</div>
<p>For instance, taking <span class="math notranslate nohighlight">\(\lambda=\sqrt{2}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\operatorname{\mathbb{P}}\left( \mu - \sqrt{2}\sigma \le X \le \mu - \sqrt{2}\sigma \right) &gt; \frac{1}{2}
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof by the law of total expectation</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sigma^{2} &amp;=\operatorname{\mathbb{E}}\left[(X-\mu)^{2}\right] \\
&amp;=  \operatorname{\mathbb{E}}\left[(X-\mu)^{2} \mid \lambda \sigma \leq| X-\mu \mid\right] \cdot \operatorname{\mathbb{P}}\left( \lambda \sigma \leq|X-\mu| \right)\\
&amp; \, +  \operatorname{\mathbb{E}}\left[(X-\mu)^{2} \mid \lambda \sigma&gt;| X-\mu \mid\right] \cdot \operatorname{\mathbb{P}}\left( \lambda \sigma&gt;|X-\mu| \right) \\
&amp; \geq(\lambda \sigma)^{2} \operatorname{\mathbb{P}}\left( \lambda \sigma \leq|X-\mu| \right)+0 \cdot \operatorname{\mathbb{P}}\left( \lambda \sigma&gt;|X-\mu| \right) \\
&amp;=\lambda^{2} \sigma^{2} \operatorname{\mathbb{P}}[\lambda \sigma \leq|X-\mu|]
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
<p>Some variation:</p>
<ul>
<li><p>Substituting <span class="math notranslate nohighlight">\(\lambda \leftarrow \frac{\epsilon\mu}{\sigma}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{P}}(|X-\mu| \geq \epsilon \mu ) \leq \frac{\sigma ^2}{(\epsilon \mu)^{2}}
  \]</div>
<p>which can be used to construct a <span class="math notranslate nohighlight">\((\epsilon, \delta)\)</span> estimator.</p>
</li>
<li><p>Substituting <span class="math notranslate nohighlight">\(\lambda \leftarrow \frac{\lambda}{\sigma}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{P}}(|X-\mu| \geq \lambda ) \leq \frac{\sigma ^2}{\lambda^{2}}
  \]</div>
<p>In general,</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{P}}(|\mathrm{X}-\mu|&gt;\lambda) \leq \frac{\mathbb{E}\left[(X-\mu)^{p}\right]}{\lambda^{p}}
  \]</div>
</li>
</ul>
</div>
<div class="section" id="cauchy-schewarz-inequality-in-probability">
<h3>Cauchy-Schewarz Inequality in Probability<a class="headerlink" href="#cauchy-schewarz-inequality-in-probability" title="Permalink to this headline">¶</a></h3>
<p>For two random variables <span class="math notranslate nohighlight">\(X, Y\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\left[ \operatorname{Cov}\left( X,Y \right) \right]^2 \le \operatorname{Var}\left( X \right) \operatorname{Var}\left( Y \right)
\]</div>
<p>The equality holds iff there is a deterministic linear relation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(Y = aX + b\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Recall the Cauchy-Schewarz inequality for vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}, \boldsymbol{v}\)</span> of an innver product space,</p>
<div class="math notranslate nohighlight">
\[
|\langle\mathbf{u}, \mathbf{v}\rangle|^{2} \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle
\]</div>
<p>Define an inner product on the set of random variables using the expectation of their product</p>
<div class="math notranslate nohighlight">
\[
\langle X, Y\rangle:=\operatorname{\mathbb{E}}(X Y)
\]</div>
<p>Then the Cauchy-Schewrz inequality becomes</p>
<div class="math notranslate nohighlight">
\[
|\operatorname{\mathbb{E}}(X Y)|^{2} \leq \operatorname{\mathbb{E}}\left(X^{2}\right) \operatorname{\mathbb{E}}\left(Y^{2}\right)
\]</div>
<p>Substituting <span class="math notranslate nohighlight">\(X\)</span> by <span class="math notranslate nohighlight">\(X-\mu_X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> by <span class="math notranslate nohighlight">\(Y-\mu_Y\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
|\operatorname{Cov}(X, Y)|^{2} &amp;=\left\vert \operatorname{\mathbb{E}}\left[ (X-\mu_X)(Y-\mu_Y) \right] \right\vert^{2} \\
&amp;\le \operatorname{\mathbb{E}}\left[ (X-\mu_X)^{2} \right] \operatorname{\mathbb{E}}\left[ (Y-\mu_Y)^2\right]\\
&amp;=\operatorname{Var}(X) \operatorname{Var}(Y)
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<div class="section" id="coin-flips">
<h3>Coin Flips<a class="headerlink" href="#coin-flips" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Count trials: What is the expected number of coin flips to get two heads in a row?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution 1: Law of Total Expectation</em></p>
<p>Denote the required number of flips by <span class="math notranslate nohighlight">\(X\)</span>. We can partition the sample space into <strong>three</strong> parts:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A_T\)</span>: the first flip is a tail</p></li>
<li><p><span class="math notranslate nohighlight">\(A_{HT}\)</span>: the first two flips are head, tail</p></li>
<li><p><span class="math notranslate nohighlight">\(A_{HH}\)</span>: the first two flips are head, head</p></li>
</ul>
<p>It’s easy to see</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{P}}\left( A_T \right) = \frac{1}{2}, \operatorname{\mathbb{P}}\left( A_{HT} \right) = \operatorname{\mathbb{P}}\left( A_{HH} \right) = \frac{1}{4}
  \]</div>
<p>But what are <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A_T \right), \operatorname{\mathbb{E}}\left( X \mid A_{HT} \right), \operatorname{\mathbb{E}}\left( X \mid A_{HH} \right)\)</span>?</p>
<ul class="simple">
<li><p>If the first flip is T, then we start over, and waste 1 flip</p></li>
<li><p>If the first two flips are HT, then we start over, and waste 2 flips</p></li>
<li><p>If the first two flips are HH, then done! We use 2 flips</p></li>
</ul>
<p>As a result, we have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A_T \right) = \operatorname{\mathbb{E}}\left( X \right) + 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A_{HT} \right) = \operatorname{\mathbb{E}}\left( X \right)+ 2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A_{HH} \right) = 2\)</span></p></li>
</ul>
<p>Then by the law of total expectation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align}
  \operatorname{\mathbb{E}}\left( X \right)
  &amp;= \operatorname{\mathbb{E}}\left( X \mid A_T \right) \operatorname{\mathbb{P}}\left( A_T \right)
  +  \operatorname{\mathbb{E}}\left( X \mid A_{HT} \right) \operatorname{\mathbb{P}}\left( A_{HT} \right)
  +  \operatorname{\mathbb{E}}\left( X \mid A_{HH} \right) \operatorname{\mathbb{P}}\left( A_{HH} \right) \\
  &amp;= \left[ \operatorname{\mathbb{E}}\left( X \right)
   + 1 \right]\cdot \frac{1}{2} + \left[ \operatorname{\mathbb{E}}\left( X \right) + 2\right] \cdot \frac{1}{4}
   + 2 \cdot \frac{1}{4} \\
  \end{align}
  \end{split}\]</div>
<p>Solving the equation gives <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \right) = 6\)</span></p>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<p>One may also partition the sample space to two parts <span class="math notranslate nohighlight">\({A_H}\)</span> and <span class="math notranslate nohighlight">\(A_T\)</span>, but to compute <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X \mid A_H \right)\)</span>, it requires to partition <span class="math notranslate nohighlight">\(A_H\)</span> into <span class="math notranslate nohighlight">\(A_{HT}\)</span> and <span class="math notranslate nohighlight">\(A_{HH}\)</span>, and then use the law of total expectation again, which is complicated and easy to make mistakes. So it would be better to partition <span class="math notranslate nohighlight">\(A\)</span> to three parts at the beginning.</p>
</div>
<p>In general, what is the expected number of coin flips to get <span class="math notranslate nohighlight">\(n\)</span> heads in a row? In fact, we just need to continue to partition <span class="math notranslate nohighlight">\(A_{HH}\)</span> into <span class="math notranslate nohighlight">\(A_{HHT}\)</span> and <span class="math notranslate nohighlight">\(A_{HHH}\)</span>, and so on. By the law of total expectation the equation becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \operatorname{\mathbb{E}}\left( X_n \right)
  = \left[ \operatorname{\mathbb{E}}\left( X_n \right) + 1 \right]\cdot \frac{1}{2}
  + \left[ \operatorname{\mathbb{E}}\left( X_n \right) + 2\right] \cdot \frac{1}{4}
   + \ldots
   + \left[ \operatorname{\mathbb{E}}\left( X_n \right) + n\right] \cdot \frac{1}{2^n}
   + n \cdot \frac{1}{2^n} \\
  \end{split}\]</div>
<p>The solution is</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{E}}\left( X_n \right) = 2 \left( 2^n-1 \right)
  \]</div>
</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution 2: Recurrence Relation</em></p>
<p>One can also derive the solution from a recurrence relation between <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X_n \right)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X_{n-1} \right)\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(Y_{n} = X_n - X_{n-1}\)</span> be the number of additional flips required to get <span class="math notranslate nohighlight">\(n\)</span> heads in a row, given that we already got <span class="math notranslate nohighlight">\(n-1\)</span> heads in a row. Then by the law of total expectation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align}
  \operatorname{\mathbb{E}}\left( Y_{n} \right)
  &amp;= \operatorname{\mathbb{E}}\left( Y_n \mid \text{the $n$-th flip is H} \right) \operatorname{\mathbb{P}}\left( \text{the $n$-th flip is H}  \right) \\
    &amp;\ + \operatorname{\mathbb{E}}\left( Y_n \mid \text{the $n$-th flip is T} \right) \operatorname{\mathbb{P}}\left( \text{the $n$-th flip is T}  \right) \\
  &amp;= 1 \cdot \frac{1}{2} + \left[ 1 + \operatorname{\mathbb{E}}\left( X_n \right) \right] \cdot \frac{1}{2}
   \end{align}
  \end{split}\]</div>
<p>Hence, we have the recurrence relation</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{\mathbb{E}}\left( X_n \right) = 2 \operatorname{\mathbb{E}}\left( X_{n-1} \right) + 2
  \]</div>
<p>Let <span class="math notranslate nohighlight">\(f(n) = \operatorname{\mathbb{E}}\left( X_n\right) + 2\)</span> then we have <span class="math notranslate nohighlight">\(f(n) = 2f(n-1)\)</span>. Since <span class="math notranslate nohighlight">\(f(1) = \operatorname{\mathbb{E}}\left( X_1 \right)+2 = 4\)</span>, we have <span class="math notranslate nohighlight">\(f(n) = 2^{n+1}\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[\operatorname{\mathbb{E}}\left( X_n \right) = 2^{n+1}-2\]</div>
</div>
</li>
<li><p><strong>Count rows</strong>: what is the expected number of times to see <span class="math notranslate nohighlight">\(k\)</span> heads in a row, i.e., HH…HH, in <span class="math notranslate nohighlight">\(n\)</span> flips of a coin?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>In <span class="math notranslate nohighlight">\(n\)</span> flips of a coin, there are <span class="math notranslate nohighlight">\(n-k+1\)</span> places where the string HH…HH can start to appear, each with a (non-independent) probability <span class="math notranslate nohighlight">\(\frac{1}{2^k} \)</span> of happening. Let <span class="math notranslate nohighlight">\(X\)</span> be the number of times to see the string HH…HH, and <span class="math notranslate nohighlight">\(X_i\)</span> be the indicator variable that is <span class="math notranslate nohighlight">\(1\)</span> if the string starts to appear at the <span class="math notranslate nohighlight">\(i\)</span>-th flip, then</p>
<div class="math notranslate nohighlight">
\[
  X = \sum_{i=1}^{n-k+1} X_i
  \]</div>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{\mathbb{E}}\left( X \right) &amp;= \operatorname{\mathbb{E}}\left( \sum_{i=1}^{n-k+1} X_i \right)\\
  &amp;= \sum_{i=1}^{n-k+1} \operatorname{\mathbb{E}}\left( X_i \right)\\
  &amp;= \frac{n-k+1}{2^k} \\
  \end{align}\end{split}\]</div>
<p>The first second last line holds even if <span class="math notranslate nohighlight">\(X_i\)</span>’s are not independent.</p>
</div>
</li>
<li><p><strong>Count runs</strong>: a coin with a probability <span class="math notranslate nohighlight">\(p\)</span> to get a head is flipped <span class="math notranslate nohighlight">\(n\)</span> times. A “run” is a maximal sequence of consecutive flips that are all the same. For instance, HTHHHTTH has five runs and <span class="math notranslate nohighlight">\(n=8\)</span>. What is the expected number of runs?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Let <span class="math notranslate nohighlight">\(X_i\)</span> be the indicator for the event that a run starts at the <span class="math notranslate nohighlight">\(i-th\)</span> toss. Let <span class="math notranslate nohighlight">\(X = \sum_i X_i\)</span> be the total number of runs. It is easy to see <span class="math notranslate nohighlight">\(\operatorname{\mathbb{E}}\left( X_1 \right) = 1\)</span>. For <span class="math notranslate nohighlight">\(i&gt;1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \operatorname{\mathbb{E}}\left(X_{i}\right)=&amp; \operatorname{\mathbb{P}}\left(X_{i}=1\right) \\
  =&amp; \operatorname{\mathbb{P}}\left(i \text { -th toss is } \mathrm{H} \mid(i-1) \text { -th toss is } \mathrm{T}\right) \times \operatorname{\mathbb{P}}\left((i-1) \text { -th toss is } \mathrm{T}\right) \\
  &amp;+\operatorname{\mathbb{P}}\left(i \text { -th toss is } \mathrm{T} \mid(i-1)\text {-th} \text { toss is } \mathrm{H}\right) \times \operatorname{\mathbb{P}}\left((i-1)\text {-th } \text { toss is } \mathrm{H}\right) \\
  =&amp; p(1-p)+(1-p) p \\
  =&amp; 2 p(1-p)
  \end{aligned}
  \end{split}\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \operatorname{\mathbb{E}}(X) &amp;=\operatorname{\mathbb{E}}\left(X_{1}+X_{2}+\cdots+X_{n}\right) \\
  &amp;=\operatorname{\mathbb{E}}\left(X_{1}\right)+\operatorname{\mathbb{E}}\left(X_{2}\right)+\cdots+\operatorname{\mathbb{E}}\left(X_{n}\right) \\
  &amp;=\operatorname{\mathbb{E}}\left(X_{1}\right)+\left[\operatorname{\mathbb{E}}\left(X_{2}\right)+\cdots+\operatorname{\mathbb{E}}\left(X_{n}\right)\right] \\
  &amp;=1+(n-1) \times 2 p(1-p) \\
  &amp;=1+2(n-1) p(1-p)
  \end{aligned}
  \end{split}\]</div>
</div>
</li>
</ul>
</div>
<div class="section" id="incremental-update-of-mean-and-variance">
<h3>Incremental Update of Mean and Variance<a class="headerlink" href="#incremental-update-of-mean-and-variance" title="Permalink to this headline">¶</a></h3>
<p><em>Suppose you have <span class="math notranslate nohighlight">\(n\)</span> observations <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span>. Now a new value <span class="math notranslate nohighlight">\(x_{n+1}\)</span> is observed. Write down recurrence functions for <span class="math notranslate nohighlight">\(\bar{x}_n\)</span> and <span class="math notranslate nohighlight">\(s^2_n\)</span>, and use them to obtain <span class="math notranslate nohighlight">\(\bar{x}_{n+1}\)</span> and <span class="math notranslate nohighlight">\(s^2_{n+1}\)</span>.</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>To update mean,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\bar{x}_{n+1}
&amp;= \frac{\sum_{i=1}^{n+1} x_i}{n+1} \\
&amp;= \frac{x_{n+1} + \sum_{i=1}^n x_i}{n+1} \\
&amp;= \frac{x_{n+1} + n\bar{x}_n}{n+1} \\
&amp;= \bar{x}_n + \frac{1}{n+1}(x_{n+1} - \bar{x}_n) \quad (*)
\end{align}\end{split}\]</div>
<p>The last line is to avoid computing a large number <span class="math notranslate nohighlight">\(n \bar{x}_n\)</span>.</p>
<p>The second last line implies that the new sample mean <span class="math notranslate nohighlight">\(\bar{x}_{n+1}\)</span> is a weighted average of the current sample mean <span class="math notranslate nohighlight">\(\bar{x}_{n+1}\)</span> and the new observed value <span class="math notranslate nohighlight">\(x_{n+1}\)</span>.</p>
<p>To update variance, we first use the above method to obtain <span class="math notranslate nohighlight">\(\bar{x}_{n+1}\)</span>, and let <span class="math notranslate nohighlight">\(S_n = ns_{n}^2\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
S_{n+1}
&amp;=  \sum_{i=1}^{n+1}x_i^2  - (n+1) \bar{x}_{n+1}^2  \\
&amp;=  \sum_{i=1}^{n}x_i^2 - n\bar{x}_n^2 + n\bar{x}_n^2 + x_{n+1}^2  - (n+1) \bar{x}_{n+1}^2  \\
&amp;=  S_{n} + n\bar{x}_n^2 + x_{n+1}^2  - (n+1) \bar{x}_{n+1}^2  \\
&amp;=  S_{n}  + x_{n+1}^2  + (n+1)(\bar{x}_n - \bar{x}_{n+1})(\bar{x}_n + \bar{x}_{n+1}) - \bar{x}_{n}^2  \\
&amp;=  S_{n}  + x_{n+1}^2  + (\bar{x}_n - x_{n+1} )(\bar{x}_n + \bar{x}_{n+1}) - \bar{x}_{n}^2 \quad \text{by} \ (*) \\
&amp;=  S_{n}  + x_{n+1}^2  + \bar{x}_n \bar{x}_{n+1} - x_{n+1} (\bar{x}_n + \bar{x}_{n+1}) \\
&amp;=  S_{n}  + (x_{n+1}  - \bar{x}_n)(x_{n+1}  - \bar{x}_{n+1})
\end{align}\end{split}\]</div>
<p>Finally <span class="math notranslate nohighlight">\(s_{n+1}^2 = \frac{1}{n+1} S_{n+1}\)</span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The substitution <span class="math notranslate nohighlight">\(S_n = ns_n^2\)</span> avoids the computation that involves <span class="math notranslate nohighlight">\(\frac{1}{n} \)</span> and <span class="math notranslate nohighlight">\(\frac{1}{n+1} \)</span>. And the update equation of the mean is also quite useful.</p>
</div>
</div>
</div>
<div class="section" id="miscellaneous">
<span id="exp-var-ex"></span><h3>Miscellaneous<a class="headerlink" href="#miscellaneous" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Two groups of data. In group one, sample standard deviation is <span class="math notranslate nohighlight">\(s_1\)</span>, in group two it is <span class="math notranslate nohighlight">\(s_2\)</span>. After merging them, it is <span class="math notranslate nohighlight">\(s_3\)</span>. Do we always have <span class="math notranslate nohighlight">\(s_3 &gt; \max(s_1, s_2)\)</span>?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Let <span class="math notranslate nohighlight">\(\lambda = \frac{n_1}{n_1 + n_2} \in (0,1)\)</span>, then <span class="math notranslate nohighlight">\(\bar{x}_3 = \lambda \bar{x}_1 + (1-\lambda)\bar{x}_2\)</span>. WLOG assume <span class="math notranslate nohighlight">\(\bar{x}_1 - \bar{x}_2 = d \ge 0\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    s^2 _1 &amp;= \frac{TSS_1}{n_1} \\
    s^2 _2 &amp;= \frac{TSS_2}{n_2} \\
    s^2 _3 &amp;= \frac{TSS_3}{n_3}\\
    s^2 _3 &amp;= \frac{TSS_1 + TTS_2 + n_1 (\bar{x}_1 - \bar{x}_3)^2 + n_2 (\bar{x}_3 - \bar{x}_2)^2 }{n_1 + n_2}\\
    &amp;= \lambda s^2 _1 + (1-\lambda) s^2 _2  + \lambda ((1-\lambda)d )^2 + (1-\lambda) (\lambda d)^2    \\
    &amp;= \underbrace{\lambda s^2 _1 + (1-\lambda) s^2 _2}_{a}  + \underbrace{\lambda(1-\lambda)d^2}_{b}   \\
    \end{aligned}\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = \frac{n_1}{n_1 + n_2} \in (0,1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_1 = \bar{x}_1 - \bar{x}_3 = \bar{x}_1 - (\lambda \bar{x}_1 + (1-\lambda)\bar{x}_2) = (1-\lambda)(\bar{x}_1 - \bar{x}_2) = (1-\lambda)c \in [0, c)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_2 = \bar{x}_3 - \bar{x}_2 = \lambda \bar{x}_1 + (1-\lambda)\bar{x}_2 - \bar{x}_2 = \lambda(\bar{x}_1 - \bar{x}_2) = \lambda c \in [0, c)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_1 + d_2 = c\)</span></p></li>
</ul>
<p>We have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\min(s^2_1, s^2_2) \le a \le \max(s^2_1, s^2_2)\)</span> with equalities iff <span class="math notranslate nohighlight">\(s_2^2 = s_2^2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(0 \le b &lt; d^2\)</span> with equality iff <span class="math notranslate nohighlight">\(d=0\)</span>.</p></li>
</ul>
<p>Since <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are independent, we can know for sure that <span class="math notranslate nohighlight">\(\min(s^2_1, s^2_2) \le s_3^2\)</span>. The other comparison <span class="math notranslate nohighlight">\(\max(s^2_1, s^2_2) \text{ vs } s_3^2\)</span> is uncertain, depending on <span class="math notranslate nohighlight">\(d\)</span>.</p>
</div>
</li>
<li><p>Find the distribution of the sum of two independent uniform random variable.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(X, Y \overset{\text{iid}}{\sim} \mathcal{U} (0,1)\)</span> and their sum be <span class="math notranslate nohighlight">\(Z = X + Y\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    f_{Z}(z)
    &amp;=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) \mathrm{~d} x \\
    &amp;=\int_{-\infty}^{\infty} \boldsymbol{1} _{x \in (0,1)} \boldsymbol{1} _{z-x \in (0,1)} \mathrm{~d} x \\
    &amp;=\int_{0}^{1} \boldsymbol{1} _{z-x \in (0,1) } \mathrm{~d} x \\
    &amp;= \left\{\begin{array}{ll}
    \int_{0}^{z} 1 \mathrm{~d} x = z, &amp; \text { if } 0&lt;z&lt;1 \\
    \int_{z-1}^{1} 1 \mathrm{~d} x = 2 - z, &amp; \text { if } 1\le z&lt;2 \\
    \end{array}\right.\\
    \end{aligned}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(Z\)</span> follows a triangular distribution with lower limit <span class="math notranslate nohighlight">\(0\)</span>, upper limit <span class="math notranslate nohighlight">\(2\)</span>, and mode <span class="math notranslate nohighlight">\(1\)</span>. That is, it’s more likely to see <span class="math notranslate nohighlight">\(Z\)</span> around <span class="math notranslate nohighlight">\(1\)</span>, which equals the sum of two expected values. In real life, the sum of two dices is probably around 3.5.</p>
</div>
</li>
<li><p>Randomly and independently select two points in <span class="math notranslate nohighlight">\([0, \ell]\)</span>, find their expected distance.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution 1: integration</em></p>
<p>\ellet <span class="math notranslate nohighlight">\(X_1, X_2 \overset{\text{iid}}{\sim} \mathcal{U} [0, \ell]\)</span>, their joint density function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    f_{X_{1} X_{2}}\left(x_{1}, x_{2}\right)=f_{X_{1}}\left(x_{1}\right) f_{X_{2}}\left(x_{2}\right)=\left\{\begin{array}{ll}
    \frac{1}{\ell^2} &amp; \text { if } \quad x_1, x_2 \in[0, \ell] \\
    0 &amp; \text { otherwise }
    \end{array}\right.
    \end{split}\]</div>
<p>Define the distance as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    g\left(x_{1}, x_{2}\right)=\left|x_{1}-x_{2}\right|=\left\{\begin{array}{lll}
    x_{1}-x_{2} &amp; \text { if } x_{1} \geq x_{2} \\
    x_{2}-x_{1} &amp; \text { otherwise }
    \end{array}\right.
    \end{split}\]</div>
<p>Hence the expectation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \mathbb{E}(g(X_1, X_2)) &amp;=\int_{0}^{\ell} \int_{0}^{\ell} g\left(x_{1}, x_{2}\right) f_{X_{1} X_{2}}\left(x_{1}, x_{2}\right) \mathrm{~d} x_{1} \mathrm{~d} x_{2} \\
    &amp;=\frac{1}{\ell^{2}} \int_{0}^{\ell} \int_{0}^{\ell}\left|x_{1}-x_{2}\right| \mathrm{~d} x_{1} \mathrm{~d} x_{2} \\
    &amp;=\frac{1}{\ell^{2}} \int_{0}^{\ell} \int_{0}^{x_{1}}\left(x_{1}-x_{2}\right) \mathrm{~d} x_{2} \mathrm{~d} x_{1}+\frac{1}{\ell^{2}} \int_{0}^{\ell} \int_{x_{1}}^{\ell}\left(x_{2}-x_{1}\right) \mathrm{~d} x_2 \mathrm{~d} x_1 \\
    &amp;=\frac{\ell^3}{6\ell^2} + \frac{\ell^3}{6\ell^2}  \\
    &amp;=\frac{\ell}{3}
    \end{aligned}
    \end{split}\]</div>
</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution 2: random procedure</em></p>
<p>If we randomly select two points, then we cut the interval of length <span class="math notranslate nohighlight">\(\ell\)</span> into 3 segments, of length <span class="math notranslate nohighlight">\(D_1, D_2, D_3\)</span> respectively. They should be “exchangeable”, so <span class="math notranslate nohighlight">\(\mathbb{E}\left( D_1 \right) = \mathbb{E}\left( D_2 \right) = \mathbb{E}\left( D_3 \right)\)</span>. Since <span class="math notranslate nohighlight">\(\mathbb{E}\left( D_1 + D_2 + D_3 \right) = \ell\)</span>, we have <span class="math notranslate nohighlight">\(\mathbb{E}\left( D_2 \right) = \frac{\ell}{3}\)</span>.</p>
<p>Formally, <span class="math notranslate nohighlight">\((D_1, D_2, D_3) = (\min (X, Y), \max (X, Y)-\min (X, Y), \ell-\max (X, Y))\)</span>. One can show that <span class="math notranslate nohighlight">\((D_1, D_2, D_3)\)</span> is an exchangeable sequence, i.e., whose joint probability distribution does not change when the positions in the sequence in which finitely many of them appear are altered.</p>
</div>
<ul class="simple">
<li><p>How about two uniform random points in a compact convex subset in <span class="math notranslate nohighlight">\(\mathbb{R} ^n\)</span>? For example, interval, disk, square, cube? See this <a class="reference external" href="https://www.cambridge.org/core/journals/bulletin-of-the-australian-mathematical-society/article/average-distance-between-two-points/F182A617B5EC6DB5AD31042A4BDF83AE">paper</a>.</p></li>
</ul>
</li>
<li><p>Randomly and independent select <span class="math notranslate nohighlight">\(n\)</span> points from an interval of length <span class="math notranslate nohighlight">\(\ell\)</span>, let <span class="math notranslate nohighlight">\(D\)</span> be the minimum distance between two points: <span class="math notranslate nohighlight">\(d = \min_{i \ne j \in [n]} \left\vert x_i - x_j \right\vert\)</span>, find <span class="math notranslate nohighlight">\(\mathbb{E}\left( D \right)\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Consider two events:</p>
<ul class="simple">
<li><p>the minimum distance between two points equals <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p>no points are in the left section of length <span class="math notranslate nohighlight">\((n-1)d\)</span></p></li>
</ul>
<p>The two events have the same probability: imagine that we take <span class="math notranslate nohighlight">\(d\)</span> from each segment <span class="math notranslate nohighlight">\([x_{i}, x_{i+1}]\)</span>, and aggregate all <span class="math notranslate nohighlight">\((n-1)\)</span> of them to the left. Hence, <span class="math notranslate nohighlight">\(\mathbb{P}\left( D = d \right) = \left( 1 - \frac{(n-1)d}{\ell}  \right)^n\)</span>. Integration over <span class="math notranslate nohighlight">\(0 \le d \le \frac{\ell}{n-1}\)</span> gives <span class="math notranslate nohighlight">\(\frac{\ell}{n^2 - 1}\)</span>.</p>
<p>Note that if <span class="math notranslate nohighlight">\(n=2\)</span> then this problem reduces to the previous one.</p>
</div>
</li>
<li><p>Randomly select three points <span class="math notranslate nohighlight">\(A, B, C\)</span> from a circle, what’s the probability that the center <span class="math notranslate nohighlight">\(O\)</span> is in the triangle <span class="math notranslate nohighlight">\(ABC\)</span>? Extension: probability of four random points on a sphere such that the tetrahedron <span class="math notranslate nohighlight">\(ABCD\)</span> contains <span class="math notranslate nohighlight">\(O\)</span>?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution: change of random procedure</em></p>
<p>Here are some hints. For details, see the great youtube <a class="reference external" href="https://www.youtube.com/watch?v=OkmNXy7er84">video</a>.</p>
<ul class="simple">
<li><p>Suppose we have already select two points <span class="math notranslate nohighlight">\(A, B\)</span>. What’s the range of <span class="math notranslate nohighlight">\(C\)</span> such that <span class="math notranslate nohighlight">\(ABC\)</span> covers <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
<li><p>Two random points can generated as follows: generate two random diameter lines <span class="math notranslate nohighlight">\(\ell_1, \ell_2\)</span> that pass center <span class="math notranslate nohighlight">\(O\)</span>, then randomly choose one endpoint of <span class="math notranslate nohighlight">\(\ell_1\)</span> to be <span class="math notranslate nohighlight">\(B\)</span>, and one endpoint of <span class="math notranslate nohighlight">\(\ell_2\)</span> to be <span class="math notranslate nohighlight">\(C\)</span>, their are 4 combinations. Now, suppose we already have <span class="math notranslate nohighlight">\(A\)</span>, and use this random procedure to generate <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span>, which of the 4 combinations of <span class="math notranslate nohighlight">\((B,C)\)</span> gives a required triangle <span class="math notranslate nohighlight">\(ABC\)</span>?</p></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./12-probabilities"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="00-probabilities.html" title="previous page">Probabilities</a>
    <a class='right-next' id="next-link" href="13-correlation-and-dependence.html" title="next page">Correlation and Dependence</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>