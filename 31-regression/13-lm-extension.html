
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Regression - Extension &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Classification" href="../32-classification/00-classification.html" />
    <link rel="prev" title="Linear Regression - Inference" href="12-lm-inference.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-lm-inference.html">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/13-lm-extension.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/13-lm-extension.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/13-lm-extension.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/13-lm-extension.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#omit-a-variable">
   Omit a Variable
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#include-a-variable">
   Include a Variable
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-observations">
   Special Observations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outliers">
     Outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multicollinearity">
   Multicollinearity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagnosis">
     Diagnosis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consequences">
     Consequences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implications">
     Implications
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#heteroscedasticity">
   Heteroscedasticity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#categorical-x">
   Categorical
   <span class="math notranslate nohighlight">
    \(X\)
   </span>
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-extension">
<h1>Linear Regression - Extension<a class="headerlink" href="#linear-regression-extension" title="Permalink to this headline">¶</a></h1>
<p>No models are perfect. In this section we introduce what happen when our model is misspecified or when some assumptions fail. We will also introduce some alternative models, e.g. Lasso, ridge regression, etc.</p>
<div class="section" id="omit-a-variable">
<span id="lm-omit-variable"></span><h2>Omit a Variable<a class="headerlink" href="#omit-a-variable" title="Permalink to this headline">¶</a></h2>
<p>Suppose the true model is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}_{n \times p} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  
\]</div>
<p>And we omit one explanatory variable <span class="math notranslate nohighlight">\(X_j\)</span>. Thus, our new design matrix has size <span class="math notranslate nohighlight">\(n \times (p-1)\)</span>, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{X}_{-j}\)</span>. Without loss of generality, let it be in the last column of the original design matrix, i.e. <span class="math notranslate nohighlight">\(\boldsymbol{X} = \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\)</span>. The new estimated coefficients vector is denoted by <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span>. The coefficient for <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in the true model is denoted by <span class="math notranslate nohighlight">\(\beta_j\)</span>, and the vector of coefficients for other explanatory variables is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{\beta} _{-j}\)</span>. Hence, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} ^\top = \left[ \boldsymbol{\beta} _{-j} \quad \beta_j \right] ^\top\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Though the common focus is on bias, omitting a variable probably decreases variance. See the relevant section <a class="reference internal" href="#lm-include-variable"><span class="std std-ref">below</span></a>, or the variance expression <a class="reference internal" href="12-lm-inference.html#lm-inference-variance"><span class="std std-ref">above</span></a>.</p>
</div>
<p><em>Question: Is <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span> unbised for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{-j}\)</span>?</em></p>
<p><em>Answer: No. Omitting a relevant variable increases bias. There is a deterministic identity for the bias.</em></p>
<p>We will see the meaning of “relevant” later.</p>
<p>We first find the expression of the new estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \hat{\boldsymbol{\beta} }_{-j}
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{y} \\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left\{ \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\left[\begin{array}{l}
\boldsymbol{\beta} _{-j}  \\
\beta _j
\end{array}\right] + \boldsymbol{\varepsilon}  \right\}\\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left( \boldsymbol{X} _{-j} \boldsymbol{\beta} _{-j} +  \boldsymbol{x}_j \beta _j + \boldsymbol{\varepsilon}  \right) \\
&amp;=  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \right]\left(  \boldsymbol{x}_j \beta _j+ \boldsymbol{\varepsilon}  \right)\\
\end{align}\end{split}\]</div>
<p>The expectation, therefore, is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{E}\left( \hat{\boldsymbol{\beta} }_{-j} \right) =  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \right]\beta _j\\
\end{split}\]</div>
<p>What is <span class="math notranslate nohighlight">\(\left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span>? You may recognize this form. It is actually the vector of estimated coefficients when we regress the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> on all other explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>. Let it be <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_{(p-1) \times 1}\)</span>.</p>
<p>Therefore, we have, for the <span class="math notranslate nohighlight">\(k\)</span>-th explanatory variable in the new model,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta} _{-j,k} \right) = \beta_{k} + \alpha_k \beta_j
\]</div>
<p>So the bias is <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>. The sign can be positive or negative.</p>
<p>This identity can be converted to the following diagram. The explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is associated with the response <span class="math notranslate nohighlight">\(Y\)</span> in two ways. First is directly by itself with strength is <span class="math notranslate nohighlight">\(\beta_k\)</span>, and second is through the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span>, with a “compound” strength <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_k \quad \overset{\quad \beta_{k} \quad }{\longrightarrow} \quad Y
\]</div>
<div class="math notranslate nohighlight">
\[
\alpha_k \searrow \qquad \nearrow \beta_j
\]</div>
<div class="math notranslate nohighlight">
\[
X_j
\]</div>
<p>When will the bias be zero?</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\alpha_k = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the concerned explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{x}_k = 0\)</span> in the design matrix.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\beta_j = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the response <span class="math notranslate nohighlight">\(Y\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{y} = 0\)</span>.</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The takeaway here is that we should include all relevant omitted factors to reduce bias. But in practice, we can never know what all relevant factors are, and rarely can we measure all relevant factors.</p>
</div>
<p>That’s how we define “relevant”.</p>
<p>What is the relation between the sample estimates? The relation has a similar form.</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta }_{-j,k} =  \hat{\beta}_k + \hat{\alpha}_k\hat{\beta}_j
\]</div>
<p>Proof: TBD. Need linear algebra about inverse.</p>
<p>Verify:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">b0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 + x3 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">lmo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">ro</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lmx</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in x3 ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">rx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmx</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reconstruction difference of b0, b1, b2 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>coefficients in y ~ x1 + x2 + x3 : [[1.00703362 0.99906076 2.00216133 2.99953742]]
coefficients in y ~ x1 + x2 : [[0.99756049 0.9550021  3.50213361]]
coefficients in x3 ~ x1 + x2 : [[-0.0031582  -0.01468848  0.50006787]]
reconstruction difference of b0, b1, b2 : [1.11022302e-15 6.66133815e-16 2.66453526e-15]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="include-a-variable">
<span id="lm-include-variable"></span><h2>Include a Variable<a class="headerlink" href="#include-a-variable" title="Permalink to this headline">¶</a></h2>
<p>What if we add a new variable <span class="math notranslate nohighlight">\(X_j\)</span>? What will happen to the existing estimator <span class="math notranslate nohighlight">\(\hat\beta_k\)</span>?</p>
<p>Increase</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}\left(\hat{\beta}_{k}\right)=\sigma^{2} \frac{1}{1-R_k^{2}} \frac{1}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}}\]</div>
<p>if <span class="math notranslate nohighlight">\(R_{k}^2\)</span> increases. When will <span class="math notranslate nohighlight">\(R^2_{k}\)</span> be unchanged? When the new variable <span class="math notranslate nohighlight">\(X_j\)</span> has no explanatory power to <span class="math notranslate nohighlight">\(X_k\)</span>. See the <a class="reference internal" href="12-lm-inference.html#lm-rss-nonincreasing"><span class="std std-ref">section</span></a>.</p>
<p>In terms of bias, if we say the model with <span class="math notranslate nohighlight">\(X_p\)</span> is “true”, then <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\beta}_k \right)\)</span> is probably closer to <span class="math notranslate nohighlight">\(\beta_k\)</span> according to the equation described in the above <a class="reference internal" href="#lm-omit-variable"><span class="std std-ref">section</span></a>.</p>
</div>
<div class="section" id="special-observations">
<h2>Special Observations<a class="headerlink" href="#special-observations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="outliers">
<h3>Outliers<a class="headerlink" href="#outliers" title="Permalink to this headline">¶</a></h3>
<p>What if an outlier exists?</p>
<ul class="simple">
<li><p>If outlier is a mistake (typo) you can drop it (or correct it)</p></li>
<li><p>If outlier is valid but unusual, look for robustness – does dropping it change answer?</p></li>
<li><p>If it does change answer, report both versions – and argue for the approach you think more appropriate</p></li>
</ul>
</div>
</div>
<div class="section" id="multicollinearity">
<h2>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h2>
<p>Definition (Multicollinearity)<br />
Multicollinearity measure the extent of pairwise correlation of variables in the design matrix.</p>
<div class="margin sidebar">
<p class="sidebar-title">Multicollinearity in computation</p>
<p>From numerical algebra’s perspective, the extent of correlation of variables in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> determines the condition number of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. As the correlation increases, its inverse becomes unstable. When perfect linear relation exists, then <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span> is not of full rank, and thus no inverse exists.</p>
</div>
<p>Definition (Perfect multicollinearity)<br />
A set of variables is perfectly multicollinear if a variable does not vary, or if there is an exact linear relationship between a set of variables:</p>
<div class="math notranslate nohighlight">
\[
X_{j}=\delta_{0}+\delta_{1} X_{1}+\cdots+\delta_{j-1} X_{j-1}+\delta_{i+1} X_{i+1}+\cdots+\delta_{k} X_{k}
\]</div>
<p>As long as the variables in the design matrix are not uncorrelated, then multicollinearity exists.</p>
<div class="section" id="diagnosis">
<h3>Diagnosis<a class="headerlink" href="#diagnosis" title="Permalink to this headline">¶</a></h3>
<p>Some common symptoms include</p>
<ul class="simple">
<li><p>Large standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta_j)\)</span></p></li>
<li><p>Overall <span class="math notranslate nohighlight">\(F\)</span>-test is significant, <span class="math notranslate nohighlight">\(R^2\)</span> is good, but individual <span class="math notranslate nohighlight">\(t\)</span>-tests are not significant due to large standard errors.</p></li>
</ul>
<p>We can measure the extent of multicollinearity by <strong>variance inflation factor</strong> (VIF) for each explanatory variable.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{VIF}_j = \frac{1}{1-R_j^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R_j^2\)</span> is the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables excluding <span class="math notranslate nohighlight">\(X_j\)</span>. The value of <span class="math notranslate nohighlight">\(\operatorname{VIF}_j\)</span> can be interpreted as: the standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta)\)</span> is <span class="math notranslate nohighlight">\(\sqrt{\operatorname{VIF}_j}\)</span> times larger than it would have been without multicollinearity.</p>
<p>A second way of measurement is the <strong>condition number</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. If it is greater than <span class="math notranslate nohighlight">\(30\)</span>, then we can conclude that the multicollinearity problem cannot be ignored.</p>
<div class="math notranslate nohighlight">
\[
\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) = \sqrt{\frac{\lambda_1 (\boldsymbol{X} ^\top \boldsymbol{X} )}{\lambda_p (\boldsymbol{X} ^\top \boldsymbol{X} )} }
\]</div>
<p>Finally, <strong>correlation matrix</strong> can also be used to measure multicollinearity since it is closely related to the condition number <span class="math notranslate nohighlight">\(\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)\)</span>.</p>
</div>
<div class="section" id="consequences">
<h3>Consequences<a class="headerlink" href="#consequences" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>It inflates <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_j \right)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \operatorname{Var}\left( \hat{\beta}_j \right)
     &amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2}  \\
     &amp;=  \sigma^2 \frac{\operatorname{VIF}_j}{\operatorname{Var}\left( X_j \right)}  
     \end{align}\end{split}\]</div>
<p>When perfect multicollinearity exists, the variance goes to infinity since <span class="math notranslate nohighlight">\(R^2_{j} = 1\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>-tests fail to reveal significant predictors, due to 1.</p></li>
<li><p>Estimated coefficients are sensitive to randomness in <span class="math notranslate nohighlight">\(Y\)</span>, i.e. unreliable. If you run the experiment again, the coefficients can change dramatically, which is measured by <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( X_1, X_2 \right)\)</span> is large, then we expect to have large <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right), \operatorname{Var}\left( \hat{\beta}_2 \right), \operatorname{Var}\left( \hat{\beta}_1, \hat{\beta}_2 \right)\)</span>, but <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)\)</span> can be small. This means we cannot distinguish the effect of <span class="math notranslate nohighlight">\(X_1 + X_2\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> is from <span class="math notranslate nohighlight">\(X_1\)</span> or <span class="math notranslate nohighlight">\(X_2\)</span>, i.e. <strong>non-identifiable</strong>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">By the fact that, for symmetric positive definite matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} \boldsymbol{b} = \sum \lambda_i b_i ^2
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} ^{-1}  \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} ^{-1}  \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} ^{-1}  \boldsymbol{b} = \sum \frac{1}{\lambda_i}  b_i ^2
 \]</div>
<p class="card-text">we have:</p>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx 0
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 - \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx \infty
 \]</div>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
</div>
</details></li>
</ol>
</div>
<div class="section" id="implications">
<h3>Implications<a class="headerlink" href="#implications" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> show high correlation, then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1\)</span> may be a proxy of <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1 - X_2\)</span> may just be noise.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_2\)</span> is removed, <span class="math notranslate nohighlight">\(X_1\)</span> may still be good for prediction.</p></li>
</ol>
</div>
</div>
<div class="section" id="heteroscedasticity">
<h2>Heteroscedasticity<a class="headerlink" href="#heteroscedasticity" title="Permalink to this headline">¶</a></h2>
<p>TBD</p>
</div>
<div class="section" id="categorical-x">
<h2>Categorical <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#categorical-x" title="Permalink to this headline">¶</a></h2>
<p>dummy variables <span class="math notranslate nohighlight">\(X_ij\)</span></p>
<p>when <span class="math notranslate nohighlight">\(c = 2\)</span>,</p>
<p>interpretation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>: difference in means between the group with <span class="math notranslate nohighlight">\(X=1\)</span> and <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span>: mean of the group with <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
</ul>
<p>TBD</p>
<p>https://www.1point3acres.com/bbs/thread-703302-1-1.html</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="12-lm-inference.html" title="previous page">Linear Regression - Inference</a>
    <a class='right-next' id="next-link" href="../32-classification/00-classification.html" title="next page">Classification</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>