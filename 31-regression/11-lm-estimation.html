
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models - Estimation &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Models - Inference" href="12-lm-inference.html" />
    <link rel="prev" title="Regression" href="00-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../15-programming/00-programming.html">
   Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../15-programming/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-programming/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-programming/31-sql.html">
     SQL
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/11-lm-estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/11-lm-estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/11-lm-estimation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/11-lm-estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation">
   Estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary-least-squares">
     Ordinary Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#by-assumptions">
     By Assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood">
     Maximum Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-of-estimated-coefficients">
     Value of Estimated Coefficients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partialling-out-explanation-for-mlr">
     Partialling Out Explanation for MLR
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models-estimation">
<h1>Linear Models - Estimation<a class="headerlink" href="#linear-models-estimation" title="Permalink to this headline">¶</a></h1>
<p>From this section we introduce linear models from a statistics’ perspective. There are three sections in total.</p>
<ol class="simple">
<li><p>The first section covers the model fundamentals, including assumptions, estimation, interpretation and some exercise.</p></li>
<li><p>The next section introduce statistical inference for linear models, such as distribution of the estimated coefficients, <span class="math notranslate nohighlight">\(t\)</span>-test, <span class="math notranslate nohighlight">\(F\)</span>-test, etc. Usually machine learning community focuses more on prediction, less on inference. But inference does matters. It analyzes how important each variable is in the model from a rigorous approach.</p></li>
<li><p>The third section introduce some issues in linear models, e.g. omitted variables bias, multicollinearity, heteroskedasticity, and some alternative models, e.g. Lasso, ridge regression, etc.</p></li>
</ol>
<div class="dropdown note admonition">
<p class="admonition-title"> Statistics’ perspective vs social science’s perspective</p>
<p>The introduction from econometrics’ perspective or social science’s perspective may be different. In short, the statistics’ perspective focuses on general multivariate cases and heavily rely on linear algebra for derivation, while the econometrics’ or the social science’s perspective prefers to introduce models in univariate cases by basic arithmetics (whose form can be complicated without linear algebra notations) and extend the intuitions and conclusions into multivariate cases.</p>
<p>Personally, I involved in four courses that introduced linear models, i.e. at undergrad/grad level offered by stat/social science department. The style of the two courses offered by the stat departments were quite alike while the graduate level one covered more topics. In both undergrad/grad level courses offered by the social science departments, sometimes I got confused by the course materials that were contradictory to my statistics training , but the instructors had no clear response or even no response at all…</p>
<p>In sum, to fully understand the most fundamental and widely used statistical model, I highly suggest to take a linear algebra course first and take the regression course offered by math/stat department.</p>
</div>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Linear models aim to model the relationship between a scalar response and one or more explanatory variables in a linear format:</p>
<div class="math notranslate nohighlight">
\[Y_i  = \beta_0 + \beta_1 x_{i,1} + \ldots + \beta_{p-1} x_{i,p-1}  + \varepsilon_i \]</div>
<p>for observations <span class="math notranslate nohighlight">\(i=1, 2, \ldots, n\)</span>.</p>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}_{n\times p}\)</span> is called the design matrix. The first column is usually set to be <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span>, i.e., intercept. The remaining <span class="math notranslate nohighlight">\(p-1\)</span> columns are designed values <span class="math notranslate nohighlight">\(x_{ij}\)</span> where <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, n\)</span> and <span class="math notranslate nohighlight">\(j=1, \ldots, p-1\)</span>. These <span class="math notranslate nohighlight">\(p-1\)</span> columns are called explanatory/independent variables, or covariates.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}_{n \times 1}\)</span> is a vector of response/dependent variables <span class="math notranslate nohighlight">\(Y_1, Y_2, \ldots, Y_n\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{p \times 1}\)</span> is a vector of coefficients to be estimated.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}_{n \times 1}\)</span> is a vector of unobserved random errors, which includes everything that we have not measured and included in the model.</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</div>
<p>which is called <strong>simple linear regression</strong>.</p>
<p>When <span class="math notranslate nohighlight">\(p&gt;2\)</span>, it is called <strong>multiple linear regression</strong>. For instance, when <span class="math notranslate nohighlight">\(p=3\)</span></p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon_i
\]</div>
<p>When there are multiple dependent variables, we call it <strong>multivariate regression</strong>, which will be introduced in another section.</p>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span>,</p>
<ul class="simple">
<li><p>if we include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta_0\)</span> means that we use a single constant to predict <span class="math notranslate nohighlight">\(y_i\)</span>. The estimator, <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, by ordinary least square, should be the sample mean <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>if we do not include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta x_i\)</span> means that we expect that <span class="math notranslate nohighlight">\(y\)</span> is proportional to <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Fixed or random <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>In natural science, researchers design <span class="math notranslate nohighlight">\(n\times p\)</span> values in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and run experiments to obtain the response <span class="math notranslate nohighlight">\(y_i\)</span>. We call this kind of data <strong>experimental data</strong>. In this sense, the explanatory variables <span class="math notranslate nohighlight">\(x_{ij}\)</span>’s are designed before the experiment, so they are also constants. The coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>’s are unknown constants. The error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is random. The response variable <span class="math notranslate nohighlight">\(Y_i\)</span> on the left hand side is random due to the randomness in the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<p>In social science, most of data is <strong>observational data</strong>. That is, researchers obtain the values of many variables at the same time, and choose one of interest to be the response variable <span class="math notranslate nohighlight">\(y_i\)</span> and some others to be the explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>. In this case, <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is viewed as a data set, and we can talk about descriptive statistics, such as variance of each explanatory variable, or covariance between pair of explanatory variables. This is valid since we often view the columns of a data set as random variables.</p>
<p>However, the inference methods of the coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> are developed based on the natural science setting, i.e., the values of explanatory variables are pre-designed constants. Many social science courses frequently use descriptive statistics of the explanatory variables which assumes they are random, and apply inference methods which assumes they are constant. This is quite confusing for beginners to linear models.</p>
<p>To be clear, we stick to the natural science setting and make the second assumption below. We use subscript <span class="math notranslate nohighlight">\(i\)</span> in every <span class="math notranslate nohighlight">\(y_i, x_i, \varepsilon_i\)</span> instead of <span class="math notranslate nohighlight">\(y, x, \varepsilon\)</span> which gives a sense that <span class="math notranslate nohighlight">\(x\)</span> is random. And we use descriptive statistics for the explanatory variables only when necessary.</p>
</div>
</div>
<div class="section" id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h2>
<p>Basic assumptions</p>
<ol>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span> is <strong>linear</strong> in covariates <span class="math notranslate nohighlight">\(X_j\)</span>.</p></li>
<li><p>The values of explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> are known and fixed. Randomness only comes from <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p></li>
<li><p>No <span class="math notranslate nohighlight">\(X_j\)</span> is constant for all observations. No exact linear relationships among the explanatory variables (aka no perfect multicollinearity, or the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is of full rank).</p></li>
<li><p>The error terms are uncorrelated <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right)= 0\)</span>, with common mean <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> and variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon_i \right) = \sigma^2\)</span> (homoskedasticity).</p>
<p>As a result, <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y} \mid \boldsymbol{X} \right) = \boldsymbol{X} \boldsymbol{\beta}\)</span>, or <span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \mid x_i \right) = \beta_0 + \beta_1 x_i\)</span> when <span class="math notranslate nohighlight">\(p=2\)</span>, which can be illustrated by the plots below.</p>
<div class="figure align-default" id="lm-distribution-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm-cond-distribution.png"><img alt="" src="../_images/lm-cond-distribution.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 45 </span><span class="caption-text">Distributions of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-distribution-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="lm-observation-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm-xyplane-dots.png"><img alt="" src="../_images/lm-xyplane-dots.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 46 </span><span class="caption-text">Observations of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-observation-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<p>To predict <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, we just use <span class="math notranslate nohighlight">\(\hat{y}_i = \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\)</span> .</p>
</li>
<li><p>The error terms are independent and follow Gaussian distribution <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _n)\)</span>.</p>
<p>As a result, we have <span class="math notranslate nohighlight">\(Y_i \sim N(\boldsymbol{x}_i ^\top \boldsymbol{\beta} , \sigma^2 )\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_n(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} _n)\)</span></p>
</li>
</ol>
<p>These assumptions are used for different objectives. The first 3 assumptions are the base, and in additiona to them,</p>
<ul class="simple">
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by least squares uses no more assumptions.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by maximal likelihood uses assumptions 4 and 5.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> in 4.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses 1, 2, <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right) = 0\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \epsilon_i \right) = \sigma^2\)</span> in 4.</p></li>
<li><p>proof of Gaussian-Markov Theorem (BLUE) uses 4.</p></li>
<li><p>derivation of the distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span> uses 4 and 5.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Zero conditional mean assumption</p>
<p>In some social science or econometrics courses, they follow the “Gauss-Markov assumptions” that are roughly the same to the assumptions, but in different formats. One of them is zero conditional mean assumption.</p>
<p>In general, it says</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \varepsilon \mid x_1, x_2, \ldots, x_p\right) = 0
\]</div>
<p>For <span class="math notranslate nohighlight">\(p=2\)</span>, it is</p>
<div class="math notranslate nohighlight">
\[\operatorname{E}\left( \varepsilon \mid x  \right) = 0\]</div>
<p>which (in their setting) implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \varepsilon \right)
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon \mid x \right) \right)\\
&amp;= 0\\
\operatorname{Cov}\left( \varepsilon, x \right)
&amp;= \operatorname{E}\left( \varepsilon x \right) - \operatorname{E}\left( \varepsilon \right)\operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon x \mid x \right) \right)- 0 \times \operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( x \operatorname{E}\left( \varepsilon \mid x \right) \right) \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>Then they these two corollaries are used for <a class="reference internal" href="#lm-estimation-by-assumpation"><span class="std std-ref">estimation</span></a>.</p>
<p>As discussed above, in their setting <span class="math notranslate nohighlight">\(x\)</span> is random (at this stage), so they use notations such as <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( x, \varepsilon \right)\)</span>. It also seems that they view <span class="math notranslate nohighlight">\(\varepsilon\)</span> as an “overall” measure of random error, instead of <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> for specific <span class="math notranslate nohighlight">\(i\)</span> in the natural science setting. But they can mean so by using the conditional notation <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span>.</p>
</div>
</div>
<div class="section" id="estimation">
<h2>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h2>
<p>We introduce various methods to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="section" id="ordinary-least-squares">
<h3>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<p>The most common way is to estimate the parameter <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by minimizing the sum of squared errors <span class="math notranslate nohighlight">\(\sum_i(y_i-\hat{y}_i)^2\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title">A note on substitution</p>
<p>We substitute the predicted <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} }\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{X} \boldsymbol{\beta}\)</span>. The <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> here just means a variable in the optimization problem, not the unknown constant coefficients in our model.</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \hat{\boldsymbol{y}}  \right\Vert ^2 \\
&amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 \\
\end{align}\end{split}\]</div>
<p>The gradient w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\nabla_{\boldsymbol{\beta}} &amp;= -2 \boldsymbol{X}  ^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )  \\
&amp;\overset{\text{set}}{=} \boldsymbol{0}
\end{align}\end{split}\]</div>
<p>Hence, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}
\]</div>
<p>This linear system is called the <strong>normal equation</strong>.</p>
<p>The closed form solution is</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = \left( \boldsymbol{X} ^\top \boldsymbol{X}   \right)^{-1}\boldsymbol{X} ^\top  \boldsymbol{y}  \]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \boldsymbol{y}\)</span> is a random variable, since it is a linear combination of the random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. This means that, keeping <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> fixed, repeat the experiment, we will probably get different response values <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, and hence different <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. As a result, there is a sampling distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, and we can find its mean, variance, and conduct hypothesis testing.</p>
<div class="dropdown tip admonition">
<p class="admonition-title"> View least squares as projection</p>
<p>Substitute the solve <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> into the prediction <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{y}} = \boldsymbol{X} \hat{\boldsymbol{\beta}} = \underbrace {\boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top}_{\boldsymbol{H}} \boldsymbol{y}
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> is a projection matrix onto the column space (image) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. Recall that a projection matrix onto the column span of a matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> has the form <span class="math notranslate nohighlight">\(\boldsymbol{P} _{\operatorname{col}(\boldsymbol{X} )} = \boldsymbol{X} \boldsymbol{X} ^\dagger\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\dagger =  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top\)</span> is the pseudo inverse of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<p>Essentially, we are trying to find a vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}\)</span> in the column space of the data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> that is as close to <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> as possible, and the closest one is just the projection of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> onto <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X})\)</span>, which is <span class="math notranslate nohighlight">\(\boldsymbol{H}\boldsymbol{y}\)</span>. The distance is measured by the norm <span class="math notranslate nohighlight">\(\left\| \boldsymbol{y} - \hat{\boldsymbol{y}}  \right\|\)</span>, which is the squared root of sum of squared errors. Note that <span class="math notranslate nohighlight">\(\boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X}) ^ \bot\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{H} = \boldsymbol{I}  - \boldsymbol{P}_{\operatorname{col}(\boldsymbol{X}) } = \boldsymbol{P}_{\operatorname{col}(\boldsymbol{X}) ^ \bot}\)</span> is the projection matrix onto the orthogonal complement <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X}) ^ \bot\)</span>.</p>
<div class="figure align-default" id="lm-projection">
<a class="reference internal image-reference" href="../_images/lm-projection.png"><img alt="" src="../_images/lm-projection.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 47 </span><span class="caption-text">Least squares as a projection <a class="reference external" href="https://waterprogramming.wordpress.com/2017/05/12/an-introduction-to-econometrics-part-1-classical-ordinary-least-squares-regression/">[Gold 2017]</a></span><a class="headerlink" href="#lm-projection" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Solving the linear system by software</p>
<p>Computing software use specific functions to solve the normal equation <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, instead of using the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X}) ^{-1}\)</span> directly which can be slow and numerically unstable. For instance, one can use QR factorization of <span class="math notranslate nohighlight">\(X\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \boldsymbol{Q} \left[\begin{array}{l}
\boldsymbol{R}_{p \times p}  \\
\boldsymbol{0}_{(n-p) \times p}
\end{array}\right]
\end{split}\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\| \boldsymbol{y} - \boldsymbol{X}  \boldsymbol{\beta}  \|^{2}
&amp;=\left\|\boldsymbol{Q} ^{\top} \boldsymbol{y}  - \boldsymbol{Q} ^\top \boldsymbol{X} \boldsymbol{\beta}  \right\|^{2} \\
&amp;=\left\|\left(\begin{array}{c}
\boldsymbol{f}  \\
\boldsymbol{r}
\end{array}\right)-\left(\begin{array}{c}
\boldsymbol{R} \boldsymbol{\beta}  \\
\boldsymbol{0}
\end{array}\right)\right\|^{2} \\
&amp;=\|\boldsymbol{f} - \boldsymbol{R} \boldsymbol{\beta} \|^{2}+\|\boldsymbol{r} \|^{2}
\end{aligned}
\end{split}\]</div>
<p>Finally</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\beta} = \boldsymbol{R} ^{-1} \boldsymbol{f}
\]</div>
</div>
<p>An unbiased estimator of the error variance <span class="math notranslate nohighlight">\(\sigma^2 = \operatorname{Var}\left( \varepsilon \right)\)</span> is (to be discussed [later])</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\Vert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \right\Vert ^2}{n-p}
\]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0, \hat{\beta}_1 =  \underset{\beta_0, \beta_1 }{\mathrm{argmin}} \, \sum_i \left( y_i - \beta_0 - \beta_1 x_i \right)^2\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0
\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) = 0
\]</div>
<p>Solve the system of the equations, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\hat{\beta}_{0} &amp;=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{align}\end{split}\]</div>
<p>The expression for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> implies that the fitted line cross the sample mean point <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>.</p>
<p>Moreover,</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{n-2} \sum_i \hat\varepsilon_i^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\varepsilon_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\)</span>.</p>
<div class="note admonition">
<p class="admonition-title"> Minimizing mean squared error</p>
<p>The objective function, <strong>sum of squared errors</strong>,</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 = \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>can be replaced by <strong>mean squared error</strong>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>and the results are the same.</p>
</div>
</div>
<div class="section" id="by-assumptions">
<span id="lm-estimation-by-assumpation"></span><h3>By Assumptions<a class="headerlink" href="#by-assumptions" title="Permalink to this headline">¶</a></h3>
<p>In some social science courses, the estimation is done by using the assumptions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid X \right) = 0\)</span></p></li>
</ul>
<p>The first one gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>The second one gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Cov}\left( X, \varepsilon \right)
&amp;= \operatorname{E}\left( X \varepsilon \right) - \operatorname{E}\left( X \right) \operatorname{E}\left( \varepsilon \right) \\
&amp;= \operatorname{E}\left[ \operatorname{E}\left( X \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= \operatorname{E}\left[ X \operatorname{E}\left( \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= 0
\end{align}\end{split}\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>Therefore, we have the same normal equations to solve for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>Estimation by the two assumptions derived from the zero conditional mean assumption can be problematic. Consider a model without intercept <span class="math notranslate nohighlight">\(y_i = \beta x_i + \varepsilon_i\)</span>. Fitting by OLS, we have only ONE first order condition</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>If we fit by assumptions, then in addition to the condition above, the first assumption <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span> also gives</p>
<div class="math notranslate nohighlight">
\[
 \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>These two conditions may not hold at the same time.</p>
</div>
</div>
<div class="section" id="maximum-likelihood">
<h3>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<p>biased. TBD.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="value-of-estimated-coefficients">
<h3>Value of Estimated Coefficients<a class="headerlink" href="#value-of-estimated-coefficients" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">Non-linear term in linear models.</p>
<p>“Linear” model is linear in predictors. A predictor can be a non-linear feature of inputs <span class="math notranslate nohighlight">\(x_1, \ldots, x_p\)</span>, say <span class="math notranslate nohighlight">\(x_1^2, ln^{x_1}, x_1 x_2, \)</span> etc.</p>
</div>
<ol>
<li><p>For a model in a standard form,</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\beta_j\)</span> is the expected change in the value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if the value of the covariate <span class="math notranslate nohighlight">\(x_j\)</span> increases by 1, holding other covariates fixed, aka <em>ceteris paribus</em>.</p>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>Sometimes other covariates is unlikely to be fixed as we increase <span class="math notranslate nohighlight">\(x_j\)</span>, and in these cases the <em>ceteris paribus</em> interpretation is not appropriate. So for interpretation purpose, don’t include</p>
<ul class="simple">
<li><p>multiple measures of the same economic concept,</p></li>
<li><p>intermediate outcomes or alternative forms of the dependent variable</p></li>
</ul>
</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the expected value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if all covariates have values of zero.</p></li>
</ul>
</li>
<li><p>If the covariate is an interaction term, for instance,</p>
<div class="math notranslate nohighlight">
\[
    Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \varepsilon
    \]</div>
<p>Then <span class="math notranslate nohighlight">\((\hat{\beta}_1 + \hat{\beta}_{12}x_2)\)</span> can be interpreted as the estimated effect of one unit change in <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> given a fixed value <span class="math notranslate nohighlight">\(x_2\)</span>. Usually <span class="math notranslate nohighlight">\(x_2\)</span> is an dummy variable, say gender.</p>
<p>In short, we build this model because we believe the effect of <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> depends on <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<p>Often higher-order interactions are added after lower-order interactions are included.</p>
</li>
<li><p>For polynomial covariates, say,</p>
<div class="math notranslate nohighlight">
\[
    Y = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_{3} x_1^3 + \varepsilon
    \]</div>
<p>the interpretation of the marginal effect of <span class="math notranslate nohighlight">\(x_1\)</span> is simply the partial derivative <span class="math notranslate nohighlight">\(\beta_1 + 2\beta_2 x_1 + 3 \beta_3 x_1^2\)</span>. We build such model because the plot suggests a non-linear relation between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, or we believe the effect of <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> depends on the value of <span class="math notranslate nohighlight">\(x_1\)</span>. Note in this case the effect can change sign.</p>
</li>
<li><p>For a model that involves log, we need some approximation.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 \ln(x) + \mu\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        \ln(1+0.01)&amp;\approx 0.01\\
        \Rightarrow \quad \ln(x + 0.01x) &amp;\approx \ln(x)  + 0.01 \quad \forall x\\
        \end{aligned}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(0.01x\)</span> change in <span class="math notranslate nohighlight">\(x\)</span>, or <span class="math notranslate nohighlight">\(1\%\)</span> change in <span class="math notranslate nohighlight">\(x\)</span>, is associated with <span class="math notranslate nohighlight">\(0.01\beta_1\)</span> change in value of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(\ln(Y) = \beta_0 + \beta_1 x + \mu\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        Y ^\prime
        &amp;= \exp(\beta_0 + \beta_1 (x + 1) + \varepsilon)   \\
        &amp;= \exp(\beta_0 + \beta_1 + \varepsilon) \exp(\beta_1)  \\
        &amp;\approx Y (1 + \beta_1) \quad \text{if $\beta_1$ is close to 0}  \\
        \end{aligned}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(1\)</span> unit change in <span class="math notranslate nohighlight">\(x\)</span> is associated with <span class="math notranslate nohighlight">\(100\beta_1 \%\)</span> change in <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(\ln(Y) = \beta_0 + \beta_1 \ln(x) + \mu\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        Y ^\prime
        &amp;= \exp(\beta_0 + \beta_1 \ln(x + 0.01x) + \varepsilon)   \\
        &amp;\approx \exp(\beta_0 + \beta_1 (\ln(x) + 0.01) + \varepsilon)   \\
        &amp;= \exp(\beta_0 + \beta_1 \ln(x) + \varepsilon)\exp(0.01\beta_1)   \\
        &amp;\approx Y (1 + 0.01\beta_1) \quad \text{if $0.01\beta_1$ is close to 0}  \\
        \end{aligned}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(1\%\)</span> change in <span class="math notranslate nohighlight">\(x\)</span> is associated with <span class="math notranslate nohighlight">\(\beta_1 \%\)</span> change in <span class="math notranslate nohighlight">\(Y\)</span>, i.e. <span class="math notranslate nohighlight">\(\beta_1\)</span> measures <strong>elasticity</strong>.</p>
<div class="note admonition">
<p class="admonition-title"> When to use log?</p>
<p>Log is often used</p>
<ul class="simple">
<li><p>when the variable has a right skewed distribution, e.g. wages, prices</p></li>
<li><p>to reduce heteroskedasticity</p></li>
</ul>
<p>not used when</p>
<ul class="simple">
<li><p>the variable has negative values</p></li>
<li><p>the variable are in percentages or proportions (hard to interpret)</p></li>
</ul>
<p>Also note that logging can change significance tests.</p>
</div>
</li>
</ul>
</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Linear regression models only reveal linear associations between the response variable and the independent variables. But association does not imply causation. Simple example: in SLR, regress <span class="math notranslate nohighlight">\(X\)</span> over <span class="math notranslate nohighlight">\(Y\)</span>, the coefficient has same sign and significance, but causation cannot be reversed.</p>
<p>Only when the data is from a randomized controlled trial, correlation will imply causation.</p>
</div>
<p>We can measure if a coefficient is statistically significant by <a class="reference internal" href="12-lm-inference.html#lm-t-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(t\)</span>-test</span></a>.</p>
</div>
<div class="section" id="r-squared">
<h3><span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>We will introduce <span class="math notranslate nohighlight">\(R\)</span>-squared in detail in next section.</p>
<dl class="simple myst">
<dt>Definition (<span class="math notranslate nohighlight">\(R\)</span>-squared)</dt><dd><p><span class="math notranslate nohighlight">\(R\)</span>-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2}
\]</div>
</div>
<div class="section" id="partialling-out-explanation-for-mlr">
<span id="lm-partialling-out"></span><h3>Partialling Out Explanation for MLR<a class="headerlink" href="#partialling-out-explanation-for-mlr" title="Permalink to this headline">¶</a></h3>
<p>We can interpret the coefficients in multiple linear regression from “partialling out” perspective.</p>
<p>When <span class="math notranslate nohighlight">\(p=3\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}
\]</div>
<p>We can obtain <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> by the following three steps</p>
<ol>
<li><p>regress <span class="math notranslate nohighlight">\(x_1\)</span> over <span class="math notranslate nohighlight">\(x_2\)</span> and obtain</p>
<div class="math notranslate nohighlight">
\[\hat{x}_{1}=\hat{\gamma}_{0}+\hat{\gamma}_{1} x_{2}\]</div>
</li>
<li><p>compute the residuals <span class="math notranslate nohighlight">\(\hat{u}_{i}\)</span> in the above regression</p>
<div class="math notranslate nohighlight">
\[
    \hat{u}_{i} = x_{1i} - \hat{x}_{1i}
    \]</div>
</li>
<li><p>regress <span class="math notranslate nohighlight">\(y\)</span> on the the residuals <span class="math notranslate nohighlight">\(\hat{u}_{1}\)</span>, and the estimated coefficient equals the required coefficient.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \text{Regress}\quad y_i
    &amp;\sim \alpha_{0}+\alpha_{1} \hat{u}_i \\
    \text{Obtain}\quad\hat{\alpha}_{1}
    &amp;= \frac{\sum (\hat{u}_i - \bar{\hat{u}}_i)(y_i - \bar{y})}{\sum (\hat{u}_i - \bar{\hat{u}}_i)^2} \\
    &amp;= \frac{\sum \hat{u}_{i}y_i}{\sum \hat{u}_{i}^2} \qquad \because \bar{\hat{u}}_i = 0\\
    &amp;\overset{\text{claimed}}{=} \hat{\beta}_1
    \end{align}\end{split}\]</div>
</li>
</ol>
<p>In this approach, <span class="math notranslate nohighlight">\(\hat{u}\)</span> is interpreted as the part in <span class="math notranslate nohighlight">\(x_1\)</span> that cannot be predicted by <span class="math notranslate nohighlight">\(x_2\)</span>, or is uncorrelated with <span class="math notranslate nohighlight">\(x_2\)</span>. We then regress <span class="math notranslate nohighlight">\(y\)</span> on <span class="math notranslate nohighlight">\(\hat{u}\)</span>, to get the effect of <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(y\)</span> after <span class="math notranslate nohighlight">\(x_2\)</span> has been “partialled out”.</p>
<p>It can be proved that the above method hold for any <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let’s consider the last variable <span class="math notranslate nohighlight">\(X_j\)</span> and its coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span>. First we find a formula for <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>.</p>
<p>Recall the matrix inverse formula: if</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{M}=\left[\begin{array}{cc}
\boldsymbol{A} &amp; \boldsymbol{b} \\
\boldsymbol{b}^{\top} &amp; c
\end{array}\right]
\end{split}\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{M}^{-1}=\left[\begin{array}{cc}
\left(\boldsymbol{A}-\frac{1}{c} \boldsymbol{b} \boldsymbol{b}^{\top}\right)^{-1} &amp; -\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \\
-\frac{1}{k} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; \frac{1}{k}
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{A}^{-1}+\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; -\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \\
-\frac{1}{k} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; \frac{1}{k}
\end{array}\right]
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k = c- \boldsymbol{b} ^\top \boldsymbol{A} ^{-1} \boldsymbol{b}\)</span>.</p>
<p>In this case,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{b} = \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c = \boldsymbol{x}_j ^\top \boldsymbol{x}_j = \left\| \boldsymbol{x}_j  \right\|^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k = \boldsymbol{x}_j ^\top \boldsymbol{x}_j  - \boldsymbol{x}_j ^\top \boldsymbol{X} _{-j} \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j = \boldsymbol{x}_j ^\top (\boldsymbol{I} - \boldsymbol{H} _{-j}) \boldsymbol{x}_j\)</span></p></li>
</ul>
<p>Substituting the above expression to the formula gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left( \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1}=\left[\begin{array}{cc}
\left(\boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j}-\frac{1}{c} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \boldsymbol{x}_j ^\top \boldsymbol{X} _{-j} \right)^{-1} &amp; -\frac{1}{k} \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right)^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \\
-\frac{1}{k} \boldsymbol{x}_j^{\top} \boldsymbol{X} _{-j}  \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right)^{-1} &amp; \frac{1}{k}
\end{array}\right]
\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\beta}_j &amp;=\hat{\boldsymbol{\beta}} _{[j]} \\
&amp;= \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \right]_j \\
&amp;= \frac{1}{k} \left[\begin{array}{cc}
- \boldsymbol{x}_j^{\top} \boldsymbol{X} _{-j}  \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right)^{-1} &amp; 1 \\
\end{array}\right] \left[\begin{array}{cc}
\boldsymbol{X} ^\top _{-j} \boldsymbol{y} \\
\boldsymbol{x}_j ^\top \boldsymbol{y}
\end{array}\right] \\
&amp;= \frac{1}{k} \left( - \boldsymbol{x}_j^{\top} \boldsymbol{X} _{-j}  \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right)^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{y}  + \boldsymbol{x} ^\top _j \boldsymbol{y}   \right) \\
&amp;= \frac{1}{k} \left(- \boldsymbol{x}_j^{\top}\boldsymbol{H} _{-j} \boldsymbol{y}  + \boldsymbol{x} ^\top _j \boldsymbol{y}   \right) \\
&amp;= \frac{1}{k} \boldsymbol{x}_j ^\top \left(\boldsymbol{I}_{n} - \boldsymbol{H} _{-j}   \right) \boldsymbol{y} \\
&amp;= \frac{1}{k}  \hat{\boldsymbol{u}} ^\top \boldsymbol{y} \\
\end{aligned}\end{split}\]</div>
<p>The partialling out formula says</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\alpha}_1
&amp;= \frac{\sum (\hat{u}_i - \bar{\hat{u}}_i)(y_i - \bar{y})}{\sum (\hat{u}_i - \bar{\hat{u}}_i)^2} \\
&amp;= \frac{\sum \hat{u}_{i}y_i}{\sum \hat{u}_{i}^2} \qquad \because \bar{\hat{u}}_i = 0 \\
&amp;= \frac{\hat{\boldsymbol{u} }^\top \boldsymbol{y} }{\hat{\boldsymbol{u} }^\top \hat{\boldsymbol{u} }} \\
\end{aligned}\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{u} }^\top \hat{\boldsymbol{u} }= \boldsymbol{x}_j ^\top (\boldsymbol{I}  - \boldsymbol{H} _{-j})^2 \boldsymbol{x}_j = \boldsymbol{x}_j ^\top (\boldsymbol{I} -\boldsymbol{H} _{-j})\boldsymbol{x}_j = k\)</span></p>
<p>Therefore, <span class="math notranslate nohighlight">\(\hat{\beta}_j = \hat{\alpha}_1\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Byproduct: since <span class="math notranslate nohighlight">\(\hat{u}_i\)</span> is actually constant (obtained from constant design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>), we can obtain a convenient formula for individual <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> (rather than the vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span>). Moreover, it can be used to compute individual variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{Var}\left( \hat{\beta}_j \right)
&amp;= \operatorname{Var}\left( \frac{\sum \hat{u}_{i}y_i}{\sum \hat{u}_{i}^2} \right)\\
&amp;= \frac{\sum \operatorname{Var} \left( \hat{u}_{i} y_i \right)}{SSR_j^2}\\
&amp;= \frac{\sum \hat{u}_{i}^2 \operatorname{Var}\left( \varepsilon_i \right)}{SSR_j^2}\\
\end{aligned}\end{split}\]</div>
<p>This holds for any type of <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon_i \right)\)</span>, i.e. heteroskedasticity. For more details see <a class="reference internal" href="12-lm-inference.html#lm-inference-variance"><span class="std std-ref">variance</span></a> section.</p>
</div>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>SLR stands for simple linear regression <span class="math notranslate nohighlight">\(y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \)</span></p>
<ol>
<li><p><em>In SLR, can you compute <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> from correlation <span class="math notranslate nohighlight">\(r_{X,Y}\)</span> and standard deviations <span class="math notranslate nohighlight">\(s_X\)</span> and <span class="math notranslate nohighlight">\(s_Y\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>In SLR, we can see from the solution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
    \hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
    \end{align}\]</div>
<p>that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \hat{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
    &amp;= r_{X,Y} \frac{s_Y}{s_X}
    \end{align}\end{split}\]</div>
<p>Thus, the slope has the same sign with the correlation <span class="math notranslate nohighlight">\(r_{X,Y}\)</span>, and equals to the correlation times a ratio of the sample standard deviations of the dependent variable over the independent variable.</p>
<p>Once can see that the magnitude of <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> increases with the magnitude of <span class="math notranslate nohighlight">\(r_{X,Y}\)</span> and <span class="math notranslate nohighlight">\(s_Y\)</span>, and decreases with <span class="math notranslate nohighlight">\(s_X\)</span>, holding others fixed.</p>
</div>
</li>
<li><p><em>In SLR, can you compute <span class="math notranslate nohighlight">\(\bar{y}\)</span> given <span class="math notranslate nohighlight">\(\hat{\beta}_0,\hat{\beta}_1\)</span> and <span class="math notranslate nohighlight">\(\bar{x}\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Since <span class="math notranslate nohighlight">\(\hat{\beta}_{0} =\bar{y}-\hat{\beta}_{1} \bar{x}\)</span>, we have <span class="math notranslate nohighlight">\(\bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \bar{x}\)</span>, i.e. the regression line always goes through the mean <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span> of the sample.</p>
<p>This also hold for multiple regression, by the first order condition w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</div>
</li>
<li><p><em>What if the mean of the error term is not zero? Can you write down an equivalent model?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>If <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = \mu_\varepsilon \ne 0\)</span>, we can just denote <span class="math notranslate nohighlight">\(\varepsilon = \mu_\varepsilon + v\)</span>, where <span class="math notranslate nohighlight">\(v\)</span> is a new error term with zero mean. Our model becomes</p>
<div class="math notranslate nohighlight">
\[
    y_i = (\beta_0 + \mu_\varepsilon) + \beta_1 x_1 + v
    \]</div>
<p>where <span class="math notranslate nohighlight">\((\beta_0 + \mu_\varepsilon)\)</span> is the new intercept. We can still apply the methods above to conduct estimation and inference.</p>
</div>
</li>
<li><p><em>Assume the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> in the model <span class="math notranslate nohighlight">\(y=\beta_0 + \beta_1 x + \varepsilon\)</span> is zero. Find the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span>, denoted <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span>. Find its mean, variance, and compare them with those of the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span> when there is an intercept term.</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>If there is no intercept, consider a simple case</p>
<div class="math notranslate nohighlight">
\[
    y_i = \beta x_i + \varepsilon_i
    \]</div>
<p>Then by minimizing sum of squared errors</p>
<div class="math notranslate nohighlight">
\[
    \min \sum_i (y_i - \beta x_i)^2
    \]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
    -2 \sum_i (y_i - \beta x_i) x_i = 0
    \]</div>
<p>and hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde{\beta}
    &amp;= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \\
    &amp;= \frac{\sum_i x_i (\beta x_i + \varepsilon_i)}{\sum_i x_i^2}\\
    &amp;= \beta + \frac{\sum x_i \varepsilon_i}{\sum_i x_i^2}
    \end{align}\end{split}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> is still an unbiased estimator of <span class="math notranslate nohighlight">\(\beta\)</span>, while its variance is smaller than the variance calculated assuming the intercept is non-zero.</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left( \tilde{\beta} \right) = \frac{\sigma^2}{\sum x_i^2} \le  \frac{\sigma^2}{\sum (x_i - \bar{x})^2} = \operatorname{Var}\left( \hat{\beta}  \right)
    \]</div>
<p>Hence, we conclude that</p>
<ul class="simple">
<li><p>if the intercept is known to be zero, better use <span class="math notranslate nohighlight">\(\tilde\beta\)</span> instead of <span class="math notranslate nohighlight">\(\hat\beta\)</span>, since the standard error of the <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is smaller, and both are unbiased.</p></li>
<li><p>If the true model has a non-zero intercept, then <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is biased for <span class="math notranslate nohighlight">\(\beta\)</span>, but it has a smaller variance, which brings a tradeoff of bias vs variance.</p></li>
</ul>
</div>
</li>
<li><p><em>What happen to <span class="math notranslate nohighlight">\(\beta\)</span>, its standard error, and its p-value, if we scale the <span class="math notranslate nohighlight">\(j\)</span>-th covariate <span class="math notranslate nohighlight">\(x_j\)</span>, or add a constant to <span class="math notranslate nohighlight">\(x_j\)</span>? How about if we change <span class="math notranslate nohighlight">\(Y\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>In short, for an affine transformation on <span class="math notranslate nohighlight">\(x_j\)</span> or <span class="math notranslate nohighlight">\(Y\)</span>, since the column space of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and the direction of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> are unchanged, the overall fitting should be unchanged, such as <span class="math notranslate nohighlight">\(R^2\)</span>, <span class="math notranslate nohighlight">\(t\)</span>-test and <span class="math notranslate nohighlight">\(F-test\)</span>. The estimates (coefficients, residuals) may change.</p>
<p>One can re-write the model and compare with the original one. Suppose the original model is</p>
<div class="math notranslate nohighlight">
\[
    Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_j x_j + \varepsilon
    \]</div>
<p>Let <span class="math notranslate nohighlight">\(x_j ^\prime = ax_j + b\)</span>, and let <span class="math notranslate nohighlight">\(\gamma_j\)</span> be the new slope, <span class="math notranslate nohighlight">\(\gamma_0\)</span> be the new intercept, and <span class="math notranslate nohighlight">\(u\)</span> be the new error term.</p>
<div class="math notranslate nohighlight">
\[
    Y = \gamma_0 + \gamma_1 x_1 + \ldots + \gamma_j (ax_j + b) + u
    \]</div>
<p>Comparing the two models, we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \gamma_j &amp;= \frac{1}{a} \beta_j  \\
    \gamma_0 &amp;= \beta_0 - \gamma_j b \\
    &amp;= \beta_0 - \beta_j \frac{b}{a}  \\
    \end{aligned}\end{split}\]</div>
<p>Others slope and the error term are unchanged.</p>
<p>The estimated variance becomes</p>
<div class="math notranslate nohighlight">
\[
    \widehat{\operatorname{Var}}(\hat{\gamma}_j) = \hat{\sigma}^2 \frac{1}{1-R_j^2} \frac{1}{\sum (x ^\prime - \bar{x} ^\prime)^2} = \frac{1}{a^2}  \widehat{\operatorname{Var} }(\hat{\beta}_j)
    \]</div>
<p>Hence, the standard error is <span class="math notranslate nohighlight">\(\operatorname{se}(\hat{\gamma}_j) = \operatorname{se}(\hat{\beta}_j)\)</span> and the <span class="math notranslate nohighlight">\(t\)</span>-test statistic is</p>
<div class="math notranslate nohighlight">
\[
    \frac{\hat{\gamma}_j}{\operatorname{se}(\hat{\gamma}_j) } = \frac{\beta_j/a}{\operatorname{se}(\hat{\beta}_j)/a}   =  \frac{\beta_j}{\operatorname{se}(\hat{\beta}_j)}
    \]</div>
<p>which is unchanged as expected.</p>
<p>For the case <span class="math notranslate nohighlight">\(Y ^\prime = c Y + d\)</span>, it is easy to write</p>
<div class="math notranslate nohighlight">
\[
    cY + d = \gamma_0 + \gamma_1 x_1 + \ldots + \gamma_j x_j + u
    \]</div>
<p>and we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \gamma_j &amp;= c \beta_j \quad \forall j\\
    \gamma_0 &amp;= c \beta_0 + d\\
    \end{aligned}\end{split}\]</div>
<p>The residuals are scaled by <span class="math notranslate nohighlight">\(c\)</span> such that the standard error is scaled by <span class="math notranslate nohighlight">\(c\)</span> too. Finally, the <span class="math notranslate nohighlight">\(t\)</span>-test statistic remains unchanged.</p>
<p>The takeaway is that, one can scale the variable to a proper unit for better interpretation.</p>
</div>
</li>
<li><p><em>True or False: In SLR, exchange <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, the new slope estimate equals the reciprocal of the original one</em>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>False.</p>
<p>Since <span class="math notranslate nohighlight">\(\hat{\beta}_1 = r_{X,Y}\frac{s_Y}{s_X}\)</span>, the new slope estimate is <span class="math notranslate nohighlight">\(\hat{\gamma}_1 = r_{X,Y}\frac{s_X}{s_Y}\)</span>. We only have <span class="math notranslate nohighlight">\(\hat{\beta}_1 \hat{\gamma}_1 = r_{X,Y}^2 = R^2\)</span>. The last equality holds in SLR, see <a class="reference internal" href="12-lm-inference.html#lm-rsquared"><span class="std std-ref">proof</span></a>.</p>
<p>More analysis:</p>
<ul>
<li><p>Since in this case <span class="math notranslate nohighlight">\(F\)</span>-test depends only on <span class="math notranslate nohighlight">\(R^2\)</span> (<a class="reference internal" href="12-lm-inference.html#lm-f-test"><span class="std std-ref">proof</span></a>), then the <span class="math notranslate nohighlight">\(F\)</span>-test are the same.</p></li>
<li><p>Since in this case <span class="math notranslate nohighlight">\(F\)</span>-test is equivalent to <span class="math notranslate nohighlight">\(t\)</span>-test (<a class="reference internal" href="12-lm-inference.html#lm-f-test"><span class="std std-ref">proof</span></a>), the <span class="math notranslate nohighlight">\(t\)</span>-test for <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> and <span class="math notranslate nohighlight">\(\hat{\gamma}_1\)</span> are the same.</p></li>
<li><p>Hence, we have</p>
<div class="math notranslate nohighlight">
\[
        \frac{\sqrt{\hat{\sigma}_1^2 / s_X^2}}{\sqrt{\hat{\sigma}_2^2 / s_Y^2}} =  \frac{\operatorname{se}(\hat{\beta}_1)}{\operatorname{se}(\hat{\gamma_1})} = \frac{\hat{\beta}_1}{\hat{\gamma_1}} = \frac{s_Y^2}{s_X^2}
        \]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
        \frac{\hat{\sigma}_1}{\hat{\sigma}_2} = \frac{s_Y}{s_X} = \sqrt{\frac{\hat{\beta}_1}{\hat{\gamma_1}}}
        \]</div>
</li>
</ul>
</div>
</li>
<li><p><em>True or False: if <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y, X_j \right) = 0\)</span> then <span class="math notranslate nohighlight">\(\beta_j= 0\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>In SLR, this is true, but in MLR, this is generally not true. See <a class="reference internal" href="12-lm-inference.html#lm-rss-nonincreasing"><span class="std std-ref">here</span></a> for explanation.</p>
</div>
</li>
<li><p><em>What affect estimation precision?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Recall</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \operatorname{Var}\left(\hat{\beta}_{j}\right) &amp;=\sigma^{2}\left[\left(\boldsymbol{X}^{\top} \boldsymbol{X}\right)^{-1}\right]_{[j, j]} \\
    &amp;=\sigma^{2} \frac{1}{1-R_{j}^{2}} \frac{1}{\sum_{i}\left(x_{i j}-\bar{x}_{j}\right)^{2}}
    \end{aligned}
    \end{split}\]</div>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the linear relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{j}\)</span>, and hence decrease the variance.</p></li>
</ul>
</div>
</li>
<li><p><em>To compare the effects of two variable <span class="math notranslate nohighlight">\(X_j, X_k\)</span>, can we say they have the same effect since the confidence interval of <span class="math notranslate nohighlight">\(\beta_j, \beta_k\)</span> overlaps?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>No, since</p>
<ul class="simple">
<li><p>the two coefficients are probably correlated <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \boldsymbol{\beta} _j, \beta_k \right) \ne 0\)</span></p></li>
<li><p>even if they are not correlated, we still need to find a pivot quantity for <span class="math notranslate nohighlight">\(\theta = \beta_j - \beta_k\)</span> and conduct a hypothesis testing on <span class="math notranslate nohighlight">\(\theta=0\)</span>. See the <a class="reference internal" href="12-lm-inference.html#lm-t-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(t\)</span>-test section</span></a>.</p></li>
</ul>
</div>
</li>
<li><p><em>Does the partialling out method holds for <span class="math notranslate nohighlight">\(p \ge 3\)</span>?</em> Yes.</p></li>
<li><p><em>How do you compare two linear models?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F\)</span>-test if nested</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared if same number of covariates (essentially comparing <span class="math notranslate nohighlight">\(RSS\)</span>)</p></li>
<li><p>adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared</p></li>
<li><p>Log-likelihood</p></li>
<li><p>out-of-sample prediction</p></li>
</ul>
</div>
</li>
<li><p><em>What happens if you exclude a relevant regressor <span class="math notranslate nohighlight">\(X_j\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>Assumes the word “relevant” here means <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> is significant.</p>
<ul class="simple">
<li><p>lower fitting performance, i.e. RSS increases,</p></li>
<li><p>lower predicting performance since we exclude a good predictor</p></li>
<li><p>probably higher standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\hat{\beta}_k)\)</span> since <span class="math notranslate nohighlight">\(RSS\)</span> increases (also need to look at how <span class="math notranslate nohighlight">\(1-R_k^2\)</span> increases)</p></li>
<li><p>bias estimates of other covariates <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\beta}_k \right)\)</span>, if the omitted variable has linear relation with them (<span class="math notranslate nohighlight">\(\alpha_k \ne 0\)</span>)</p></li>
</ul>
<p>See detailed <a class="reference internal" href="13-lm-diagnosis.html#lm-omit-variable"><span class="std std-ref">discussion</span></a>.</p>
</div>
</li>
<li><p><em>What happens if you include an irrelevant regressor <span class="math notranslate nohighlight">\(X_j\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Slution</em></p>
<p>Assume the word “irrelevant” means <span class="math notranslate nohighlight">\(\beta_j=0\)</span>.</p>
<ul class="simple">
<li><p>slightly higher fitting performance since <span class="math notranslate nohighlight">\(RSS\)</span> decreases</p></li>
<li><p>lower predicting performance, since we fit noise as a variable</p></li>
<li><p>probably higher standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\hat{\beta}_k)\)</span> since <span class="math notranslate nohighlight">\((1-R_k^2)\)</span> decreases (also need to look at how <span class="math notranslate nohighlight">\(RSS\)</span> decreases)</p></li>
<li><p>will NOT bias estimates of other covariates <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\beta}_k \right)\)</span>, since <span class="math notranslate nohighlight">\(\beta_j=0\)</span>.</p></li>
</ul>
<p>See detailed <a class="reference internal" href="13-lm-diagnosis.html#lm-add-variable"><span class="std std-ref">discussion</span></a>.</p>
</div>
</li>
<li><p><em>Describe missing data problems in linear regression</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<ul class="simple">
<li><p>if completely at random, then it amounts to a smaller sample. larger standard error, but still unbiased.</p></li>
<li><p>if missing depends on values of <span class="math notranslate nohighlight">\(x\)</span> (Exogenous Sample Selection)</p>
<ul>
<li><p>if the true relation is linear, then no problem. still unbiased since the slope of the hyperplane is unchanged. the standard error changes inversely according to how <span class="math notranslate nohighlight">\(\operatorname{Var}\left( X \right)\)</span> change.</p></li>
<li><p>if the true relation is not linear, then we will get quite different estimates</p></li>
</ul>
</li>
<li><p>if missing depends on <span class="math notranslate nohighlight">\(Y\)</span> (Endogenous sample selection)</p>
<ul>
<li><p>if the true relation is linear, then we may have biased estimates.</p></li>
</ul>
</li>
</ul>
<p>See detailed <a class="reference internal" href="13-lm-diagnosis.html#lm-missing-values"><span class="std std-ref">discussion</span></a></p>
</div>
</li>
<li><p><em>Does <span class="math notranslate nohighlight">\(\boldsymbol{x}_{p} ^\top \boldsymbol{y} = 0 \Leftrightarrow \hat{\beta}_{p}=0\)</span>?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>In general, NO. Whether <span class="math notranslate nohighlight">\(\boldsymbol{x}_{p} ^\top \boldsymbol{y} = 0\)</span> depends only on these two vectors, but whether <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span> depends on existing covariates in the model. See detailed <a class="reference internal" href="12-lm-inference.html#lm-rss-nonincreasing"><span class="std std-ref">discussion</span></a>.</p>
</div>
</li>
<li><p>Causal?</p>
<p>313.qz1.q2</p>
<p>TBD.</p>
</li>
<li><p>Add/Remove an observation</p>
<p>E(b), Var(b), RSS, TSS, R^2</p>
</li>
<li><p>More</p>
<p>https://www.1point3acres.com/bbs/thread-703302-1-1.html</p>
</li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="00-regression.html" title="previous page">Regression</a>
    <a class='right-next' id="next-link" href="12-lm-inference.html" title="next page">Linear Models - Inference</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>