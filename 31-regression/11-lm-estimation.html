
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models - Estimation &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression - Inference" href="12-lm-inference.html" />
    <link rel="prev" title="Regression" href="00-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-lm-inference.html">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-lm-extension.html">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-t-SNE.html">
     SNE and $t$-SNE
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     K-means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/11-lm-estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/11-lm-estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/11-lm-estimation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/11-lm-estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation">
   Estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary-least-squares">
     Ordinary Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#by-assumptions">
     By Assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood">
     Maximum Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-of-estimated-coefficients">
     Value of Estimated Coefficients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     $R$-squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partialling-out-explanation-for-mlr">
     Partialling Out Explanation for MLR
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models-estimation">
<h1>Linear Models - Estimation<a class="headerlink" href="#linear-models-estimation" title="Permalink to this headline">¶</a></h1>
<p>From this section we introduce linear models from a statistics’ perspective. There are three sections in total.</p>
<ol class="simple">
<li><p>The first section covers the model fundamentals, including assumptions, estimation, interpretation and some exercise.</p></li>
<li><p>The next section introduce statistical inference for linear models, such as distribution of the estimated coefficients, $t$-test, $F$-test, etc. Usually machine learning community focuses more on prediction, less on inference. But inference does matters. It analyzes how important each variable is in the model from a rigorous approach.</p></li>
<li><p>The third section introduce some issues in linear models, e.g. omitted variables bias, multicollinearity, heteroscedasticity, and some alternative models, e.g. Lasso, ridge regression, etc.</p></li>
</ol>
<p>:::{admonition,dropdown,note} Statistics’ perspective vs social science’s perspective</p>
<p>The introduction from econometrics’ perspective or social science’s perspective may be different. In short, the statistics’ perspective focuses on general multivariate cases and heavily rely on linear algebra for derivation, while the econometrics’ or the social science’s perspective prefers to introduce models in univariate cases by basic arithmetics (whose form can be complicated without linear algebra notations) and extend the intuitions and conclusions into multivariate cases.</p>
<p>Personally, I involved in four courses that introduced linear models, i.e. at undergrad/grad level offered by stat/social science department. The style of the two courses offered by the stat departments were quite alike while the graduate level one covered more topics. In both undergrad/grad level courses offered by the social science departments, sometimes I got confused by the course materials that were contradictory to my statistics training , but the instructors had no clear response or even no response at all…</p>
<p>In sum, to fully understand the most fundamental and widely used statistical model, I highly suggest to take a linear algebra course first and take the regression course offered by math/stat department.</p>
<p>:::</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Linear models aim to model the relationship between a scalar response and one or more explanatory variables in a linear format:</p>
<p>$$Y_i  = \beta_0 + \beta_1 x_{i,1} + \ldots + \beta_{p-1} x_{i,p-1}  + \varepsilon_i $$</p>
<p>for observations $i=1, 2, \ldots, n$.</p>
<p>In matrix form,</p>
<p>$$
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
$$</p>
<p>where</p>
<ul class="simple">
<li><p>$\boldsymbol{X}<em>{n\times p}$ is called the design matrix. The first column is usually set to be $\boldsymbol{1}$, i.e., intercept. The remaining $p-1$ columns are designed values $x</em>{ij}$ where $i = 1, 2, \ldots, n$ and $j=1, \ldots, p-1$. These $p-1$ columns are called explanatory/independent variables, or covariates.</p></li>
<li><p>$\boldsymbol{y}_{n \times 1}$ is a vector of response/dependent variables $Y_1, Y_2, \ldots, Y_n$.</p></li>
<li><p>$\boldsymbol{\beta}_{p \times 1}$ is a vector of coefficients to be estimated.</p></li>
<li><p>$\boldsymbol{\varepsilon}_{n \times 1}$ is a vector of unobserved random errors, which includes everything that we have not measured and included in the model.</p></li>
</ul>
<p>When $p=2$, we have</p>
<p>$$
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$</p>
<p>which is called <strong>simple linear regression</strong>.</p>
<p>When $p&gt;2$, it is called <strong>multiple linear regression</strong>. For instance, when $p=3$</p>
<p>$$
Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon_i
$$</p>
<p>When there are multiple dependent variables, we call it <strong>multivariate regression</strong>, which will be introduced in another section.</p>
<p>When $p=1$,</p>
<ul class="simple">
<li><p>if we include intercept, then the regression model $y_i = \beta_0$ means that we use a single constant to predict $y_i$. The estimator, $\hat{\beta}_0$, by ordinary least square, should be the sample mean $\hat{y}$.</p></li>
<li><p>if we do not include intercept, then the regression model $y_i = \beta x_i$ means that we expect that $y$ is proportional to $x$.</p></li>
</ul>
<p>:::{admonition,dropdown,note} Fixed or random $\boldsymbol{X}$?
In natural science, researchers design $n\times p$ values in the design matrix $\boldsymbol{X}$ and run experiments to obtain the response $y_i$. We call this kind of data <strong>experimental data</strong>. In this sense, the explanatory variables $x_{ij}$’s are designed before the experiment, so they are also constants. The coefficients $\beta_j$’s are unknown constants. The error term $\varepsilon_i$ is random. The response variable $Y_i$ on the left hand side is random due to the randomness in the error term $\varepsilon_i$.</p>
<p>In social science, most of data is <strong>observational data</strong>. That is, researchers obtain the values of many variables at the same time, and choose one of interest to be the response variable $y_i$ and some others to be the explanatory variables $\boldsymbol{x}_i$. In this case, $\boldsymbol{X}$ is viewed as a data set, and we can talk about descriptive statistics, such as variance of each explanatory variable, or covariance between pair of explanatory variables. This is valid since we often view the columns of a data set as random variables.</p>
<p>However, the inference methods of the coefficients $\boldsymbol{\beta}$ are developed based on the natural science setting, i.e., the values of explanatory variables are pre-designed constants. Many social science courses frequently use descriptive statistics of the explanatory variables which assumes they are random, and apply inference methods which assumes they are constant. This is quite confusing for beginners to linear models.</p>
<p>To be clear, we stick to the natural science setting and make the second assumption below. We use subscript $i$ in every $y_i, x_i, \varepsilon_i$ instead of $y, x, \varepsilon$ which gives a sense that $x$ is random. And we use descriptive statistics for the explanatory variables only when necessary.
:::</p>
</div>
<div class="section" id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h2>
<p>Basic assumptions</p>
<ol>
<li><p>$\operatorname{E}\left( y_i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}$ is <strong>linear</strong> in covariates $X_j$.</p></li>
<li><p>The values of explanatory variables $\boldsymbol{x}_i$ are known and fixed. Randomness only comes from $\varepsilon_i$.</p></li>
<li><p>No $X_j$ is constant for all observations. No exact linear relationships among the explanatory variables (aka no perfect multicollinearity, or the design matrix $\boldsymbol{X}$ is of full rank).</p></li>
<li><p>The error terms are uncorrelated $\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right)= 0$, with common mean $\operatorname{E}\left( \varepsilon_i \right) = 0$ and variance $\operatorname{Var}\left( \varepsilon_i \right) = \sigma^2$ (homoskedasticity).</p>
<p>As a result, $\operatorname{E}\left( \boldsymbol{y} \mid \boldsymbol{X} \right) = \boldsymbol{X} \boldsymbol{\beta}$, or $\operatorname{E}\left( y_i \mid x_i \right) = \beta_0 + \beta_1 x_i$ when $p=2$, which can be illustrated by the plots below.</p>
<p>:::{figure} lm-distribution-of-y-given-x
<img src="../imgs/lm-cond-distribution.png" width = "40%" alt=""/></p>
<p>Distributions of $y$ given $x$ [Meyer 2021]
:::</p>
<p>:::{figure} lm-observation-of-y-given-x
<img src="../imgs/lm-xyplane-dots.png" width = "50%" alt=""/></p>
<p>Observations of $y$ given $x$ [Meyer 2021]
:::</p>
<p>To predict $\hat{y}_i$, we just use $\hat{y}_i = \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}$ .</p>
</li>
<li><p>The error terms are independent and follow Gaussian distribution $\varepsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)$, or $\boldsymbol{\varepsilon} \sim N_n (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _n)$.</p>
<p>As a result, we have $Y_i \sim N(\boldsymbol{x}_i ^\top \boldsymbol{\beta} , \sigma^2 )$ or $\boldsymbol{y} \sim N_n(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} _n)$</p>
</li>
</ol>
<p>These assumptions are used for different objectives. The first 3 assumptions are the base, and in additiona to them,</p>
<ul class="simple">
<li><p>derivation of $\hat{\boldsymbol{\beta}}$ by least squares uses no more assumptions.</p></li>
<li><p>derivation of $\hat{\boldsymbol{\beta}}$ by maximal likelihood uses assumptions 4 and 5.</p></li>
<li><p>derivation of $\operatorname{E}\left( \hat{\boldsymbol{\beta}} \right)$ uses $\operatorname{E}\left( \varepsilon_i \right) = 0$ in 4.</p></li>
<li><p>derivation of $\operatorname{Var}\left( \hat{\boldsymbol{\beta}} \right)$ uses 1, 2, $\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right) = 0$ and $\operatorname{Var}\left( \epsilon_i \right) = \sigma^2$ in 4.</p></li>
<li><p>proof of Gaussian-Markov Theorem (BLUE) uses 4.</p></li>
<li><p>derivation of the distribution of $\hat{\boldsymbol{\beta} }$ uses 4 and 5.</p></li>
</ul>
<p>:::{admonition,dropdown,note} Zero conditional mean assumption
In some social science or econometrics courses, they follow the “Gauss-Markov assumptions” that are roughly the same to the assumptions, but in different formats. One of them is zero conditional mean assumption.</p>
<p>In general, it says</p>
<p>$$
\operatorname{E}\left( \varepsilon \mid x_1, x_2, \ldots, x_p\right) = 0
$$</p>
<p>For $p=2$, it is</p>
<p>$$\operatorname{E}\left( \varepsilon \mid x  \right) = 0$$</p>
<p>which (in their setting) implies</p>
<p>$$\begin{align}
\operatorname{E}\left( \varepsilon \right)
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon \mid x \right) \right)\
&amp;= 0\
\operatorname{Cov}\left( \varepsilon, x \right)
&amp;= \operatorname{E}\left( \varepsilon x \right) - \operatorname{E}\left( \varepsilon \right)\operatorname{E}\left( x \right)\
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon x \mid x \right) \right)- 0 \times \operatorname{E}\left( x \right)\
&amp;= \operatorname{E}\left( x \operatorname{E}\left( \varepsilon \mid x \right) \right) \
&amp;= 0
\end{align}$$</p>
<p>Then they these two corollaries are used for <a class="reference internal" href="#lm-estimation-by-assumpation"><span class="std std-ref">estimation</span></a>.</p>
<p>As discussed above, in their setting $x$ is random (at this stage), so they use notations such as $\operatorname{E}\left( \varepsilon \mid x \right)$ and $\operatorname{Cov}\left( x, \varepsilon \right)$. It also seems that they view $\varepsilon$ as an “overall” measure of random error, instead of $\varepsilon_i$ for specific $i$ in the natural science setting. But they can mean so by using the conditional notation $\operatorname{E}\left( \varepsilon \mid x \right)$.
:::</p>
</div>
<div class="section" id="estimation">
<h2>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h2>
<p>We introduce various methods to estimate the parameters $\boldsymbol{\beta}$ and $\sigma^2$.</p>
<div class="section" id="ordinary-least-squares">
<h3>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<p>The most common way is to estimate the parameter $\hat{\boldsymbol{\beta}}$ by minimizing the sum of squared errors $\sum_i(y_i-\hat{y}_i)^2$.</p>
<div class="margin sidebar">
<p class="sidebar-title">A note on substitution</p>
<p>We substitute the predicted $\hat{\boldsymbol{y} }$ by $\boldsymbol{X} \boldsymbol{\beta}$. The $\boldsymbol{\beta}$ here just means a variable in the optimization problem, not the unknown constant coefficients in our model.</p>
</div>
<p>$$\begin{align}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} , \left\Vert \boldsymbol{y}  - \hat{\boldsymbol{y}}  \right\Vert ^2 \
&amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} , \left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 \
\end{align}$$</p>
<p>The gradient w.r.t. $\boldsymbol{\beta}$ is</p>
<p>$$\begin{align}
\nabla_{\boldsymbol{\beta}} &amp;= -2 \boldsymbol{X}  ^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )  \
&amp;\overset{\text{set}}{=} \boldsymbol{0}
\end{align}$$</p>
<p>Hence, we have</p>
<p>$$
\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}
$$</p>
<p>This linear system is called the <strong>normal equation</strong>.</p>
<p>The closed form solution is</p>
<p>$$\hat{\boldsymbol{\beta}} = \left( \boldsymbol{X} ^\top \boldsymbol{X}   \right)^{-1}\boldsymbol{X} ^\top  \boldsymbol{y}  $$</p>
<p>Note that $\hat{\boldsymbol{\beta}}=(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \boldsymbol{y}$ is a random variable, since it is a linear combination of the random vector $\boldsymbol{y}$. This means that, keeping $\boldsymbol{X}$ fixed, repeat the experiment, we will probably get different response values $\boldsymbol{y}$, and hence different $\hat{\boldsymbol{\beta}}$. As a result, there is a sampling distribution of $\hat{\boldsymbol{\beta}}$, and we can find its mean, variance, and conduct hypothesis testing.</p>
<p>::::{admonition,dropdown,tip} View least squares as projection
Substitute the solve $\hat{\boldsymbol{\beta}}$ into the prediction $\hat{\boldsymbol{y}}$ we have</p>
<p>$$
\hat{\boldsymbol{y}} = \boldsymbol{X} \hat{\boldsymbol{\beta}} = \underbrace {\boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top}_{\boldsymbol{H}} \boldsymbol{y}
$$</p>
<p>Here $\boldsymbol{H}$ is a projection matrix onto the column space (image) of $\boldsymbol{X}$. Recall that a projection matrix onto the column span of a matrix $\boldsymbol{X}$ has the form $\boldsymbol{P} _{\operatorname{col}(\boldsymbol{X} )} = \boldsymbol{X} \boldsymbol{X} ^\dagger$ where $\boldsymbol{X} ^\dagger =  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top$ is the pseudo inverse of $\boldsymbol{X}$.</p>
<p>Essentially, we are trying to find a vector $\hat{\boldsymbol{y}}$ in the column space of the data matrix $\boldsymbol{X}$ that is as close to $\boldsymbol{y}$ as possible, and the closest one is just the projection of $\boldsymbol{y}$ onto $\operatorname{col}(\boldsymbol{X})$, which is $\boldsymbol{H}\boldsymbol{y}$. The distance is measured by the norm $\left| \boldsymbol{y} - \hat{\boldsymbol{y}}  \right|$, which is the squared root of sum of squared errors. Note that $\boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X}) ^ \bot$ since $\boldsymbol{I} - \boldsymbol{H} = \boldsymbol{I}  - \boldsymbol{P}<em>{\operatorname{col}(\boldsymbol{X}) } = \boldsymbol{P}</em>{\operatorname{col}(\boldsymbol{X}) ^ \bot}$ is the projection matrix onto the orthogonal complement $\operatorname{col}(\boldsymbol{X}) ^ \bot$.</p>
<p>:::{figure} lm-projection
<img src="../imgs/lm-projection.png" width = "50%" alt=""/></p>
<p>Least squares as a projection <a class="reference external" href="https://waterprogramming.wordpress.com/2017/05/12/an-introduction-to-econometrics-part-1-classical-ordinary-least-squares-regression/">[Gold 2017]</a>
:::</p>
<p>::::</p>
<p>:::{admonition,dropdown,note} Solving the linear system by software
Computing software use specific functions to solve the normal equation $\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}$ for $\boldsymbol{\beta}$, instead of using the inverse $(\boldsymbol{X} ^\top \boldsymbol{X}) ^{-1}$ directly which can be slow and numerically unstable. For instance, one can use QR factorization of $X$,</p>
<p>$$
\boldsymbol{X} = \boldsymbol{Q} \left[\begin{array}{l}
\boldsymbol{R}<em>{p \times p}  \
\boldsymbol{0}</em>{(n-p) \times p}
\end{array}\right]
$$</p>
<p>Hence,</p>
<p>$$
\begin{aligned}
| \boldsymbol{y} - \boldsymbol{X}  \boldsymbol{\beta}  |^{2}
&amp;=\left|\boldsymbol{Q} ^{\top} \boldsymbol{y}  - \boldsymbol{Q} ^\top \boldsymbol{X} \boldsymbol{\beta}  \right|^{2} \
&amp;=\left|\left(\begin{array}{c}
\boldsymbol{f}  \
\boldsymbol{r}
\end{array}\right)-\left(\begin{array}{c}
\boldsymbol{R} \boldsymbol{\beta}  \
\boldsymbol{0}
\end{array}\right)\right|^{2} \
&amp;=|\boldsymbol{f} - \boldsymbol{R} \boldsymbol{\beta} |^{2}+|\boldsymbol{r} |^{2}
\end{aligned}
$$</p>
<p>Finally</p>
<p>$$
\boldsymbol{\beta} = \boldsymbol{R} ^{-1} \boldsymbol{f}
$$
:::</p>
<p>An unbiased estimator of the error variance $\sigma^2 = \operatorname{Var}\left( \varepsilon \right)$ is (to be discussed [later])</p>
<p>$$
\hat{\sigma}^2 = \frac{\left\Vert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \right\Vert ^2}{n-p}
$$</p>
<p>When $p=2$, we have</p>
<p>$$\hat{\beta}_0, \hat{\beta}_1 =  \underset{\beta_0, \beta_1 }{\mathrm{argmin}} , \sum_i \left( y_i - \beta_0 - \beta_1 x_i \right)^2$$</p>
<p>Differentiation w.r.t. $\beta_1$ gives</p>
<p>$$</p>
<ul class="simple">
<li><p>2\sum_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0
$$</p></li>
</ul>
<p>Differentiation w.r.t. $\beta_0$ gives</p>
<p>$$</p>
<ul class="simple">
<li><p>2\sum_i (y_i - \beta_0 - \beta_1 x_i) = 0
$$</p></li>
</ul>
<p>Solve the system of the equations, we have</p>
<p>$$\begin{align}
\hat{\beta}<em>{1} &amp;=\frac{\sum</em>{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \
\hat{\beta}<em>{0} &amp;=\bar{y}-\hat{\beta}</em>{1} \bar{x}
\end{align}$$</p>
<p>The expression for $\hat{\beta}_0$ implies that the fitted line cross the sample mean point $(\bar{x}, \bar{y})$.</p>
<p>Moreover,</p>
<p>$$
\hat{\sigma}^2 = \frac{1}{n-2} \sum_i \hat\varepsilon_i^2
$$</p>
<p>where $\hat\varepsilon_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i$.</p>
<p>:::{admonition,note} Minimizing mean squared error
The objective function, <strong>sum of squared errors</strong>,</p>
<p>$$
\left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 = \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
$$</p>
<p>can be replaced by <strong>mean squared error</strong>,</p>
<p>$$
\frac{1}{n} \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
$$</p>
<p>and the results are the same.
:::</p>
</div>
<div class="section" id="by-assumptions">
<span id="lm-estimation-by-assumpation"></span><h3>By Assumptions<a class="headerlink" href="#by-assumptions" title="Permalink to this headline">¶</a></h3>
<p>In some social science courses, the estimation is done by using the assumptions</p>
<ul class="simple">
<li><p>$\operatorname{E}\left( \varepsilon \right) = 0$</p></li>
<li><p>$\operatorname{E}\left( \varepsilon \mid X \right) = 0$</p></li>
</ul>
<p>The first one gives</p>
<p>$$
\frac{1}{n}  \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}<em>{0}-\hat{\beta}</em>{1} x_{i}\right)=0
$$</p>
<p>The second one gives</p>
<p>$$\begin{align}
\operatorname{Cov}\left( X, \varepsilon \right)
&amp;= \operatorname{E}\left( X \varepsilon \right) - \operatorname{E}\left( X \right) \operatorname{E}\left( \varepsilon \right) \
&amp;= \operatorname{E}\left[ \operatorname{E}\left( X \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\
&amp;= \operatorname{E}\left[ X \operatorname{E}\left( \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\
&amp;= 0
\end{align}$$</p>
<p>which gives</p>
<p>$$
\frac{1}{n}  \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}<em>{0}-\hat{\beta}</em>{1} x_{i}\right)=0
$$</p>
<p>Therefore, we have the same normal equations to solve for $\hat{\beta}_0$ and $\hat{\beta}_1$.</p>
<p>:::{admonition,warning} Warning</p>
<p>Estimation by the two assumptions derived from the zero conditional mean assumption can be problematic. Consider a model without intercept $y_i = \beta x_i + \varepsilon_i$. Fitting by OLS, we have only ONE first order condition</p>
<p>$$
\sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}<em>{1} x</em>{i}\right)=0
$$</p>
<p>If we fit by assumptions, then in addition to the condition above, the first assumption $\operatorname{E}\left( \varepsilon \right) = 0$ also gives</p>
<p>$$
\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}<em>{1} x</em>{i}\right)=0
$$</p>
<p>These two conditions may not hold at the same time.
:::</p>
</div>
<div class="section" id="maximum-likelihood">
<h3>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<p>biased. TBD.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="value-of-estimated-coefficients">
<h3>Value of Estimated Coefficients<a class="headerlink" href="#value-of-estimated-coefficients" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">Non-linear term in linear models.</p>
<p>“Linear” model is linear in predictors. A predictor can be a non-linear feature of inputs $x_1, \ldots, x_p$, say $x_1^2, ln^{x_1}, x_1 x_2, $ etc.</p>
</div>
<ol>
<li><p>For a model in a standard form,</p>
<ul class="simple">
<li><p>$\beta_j$ is the expected change in the value of the response variable $y$ if the value of the covariate $x_j$ increases by 1, holding other covariates fixed.</p></li>
<li><p>$\beta_0$ is the expected value of the response variable $y$ if all covariates have values of zero.</p></li>
</ul>
</li>
<li><p>If the covariate is an interaction term, for instance,</p>
<p>$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \varepsilon
$$</p>
<p>Then $(\hat{\beta}<em>1 + \hat{\beta}</em>{12}x_2)$ can be interpreted as the estimated effect of one unit change in $x_1$ on $Y$ given a fixed value $x_2$. Usually $x_2$ is an dummy variable, say gender.</p>
<p>In short, we build this model because we believe the effect of $x_1$ on $Y$ depends on $x_2$.</p>
<p>Often higher-order interactions are added after lower-order interactions are included.</p>
</li>
<li><p>For polynomial covariates, say,</p>
<p>$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_{3} x_1^3 + \varepsilon
$$</p>
<p>the interpretation of the marginal effect of $x_1$ is simply the partial derivative $\beta_1 + 2\beta_2 x_1 + 3 \beta_3 x_1^2$. We build such model because the plot suggests a non-linear relation between $Y$ and $x_1$, or we believe the effect of $x_1$ on $Y$ depends on the value of $x_1$. Note in this case the effect can change sign.</p>
</li>
<li><p>For a model that involves log, we need some approximation.</p>
<ul>
<li><p>$Y = \beta_0 + \beta_1 \ln(x) + \mu$</p>
<p>$$\begin{aligned}
\ln(1+0.01)&amp;\approx 0.01\
\Rightarrow \quad \ln(x + 0.01x) &amp;\approx \ln(x)  + 0.01 \quad \forall x\
\end{aligned}$$</p>
<p>Hence, $0.01x$ change in $x$, or $1%$ change in $x$, is associated with $0.01\beta_1$ change in value of $Y$.</p>
</li>
<li><p>$\ln(Y) = \beta_0 + \beta_1 x + \mu$</p>
<p>$$\begin{aligned}
Y ^\prime
&amp;= \exp(\beta_0 + \beta_1 (x + 1) + \varepsilon)   \
&amp;= \exp(\beta_0 + \beta_1 + \varepsilon) \exp(\beta_1)  \
&amp;\approx Y (1 + \beta_1) \quad \text{if $\beta_1$ is close to 0}  \
\end{aligned}$$</p>
<p>Hence, $1$ unit change in $x$ is associated with $100\beta_1 %$ change in $Y$.</p>
</li>
<li><p>$\ln(Y) = \beta_0 + \beta_1 \ln(x) + \mu$</p>
<p>$$\begin{aligned}
Y ^\prime
&amp;= \exp(\beta_0 + \beta_1 \ln(x + 0.01x) + \varepsilon)   \
&amp;\approx \exp(\beta_0 + \beta_1 (\ln(x) + 0.01) + \varepsilon)   \
&amp;= \exp(\beta_0 + \beta_1 \ln(x) + \varepsilon)\exp(0.01\beta_1)   \
&amp;\approx Y (1 + 0.01\beta_1) \quad \text{if $0.01\beta_1$ is close to 0}  \
\end{aligned}$$</p>
<p>Hence, $1%$ change in $x$ is associated with $\beta_1 %$ change in $Y$, i.e. $\beta_1$ measures <strong>elasticity</strong>.</p>
<p>:::{admonition,note} When to use log?</p>
<p>Log is often used</p>
<ul class="simple">
<li><p>when the variable has a right skewed distribution, e.g. wages, prices</p></li>
<li><p>to reduce heteroskedasticity</p></li>
</ul>
<p>not used when</p>
<ul class="simple">
<li><p>the variable has negative values</p></li>
<li><p>the variable are in percentages or proportions (hard to interpret)</p></li>
</ul>
<p>Also note that logging can change significance tests.</p>
<p>:::</p>
</li>
</ul>
</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Linear regression models only reveal linear associations between the response variable and the independent variables. But association does not imply causation. Simple example: in SLR, regress $X$ over $Y$, the coefficient has same sign and significance, but causation cannot be reversed.</p>
<p>Only when the data is from a randomized controlled trial, correlation will imply causation.</p>
</div>
<p>We can measure if a coefficient is statistically significant by <a class="reference internal" href="12-lm-inference.html#lm-t-test"><span class="std std-ref">$t$-test</span></a>.</p>
</div>
<div class="section" id="r-squared">
<h3>$R$-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>We will introduce $R$-squared in detail in next section.</p>
<dl class="simple myst">
<dt>Definition ($R$-squared)</dt><dd><p>$R$-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
</dd>
</dl>
<p>$$
R^2 = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2}
$$</p>
</div>
<div class="section" id="partialling-out-explanation-for-mlr">
<h3>Partialling Out Explanation for MLR<a class="headerlink" href="#partialling-out-explanation-for-mlr" title="Permalink to this headline">¶</a></h3>
<p>We can interpret the coefficients in multiple linear regression from “partialling out” perspective.</p>
<p>When $p=3$, i.e.,</p>
<p>$$
\hat{y}=\hat{\beta}<em>{0}+\hat{\beta}</em>{1} x_{1}+\hat{\beta}<em>{2} x</em>{2}
$$</p>
<p>We can obtain $\hat{\beta}_1$ by the following three steps</p>
<ol>
<li><p>regress $x_1$ over $x_2$ and obtain</p>
<p>$$\hat{x}<em>{1}=\hat{\gamma}</em>{0}+\hat{\gamma}<em>{1} x</em>{2}$$</p>
</li>
<li><p>compute the residuals $\hat{u}_{1}$ in the above regression</p>
<p>$$
\hat{u}<em>{i} = x</em>{1i} - \hat{x}_{1i}
$$</p>
</li>
<li><p>regress $y$ on the the residuals $\hat{u}_{1}$, and the estimated coefficient equals the required coefficient.</p>
<p>$$\begin{align}
\hat{y}
&amp;=\hat{\alpha}<em>{0}+\hat{\alpha}</em>{1} \hat{u} \
\hat{\alpha}_{1}
&amp;= \frac{\sum (\hat{u}_i - \bar{\hat{u}}_i)(y_i - \bar{y})}{\sum (\hat{u}<em>i - \bar{\hat{u}}<em>i)^2} \
&amp;= \frac{\sum \hat{u}</em>{i}y_i}{\sum \hat{u}</em>{i}^2} \qquad \because \bar{\hat{u}}_i = 0\
&amp;\overset{\text{claimed}}{=} \hat{\beta}_1
\end{align}$$</p>
</li>
</ol>
<p>In this approach, $\hat{u}$ is interpreted as the part in $x_1$ that cannot be predicted by $x_2$, or is uncorrelated with $x_2$. We then regress $y$ on $\hat{u}$, to get the effect of $x_1$ on $y$ after $x_2$ has been “partialled out”.</p>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>SLR stands for simple linear regression $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i $</p>
<ol>
<li><p><em>In SLR, can you compute $\hat{\beta}<em>1$ from correlation $r</em>{X,Y}$ and standard deviations $s_X$ and $s_Y$?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>In SLR, we can see from the solution</p>
<p>$$\begin{align}
\hat{\beta}<em>{1} &amp;=\frac{\sum</em>{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\end{align}$$</p>
<p>that</p>
<p>$$\begin{align}
\hat{\beta}<em>1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \
&amp;= r</em>{X,Y} \frac{s_Y}{s_X}
\end{align}$$</p>
<p>Thus, the slope has the same sign with the correlation $r_{X,Y}$, and equals to the correlation times a ratio of the sample standard deviations of the dependent variable over the independent variable.</p>
<p>Once can see that the magnitude of $\hat\beta_1$ increases with the magnitude of $r_{X,Y}$ and $s_Y$, and decreases with $s_X$, holding others fixed.</p>
<p>:::</p>
</li>
<li><p><em>In SLR, can you compute $\bar{y}$ given $\hat{\beta}_0,\hat{\beta}_1$ and $\bar{x}$?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>Since $\hat{\beta}<em>{0} =\bar{y}-\hat{\beta}</em>{1} \bar{x}$, we have $\bar{y} = \hat{\beta}<em>{0} + \hat{\beta}</em>{1} \bar{x}$, i.e. the regression line always goes through the mean $(\bar{x}, \bar{y})$ of the sample.</p>
<p>This also hold for multiple regression, by the first order condition w.r.t. $\beta_0$.</p>
<p>:::</p>
</li>
<li><p><em>What if the mean of the error term is not zero? Can you write down an equivalent model?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>If $\operatorname{E}\left( \varepsilon \right) = \mu_\varepsilon \ne 0$, we can just denote $\varepsilon = \mu_\varepsilon + v$, where $v$ is a new error term with zero mean. Our model becomes</p>
<p>$$
y_i = (\beta_0 + \mu_\varepsilon) + \beta_1 x_1 + v
$$</p>
<p>where $(\beta_0 + \mu_\varepsilon)$ is the new intercept. We can still apply the methods above to conduct estimation and inference.</p>
<p>:::</p>
</li>
<li><p><em>Assume the intercept $\beta_0$ in the model $y=\beta_0 + \beta_1 x + \varepsilon$ is zero. Find the OLS estimate for $\beta_1$, denoted $\tilde{\beta}$. Find its mean, variance, and compare them with those of the OLS estimate for $\beta_1$ when there is an intercept term.</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>If there is no intercept, consider a simple case</p>
<p>$$
y_i = \beta x_i + \varepsilon_i
$$</p>
<p>Then by minimizing sum of squared errors</p>
<p>$$
\min \sum_i (y_i - \beta x_i)^2
$$</p>
<p>we have</p>
<p>$$
-2 \sum_i (y_i - \beta x_i) x_i = 0
$$</p>
<p>and hence,</p>
<p>$$\begin{align}
\tilde{\beta}
&amp;= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \
&amp;= \frac{\sum_i x_i (\beta x_i + \varepsilon_i)}{\sum_i x_i^2}\
&amp;= \beta + \frac{\sum x_i \varepsilon_i}{\sum_i x_i^2}
\end{align}$$</p>
<p>Therefore, $\tilde{\beta}$ is still an unbiased estimator of $\beta$, while its variance is smaller than the variance calculated assuming the intercept is non-zero.</p>
<p>$$
\operatorname{Var}\left( \tilde{\beta} \right) = \frac{\sigma^2}{\sum x_i^2} \le  \frac{\sigma^2}{\sum (x_i - \bar{x})^2} = \operatorname{Var}\left( \hat{\beta}  \right)
$$</p>
<p>Hence, we conclude that</p>
<ul class="simple">
<li><p>if the intercept is known to be zero, better use $\tilde\beta$ instead of $\hat\beta$, since the standard error of the $\tilde\beta$ is smaller, and both are unbiased.</p></li>
<li><p>If the true model has a non-zero intercept, then $\tilde\beta$ is biased for $\beta$, but it has a smaller variance, which brings a tradeoff of bias vs variance.</p></li>
</ul>
<p>:::</p>
</li>
<li><p><em>What happen to $\beta$, its standard error, and its p-value, if we scale the $j$-th covariate $x_j$, or add a constant to $x_j$? How about if we change $Y$?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em></p>
<p>In short, for an affine transformation on $x_j$ or $Y$, since the column space of $\boldsymbol{X}$ and the direction of $\boldsymbol{y}$ are unchanged, the overall fitting should be unchanged, such as $R^2$, $t$-test and $F-test$. The estimates (coefficients, residuals) may change.</p>
<p>One can re-write the model and compare with the original one. Suppose the original model is</p>
<p>$$
Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_j x_j + \varepsilon
$$</p>
<p>Let $x_j ^\prime = ax_j + b$, and let $\gamma_j$ be the new slope, $\gamma_0$ be the new intercept, and $u$ be the new error term.</p>
<p>$$
Y = \gamma_0 + \gamma_1 x_1 + \ldots + \gamma_j (ax_j + b) + u
$$</p>
<p>Comparing the two models, we obtain</p>
<p>$$\begin{aligned}
\gamma_j &amp;= \frac{1}{a} \beta_j  \
\gamma_0 &amp;= \beta_0 - \gamma_j b \
&amp;= \beta_0 - \beta_j \frac{b}{a}  \
\end{aligned}$$</p>
<p>Others slope and the error term are unchanged.</p>
<p>The estimated variance becomes</p>
<p>$$
\widehat{Var}(\hat{\gamma}_j) = \hat{\sigma}^2 \frac{1}{1-R_j^2} \frac{1}{\sum (x ^\prime - \bar{x} ^\prime)^2} = \frac{1}{a^2}  \widehat{Var}(\hat{\beta}_j)
$$</p>
<p>Hence, the standard error is $\operatorname{se}(\hat{\gamma}_j) = \operatorname{se}(\hat{\beta}_j)$ and the $t$-test statistic is</p>
<p>$$
\frac{\hat{\gamma}_j}{\operatorname{se}(\hat{\gamma}_j) } = \frac{\beta_j/a}{\operatorname{se}(\hat{\beta}_j)/a}   =  \frac{\beta_j}{\operatorname{se}(\hat{\beta}_j)}
$$</p>
<p>which is unchanged as expected.</p>
<p>For the case $Y ^\prime = c Y + d$, it is easy to write</p>
<p>$$
cY + d = \gamma_0 + \gamma_1 x_1 + \ldots + \gamma_j x_j + u
$$</p>
<p>and we have</p>
<p>$$\begin{aligned}
\gamma_j &amp;= c \beta_j \quad \forall j\
\gamma_0 &amp;= c \beta_0 + d\
\end{aligned}$$</p>
<p>The residuals are scaled by $c$ such that the standard error is scaled by $c$ too. Finally, the $t$-test statistic remains unchanged.</p>
<p>The takeaway is that, one can scale the variable to a proper unit for better interpretation.</p>
<p>:::</p>
</li>
<li><p><em>True or False: In SLR, exchange $X$ and $Y$, the new slope estimate equals the reciprocal of the original one</em>.</p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>False.</p>
<p>Since $\hat{\beta}<em>1 = r</em>{X,Y}\frac{s_Y}{s_X}$, the new slope estimate is $\hat{\gamma}<em>1 = r</em>{X,Y}\frac{s_X}{s_Y}$. We only have $\hat{\beta}_1 \hat{\gamma}<em>1 = r</em>{X,Y}^2 = R^2$. The last equality holds in SLR, see <a class="reference internal" href="12-lm-inference.html#lm-rsquared"><span class="std std-ref">proof</span></a>.</p>
<p>More analysis:</p>
<ul>
<li><p>Since in this case $F$-test depends only on $R^2$ (<a class="reference internal" href="12-lm-inference.html#lm-f-test"><span class="std std-ref">proof</span></a>), then the $F$-test are the same.</p></li>
<li><p>Since in this case $F$-test is equivalent to $t$-test (<a class="reference internal" href="12-lm-inference.html#lm-f-test"><span class="std std-ref">proof</span></a>), the $t$-test for $\hat{\beta}_1$ and $\hat{\gamma}_1$ are the same.</p></li>
<li><p>Hence, we have</p>
<p>$$
\frac{\sqrt{\hat{\sigma}_1^2 / s_X^2}}{\sqrt{\hat{\sigma}_2^2 / s_Y^2}} =  \frac{\operatorname{se}(\hat{\beta}_1)}{\operatorname{se}(\hat{\gamma_1})} = \frac{\hat{\beta}_1}{\hat{\gamma_1}} = \frac{s_Y^2}{s_X^2}
$$</p>
<p>then</p>
<p>$$
\frac{\hat{\sigma}_1}{\hat{\sigma}_2} = \frac{s_Y}{s_X} = \sqrt{\frac{\hat{\beta}_1}{\hat{\gamma_1}}}
$$</p>
</li>
</ul>
<p>:::</p>
</li>
<li><p><em>True or False: if $\operatorname{Cov}\left( Y, X_j \right) = 0$ then $\beta_j= 0$?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>In SLR, this is true, but in MLR, this is generally not true. See <a class="reference internal" href="12-lm-inference.html#lm-rss-nonincreasing"><span class="std std-ref">here</span></a> for explanation.</p>
<p>:::</p>
</li>
<li><p><em>What affect estimation precision?</em></p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>Recall</p>
<p>$$
\begin{aligned}
\operatorname{Var}\left(\hat{\beta}<em>{j}\right) &amp;=\sigma^{2}\left[\left(\boldsymbol{X}^{\top} \boldsymbol{X}\right)^{-1}\right]</em>{[j, j]} \
&amp;=\sigma^{2} \frac{1}{1-R_{j}^{2}} \frac{1}{\sum_{i}\left(x_{i j}-\bar{x}_{j}\right)^{2}}
\end{aligned}
$$</p>
<ul class="simple">
<li><p>The larger the error variance, $\sigma^2$, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the $x_i$, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between $X_j$ and other covariates (e.g. by orthogonal design) can decreases $R^2_{j}$, and hence decrease the variance.</p></li>
</ul>
<p>:::</p>
</li>
<li><p>To compare the effects of two variable $X_j, X_k$, can we say they have the same effect since the confidence interval of $\beta_j, \beta_k$ overlaps?</p>
<p>:::{admonition,dropdown,seealso} <em>Solution</em></p>
<p>No, since</p>
<ul class="simple">
<li><p>the two coefficients are probably correlated $\operatorname{Cov}\left( \boldsymbol{\beta} _j, \beta_k \right) \ne 0$</p></li>
<li><p>even if they are not correlated, we still need to find a pivot quantity for $\theta = \beta_j - \beta_k$ and conduct a hypothesis testing on $\theta=0$. See the <a class="reference internal" href="12-lm-inference.html#lm-t-test"><span class="std std-ref">$t$-test section</span></a>.
:::</p></li>
</ul>
</li>
<li><p><em>Does the partialling out method holds for $p \ge 3$</em>?</p></li>
<li><p>Causal?</p>
<p>313.qz1.q2</p>
<p>TBD.</p>
</li>
<li><p>Add/Remove a Variable/Observation</p>
<p>TBD</p>
<p>Table summary.</p>
<p>Rows: E(b), Var(b), RSS, TSS, R^2</p>
</li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-regression.html" title="previous page">Regression</a>
    <a class='right-next' id="next-link" href="12-lm-inference.html" title="next page">Linear Regression - Inference</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>