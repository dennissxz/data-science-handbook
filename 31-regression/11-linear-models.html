
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Classification" href="../32-classification/00-classification.html" />
    <link rel="prev" title="Regression" href="00-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-stat-sampling.html">
     Statistical Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/13-denominations.html">
     Denominations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted JISP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/01-information-theory.html">
     Information Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/11-linear-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/11-linear-models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/11-linear-models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/11-linear-models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-learning">
   Estimation (Learning)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary-least-squares">
     Ordinary Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#by-assumptions">
     By Assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood">
     Maximum Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties">
   Properties
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coefficients">
     Coefficients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unbiasedness">
       Unbiasedness
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance">
       Variance
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blue">
     BLUE
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hypothesis-testing">
       Hypothesis Testing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals">
     Residuals
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-term-variance-estimation">
     Error Term Variance Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
     Independence of
     <span class="math notranslate nohighlight">
      \(\hat{\boldsymbol{\beta}}\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\hat{\sigma}^2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition-of-total-sum-of-squares">
     Decomposition of Total Sum of Squares
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Coefficients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value">
       Value
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partialling-out">
       Partialling Out
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Hypothesis Testing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection">
   Model Selection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared-and-adjusted-r-squared">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared and Adjusted
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anova">
     ANOVA?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stepwise">
     Stepwise
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-cases">
   Special Cases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omit-a-variables">
     Omit a Variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#include-a-variable">
     Include a Variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multicollinearity">
     Multicollinearity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diagnosis">
       Diagnosis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consequences">
       Consequences
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implications">
       Implications
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heteroscedasticity">
     Heteroscedasticity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-x">
     Categorical
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slope-vs-correlation">
     Slope vs Correlation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitted-line-passes-sample-mean">
     Fitted Line Passes Sample Mean
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-zero-mean-of-error-term">
     Non-zero Mean of Error Term
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#no-intercept">
     No Intercept
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformation-of-variables">
     Transformation of Variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exchange-x-and-y">
     Exchange
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(Y\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance-and-beta-j">
     Covariance and
     <span class="math notranslate nohighlight">
      \(\beta_j\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#increase-estimation-precision">
     Increase Estimation Precision
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared-vs-hat-boldsymbol-beta">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared vs
     <span class="math notranslate nohighlight">
      \(\hat{\boldsymbol{\beta}}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partialling-out-in-general-cases">
     Partialling Out in General Cases
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#causal">
     Causal?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared-non-decreasing">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared Non-decreasing
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models">
<h1>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this headline">¶</a></h1>
<p>In this section we introduce linear models from a statistics’ perspective. The introduction from econometrics’ perspective or social science’s perspective may be different. In short, the statistics’ perspective focuses on general multivariate cases and heavily rely on linear algebra for derivation, while the econometrics’ or the social science’s perspective prefers to introduce models in univariate cases by basic arithmetics (whose form can be complicated without linear algebra notations) and extend the intuitions and conclusions into multivariate cases.</p>
<!---
My handwritten notes for the graduate level course STAT 343 offered by UChicago statistics department can be found [here](../imgs/lm-notes-applied-stat.pdf).
-->
<p>Personally, I involved in four courses that introduced linear models, i.e. at undergrad/grad level offered by stat/social science department. The style of the two courses offered by the stat departments were quite alike while the graduate level one covered more topics. In both undergrad/grad level courses offered by the social science departments, sometimes I got confused by the course materials that were contradictory to my statistics training , but the instructors had no clear response or even no response at all…</p>
<p>In sum, to fully understand the most fundamental and widely used statistical model, I highly suggest to take a linear algebra course first and take the regression course offered by math/stat department.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Linear models aim to model the relationship between a scalar response and one or more explanatory variables in a linear format:</p>
<div class="math notranslate nohighlight">
\[Y_i  = \beta_0 + \beta_1 x_{i,1} + \ldots + \beta_{p-1} x_{i,p-1}  + \varepsilon_i \]</div>
<p>for observations <span class="math notranslate nohighlight">\(i=1, 2, \ldots, n\)</span>.</p>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}_{n\times p}\)</span> is called the design matrix. The first column is usually set to be <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span>, i.e., intercept. The remaining <span class="math notranslate nohighlight">\(p-1\)</span> columns are designed values <span class="math notranslate nohighlight">\(x_{ij}\)</span> where <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, n\)</span> and <span class="math notranslate nohighlight">\(j=1, \ldots, p-1\)</span>. These <span class="math notranslate nohighlight">\(p-1\)</span> columns are called explanatory/independent variables, or covariates.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}_{n \times 1}\)</span> is a vector of response/dependent variables <span class="math notranslate nohighlight">\(Y_1, Y_2, \ldots, Y_n\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{p \times 1}\)</span> is a vector of coefficients to be estimated.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}_{n \times 1}\)</span> is a vector of unobserved random errors, which includes everything that we have not measured and included in the model.</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</div>
<p>which is called <strong>simple linear regression</strong>.</p>
<p>When <span class="math notranslate nohighlight">\(p&gt;2\)</span>, it is called <strong>multiple linear regression</strong>. For instance, when <span class="math notranslate nohighlight">\(p=3\)</span></p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon_i
\]</div>
<p>When there are multiple dependent variables, we call it <strong>multivariate regression</strong>, which will be introduced in another section.</p>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span>,</p>
<ul class="simple">
<li><p>if we include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta_0\)</span> means that we use a single constant to predict <span class="math notranslate nohighlight">\(y_i\)</span>. The estimator, <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, by ordinary least square, should be the sample mean <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>if we do not include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta x_i\)</span> means that we expect that <span class="math notranslate nohighlight">\(y\)</span> is proportional to <span class="math notranslate nohighlight">\(x\)</span>. See <a class="reference internal" href="#lm-proportional-model"><span class="std std-ref">here</span></a> for details.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Fixed or random <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>In natural science, researchers design <span class="math notranslate nohighlight">\(n\times p\)</span> values in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and run experiments to obtain the response <span class="math notranslate nohighlight">\(y_i\)</span>. We call this kind of data <strong>experimental data</strong>. In this sense, the explanatory variables <span class="math notranslate nohighlight">\(x_{ij}\)</span>’s are designed before the experiment, so they are also constants. The coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>’s are unknown constants. The error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is random. The response variable <span class="math notranslate nohighlight">\(Y_i\)</span> on the left hand side is random due to the randomness in the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<p>In social science, most of data is <strong>observational data</strong>. That is, researchers obtain the values of many variables at the same time, and choose one of interest to be the response variable <span class="math notranslate nohighlight">\(y_i\)</span> and some others to be the explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>. In this case, <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is viewed as a data set, and we can talk about descriptive statistics, such as variance of each explanatory variable, or covariance between pair of explanatory variables. This is valid since we often view the columns of a data set as random variables.</p>
<p>However, the inference methods of the coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> are developed based on the natural science setting, i.e., the values of explanatory variables are pre-designed constants. Many social science courses frequently use descriptive statistics of the explanatory variables which assumes they are random, and apply inference methods which assumes they are constant. This is quite confusing for beginners to linear models.</p>
<p>To be clear, we stick to the natural science setting and make the second assumption below. We use subscript <span class="math notranslate nohighlight">\(i\)</span> in every <span class="math notranslate nohighlight">\(y_i, x_i, \varepsilon_i\)</span> instead of <span class="math notranslate nohighlight">\(y, x, \varepsilon\)</span> which gives a sense that <span class="math notranslate nohighlight">\(x\)</span> is random. And we use descriptive statistics for the explanatory variables only when necessary.</p>
</div>
</div>
<div class="section" id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h2>
<p>Basic assumptions</p>
<ol>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span> is <strong>linear</strong> in covariates <span class="math notranslate nohighlight">\(X_j\)</span>.</p></li>
<li><p>The values of explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> are known and fixed. Randomness only comes from <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p></li>
<li><p>No <span class="math notranslate nohighlight">\(X_j\)</span> is constant for all observations. No exact linear relationships among the explanatory variables (no perfect multicollinearity).</p></li>
<li><p>The error terms are uncorrelated <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right)= 0\)</span>, with common mean <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> and variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon_i \right) = \sigma^2\)</span> (homoskedasticity).</p>
<p>As a result, <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y} \mid \boldsymbol{X} \right) = \boldsymbol{X} \boldsymbol{\beta}\)</span>, or <span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \mid x_i \right) = \beta_0 + \beta_1 x_i\)</span> when <span class="math notranslate nohighlight">\(p=2\)</span>, which can be illustrated by the plots below.</p>
<div class="figure align-default" id="lm-distribution-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm_cond_distribution.png"><img alt="" src="../_images/lm_cond_distribution.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Distributions of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-distribution-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="lm-observation-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm_xyplane_dots.png"><img alt="" src="../_images/lm_xyplane_dots.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Observations of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-observation-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<p>To predict <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, we just use <span class="math notranslate nohighlight">\(\hat{y}_i = \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\)</span> .</p>
</li>
<li><p>The error terms are independent and follow Gaussian distribution <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _n)\)</span>.</p>
<p>As a result, we have <span class="math notranslate nohighlight">\(Y_i \sim N(\boldsymbol{x}_i ^\top \boldsymbol{\beta} , \sigma^2 )\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_n(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} _n)\)</span></p>
</li>
</ol>
<p>These assumptions are used for different objectives. The first 3 assumptions are the base, and in additiona to them,</p>
<ul class="simple">
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by least squares uses no more assumptions.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by maximal likelihood uses assumptions 4 and 5.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> in 4.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses 1, 2, <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right) = 0\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \epsilon_i \right) = \sigma^2\)</span> in 4.</p></li>
<li><p>proof of Gaussian-Markov Theorem (BLUE) uses 4.</p></li>
<li><p>derivation of the distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span> uses 4 and 5.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Zero conditional mean assumption</p>
<p>In some social science or econometrics courses, they follow the “Gauss-Markov assumptions” that are roughly the same to the assumptions, but in different formats. One of them is zero conditional mean assumption.</p>
<p>In general, it says</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \varepsilon \mid x_1, x_2, \ldots, x_p\right) = 0
\]</div>
<p>For <span class="math notranslate nohighlight">\(p=2\)</span>, it is</p>
<div class="math notranslate nohighlight">
\[\operatorname{E}\left( \varepsilon \mid x  \right) = 0\]</div>
<p>which (in their setting) implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \varepsilon \right)
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon \mid x \right) \right)\\
&amp;= 0\\
\operatorname{Cov}\left( \varepsilon, x \right)
&amp;= \operatorname{E}\left( \varepsilon x \right) - \operatorname{E}\left( \varepsilon \right)\operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon x \mid x \right) \right)- 0 \times \operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( x \operatorname{E}\left( \varepsilon \mid x \right) \right) \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>Then they these two corollaries are used for <a class="reference internal" href="#lm-estimation-by-assumpation"><span class="std std-ref">estimation</span></a>.</p>
<p>As discussed above, in their setting <span class="math notranslate nohighlight">\(x\)</span> is random (at this stage), so they use notations such as <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( x, \varepsilon \right)\)</span>. It also seems that they view <span class="math notranslate nohighlight">\(\varepsilon\)</span> as an “overall” measure of random error, instead of <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> for specific <span class="math notranslate nohighlight">\(i\)</span> in the natural science setting. But they can mean so by using the conditional notation <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span>.</p>
</div>
</div>
<div class="section" id="estimation-learning">
<h2>Estimation (Learning)<a class="headerlink" href="#estimation-learning" title="Permalink to this headline">¶</a></h2>
<p>We introduce various methods to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="section" id="ordinary-least-squares">
<h3>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<p>The most common way is to estimate the parameter <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by minimizing the sum of squared errors <span class="math notranslate nohighlight">\(\sum_i(y_i-\hat{y}_i)^2\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title">A note on substitution</p>
<p>We substitute the predicted <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} }\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{X} \boldsymbol{\beta}\)</span>. The <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> here just means a variable in the optimization problem, not the unknown constant coefficients in our model.</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \hat{\boldsymbol{y}}  \right\Vert ^2 \\
&amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 \\
\end{align}\end{split}\]</div>
<p>The gradient w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\nabla_{\boldsymbol{\beta}} &amp;= -2 \boldsymbol{X}  ^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )  \\
&amp;\overset{\text{set}}{=} \boldsymbol{0}
\end{align}\end{split}\]</div>
<p>Hence, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}
\]</div>
<p>This linear system is called the <strong>normal equation</strong>.</p>
<p>The closed form solution is</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = \left( \boldsymbol{X} ^\top \boldsymbol{X}   \right)^{-1}\boldsymbol{X} ^\top  \boldsymbol{y}  \]</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Solving the linear system by software</p>
<p>Computing software use specific functions to solve the normal equation <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, instead of using the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X}) ^{-1}\)</span> directly which can be slow and numerically unstable. For instance, one can use QR factorization of <span class="math notranslate nohighlight">\(X\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \boldsymbol{Q} \left[\begin{array}{l}
\boldsymbol{R}_{p \times p}  \\
\boldsymbol{0}_{(n-p) \times p}
\end{array}\right]
\end{split}\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\| \boldsymbol{y} - \boldsymbol{X}  \boldsymbol{\beta}  \|^{2}
&amp;=\left\|\boldsymbol{Q} ^{\top} \boldsymbol{y}  - \boldsymbol{Q} ^\top \boldsymbol{X} \boldsymbol{\beta}  \right\|^{2} \\
&amp;=\left\|\left(\begin{array}{c}
\boldsymbol{f}  \\
\boldsymbol{r}
\end{array}\right)-\left(\begin{array}{c}
\boldsymbol{R} \boldsymbol{\beta}  \\
\boldsymbol{0}
\end{array}\right)\right\|^{2} \\
&amp;=\|\boldsymbol{f} - \boldsymbol{R} \boldsymbol{\beta} \|^{2}+\|\boldsymbol{r} \|^{2}
\end{aligned}
\end{split}\]</div>
<p>Finally</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\beta} = \boldsymbol{R} ^{-1} \boldsymbol{f}
\]</div>
</div>
<p>An unbiased estimator of the error variance <span class="math notranslate nohighlight">\(\sigma^2 = \operatorname{Var}\left( \varepsilon \right)\)</span> is (to be discussed [later])</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\Vert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \right\Vert ^2}{n-p}
\]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0, \hat{\beta}_1 =  \underset{\beta_0, \beta_1 }{\mathrm{argmin}} \, \sum_i \left( y_i - \beta_0 - \beta_1 x_i \right)^2\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0
\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) = 0
\]</div>
<p>Solve the system of the equations, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\hat{\beta}_{0} &amp;=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{align}\end{split}\]</div>
<p>The expression for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> implies that the fitted line cross the sample mean point <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>.</p>
<p>Moreover,</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{n-2} \sum_i \hat\varepsilon_i^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\varepsilon_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\)</span>.</p>
<div class="admonition">
<p class="admonition-title"> Minimizing mean squared error</p>
<p>The objective function, <strong>sum of squared errors</strong>,</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 = \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>can be replaced by <strong>mean squared error</strong>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>and the results are the same.</p>
</div>
</div>
<div class="section" id="by-assumptions">
<span id="lm-estimation-by-assumpation"></span><h3>By Assumptions<a class="headerlink" href="#by-assumptions" title="Permalink to this headline">¶</a></h3>
<p>In some social science courses, the estimation is done by using the assumptions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid X \right) = 0\)</span></p></li>
</ul>
<p>The first one gives</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{1}{n}  \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\end{equation}
\]</div>
<p>The second one gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Cov}\left( X, \varepsilon \right)
&amp;= \operatorname{E}\left( X \varepsilon \right) - \operatorname{E}\left( X \right) \operatorname{E}\left( \varepsilon \right) \\
&amp;= \operatorname{E}\left[ \operatorname{E}\left( X \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= \operatorname{E}\left[ X \operatorname{E}\left( \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= 0
\end{align}\end{split}\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>Therefore, we have the same normal equations to solve for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
</div>
<div class="section" id="maximum-likelihood">
<h3>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<p>biased. TBD.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h2>
<p>We describe the properties of OLS estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and the corresponding residuals <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\varepsilon} }\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \boldsymbol{y}\)</span> is a random variable, since it is a linear combination of the random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. This means that, keeping <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> fixed, repeat the experiment, we will probably get different response values <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, and hence different <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. As a result, there is a sampling distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, and we can find its mean, variance, and conduct hypothesis testing.</p>
<div class="section" id="coefficients">
<h3>Coefficients<a class="headerlink" href="#coefficients" title="Permalink to this headline">¶</a></h3>
<div class="section" id="unbiasedness">
<h4>Unbiasedness<a class="headerlink" href="#unbiasedness" title="Permalink to this headline">¶</a></h4>
<p>The OLS estimators are unbiased since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \hat{\boldsymbol{\beta} } \right) &amp;= \operatorname{E}\left( (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \operatorname{E}\left( \boldsymbol{y} \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \boldsymbol{X} \boldsymbol{\beta} \\
&amp;= \boldsymbol{\beta}
\end{align}\end{split}\]</div>
<p>when <span class="math notranslate nohighlight">\(p=2\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1}
&amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\end{align}\end{split}\]</div>
<p>To prove unbiasedness, using the fact that for any constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_i (x_i - \bar{x})c = 0
\]</div>
<p>Then, the numerator becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
&amp;=\sum\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+u_{i}\right) \\
&amp;=\sum\left(x_{i}-\bar{x}\right) \beta_{0}+\sum\left(x_{i}-\bar{x}\right) \beta_{1} x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{0} \sum\left(x_{i}-\bar{x}\right)+\beta_{1} \sum\left(x_{i}-\bar{x}\right) x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{1} \sum\left(x_{i}-\bar{x}\right)^2 +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
\end{align}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\hat{\beta}_{1}=\beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}}
\end{equation}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta}_1 \right) = \beta_1
\]</div>
</div>
<div class="section" id="variance">
<span id="lm-inference-variance"></span><h4>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h4>
<p>The variance (covariance matrix) of the coefficients is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \boldsymbol{\beta}  \right) &amp;= \operatorname{Var}\left(  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right)  \\
&amp;=   (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \operatorname{Var}\left( \boldsymbol{y}  \right)  \boldsymbol{X}  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;= \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\\
\end{align}\end{split}\]</div>
<div class="admonition">
<p class="admonition-title"> Note</p>
<p>More specifically, for the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient estimator <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, its variance is,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\operatorname{Var}\left( \hat{\beta}_j \right)
&amp;= \sigma^2 \frac{1}{1- R^2_{-j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(R_{-j}^2\)</span> is the value of <a class="reference internal" href="#lm-rsquared"><span class="std std-ref"><span class="math notranslate nohighlight">\(R\)</span>-squared</span></a> when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables.</p>
<p>Note that the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regressing <span class="math notranslate nohighlight">\(X_1\)</span> to an constant intercept is 0. So we have the particular result below.</p>
</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, the variance of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_1 \right)
&amp;= \operatorname{Var}\left( \beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}} \right)\\
&amp;= \frac{\operatorname{Var}\left( \sum\left(x_{i}-\bar{x}\right) u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sum\left(x_{i}-\bar{x}\right)^2 \operatorname{Var}\left( u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \sigma^2 \frac{\sum\left(x_{i}-\bar{x}\right)^2 }{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sigma^2}{\sum_{i=1}^n \left(x_{i}-\bar{x}\right)^{2}}\\
\end{align}\end{split}\]</div>
<p>We conclude that</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{-j}\)</span>, and hence decrease the variance.</p></li>
</ul>
<p>A problem is that the error <span class="math notranslate nohighlight">\(\sigma^2\)</span> variance is <strong>unknown</strong>. In practice, we can estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> by its unbiased estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2=\frac{\sum_i (x_i - \bar{x})}{n-2}\)</span> (to be shown [link]), and substitute it into <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span>. Since the error variance <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is estimated, the slope variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span> is estimated too, and hence the square root is called standard error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, instead of standard deviation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{se}\left(\hat{\beta}_{1}\right)
&amp;= \sqrt{\widehat{\operatorname{Var}}\left( \hat{\beta}_1 \right)}\\
&amp;= \frac{\hat{\sigma}}{\sqrt{\sum \left(x_{i}-\bar{x}\right)^{2}}}
\end{align}\end{split}\]</div>
</div>
</div>
<div class="section" id="blue">
<h3>BLUE<a class="headerlink" href="#blue" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Theorem (Gauss–Markov)</dt><dd><p>The ordinary least squares (OLS) estimator has the <strong>lowest</strong> sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. In abbreviation, the OLS estimator is BLUE: Best (lowest variance) Linear Unbiased Estimator.</p>
</dd>
<dd><details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">Let <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \boldsymbol{C} \boldsymbol{y}\)</span> be another linear estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. We can write <span class="math notranslate nohighlight">\(\boldsymbol{C} = \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \boldsymbol{X} ^\top + \boldsymbol{D}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{D} \ne \boldsymbol{0}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{E}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \operatorname{E}\left( \boldsymbol{C} \boldsymbol{y}   \right)\\
  &amp;= \boldsymbol{C} \operatorname{E}\left( \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  \right)\\
  &amp;= \boldsymbol{\beta} + \boldsymbol{D} \boldsymbol{X} \boldsymbol{\beta} \\
  \end{align}\end{split}\]</div>
<p class="card-text">Hence, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}}\)</span> is unbiased iff <span class="math notranslate nohighlight">\(\boldsymbol{D} \boldsymbol{X} = 0\)</span>.</p>
<p class="card-text">The variance is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \boldsymbol{C}\operatorname{Var}\left( \boldsymbol{y}  \right) \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \boldsymbol{C} \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  + (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{D} ^\top + \boldsymbol{D} \boldsymbol{X} \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^\top  + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right) + \sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \\
  \end{align}\end{split}\]</div>
<p class="card-text">Since <span class="math notranslate nohighlight">\(\sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \in \mathrm{PSD}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right) \succeq \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)
  \]</div>
<p class="card-text">The equality holds iff <span class="math notranslate nohighlight">\(\boldsymbol{D} ^\top \boldsymbol{D} = 0\)</span>, which implies that <span class="math notranslate nohighlight">\(\operatorname{tr}\left( \boldsymbol{D} \boldsymbol{D} ^\top \right) = 0\)</span>, then <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{D} \right\Vert _F^2 = 0\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{D} = 0\)</span>, i.e. <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta} } = \hat{\boldsymbol{\beta} }\)</span>. Therefore, BLUE is unique.</p>
</div>
</details></dd>
</dl>
<p>If error term is normally distributed, then OLS is most efficient among all consistent estimators (not just linear ones).</p>
<p>When the distribution of error term is non-normal, other estimators may have lower variance than OLS such as least absolute deviation (median regression).</p>
<div class="section" id="hypothesis-testing">
<h4>Hypothesis Testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">¶</a></h4>
<p>TBD</p>
</div>
</div>
<div class="section" id="residuals">
<h3>Residuals<a class="headerlink" href="#residuals" title="Permalink to this headline">¶</a></h3>
<p>Definition<br />
The residual is defined as the difference between the true response value <span class="math notranslate nohighlight">\(y\)</span> and our fitted response value <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat\varepsilon_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\]</div>
<p>It is an estimate of the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Sometimes the second property is described as “the residuals and the explanatory variables are uncorrelated”.</p>
</div>
<p>Properties</p>
<ul class="simple">
<li><p>The sum of the residual is zero: <span class="math notranslate nohighlight">\(\sum_i \hat{\varepsilon}_i = 0\)</span></p></li>
<li><p>The sum of the product of residual and any covariate: <span class="math notranslate nohighlight">\(\sum_i x_{ij} \hat{\varepsilon}_i = 0\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">Recall the normal equation</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top (\boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta} }) = \boldsymbol{0}
\]</div>
<p class="card-text">We obtain</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
<p class="card-text">Since the first column of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> , we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_i \hat{\varepsilon}_i  
&amp;= \sum_i(y_i - \hat{y}_i)  \\
&amp;= \sum_i(y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta} }_i)  \\
&amp;= 0
\end{align}\end{split}\]</div>
<p class="card-text">For other columns <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_j ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
</div>
</details></div>
<div class="section" id="error-term-variance-estimation">
<h3>Error Term Variance Estimation<a class="headerlink" href="#error-term-variance-estimation" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
<div class="section" id="independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
<h3>Independence of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span><a class="headerlink" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
<div class="section" id="decomposition-of-total-sum-of-squares">
<span id="lm-tss-identity"></span><h3>Decomposition of Total Sum of Squares<a class="headerlink" href="#decomposition-of-total-sum-of-squares" title="Permalink to this headline">¶</a></h3>
<p>We can think of each observation as being made up of an explained part, and an unexplained part.</p>
<ul class="simple">
<li><p>Total sum of squares: <span class="math notranslate nohighlight">\(TSS = \sum\left(y_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Explained sum of squares: <span class="math notranslate nohighlight">\(ESS = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Residual sum of squares: <span class="math notranslate nohighlight">\(RSS = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
<p>Then we have the decomposition identity</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
TSS
&amp;=\sum\left(y_{i}-\bar{y}\right)^{2} \\
&amp;=\sum\left[\left(y_{i}-\hat{y}_{i}\right)+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum\left[\hat{\varepsilon}_{i}+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum \hat{\varepsilon}_{i}^{2}+2 \sum \hat{\varepsilon}_{i}\left(\hat{y}_{i}-\bar{y}\right)+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2} \\
&amp;= RSS + 2  \sum \hat{\varepsilon}_{i}\left(\hat{\beta}_0 + \hat{\beta}_1 x_{i}-\bar{y}\right)+ ESS \\
&amp;= RSS + ESS
\end{align}\end{split}\]</div>
<p>where use the fact that <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i x_i = 0\)</span> shown [above].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some courses use the letters <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(E\)</span> to denote the opposite quantity in statistics courses.</p>
<ul class="simple">
<li><p>Sum of squares due to regression: <span class="math notranslate nohighlight">\(SSR = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Sum of squared errors: <span class="math notranslate nohighlight">\(SSE = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
</div>
<p>From linear algebra’s perspective, the identity is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y} - \bar{y} \boldsymbol{1} _n  \right\Vert ^2 = \left\Vert \boldsymbol{y} - \hat{\boldsymbol{y} }  \right\Vert ^2 + \left\Vert \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n\right\Vert ^2
\]</div>
<p>which holds because the LHS vector <span class="math notranslate nohighlight">\(\boldsymbol{y} - \bar{y}\boldsymbol{1} _n\)</span> is the the sum of the two RHS vectors, and the two vectors are orthogonal</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{y}  - \bar{y} \boldsymbol{1} _n = (\boldsymbol{y} - \hat{\boldsymbol{y} }) &amp;+ (\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n) \\
\boldsymbol{y} - \hat{\boldsymbol{y} } &amp;\perp \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n
\end{aligned}\end{split}\]</div>
<p>More specifically, they are orthogonal because</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} - \hat{\boldsymbol{y} } \in \operatorname{im}(\boldsymbol{X} )^\perp \quad  \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \in \operatorname{im}(\boldsymbol{X} )
\]</div>
<p>since <span class="math notranslate nohighlight">\(\boldsymbol{1} _n \in \operatorname{im}(\boldsymbol{X})\)</span>, if an intercept term is included in the model.</p>
<p>drawing [here]</p>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Coefficients<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="section" id="value">
<h4>Value<a class="headerlink" href="#value" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(\beta_j\)</span> is the expected change in the value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if the value of the covariate <span class="math notranslate nohighlight">\(x_j\)</span> increases by 1, holding other covariates fixed.</p>
<p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the expected value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if all covariates have values of zero.</p>
<p>If the response is in log format, i.e. <span class="math notranslate nohighlight">\(\log(Y)\)</span>, then the <span class="math notranslate nohighlight">\(\beta_j\)</span> can be interpreted as the percentage change in <span class="math notranslate nohighlight">\(Y\)</span> associated with one unit increase of <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Linear regression models only reveal linear associations between the response variable and the independent variables. But association does not imply causation. Simple example: in SLR, regress <span class="math notranslate nohighlight">\(X\)</span> over <span class="math notranslate nohighlight">\(Y\)</span>, the coefficient has same sign and significance??, but causation cannot be reversed.</p>
<p>Only when the data is from a randomized controlled trial, correlation will imply causation.</p>
</div>
</div>
<div class="section" id="partialling-out">
<h4>Partialling Out<a class="headerlink" href="#partialling-out" title="Permalink to this headline">¶</a></h4>
<p>We can interpret the coefficients in multiple linear regression from “partialling out” perspective.</p>
<p>When <span class="math notranslate nohighlight">\(p=3\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}
\]</div>
<p>We can obtain <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> by the following three steps</p>
<ol>
<li><p>regress <span class="math notranslate nohighlight">\(x_1\)</span> over <span class="math notranslate nohighlight">\(x_2\)</span> and obtain</p>
<div class="math notranslate nohighlight">
\[\hat{x}_{1}=\hat{\gamma}_{0}+\hat{\gamma}_{1} x_{2}\]</div>
</li>
<li><p>compute the residuals <span class="math notranslate nohighlight">\(\hat{u}_{1}\)</span> in the above regression</p>
<div class="math notranslate nohighlight">
\[
     \hat{u}_{i} = x_{1i} - \hat{x}_{1i}
     \]</div>
</li>
<li><p>regress <span class="math notranslate nohighlight">\(y\)</span> on the the residuals <span class="math notranslate nohighlight">\(\hat{u}_{1}\)</span>, and the estimated coefficient equals the required coefficient.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \hat{y}
     &amp;=\hat{\alpha}_{0}+\hat{\alpha}_{1} \hat{u} \\
     \hat{\alpha}_{1}
     &amp;= \frac{\sum (\hat{u}_i - \bar{\hat{u}}_i)(y_i - \bar{y})}{\sum (\hat{u}_i - \bar{\hat{u}}_i)^2} \\
     &amp;= \frac{\sum \hat{u}_{i}y_i}{\sum \hat{u}_{i}^2} \qquad \because \bar{\hat{u}}_i = 0\\
     &amp;\overset{\text{claimed}}{=} \hat{\beta}_1
     \end{align}\end{split}\]</div>
</li>
</ol>
<p>In this approach, <span class="math notranslate nohighlight">\(\hat{u}\)</span> is interpreted as the part in <span class="math notranslate nohighlight">\(x_1\)</span> that cannot be predicted by <span class="math notranslate nohighlight">\(x_2\)</span>, or is uncorrelated with <span class="math notranslate nohighlight">\(x_2\)</span>. We then regress <span class="math notranslate nohighlight">\(y\)</span> on <span class="math notranslate nohighlight">\(\hat{u}\)</span>, to get the effect of <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(y\)</span> after <span class="math notranslate nohighlight">\(x_2\)</span> has been “partialled out”.</p>
</div>
<div class="section" id="id2">
<h4>Hypothesis Testing<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="model-selection">
<h2>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this headline">¶</a></h2>
<div class="section" id="r-squared-and-adjusted-r-squared">
<span id="lm-rsquared"></span><h3><span class="math notranslate nohighlight">\(R\)</span>-squared and Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#r-squared-and-adjusted-r-squared" title="Permalink to this headline">¶</a></h3>
<p>Assuming the <a class="reference internal" href="#lm-tss-identity"><span class="std std-ref">decomposition identity</span></a> of <span class="math notranslate nohighlight">\(TSS\)</span> holds, we can define <span class="math notranslate nohighlight">\(R\)</span>-squared.</p>
<p>Defintion<br />
<span class="math notranslate nohighlight">\(R\)</span>-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
<div class="math notranslate nohighlight">
\[
  R^2 = \frac{SSR}{SST}  = 1 - \frac{SSE}{SST} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
  \]</div>
<p>Properties</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared can never decrease when an additional explanatory variable is added to the model.</p></li>
</ul>
<p>As long as <span class="math notranslate nohighlight">\(Cov(Y, X_j) \ne 0\)</span>, then <span class="math notranslate nohighlight">\(X_j\)</span> has some explanatory power to <span class="math notranslate nohighlight">\(Y\)</span>, and thus SST decreases. As a result, it is not a good measure for model selection, which can cause overfitting.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared equals the squared correlation coefficient between the actual value of the response and the fitted value <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( y, \hat{y} \right)^2\)</span>.</p>
<p>In particular, in simple linear regression, <span class="math notranslate nohighlight">\(R^2 = \rho_{X,Y}^2\)</span>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">By the definition of correlation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Corr}\left( y, \hat{y} \right)^2
&amp;= \frac{\operatorname{Cov}\left( y, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
&amp;= \frac{\operatorname{Cov}\left( \hat{y} + \hat{\varepsilon}, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
&amp;= \frac{\operatorname{Cov}\left( \hat{y} , \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
&amp;= \frac{\operatorname{Var}\left( \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
&amp;= \frac{\operatorname{Var}\left( \hat{y} \right)}{\operatorname{Var}\left( y \right)} \\
&amp;= \frac{SSR}{SST} \\
&amp;= R^2 \\
\end{align}\end{split}\]</div>
<p class="card-text">The third equality holds since</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Cov}\left( \hat{\varepsilon}, \hat{y} \right) = \operatorname{Cov}\left( \hat{\varepsilon}, \sum_j x_j \hat{\beta}_j  \right) = \sum_j \hat{\beta}_j \operatorname{Cov}\left( \hat{\varepsilon},  x_j \right) = 0
\]</div>
<p class="card-text">When <span class="math notranslate nohighlight">\(p=2\)</span>, since <span class="math notranslate nohighlight">\(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
R^2 = \operatorname{Corr}\left(y, \hat{y} \right)^2 = \operatorname{Corr}\left(y, x \right)^2
\]</div>
</div>
</details></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> <span class="math notranslate nohighlight">\(R\)</span>-squared when there is no intercept</p>
<p>When there is no intercept, then <span class="math notranslate nohighlight">\(\bar{y} \boldsymbol{1} _n \notin \operatorname{im}(X)\)</span> and hence <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \notin \operatorname{im}(X)\)</span>. The decomposition identity may not hold. Thus, the value of <span class="math notranslate nohighlight">\(R\)</span>-squared may no longer be in <span class="math notranslate nohighlight">\([0,1]\)</span>, and its interpretation is no longer valid. What actually happen to the value <span class="math notranslate nohighlight">\(R\)</span>-squared depends on whether we define it using <span class="math notranslate nohighlight">\(TSS\)</span> with <span class="math notranslate nohighlight">\(RSS\)</span> or <span class="math notranslate nohighlight">\(ESS\)</span>.</p>
<p>If we define <span class="math notranslate nohighlight">\(R^2 = \frac{ESS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{ESS} = \left\Vert \hat{\boldsymbol{y} } - \bar{y}\boldsymbol{1} _n \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(ESS &gt; TSS\)</span>, i.e., <span class="math notranslate nohighlight">\(R^2 &gt; 1\)</span>.</p>
<p>On the other hand, if we define <span class="math notranslate nohighlight">\(R^2 = 1 - \frac{RSS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{RSS} = \left\Vert \hat{\boldsymbol{y} } -  \boldsymbol{y} \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(RSS &gt; TSS\)</span>, i.e. <span class="math notranslate nohighlight">\(R^2 &lt; 0\)</span>.</p>
</div>
<p>Due to the non-decrease property of <span class="math notranslate nohighlight">\(R\)</span>-squared, we define adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared which is a better measure of goodness of fitting.</p>
<p>Definition<br />
Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared, denoted by <span class="math notranslate nohighlight">\(\bar{R}^2\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[
  \bar{R}^2 = 1-\frac{RSS / (n-p)}{ TSS / (n-1)}
  \]</div>
<p>Properties</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{R}^2\)</span> can increase or decrease. When a new variable is included, <span class="math notranslate nohighlight">\(RSS\)</span> decreases, but <span class="math notranslate nohighlight">\((n-p)\)</span> also decreases.</p>
<ul>
<li><p>Relation to <span class="math notranslate nohighlight">\(R\)</span>-squared is</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
\bar{R}^2 = 1-\frac{n-1}{ n-p}(1 - R^2) &lt; R^2
\]</div>
<ul>
<li><p>Relation to estimated variance of random error and variance of response</p>
<div class="math notranslate nohighlight">
\[
    \bar{R}^2 = 1-\frac{\hat{\sigma}^2}{\operatorname{Var}\left( y \right)}
    \]</div>
</li>
<li><p>Can be negative when</p>
<div class="math notranslate nohighlight">
\[
    R^2 &lt; \frac{p-1}{n-p}
    \]</div>
<p>If <span class="math notranslate nohighlight">\(p &gt; \frac{n+1}{2}\)</span> then the above inequality always hold, and adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared is always negative.</p>
</li>
</ul>
</div>
<div class="section" id="anova">
<h3>ANOVA?<a class="headerlink" href="#anova" title="Permalink to this headline">¶</a></h3>
<p>The Analysis Of Variance, popularly known as the ANOVA, can be used in cases where there are more than two groups.</p>
<p>TBD</p>
</div>
<div class="section" id="stepwise">
<h3>Stepwise<a class="headerlink" href="#stepwise" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
</div>
<div class="section" id="special-cases">
<h2>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">¶</a></h2>
<p>No models are perfect. In this section we introduce what happen when our model is misspecified or when some assumptions fail.</p>
<div class="section" id="omit-a-variables">
<span id="lm-omit-variable"></span><h3>Omit a Variables<a class="headerlink" href="#omit-a-variables" title="Permalink to this headline">¶</a></h3>
<p>Suppose the true model is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}_{n \times p} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  
\]</div>
<p>And we omit one explanatory variable <span class="math notranslate nohighlight">\(X_j\)</span>. Thus, our new design matrix has size <span class="math notranslate nohighlight">\(n \times (p-1)\)</span>, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{X}_{-j}\)</span>. Without loss of generality, let it be in the last column of the original design matrix, i.e. <span class="math notranslate nohighlight">\(\boldsymbol{X} = \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\)</span>. The new estimated coefficients vector is denoted by <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span>. The coefficient for <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in the true model is denoted by <span class="math notranslate nohighlight">\(\beta_j\)</span>, and the vector of coefficients for other explanatory variables is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{\beta} _{-j}\)</span>. Hence, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} ^\top = \left[ \boldsymbol{\beta} _{-j} \quad \beta_j \right] ^\top\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Though the common focus is on bias, omitting a variable probably decreases variance. See the relevant section <a class="reference internal" href="#lm-include-variable"><span class="std std-ref">below</span></a>, or the variance expression <a class="reference internal" href="#lm-inference-variance"><span class="std std-ref">above</span></a>.</p>
</div>
<p><em>Question: Is <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span> unbised for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{-j}\)</span>?</em></p>
<p><em>Answer: No. Omitting a relevant variable increases bias. There is a deterministic identity for the bias.</em></p>
<p>We will see the meaning of “relevant” later.</p>
<p>We first find the expression of the new estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \hat{\boldsymbol{\beta} }_{-j}
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{y} \\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left\{ \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\left[\begin{array}{l}
\boldsymbol{\beta} _{-j}  \\
\beta _j
\end{array}\right] + \boldsymbol{\varepsilon}  \right\}\\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left( \boldsymbol{X} _{-j} \boldsymbol{\beta} _{-j} +  \boldsymbol{x}_j \beta _j + \boldsymbol{\varepsilon}  \right) \\
&amp;=  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \right]\left(  \boldsymbol{x}_j \beta _j+ \boldsymbol{\varepsilon}  \right)\\
\end{align}\end{split}\]</div>
<p>The expectation, therefore, is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{E}\left( \hat{\boldsymbol{\beta} }_{-j} \right) =  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \right]\beta _j\\
\end{split}\]</div>
<p>What is <span class="math notranslate nohighlight">\(\left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span>? You may recognize this form. It is actually the vector of estimated coefficients when we regress the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> on all other explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>. Let it be <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_{(p-1) \times 1}\)</span>.</p>
<p>Therefore, we have, for the <span class="math notranslate nohighlight">\(k\)</span>-th explanatory variable in the new model,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta} _{-j,k} \right) = \beta_{k} + \alpha_k \beta_j
\]</div>
<p>So the bias is <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>. The sign can be positive or negative.</p>
<p>This identity can be converted to the following diagram. The explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is associated with the response <span class="math notranslate nohighlight">\(Y\)</span> in two ways. First is directly by itself with strength is <span class="math notranslate nohighlight">\(\beta_k\)</span>, and second is through the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span>, with a “compound” strength <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_k \quad \overset{\quad \beta_{k} \quad }{\longrightarrow} \quad Y
\]</div>
<div class="math notranslate nohighlight">
\[
\alpha_k \searrow \qquad \nearrow \beta_j
\]</div>
<div class="math notranslate nohighlight">
\[
X_j
\]</div>
<p>When will the bias be zero?</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\alpha_k = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the concerned explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{x}_k = 0\)</span> in the design matrix.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\beta_j = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the response <span class="math notranslate nohighlight">\(Y\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{y} = 0\)</span>.</p></li>
</ul>
<p>That’s how we define “relevant”.</p>
<p>What is the relation between the sample estimates? The relation has a similar form.</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta }_{-j,k} =  \hat{\beta}_k + \hat{\alpha}_k\hat{\beta}_j
\]</div>
<p>Proof: TBD. Need linear algebra about inverse.</p>
<p>Verify:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">b0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 + x3 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">lmo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">ro</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lmx</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in x3 ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">rx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmx</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reconstruction difference of b0, b1, b2 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">eefce4fb76de</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;sklearn&#39;
</pre></div>
</div>
</div>
</div>
<p>The takeaway here is that we should include the omitted factors to reduce bias. But in practice, we can never know what all relevant factors are, and rarely can we measure all relevant factors.</p>
</div>
<div class="section" id="include-a-variable">
<span id="lm-include-variable"></span><h3>Include a Variable<a class="headerlink" href="#include-a-variable" title="Permalink to this headline">¶</a></h3>
<p>What if we add a new variable <span class="math notranslate nohighlight">\(x_p\)</span>? What will happen to the existing estimator <span class="math notranslate nohighlight">\(\hat\beta_k\)</span>?</p>
<p>Increase <span class="math notranslate nohighlight">\(\operatorname{Var}\left(\hat{\beta}_{k}\right)=\sigma^{2} \frac{1}{1-R_{-j}^{2}} \frac{1}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}}\)</span> if <span class="math notranslate nohighlight">\(R_{-k}^2\)</span> increases. When will <span class="math notranslate nohighlight">\(R^2_{-k}\)</span> be unchanged? When the new variable <span class="math notranslate nohighlight">\(x_p\)</span> has no explanatory power to <span class="math notranslate nohighlight">\(x_k\)</span>. See the <a class="reference internal" href="#lm-rsq-non-decreasing"><span class="std std-ref">exercise</span></a>.</p>
<p>In terms of bias, if we say the model with <span class="math notranslate nohighlight">\(x_p\)</span> is “true”, then <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\beta}_k \right)\)</span> is probably closer to <span class="math notranslate nohighlight">\(\beta_k\)</span> according to the equation described in the above <a class="reference internal" href="#lm-omit-variable"><span class="std std-ref">section</span></a>.</p>
</div>
<div class="section" id="multicollinearity">
<h3>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Definition (Multicollinearity)<br />
Multicollinearity measure the extent of pairwise correlation of variables in the design matrix.</p>
<div class="margin sidebar">
<p class="sidebar-title">Multicollinearity in computation</p>
<p>From numerical algebra’s perspective, the extent of correlation of variables in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> determines the condition number of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. As the correlation increases, its inverse becomes unstable. When perfect linear relation exists, then <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span> is not of full rank, and thus no inverse exists.</p>
</div>
<p>Definition (Perfect multicollinearity)<br />
A set of variables is perfectly multicollinear if a variable does not vary, or if there is an exact linear relationship between a set of variables:</p>
<div class="math notranslate nohighlight">
\[
X_{j}=\delta_{0}+\delta_{1} X_{1}+\cdots+\delta_{j-1} X_{j-1}+\delta_{i+1} X_{i+1}+\cdots+\delta_{k} X_{k}
\]</div>
<p>As long as the variables in the design matrix are not uncorrelated, then multicollinearity exists.</p>
<div class="section" id="diagnosis">
<h4>Diagnosis<a class="headerlink" href="#diagnosis" title="Permalink to this headline">¶</a></h4>
<p>Some common symptoms include</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F\)</span>-test is significant, <span class="math notranslate nohighlight">\(R^2\)</span> is good, but <span class="math notranslate nohighlight">\(t\)</span>-test is not significant.</p></li>
<li><p>Large magnitude of <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span></p></li>
<li><p>Large standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta_j)\)</span></p></li>
</ul>
<p>We can measure the extent of multicollinearity by <strong>variance inflation factor</strong> (VIF) for each explanatory variable.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{VIF}_j = \frac{1}{1-R_{-j}^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R_{-j}^2\)</span> is the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables excluding <span class="math notranslate nohighlight">\(X_j\)</span>. The value of <span class="math notranslate nohighlight">\(\operatorname{VIF}_j\)</span> can be interpreted as: the standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta)\)</span> is <span class="math notranslate nohighlight">\(\sqrt{\operatorname{VIF}_j}\)</span> times larger than it would have been without multicollinearity.</p>
<p>A second way of measurement is the <strong>condition number</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. If it is greater than <span class="math notranslate nohighlight">\(30\)</span>, then we can conclude that the multicollinearity problem cannot be ignored.</p>
<div class="math notranslate nohighlight">
\[
\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) = \sqrt{\frac{\lambda_1 (\boldsymbol{X} ^\top \boldsymbol{X} )}{\lambda_p (\boldsymbol{X} ^\top \boldsymbol{X} )} }
\]</div>
<p>Finally, <strong>correlation matrix</strong> can also be used to measure multicollinearity since it is closely related to the condition number <span class="math notranslate nohighlight">\(\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)\)</span>.</p>
</div>
<div class="section" id="consequences">
<h4>Consequences<a class="headerlink" href="#consequences" title="Permalink to this headline">¶</a></h4>
<ol>
<li><p>It inflates <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_j \right)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \operatorname{Var}\left( \hat{\beta}_j \right)
     &amp;= \sigma^2 \frac{1}{1- R^2_{-j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2}  \\
     &amp;=  \sigma^2 \frac{\operatorname{VIF}_j}{\operatorname{Var}\left( X_j \right)}  
     \end{align}\end{split}\]</div>
<p>When perfect multicollinearity exists, the variance goes to infinity since <span class="math notranslate nohighlight">\(R^2_{-j} = 1\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>-tests fail to reveal significant predictors, due to 1.</p></li>
<li><p>Estimated coefficients are sensitive to randomness in <span class="math notranslate nohighlight">\(Y\)</span>, i.e. unreliable. If you run the experiment again, the coefficients can change dramatically, which is measured by <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( X_1, X_2 \right)\)</span> is large, then we expect to have large <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right), \operatorname{Var}\left( \hat{\beta}_2 \right), \operatorname{Var}\left( \hat{\beta}_1, \hat{\beta}_2 \right)\)</span>, but <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)\)</span> can be small. This means we cannot distinguish the effect of <span class="math notranslate nohighlight">\(X_1 + X_2\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> is from <span class="math notranslate nohighlight">\(X_1\)</span> or <span class="math notranslate nohighlight">\(X_2\)</span>, i.e. <strong>non-identifiable</strong>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">By the fact that, for symmetric positive definite matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} \boldsymbol{b} = \sum \lambda_i b_i ^2
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} ^{-1}  \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} ^{-1}  \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} ^{-1}  \boldsymbol{b} = \sum \frac{1}{\lambda_i}  b_i ^2
 \]</div>
<p class="card-text">we have:</p>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx 0
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 - \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx \infty
 \]</div>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
</div>
</details></li>
</ol>
</div>
<div class="section" id="implications">
<h4>Implications<a class="headerlink" href="#implications" title="Permalink to this headline">¶</a></h4>
<p>If <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> show high correlation, then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1\)</span> may be a proxy of <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1 - X_2\)</span> may just be noise.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_2\)</span> is removed, <span class="math notranslate nohighlight">\(X_1\)</span> may still be good for prediction.</p></li>
</ol>
</div>
</div>
<div class="section" id="heteroscedasticity">
<h3>Heteroscedasticity<a class="headerlink" href="#heteroscedasticity" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
<div class="section" id="categorical-x">
<h3>Categorical <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#categorical-x" title="Permalink to this headline">¶</a></h3>
<p>dummy variables <span class="math notranslate nohighlight">\(X_ij\)</span></p>
<p>when <span class="math notranslate nohighlight">\(c = 2\)</span>,</p>
<p>interpretation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>: difference in means between the group with <span class="math notranslate nohighlight">\(X=1\)</span> and <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span>: mean of the group with <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
</ul>
<p>TBD</p>
<p>https://www.1point3acres.com/bbs/thread-703302-1-1.html</p>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<div class="section" id="slope-vs-correlation">
<h3>Slope vs Correlation<a class="headerlink" href="#slope-vs-correlation" title="Permalink to this headline">¶</a></h3>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we can see from the solution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\end{align}\]</div>
<p>that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= r_{X,Y} \frac{s_Y}{s_X}
\end{align}\end{split}\]</div>
<p>Thus, the slope has the same sign with the correlation <span class="math notranslate nohighlight">\(r_{X,Y}\)</span>, and equals to the correlation times a ratio of the sample standard deviations of the dependent variable over the independent variable.</p>
<p>Once can see that the magnitude of <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> increases with the magnitude of <span class="math notranslate nohighlight">\(\rho_{X,Y}\)</span> and <span class="math notranslate nohighlight">\(s_Y\)</span>, and decreases with <span class="math notranslate nohighlight">\(s_X\)</span>, holding others fixed.</p>
</div>
<div class="section" id="fitted-line-passes-sample-mean">
<h3>Fitted Line Passes Sample Mean<a class="headerlink" href="#fitted-line-passes-sample-mean" title="Permalink to this headline">¶</a></h3>
<p>Since <span class="math notranslate nohighlight">\(\hat{\beta}_{0} =\bar{y}-\hat{\beta}_{1} \bar{x}\)</span>, we have <span class="math notranslate nohighlight">\(\bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \bar{x}\)</span>, i.e. the regression line always goes through the mean <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span> of the sample.</p>
<p>This also hold for multiple regression, by the first order condition w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</div>
<div class="section" id="non-zero-mean-of-error-term">
<h3>Non-zero Mean of Error Term<a class="headerlink" href="#non-zero-mean-of-error-term" title="Permalink to this headline">¶</a></h3>
<p><em>What if the mean of the error term is not zero?</em></p>
<p>If <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = \mu_\varepsilon \ne 0\)</span>, we can just denote <span class="math notranslate nohighlight">\(\varepsilon = \mu_\varepsilon + v\)</span>, where <span class="math notranslate nohighlight">\(v\)</span> is a new error term with zero mean. Our model becomes</p>
<div class="math notranslate nohighlight">
\[
y_i = (\beta_0 + \mu_\varepsilon) + \beta_1 x_1 + v
\]</div>
<p>where <span class="math notranslate nohighlight">\((\beta_0 + \mu_\varepsilon)\)</span> is the new intercept. We can still apply the methods above to conduct estimation and inference.</p>
</div>
<div class="section" id="no-intercept">
<span id="lm-proportional-model"></span><h3>No Intercept<a class="headerlink" href="#no-intercept" title="Permalink to this headline">¶</a></h3>
<p><em>Assume the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> in the model <span class="math notranslate nohighlight">\(y=\beta_0 + \beta_1 x + \varepsilon\)</span> is zero. Find the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span>, denoted <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span>. Find its mean, variance, and compare them with those of the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span> when there is an intercept term.</em></p>
<p>If there is no intercept, consider a simple case</p>
<div class="math notranslate nohighlight">
\[
y = \beta x + \varepsilon
\]</div>
<p>Then by minimizing sum of squared errors</p>
<div class="math notranslate nohighlight">
\[
\min \sum_i (y_i - \beta x)^2
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
-2 \sum_i (y_i - \beta x) x = 0
\]</div>
<p>and hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}
&amp;= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \\
&amp;= \frac{\sum_i x_i (\beta x_i + \varepsilon_i)}{\sum_i x_i^2}\\
&amp;= \beta + \frac{\sum x_i \varepsilon_i}{\sum_i x_i^2}
\end{align}\end{split}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> is still an unbiased estimator of <span class="math notranslate nohighlight">\(\beta\)</span>, while its variance is smaller than the variance calculated assuming the intercept is non-zero.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}\left( \tilde{\beta} \right) = \frac{\sigma^2}{\sum x_i^2} \le  \frac{\sigma^2}{\sum (x_i - \bar{x})^2} = \operatorname{Var}\left( \hat{\beta}  \right)
\]</div>
<p>Hence, if the intercept is known to be zero, better use <span class="math notranslate nohighlight">\(\tilde\beta\)</span> instead of <span class="math notranslate nohighlight">\(\hat\beta\)</span>, since the standard error of the <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is smaller, and both are unbiased.</p>
<p>If the true model has a non-zero intercept, then <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is biased for <span class="math notranslate nohighlight">\(\beta\)</span>, but it has a smaller variance, which brings a tradeoff of bias vs variance.</p>
</div>
<div class="section" id="transformation-of-variables">
<h3>Transformation of Variables<a class="headerlink" href="#transformation-of-variables" title="Permalink to this headline">¶</a></h3>
<p>First, we take simple linear regression as an example.</p>
<p>If <span class="math notranslate nohighlight">\(X ^\prime = aX + b\)</span>, then the new slope estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
&amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, aX + b  \right)}{\widehat{\operatorname{Var}}\left( aX+b \right)}  \\
&amp;= \frac{a\widehat{\operatorname{Cov}}\left( Y, X \right)}{a^2\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= \frac{1}{a} \hat\beta_1 \\
\end{align}\end{split}\]</div>
<p>and the new intercept is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde\beta_0
&amp;= \bar{y} - \tilde\beta_1 \bar{x} ^\prime \\
&amp;= \bar{y} - \hat\beta_1 \frac{1}{a}  (a\bar{x}+b) \\
&amp;= \hat\beta_0 - \hat\beta_1 \frac{b}{a} \\
\end{align}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(Y ^\prime = cY + d\)</span> then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y ^\prime, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
&amp;= \frac{\widehat{\operatorname{Cov}}\left( cY+d, X  \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= \frac{c\widehat{\operatorname{Cov}}\left( Y, X \right)}{c\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= c \hat\beta_1 \\
\end{align}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde\beta_0
&amp;= \bar{y}^\prime - \tilde\beta_1 \bar{x} \\
&amp;= (c\bar{y}+d) - c\hat\beta_1 \bar{x} \\
&amp;= c\hat\beta_0 + d\\
\end{align}\end{split}\]</div>
<p>Can the conclusions be extended to multiple regression?</p>
<p>TBD.</p>
</div>
<div class="section" id="exchange-x-and-y">
<h3>Exchange <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span><a class="headerlink" href="#exchange-x-and-y" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
<div class="section" id="covariance-and-beta-j">
<h3>Covariance and <span class="math notranslate nohighlight">\(\beta_j\)</span><a class="headerlink" href="#covariance-and-beta-j" title="Permalink to this headline">¶</a></h3>
<p>In multiple regression, if <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y, X_j \right) = 0\)</span> then <span class="math notranslate nohighlight">\(\beta_j= 0\)</span>?</p>
<p>Is it possible that <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( X_j, X_k \right) \ne 0, \operatorname{Cov}\left( Y, X_k \right) \ne 0\)</span> but <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y, X_j \right) = 0\)</span>?</p>
<p>TBD.</p>
</div>
<div class="section" id="increase-estimation-precision">
<h3>Increase Estimation Precision<a class="headerlink" href="#increase-estimation-precision" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{-j}\)</span>, and hence decrease the variance.</p></li>
</ul>
</div>
<div class="section" id="r-squared-vs-hat-boldsymbol-beta">
<h3><span class="math notranslate nohighlight">\(R\)</span>-squared vs <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span><a class="headerlink" href="#r-squared-vs-hat-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
<div class="section" id="partialling-out-in-general-cases">
<h3>Partialling Out in General Cases<a class="headerlink" href="#partialling-out-in-general-cases" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
<div class="section" id="causal">
<h3>Causal?<a class="headerlink" href="#causal" title="Permalink to this headline">¶</a></h3>
<p>313.qz1.q2</p>
<p>TBD.</p>
</div>
<div class="section" id="r-squared-non-decreasing">
<span id="lm-rsq-non-decreasing"></span><h3><span class="math notranslate nohighlight">\(R\)</span>-squared Non-decreasing<a class="headerlink" href="#r-squared-non-decreasing" title="Permalink to this headline">¶</a></h3>
<p>Given a data set, when we add an new explanatory variable into a regression model, <span class="math notranslate nohighlight">\(R\)</span>-squared is non-decreasing.</p>
<p>This is equivalent to say <span class="math notranslate nohighlight">\(RSS\)</span> is always decreases or unchanged.</p>
<p>Since we are comparing two nested minimization problems</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{with } x_{p}  \quad \min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} _{n \times p} \boldsymbol{\beta} _{p\times 1} \right\Vert ^2  \\
\text{without } x_{p} \quad \min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} _{n \times p} \boldsymbol{\beta} _{p\times 1} \right\Vert ^2  \\
\text{s.t.}  &amp;\ \beta_{p} = 0
\end{aligned}\end{split}\]</div>
<p>As a result, the minimum value of the first problem should be no larger than the minimum value of the second problem. When will they be equal? Only when the solution <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span> in problem 1.</p>
<p>When will <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span> in problem 1? No clear condition.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>’s are orthogonal such that <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} = I_{p}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{x}_{p} ^\top \boldsymbol{y} = 0 \Leftrightarrow \hat{\beta}_{p}=0
  \]</div>
</li>
<li><p>Note that in general, <span class="math notranslate nohighlight">\(\not\Rightarrow\)</span>. An example is shown below where <span class="math notranslate nohighlight">\(\hat{\beta}_{2} \ne 0\)</span>. Also, in general, <span class="math notranslate nohighlight">\(\not\Leftarrow\)</span>. An simple example can be a data set of two points <span class="math notranslate nohighlight">\((1,0), (1,1)\)</span>. The fitted line is <span class="math notranslate nohighlight">\(y=0.5\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-regression.html" title="previous page">Regression</a>
    <a class='right-next' id="next-link" href="../32-classification/00-classification.html" title="next page">Classification</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>