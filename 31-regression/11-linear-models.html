
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dimensionality Reduction" href="../33-dimensionality-reduction/00-dimensionality-reduction.html" />
    <link rel="prev" title="Regression" href="00-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesianâ€™s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-stat-sampling.html">
     Statistical Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/13-denominations.html">
     Denominations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/01-information-theory.html">
     Information Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/11-linear-models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/11-linear-models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-learning">
   Estimation (Learning)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-squares">
     Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#by-assumptions">
     By Assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood">
     Maximum Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties">
   Properties
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slope-vs-correlation">
     Slope vs Correlation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitted-line-passes-sample-mean">
     Fitted Line Passes Sample Mean
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference">
     Inference
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#slope">
       Slope
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#residual">
       Residual
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#independence">
       Independence
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition-of-total-sum-of-squares">
     Decomposition of Total Sum of Squares
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection">
   Model Selection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-anova">
     What is ANOVA?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-cases">
   Special Cases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-zero-mean-of-error-term">
     Non-zero Mean of Error Term
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#no-intercept">
     No Intercept
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformation-of-variables">
     Transformation of Variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omitted-variable-bias">
     Omitted Variable Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heteroscedasticity">
     Heteroscedasticity
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models">
<h1>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">Â¶</a></h2>
<p>To estimate how <span class="math notranslate nohighlight">\(y\)</span> change with <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Model:</p>
<div class="math notranslate nohighlight">
\[Y_i  = \boldsymbol{x}_i ^\top \boldsymbol{\beta}  + \varepsilon_i \]</div>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}_{n\times p}\)</span> is called the design matrix. There are <span class="math notranslate nohighlight">\(p\)</span> independent variables (aka covariates) <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span> and <span class="math notranslate nohighlight">\(n\)</span> observations. The first columns is usually <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> , i.e. intercept.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}_{n \times 1}\)</span> is a vector of dependent variable.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{n \times 1}\)</span> are coefficients to be estimate</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}_{n \times 1}\)</span> are unobserved random error</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</div>
<p>which is called <strong>simple linear regression</strong>. When <span class="math notranslate nohighlight">\(p&gt;2\)</span>, it is called <strong>multiple linear regression</strong>. When there are multiple dependent variables, we call it <strong>multivariate regression</strong>.</p>
</div>
<div class="section" id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">Â¶</a></h2>
<p>Basic assumptions</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> is known and fixed.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span> is linear in covariates <span class="math notranslate nohighlight">\(X_j\)</span>.</p></li>
<li><p>The error terms are i.i.d. distributed with mean <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> and variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon_i \right) = \sigma^2\)</span>.</p></li>
</ol>
<p>As a result, <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y}  \mid \boldsymbol{X} \right) = \boldsymbol{X} \boldsymbol{\beta}\)</span>, or <span class="math notranslate nohighlight">\(\operatorname{E}\left( y \mid x \right) = \beta_0 + \beta_1 x\)</span> when <span class="math notranslate nohighlight">\(p=2\)</span>, which can be illustrated by the plots below.</p>
  <div align="center">
  <img src="../imgs/lm_cond_distribution.png" width = "50%" alt="" align=center />
  </div>
  <div align="center">
  <img src="../imgs/lm_xyplane_dots.png" width = "50%" alt="" align=center />
  </div>
<p>To predict <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, we just use <span class="math notranslate nohighlight">\(\hat{y}_i = \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\)</span> .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some social science courses, there is an Zero Conditional Mean assumption: <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \mid \boldsymbol{x}_i  \right) = 0\)</span> and it is used for estimation.</p>
</div>
<p>An additional stronger assumption on the distribution of error term (necessary for some inference)</p>
<ol class="simple">
<li><p>The error terms follow Gaussian distribution <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _n)\)</span>.</p></li>
</ol>
<p>As a result, we have <span class="math notranslate nohighlight">\(Y_i \sim N(\boldsymbol{x}_i ^\top \boldsymbol{\beta} , \sigma^2 )\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_n(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} _n)\)</span></p>
</div>
<div class="section" id="estimation-learning">
<h2>Estimation (Learning)<a class="headerlink" href="#estimation-learning" title="Permalink to this headline">Â¶</a></h2>
<p>We introduce various methods to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="section" id="least-squares">
<h3>Least Squares<a class="headerlink" href="#least-squares" title="Permalink to this headline">Â¶</a></h3>
<p>We can estimate the parameter <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by minimizing the sum of squared errors.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \hat{\boldsymbol{y}}  \right\Vert ^2 \\
&amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 \\
\end{align}\end{split}\]</div>
<p>The gradient w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\nabla_{\boldsymbol{\beta}} &amp;= -2 \boldsymbol{X}  ^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )  \\
&amp;\overset{\text{set}}{=} \boldsymbol{0}
\end{align}\end{split}\]</div>
<p>Hence, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}
\]</div>
<p>This linear system is called the <strong>normal equation</strong>.</p>
<p>The closed form solution is</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = \left( \boldsymbol{X} ^\top \boldsymbol{X}   \right)^{-1}\boldsymbol{X} \boldsymbol{y}  \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Computing software use specific functions to solve the normal equation <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, instead of using the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X}) ^{-1}\)</span> directly which can be slow and unstable.</p>
</div>
<p>An unbiased estimator of the error variance <span class="math notranslate nohighlight">\(\sigma^2 = \operatorname{Var}\left( \varepsilon \right)\)</span> is (to be discussed [later])</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\Vert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \right\Vert ^2}{n-p}
\]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0, \hat{\beta}_1 =  \underset{\beta_0, \beta_1 }{\mathrm{argmin}} \, \sum_i \left( y_i - \beta_0 - \beta_1 x_i \right)^2\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0
\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i)= 0
\]</div>
<p>Solve the system of the equations, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\hat{\beta}_{0} &amp;=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{align}\end{split}\]</div>
<p>Moreover,</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{n-2} \sum_i \hat\varepsilon_i^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\varepsilon_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The objective function, <strong>sum of squared errors</strong>,</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 = \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>can be replaced by <strong>mean squared error</strong>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>and the results are the same.</p>
</div>
</div>
<div class="section" id="by-assumptions">
<h3>By Assumptions<a class="headerlink" href="#by-assumptions" title="Permalink to this headline">Â¶</a></h3>
<p>In some social science courses, the estimation is done by using the assumptions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid X \right) = 0\)</span></p></li>
</ul>
<p>The first one gives</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{1}{n}  \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\end{equation}
\]</div>
<p>The second one gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Cov}\left( X, \varepsilon \right)
&amp;= \operatorname{E}\left( X \varepsilon \right) - \operatorname{E}\left( X \right) \operatorname{E}\left( \varepsilon \right) \\
&amp;= \operatorname{E}\left[ \operatorname{E}\left( X \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= \operatorname{E}\left[ X \operatorname{E}\left( \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= 0
\end{align}\end{split}\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>Therefore, we have the same normal equations to solve for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
</div>
<div class="section" id="maximum-likelihood">
<h3>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">Â¶</a></h3>
<p>biased. TBD.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">Â¶</a></h3>
<p>TBD.</p>
</div>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="slope-vs-correlation">
<h3>Slope vs Correlation<a class="headerlink" href="#slope-vs-correlation" title="Permalink to this headline">Â¶</a></h3>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we can see from the solution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\end{align}\]</div>
<p>that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= r_{X,Y} \frac{s_Y}{s_X}
\end{align}\end{split}\]</div>
<p>Thus, the slope has the same sign with the correlation <span class="math notranslate nohighlight">\(r_{X,Y}\)</span>, and equals to the correlation times a ratio of the sample standard deviations of the dependent variable over the independent variable.</p>
<p>Once can see that the magnitude of <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> increases with the magnitude of <span class="math notranslate nohighlight">\(\rho_{X,Y}\)</span> and <span class="math notranslate nohighlight">\(s_Y\)</span>, and decreases with <span class="math notranslate nohighlight">\(s_X\)</span>, holding others fixed.</p>
</div>
<div class="section" id="fitted-line-passes-sample-mean">
<h3>Fitted Line Passes Sample Mean<a class="headerlink" href="#fitted-line-passes-sample-mean" title="Permalink to this headline">Â¶</a></h3>
<p>Since <span class="math notranslate nohighlight">\(\hat{\beta}_{0} =\bar{y}-\hat{\beta}_{1} \bar{x}\)</span>, we have <span class="math notranslate nohighlight">\(\bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \bar{x}\)</span>, i.e. the regression line always goes through the mean <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span> of the sample.</p>
<p>This also hold for multiple regression, by the first order condition w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</div>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">Â¶</a></h3>
<div class="section" id="slope">
<h4>Slope<a class="headerlink" href="#slope" title="Permalink to this headline">Â¶</a></h4>
<p>The OLS estimators are unbiased since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \hat{\boldsymbol{\beta} } \right) &amp;= \operatorname{E}\left( (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \operatorname{E}\left( \boldsymbol{y} \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \boldsymbol{X} \boldsymbol{\beta} \\
&amp;= \boldsymbol{\beta}
\end{align}\end{split}\]</div>
<p>when <span class="math notranslate nohighlight">\(p=2\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1}
&amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\end{align}\end{split}\]</div>
<p>To prove unbiasedness, using the fact that for any constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_i (x_i - \bar{x})c = 0
\]</div>
<p>Then, the numerator becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
&amp;=\sum\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+u_{i}\right) \\
&amp;=\sum\left(x_{i}-\bar{x}\right) \beta_{0}+\sum\left(x_{i}-\bar{x}\right) \beta_{1} x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{0} \sum\left(x_{i}-\bar{x}\right)+\beta_{1} \sum\left(x_{i}-\bar{x}\right) x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{1} \sum\left(x_{i}-\bar{x}\right)^2 +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
\end{align}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\hat{\beta}_{1}=\beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}}
\end{equation}
\]</div>
<p>The variance (matrix) is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \boldsymbol{\beta}  \right) &amp;= \operatorname{Var}\left(  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right)  \\
&amp;=   (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \operatorname{Var}\left( \boldsymbol{y}  \right)  \boldsymbol{X}  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;= \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\\
\end{align}\end{split}\]</div>
<p>when <span class="math notranslate nohighlight">\(p=2\)</span>, it is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_1 \right)
&amp;= \operatorname{Var}\left( \beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}} \right)\\
&amp;= \frac{\operatorname{Var}\left( \sum\left(x_{i}-\bar{x}\right) u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sum\left(x_{i}-\bar{x}\right)^2 \operatorname{Var}\left( u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \sigma^2 \frac{\sum\left(x_{i}-\bar{x}\right)^2 }{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sigma^2}{\sum \left(x_{i}-\bar{x}\right)^{2}}\\
&amp;= \frac{\sigma^2}{(n-1)s_X^2}\\
\end{align}\end{split}\]</div>
<p>We conclude that</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the slope estimator</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance of the slope estimator</p></li>
<li><p>A larger sample size should decrease the variance of the slope estimator</p></li>
<li><p>A problem is that the error <span class="math notranslate nohighlight">\(\sigma^2\)</span> variance is <strong>unknown</strong></p></li>
</ul>
<p>We can estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> by its unbiased estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2=\frac{\sum_i (x_i - \bar{x})}{n-2}\)</span> (to be shown [below]), and substitute it into <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span>. Since the error variance <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is estimated, the slope variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span> is estimated too, and hence the square root is called standard error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, instead of standard deviation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{se}\left(\hat{\beta}_{1}\right)
&amp;= \sqrt{\widehat{\operatorname{Var}}\left( \hat{\beta}_1 \right)}\\
&amp;= \frac{\hat{\sigma}}{\sqrt{\sum \left(x_{i}-\bar{x}\right)^{2}}}
\end{align}\end{split}\]</div>
</div>
<div class="section" id="residual">
<h4>Residual<a class="headerlink" href="#residual" title="Permalink to this headline">Â¶</a></h4>
<p>The residual <span class="math notranslate nohighlight">\(\hat\varepsilon_i = y_i - \hat{y}_i\)</span> is an estimate of the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, and is the vertical difference between the fitted regression line and the observation points.</p>
<p>The sum of the residual is zero. Recall the normal equation</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top (\boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta} }) = \boldsymbol{0}
\]</div>
<p>Since the first column of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> , we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_i \hat{\varepsilon}_i  
&amp;= \sum_i(y_i - \hat{y}_i)  \\
&amp;= \sum_i(y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta} }_i)  \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>From the normal equation [below] we can also get</p>
<div class="math notranslate nohighlight">
\[
\sum_i x_{ij} \hat{\varepsilon}_i = 0
\]</div>
<p>for all <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>These two equations together gives the zero sample covariance between residuals and each covariate <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\widehat{\operatorname{Cov}}\left(X_j, \varepsilon \right) = 0
\]</div>
<p><strong>Distribution</strong></p>
<p>TBD</p>
</div>
<div class="section" id="independence">
<h4>Independence<a class="headerlink" href="#independence" title="Permalink to this headline">Â¶</a></h4>
<p>TBD</p>
</div>
</div>
<div class="section" id="decomposition-of-total-sum-of-squares">
<h3>Decomposition of Total Sum of Squares<a class="headerlink" href="#decomposition-of-total-sum-of-squares" title="Permalink to this headline">Â¶</a></h3>
<p>We can think of each observation as being made up of an explained part, and an unexplained part.</p>
<ul class="simple">
<li><p>Total sum of squares: <span class="math notranslate nohighlight">\(SST = \sum\left(y_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Sum of squares due to regression : <span class="math notranslate nohighlight">\(SSR = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Sum of squared errors: <span class="math notranslate nohighlight">\(SSE = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
SST
&amp;=\sum\left(y_{i}-\bar{y}\right)^{2} \\
&amp;=\sum\left[\left(y_{i}-\hat{y}_{i}\right)+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum\left[\hat{\varepsilon}_{i}+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum \hat{\varepsilon}_{i}^{2}+2 \sum \hat{\varepsilon}_{i}\left(\hat{y}_{i}-\bar{y}\right)+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2} \\
&amp;=\mathrm{SSR}+2 \sum \hat{\varepsilon}_{i}\left(\hat{\beta}_0 + \hat{\beta}_1 x_{i}-\bar{y}\right)+SSE \\
&amp;= SSR + SSE
\end{align}\end{split}\]</div>
<p>where use the fact that <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i x_i = 0\)</span> shown [above].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some social science courses use <span class="math notranslate nohighlight">\(SSR\)</span> and <span class="math notranslate nohighlight">\(SSE\)</span> to denote the opposite quantity in statistics courses.</p>
<ul class="simple">
<li><p>Explained sum of squares: <span class="math notranslate nohighlight">\(SSE = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Residual sum of squares: <span class="math notranslate nohighlight">\(SSR = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="model-selection">
<h2>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="r-squared">
<h3><span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">Â¶</a></h3>
<p><span class="math notranslate nohighlight">\(R\)</span>-squared (<span class="math notranslate nohighlight">\(R\)</span>^2) is a statistical measure that represents the proportion of the variance for a dependent variable thatâ€™s explained by an independent variable or variables in a regression model.</p>
<p>Whereas correlation explains the strength of the relationship between an independent and dependent variable, <span class="math notranslate nohighlight">\(R\)</span>-squared explains to what extent the variance of one variable explains the variance of the second variable.</p>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{SSR}{SST}  = 1 - \frac{SSE}{SST}
\]</div>
<p>relation with <span class="math notranslate nohighlight">\(\beta\)</span> in simple linear models:</p>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="what-is-anova">
<h3>What is ANOVA?<a class="headerlink" href="#what-is-anova" title="Permalink to this headline">Â¶</a></h3>
<p>The Analysis Of Variance, popularly known as the ANOVA, can be used in cases where there are more than two groups.</p>
</div>
</div>
<div class="section" id="special-cases">
<h2>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="non-zero-mean-of-error-term">
<h3>Non-zero Mean of Error Term<a class="headerlink" href="#non-zero-mean-of-error-term" title="Permalink to this headline">Â¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = \mu_\varepsilon \ne 0\)</span>, we can just denote <span class="math notranslate nohighlight">\(\varepsilon = \mu_\varepsilon + v\)</span>, where <span class="math notranslate nohighlight">\(v\)</span> is a new error term with zero mean. Our model becomes</p>
<div class="math notranslate nohighlight">
\[
y_i = (\beta_0 + \mu_\varepsilon) + \beta_1 x_1 + v
\]</div>
<p>where <span class="math notranslate nohighlight">\((\beta_0 + \mu_\varepsilon)\)</span> is the new intercept. We can still apply the methods above to conduct estimation and inference.</p>
</div>
<div class="section" id="no-intercept">
<h3>No Intercept<a class="headerlink" href="#no-intercept" title="Permalink to this headline">Â¶</a></h3>
<p>If there is no intercept, consider a simple case</p>
<div class="math notranslate nohighlight">
\[
y = \beta x + \varepsilon
\]</div>
<p>Then by minimizing sum of squared errors</p>
<div class="math notranslate nohighlight">
\[
\min \sum_i (y_i - \beta x)^2
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
-2 \sum_i (y_i - \beta x) x = 0
\]</div>
<p>and hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}
&amp;= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \\
&amp;= \frac{\sum_i x_i (\beta x_i + \varepsilon_i)}{\sum_i x_i^2}\\
&amp;= \beta + \frac{\sum x_i \varepsilon_i}{\sum_i x_i^2}
\end{align}\end{split}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> is still an unbiased estimator of <span class="math notranslate nohighlight">\(\beta\)</span>, while its variance is smaller than the variance calculated assuming the intercept is non-zero.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}\left( \tilde{\beta} \right) = \frac{\sigma^2}{\sum x_i^2} \le  \frac{\sigma^2}{\sum (x_i - \bar{x})^2} = \operatorname{Var}\left( \hat{\beta}  \right)
\]</div>
<p>Hence, if the intercept is known to be zero, better use <span class="math notranslate nohighlight">\(\tilde\beta\)</span> instead of <span class="math notranslate nohighlight">\(\hat\beta\)</span>, since the standard error of the <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is smaller, and both are unbiased.</p>
<p>If the true model has a non-zero intercept, then <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is biased for <span class="math notranslate nohighlight">\(\beta\)</span>, but it has a smaller variance, which brings a tradeoff of bias vs variance.</p>
</div>
<div class="section" id="transformation-of-variables">
<h3>Transformation of Variables<a class="headerlink" href="#transformation-of-variables" title="Permalink to this headline">Â¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(X ^\prime = aX + b\)</span>, then the new slope estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
&amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, aX + b  \right)}{\widehat{\operatorname{Var}}\left( aX+b \right)}  \\
&amp;= \frac{a\widehat{\operatorname{Cov}}\left( Y, X \right)}{a^2\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= \frac{1}{a} \hat\beta_1 \\
\end{align}\end{split}\]</div>
<p>and the new intercept is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde\beta_0
&amp;= \bar{y} - \tilde\beta_1 \bar{x} ^\prime \\
&amp;= \bar{y} - \hat\beta_1 \frac{1}{a}  (a\bar{x}+b) \\
&amp;= \hat\beta_0 - \hat\beta_1 \frac{b}{a} \\
\end{align}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(Y ^\prime = cY + d\)</span> then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y ^\prime, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
&amp;= \frac{\widehat{\operatorname{Cov}}\left( cY+d, X  \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= \frac{c\widehat{\operatorname{Cov}}\left( Y, X \right)}{c\widehat{\operatorname{Var}}\left( X \right)}  \\
&amp;= c \hat\beta_1 \\
\end{align}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde\beta_0
&amp;= \bar{y}^\prime - \tilde\beta_1 \bar{x} \\
&amp;= (c\bar{y}+d) - c\hat\beta_1 \bar{x} \\
&amp;= c\hat\beta_0 + d\\
\end{align}\end{split}\]</div>
</div>
<div class="section" id="omitted-variable-bias">
<h3>Omitted Variable Bias<a class="headerlink" href="#omitted-variable-bias" title="Permalink to this headline">Â¶</a></h3>
<p>TBD</p>
</div>
<div class="section" id="heteroscedasticity">
<h3>Heteroscedasticity<a class="headerlink" href="#heteroscedasticity" title="Permalink to this headline">Â¶</a></h3>
<p>TBD</p>
<p>https://www.1point3acres.com/bbs/thread-703302-1-1.html</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-regression.html" title="previous page">Regression</a>
    <a class='right-next' id="next-link" href="../33-dimensionality-reduction/00-dimensionality-reduction.html" title="next page">Dimensionality Reduction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>