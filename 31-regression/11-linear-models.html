
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Classification" href="../32-classification/00-classification.html" />
    <link rel="prev" title="Regression" href="00-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-stat-sampling.html">
     Statistical Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/13-denominations.html">
     Denominations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted JISP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/01-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html#isomap">
     Isomap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html#laplacian-eigenmaps">
     Laplacian Eigenmaps
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html#locally-linear-embedding">
     Locally Linear Embedding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html#maximum-variance-unfolding">
     Maximum Variance Unfolding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/11-linear-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/11-linear-models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/11-linear-models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/11-linear-models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation-learning">
   Estimation (Learning)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary-least-squares">
     Ordinary Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#by-assumptions">
     By Assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood">
     Maximum Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties">
   Properties
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coefficients">
     Coefficients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unbiasedness">
       Unbiasedness
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance">
       Variance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#efficiency-blue">
       Efficiency (BLUE)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consistency">
       Consistency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#large-sample-distribution">
       Large Sample Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals-and-error-variance">
     Residuals and Error Variance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#residuals">
       Residuals
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimation-of-error-variance">
       Estimation of Error Variance
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
     Independence of
     <span class="math notranslate nohighlight">
      \(\hat{\boldsymbol{\beta}}\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\hat{\sigma}^2\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sum-of-squares">
     Sum of Squares
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decomposition-of-tss">
       Decomposition of TSS
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#non-increasing-rss">
       Non-increasing RSS
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-of-estimated-coefficients">
     Value of Estimated Coefficients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partialling-out-explanation-for-mlr">
     Partialling Out Explanation for MLR
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#t-test-of-boldsymbol-v-top-boldsymbol-beta">
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -test of
     <span class="math notranslate nohighlight">
      \(\boldsymbol{v} ^\top \boldsymbol{\beta}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-interval-for-boldsymbol-v-top-boldsymbol-beta">
     Confidence Interval for
     <span class="math notranslate nohighlight">
      \(\boldsymbol{v} ^\top \boldsymbol{\beta}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-interval-for-y-new">
     Prediction Interval for
     <span class="math notranslate nohighlight">
      \(y_{new}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-region-for-boldsymbol-beta">
     Confidence Region for
     <span class="math notranslate nohighlight">
      \(\boldsymbol{\beta}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection">
   Model Selection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjusted-r-squared">
     Adjusted
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-test">
     <span class="math notranslate nohighlight">
      \(F\)
     </span>
     -test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anova">
     ANOVA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stepwise">
     Stepwise
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-cases">
   Special Cases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omit-a-variable">
     Omit a Variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#include-a-variable">
     Include a Variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multicollinearity">
     Multicollinearity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diagnosis">
       Diagnosis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consequences">
       Consequences
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implications">
       Implications
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heteroscedasticity">
     Heteroscedasticity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-x">
     Categorical
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models">
<h1>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this headline">¶</a></h1>
<p>In this section we introduce linear models from a statistics’ perspective. The introduction from econometrics’ perspective or social science’s perspective may be different. In short, the statistics’ perspective focuses on general multivariate cases and heavily rely on linear algebra for derivation, while the econometrics’ or the social science’s perspective prefers to introduce models in univariate cases by basic arithmetics (whose form can be complicated without linear algebra notations) and extend the intuitions and conclusions into multivariate cases.</p>
<!---
My handwritten notes for the graduate level course STAT 343 offered by UChicago statistics department can be found [here](../imgs/lm-notes-applied-stat.pdf).

Ref:

http://www3.grips.ac.jp/~yamanota/Lecture%20Note%204%20to%207%20OLS.pdf
-->
<p>Personally, I involved in four courses that introduced linear models, i.e. at undergrad/grad level offered by stat/social science department. The style of the two courses offered by the stat departments were quite alike while the graduate level one covered more topics. In both undergrad/grad level courses offered by the social science departments, sometimes I got confused by the course materials that were contradictory to my statistics training , but the instructors had no clear response or even no response at all…</p>
<p>In sum, to fully understand the most fundamental and widely used statistical model, I highly suggest to take a linear algebra course first and take the regression course offered by math/stat department.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Linear models aim to model the relationship between a scalar response and one or more explanatory variables in a linear format:</p>
<div class="math notranslate nohighlight">
\[Y_i  = \beta_0 + \beta_1 x_{i,1} + \ldots + \beta_{p-1} x_{i,p-1}  + \varepsilon_i \]</div>
<p>for observations <span class="math notranslate nohighlight">\(i=1, 2, \ldots, n\)</span>.</p>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}_{n\times p}\)</span> is called the design matrix. The first column is usually set to be <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span>, i.e., intercept. The remaining <span class="math notranslate nohighlight">\(p-1\)</span> columns are designed values <span class="math notranslate nohighlight">\(x_{ij}\)</span> where <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, n\)</span> and <span class="math notranslate nohighlight">\(j=1, \ldots, p-1\)</span>. These <span class="math notranslate nohighlight">\(p-1\)</span> columns are called explanatory/independent variables, or covariates.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{y}_{n \times 1}\)</span> is a vector of response/dependent variables <span class="math notranslate nohighlight">\(Y_1, Y_2, \ldots, Y_n\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{p \times 1}\)</span> is a vector of coefficients to be estimated.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}_{n \times 1}\)</span> is a vector of unobserved random errors, which includes everything that we have not measured and included in the model.</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</div>
<p>which is called <strong>simple linear regression</strong>.</p>
<p>When <span class="math notranslate nohighlight">\(p&gt;2\)</span>, it is called <strong>multiple linear regression</strong>. For instance, when <span class="math notranslate nohighlight">\(p=3\)</span></p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon_i
\]</div>
<p>When there are multiple dependent variables, we call it <strong>multivariate regression</strong>, which will be introduced in another section.</p>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span>,</p>
<ul class="simple">
<li><p>if we include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta_0\)</span> means that we use a single constant to predict <span class="math notranslate nohighlight">\(y_i\)</span>. The estimator, <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, by ordinary least square, should be the sample mean <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>if we do not include intercept, then the regression model <span class="math notranslate nohighlight">\(y_i = \beta x_i\)</span> means that we expect that <span class="math notranslate nohighlight">\(y\)</span> is proportional to <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Fixed or random <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>In natural science, researchers design <span class="math notranslate nohighlight">\(n\times p\)</span> values in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and run experiments to obtain the response <span class="math notranslate nohighlight">\(y_i\)</span>. We call this kind of data <strong>experimental data</strong>. In this sense, the explanatory variables <span class="math notranslate nohighlight">\(x_{ij}\)</span>’s are designed before the experiment, so they are also constants. The coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>’s are unknown constants. The error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is random. The response variable <span class="math notranslate nohighlight">\(Y_i\)</span> on the left hand side is random due to the randomness in the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<p>In social science, most of data is <strong>observational data</strong>. That is, researchers obtain the values of many variables at the same time, and choose one of interest to be the response variable <span class="math notranslate nohighlight">\(y_i\)</span> and some others to be the explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>. In this case, <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is viewed as a data set, and we can talk about descriptive statistics, such as variance of each explanatory variable, or covariance between pair of explanatory variables. This is valid since we often view the columns of a data set as random variables.</p>
<p>However, the inference methods of the coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> are developed based on the natural science setting, i.e., the values of explanatory variables are pre-designed constants. Many social science courses frequently use descriptive statistics of the explanatory variables which assumes they are random, and apply inference methods which assumes they are constant. This is quite confusing for beginners to linear models.</p>
<p>To be clear, we stick to the natural science setting and make the second assumption below. We use subscript <span class="math notranslate nohighlight">\(i\)</span> in every <span class="math notranslate nohighlight">\(y_i, x_i, \varepsilon_i\)</span> instead of <span class="math notranslate nohighlight">\(y, x, \varepsilon\)</span> which gives a sense that <span class="math notranslate nohighlight">\(x\)</span> is random. And we use descriptive statistics for the explanatory variables only when necessary.</p>
</div>
</div>
<div class="section" id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h2>
<p>Basic assumptions</p>
<ol>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span> is <strong>linear</strong> in covariates <span class="math notranslate nohighlight">\(X_j\)</span>.</p></li>
<li><p>The values of explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> are known and fixed. Randomness only comes from <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p></li>
<li><p>No <span class="math notranslate nohighlight">\(X_j\)</span> is constant for all observations. No exact linear relationships among the explanatory variables (aka no perfect multicollinearity, or the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is of full rank).</p></li>
<li><p>The error terms are uncorrelated <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right)= 0\)</span>, with common mean <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> and variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon_i \right) = \sigma^2\)</span> (homoskedasticity).</p>
<p>As a result, <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y} \mid \boldsymbol{X} \right) = \boldsymbol{X} \boldsymbol{\beta}\)</span>, or <span class="math notranslate nohighlight">\(\operatorname{E}\left( y_i \mid x_i \right) = \beta_0 + \beta_1 x_i\)</span> when <span class="math notranslate nohighlight">\(p=2\)</span>, which can be illustrated by the plots below.</p>
<div class="figure align-default" id="lm-distribution-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm-cond-distribution.png"><img alt="" src="../_images/lm-cond-distribution.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Distributions of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-distribution-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="lm-observation-of-y-given-x">
<a class="reference internal image-reference" href="../_images/lm-xyplane-dots.png"><img alt="" src="../_images/lm-xyplane-dots.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Observations of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> [Meyer 2021]</span><a class="headerlink" href="#lm-observation-of-y-given-x" title="Permalink to this image">¶</a></p>
</div>
<p>To predict <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, we just use <span class="math notranslate nohighlight">\(\hat{y}_i = \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\)</span> .</p>
</li>
<li><p>The error terms are independent and follow Gaussian distribution <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n (\boldsymbol{0} , \sigma^2 \boldsymbol{I} _n)\)</span>.</p>
<p>As a result, we have <span class="math notranslate nohighlight">\(Y_i \sim N(\boldsymbol{x}_i ^\top \boldsymbol{\beta} , \sigma^2 )\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_n(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} _n)\)</span></p>
</li>
</ol>
<p>These assumptions are used for different objectives. The first 3 assumptions are the base, and in additiona to them,</p>
<ul class="simple">
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by least squares uses no more assumptions.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by maximal likelihood uses assumptions 4 and 5.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> in 4.</p></li>
<li><p>derivation of <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta}} \right)\)</span> uses 1, 2, <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \varepsilon_i, \varepsilon_j \right) = 0\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \epsilon_i \right) = \sigma^2\)</span> in 4.</p></li>
<li><p>proof of Gaussian-Markov Theorem (BLUE) uses 4.</p></li>
<li><p>derivation of the distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span> uses 4 and 5.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title"> Zero conditional mean assumption</p>
<p>In some social science or econometrics courses, they follow the “Gauss-Markov assumptions” that are roughly the same to the assumptions, but in different formats. One of them is zero conditional mean assumption.</p>
<p>In general, it says</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \varepsilon \mid x_1, x_2, \ldots, x_p\right) = 0
\]</div>
<p>For <span class="math notranslate nohighlight">\(p=2\)</span>, it is</p>
<div class="math notranslate nohighlight">
\[\operatorname{E}\left( \varepsilon \mid x  \right) = 0\]</div>
<p>which (in their setting) implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \varepsilon \right)
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon \mid x \right) \right)\\
&amp;= 0\\
\operatorname{Cov}\left( \varepsilon, x \right)
&amp;= \operatorname{E}\left( \varepsilon x \right) - \operatorname{E}\left( \varepsilon \right)\operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( \operatorname{E}\left( \varepsilon x \mid x \right) \right)- 0 \times \operatorname{E}\left( x \right)\\
&amp;= \operatorname{E}\left( x \operatorname{E}\left( \varepsilon \mid x \right) \right) \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>Then they these two corollaries are used for <a class="reference internal" href="#lm-estimation-by-assumpation"><span class="std std-ref">estimation</span></a>.</p>
<p>As discussed above, in their setting <span class="math notranslate nohighlight">\(x\)</span> is random (at this stage), so they use notations such as <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( x, \varepsilon \right)\)</span>. It also seems that they view <span class="math notranslate nohighlight">\(\varepsilon\)</span> as an “overall” measure of random error, instead of <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> for specific <span class="math notranslate nohighlight">\(i\)</span> in the natural science setting. But they can mean so by using the conditional notation <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid x \right)\)</span>.</p>
</div>
</div>
<div class="section" id="estimation-learning">
<h2>Estimation (Learning)<a class="headerlink" href="#estimation-learning" title="Permalink to this headline">¶</a></h2>
<p>We introduce various methods to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="section" id="ordinary-least-squares">
<h3>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<p>The most common way is to estimate the parameter <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by minimizing the sum of squared errors <span class="math notranslate nohighlight">\(\sum_i(y_i-\hat{y}_i)^2\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title">A note on substitution</p>
<p>We substitute the predicted <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} }\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{X} \boldsymbol{\beta}\)</span>. The <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> here just means a variable in the optimization problem, not the unknown constant coefficients in our model.</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \hat{\boldsymbol{y}}  \right\Vert ^2 \\
&amp;= \underset{\boldsymbol{\beta} }{\mathrm{argmin}} \, \left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 \\
\end{align}\end{split}\]</div>
<p>The gradient w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\nabla_{\boldsymbol{\beta}} &amp;= -2 \boldsymbol{X}  ^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )  \\
&amp;\overset{\text{set}}{=} \boldsymbol{0}
\end{align}\end{split}\]</div>
<p>Hence, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}
\]</div>
<p>This linear system is called the <strong>normal equation</strong>.</p>
<p>The closed form solution is</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = \left( \boldsymbol{X} ^\top \boldsymbol{X}   \right)^{-1}\boldsymbol{X} ^\top  \boldsymbol{y}  \]</div>
<div class="dropdown tip admonition">
<p class="admonition-title"> View least squares as projection</p>
<p>Substitute the solve <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> into the prediction <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{y}} = \boldsymbol{X} \hat{\boldsymbol{\beta}} = \underbrace {\boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top}_{\boldsymbol{H}} \boldsymbol{y}
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> is a projection matrix onto the column space (image) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. Recall that a projection matrix onto the column span of a matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> has the form <span class="math notranslate nohighlight">\(\boldsymbol{P} _{\operatorname{col}(\boldsymbol{X} )} = \boldsymbol{X} \boldsymbol{X} ^\dagger\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\dagger =  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top\)</span> is the pseudo inverse of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<p>Essentially, we are trying to find a vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}\)</span> in the column space of the data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> that is as close to <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> as possible, and the closest one is just the projection of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> onto <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X})\)</span>, which is <span class="math notranslate nohighlight">\(\boldsymbol{H}\boldsymbol{y}\)</span>. The distance is measured by the norm <span class="math notranslate nohighlight">\(\left\| \boldsymbol{y} - \hat{\boldsymbol{y}}  \right\|\)</span>, which is the squared root of sum of squared errors. Note that <span class="math notranslate nohighlight">\(\boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X}) ^ \bot\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{H} = \boldsymbol{I}  - \boldsymbol{P}_{\operatorname{col}(\boldsymbol{X}) } = \boldsymbol{P}_{\operatorname{col}(\boldsymbol{X}) ^ \bot}\)</span> is the projection matrix onto the orthogonal complement <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X}) ^ \bot\)</span>.</p>
<div class="figure align-default" id="lm-projection">
<a class="reference internal image-reference" href="../_images/lm-projection.png"><img alt="" src="../_images/lm-projection.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Least squares as a projection <a class="reference external" href="https://waterprogramming.wordpress.com/2017/05/12/an-introduction-to-econometrics-part-1-classical-ordinary-least-squares-regression/">[Gold 2017]</a></span><a class="headerlink" href="#lm-projection" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Solving the linear system by software</p>
<p>Computing software use specific functions to solve the normal equation <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} ^\top \boldsymbol{y}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, instead of using the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X}) ^{-1}\)</span> directly which can be slow and numerically unstable. For instance, one can use QR factorization of <span class="math notranslate nohighlight">\(X\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \boldsymbol{Q} \left[\begin{array}{l}
\boldsymbol{R}_{p \times p}  \\
\boldsymbol{0}_{(n-p) \times p}
\end{array}\right]
\end{split}\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\| \boldsymbol{y} - \boldsymbol{X}  \boldsymbol{\beta}  \|^{2}
&amp;=\left\|\boldsymbol{Q} ^{\top} \boldsymbol{y}  - \boldsymbol{Q} ^\top \boldsymbol{X} \boldsymbol{\beta}  \right\|^{2} \\
&amp;=\left\|\left(\begin{array}{c}
\boldsymbol{f}  \\
\boldsymbol{r}
\end{array}\right)-\left(\begin{array}{c}
\boldsymbol{R} \boldsymbol{\beta}  \\
\boldsymbol{0}
\end{array}\right)\right\|^{2} \\
&amp;=\|\boldsymbol{f} - \boldsymbol{R} \boldsymbol{\beta} \|^{2}+\|\boldsymbol{r} \|^{2}
\end{aligned}
\end{split}\]</div>
<p>Finally</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\beta} = \boldsymbol{R} ^{-1} \boldsymbol{f}
\]</div>
</div>
<p>An unbiased estimator of the error variance <span class="math notranslate nohighlight">\(\sigma^2 = \operatorname{Var}\left( \varepsilon \right)\)</span> is (to be discussed [later])</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\Vert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \right\Vert ^2}{n-p}
\]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0, \hat{\beta}_1 =  \underset{\beta_0, \beta_1 }{\mathrm{argmin}} \, \sum_i \left( y_i - \beta_0 - \beta_1 x_i \right)^2\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0
\]</div>
<p>Differentiation w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
- 2\sum_i (y_i - \beta_0 - \beta_1 x_i) = 0
\]</div>
<p>Solve the system of the equations, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\hat{\beta}_{0} &amp;=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{align}\end{split}\]</div>
<p>The expression for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> implies that the fitted line cross the sample mean point <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>.</p>
<p>Moreover,</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{n-2} \sum_i \hat\varepsilon_i^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\varepsilon_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\)</span>.</p>
<div class="note admonition">
<p class="admonition-title"> Minimizing mean squared error</p>
<p>The objective function, <strong>sum of squared errors</strong>,</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y}  - \boldsymbol{X}  \boldsymbol{\beta}  \right\Vert ^2 = \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>can be replaced by <strong>mean squared error</strong>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_i \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2
\]</div>
<p>and the results are the same.</p>
</div>
</div>
<div class="section" id="by-assumptions">
<span id="lm-estimation-by-assumpation"></span><h3>By Assumptions<a class="headerlink" href="#by-assumptions" title="Permalink to this headline">¶</a></h3>
<p>In some social science courses, the estimation is done by using the assumptions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \mid X \right) = 0\)</span></p></li>
</ul>
<p>The first one gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>The second one gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Cov}\left( X, \varepsilon \right)
&amp;= \operatorname{E}\left( X \varepsilon \right) - \operatorname{E}\left( X \right) \operatorname{E}\left( \varepsilon \right) \\
&amp;= \operatorname{E}\left[ \operatorname{E}\left( X \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= \operatorname{E}\left[ X \operatorname{E}\left( \varepsilon \mid X \right) \right] - \operatorname{E}\left(  X \right)\operatorname{E}\left[ \operatorname{E}\left( \varepsilon \mid X\right) \right]\\
&amp;= 0
\end{align}\end{split}\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}  \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>Therefore, we have the same normal equations to solve for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>Estimation by the two assumptions derived from the zero conditional mean assumption can be problematic. Consider a model without intercept <span class="math notranslate nohighlight">\(y_i = \beta x_i + \varepsilon_i\)</span>. Fitting by OLS, we have only ONE first order condition</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>If we fit by assumptions, then in addition to the condition above, the first assumption <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = 0\)</span> also gives</p>
<div class="math notranslate nohighlight">
\[
 \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{1} x_{i}\right)=0
\]</div>
<p>These two conditions may not hold at the same time.</p>
</div>
</div>
<div class="section" id="maximum-likelihood">
<h3>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<p>biased. TBD.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>TBD.</p>
</div>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h2>
<p>We describe the properties of OLS estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and the corresponding residuals <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\varepsilon} }\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}=(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \boldsymbol{y}\)</span> is a random variable, since it is a linear combination of the random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. This means that, keeping <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> fixed, repeat the experiment, we will probably get different response values <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, and hence different <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. As a result, there is a sampling distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, and we can find its mean, variance, and conduct hypothesis testing.</p>
<div class="section" id="coefficients">
<h3>Coefficients<a class="headerlink" href="#coefficients" title="Permalink to this headline">¶</a></h3>
<div class="section" id="unbiasedness">
<h4>Unbiasedness<a class="headerlink" href="#unbiasedness" title="Permalink to this headline">¶</a></h4>
<p>The OLS estimators are unbiased since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \hat{\boldsymbol{\beta} } \right) &amp;= \operatorname{E}\left( (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \operatorname{E}\left( \boldsymbol{y} \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \boldsymbol{X} \boldsymbol{\beta} \\
&amp;= \boldsymbol{\beta}
\end{align}\end{split}\]</div>
<p>when <span class="math notranslate nohighlight">\(p=2\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1}
&amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\end{align}\end{split}\]</div>
<p>To prove unbiasedness, using the fact that for any constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_i (x_i - \bar{x})c = 0
\]</div>
<p>Then, the numerator becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
&amp;=\sum\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+u_{i}\right) \\
&amp;=\sum\left(x_{i}-\bar{x}\right) \beta_{0}+\sum\left(x_{i}-\bar{x}\right) \beta_{1} x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{0} \sum\left(x_{i}-\bar{x}\right)+\beta_{1} \sum\left(x_{i}-\bar{x}\right) x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{1} \sum\left(x_{i}-\bar{x}\right)^2 +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
\end{align}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\hat{\beta}_{1}=\beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}}
\end{equation}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta}_1 \right) = \beta_1
\]</div>
</div>
<div class="section" id="variance">
<span id="lm-inference-variance"></span><h4>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h4>
<p>The variance (covariance matrix) of the coefficients is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \boldsymbol{\beta}  \right) &amp;= \operatorname{Var}\left(  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right)  \\
&amp;=   (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \operatorname{Var}\left( \boldsymbol{y}  \right)  \boldsymbol{X}  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;= \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\\
\end{align}\end{split}\]</div>
<div class="admonition">
<p class="admonition-title"> Note</p>
<p>More specifically, for the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient estimator <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, its variance is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_j \right)
&amp;= \sigma^2 \left[ (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \right]_{[j,j]} \\
&amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2} \\
&amp;= \sigma^2 \frac{TSS_j}{RSS_j} \frac{1}{TSS_j} \\
&amp;= \sigma^2 \frac{1}{\sum_i(\hat{x}_{ij} - x_{ij})} \\
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(R_j^2\)</span>, <span class="math notranslate nohighlight">\(RSS_j\)</span>, <span class="math notranslate nohighlight">\(TSS_j\)</span>, and <span class="math notranslate nohighlight">\(\hat{x}_{ij}\)</span> are the corresponding representatives when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables.</p>
<p>Note that the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regressing <span class="math notranslate nohighlight">\(X_1\)</span> to an constant intercept is 0. So we have the particular result below.</p>
</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X} )^\top\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c}
\left(\boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1}
=\frac{1}{\sum_{i=1}^{n} \left(x_{i}-\bar{x}\right)^{2}}\left[\begin{array}{cc}
\bar{x^2} &amp; - \bar{x} \\
- \bar{x} &amp; 1
\end{array}\right]
\end{array}
\end{split}\]</div>
<p>the variance of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_1 \right)
&amp;= \operatorname{Var}\left( \beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}} \right)\\
&amp;= \frac{\operatorname{Var}\left( \sum\left(x_{i}-\bar{x}\right) u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sum\left(x_{i}-\bar{x}\right)^2 \operatorname{Var}\left( u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \sigma^2 \frac{\sum\left(x_{i}-\bar{x}\right)^2 }{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sigma^2}{\sum_{i=1}^n \left(x_{i}-\bar{x}\right)^{2}}\\
\end{align}\end{split}\]</div>
<p>We conclude that</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{j}\)</span>, and hence decrease the variance.</p></li>
</ul>
<p>A problem is that the error <span class="math notranslate nohighlight">\(\sigma^2\)</span> variance is <strong>unknown</strong>. In practice, we can estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> by its unbiased estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2=\frac{\sum_i (x_i - \bar{x})}{n-2}\)</span> (to be shown [link]), and substitute it into <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span>. Since the error variance <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is estimated, the slope variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span> is estimated too, and hence the square root is called standard error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, instead of standard deviation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{se}\left(\hat{\beta}_{1}\right)
&amp;= \sqrt{\widehat{\operatorname{Var}}\left( \hat{\beta}_1 \right)}\\
&amp;= \frac{\hat{\sigma}}{\sqrt{\sum \left(x_{i}-\bar{x}\right)^{2}}}
\end{align}\end{split}\]</div>
</div>
<div class="section" id="efficiency-blue">
<h4>Efficiency (BLUE)<a class="headerlink" href="#efficiency-blue" title="Permalink to this headline">¶</a></h4>
<dl class="simple myst">
<dt>Theorem (Gauss–Markov)</dt><dd><p>The ordinary least squares (OLS) estimator has the <strong>lowest</strong> sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. In abbreviation, the OLS estimator is BLUE: Best (lowest variance) Linear Unbiased Estimator.</p>
</dd>
</dl>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \boldsymbol{C} \boldsymbol{y}\)</span> be another linear estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. We can write <span class="math notranslate nohighlight">\(\boldsymbol{C} = \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \boldsymbol{X} ^\top + \boldsymbol{D}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{D} \ne \boldsymbol{0}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{E}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \operatorname{E}\left( \boldsymbol{C} \boldsymbol{y}   \right)\\
  &amp;= \boldsymbol{C} \operatorname{E}\left( \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  \right)\\
  &amp;= \boldsymbol{\beta} + \boldsymbol{D} \boldsymbol{X} \boldsymbol{\beta} \\
  \end{align}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}}\)</span> is unbiased iff <span class="math notranslate nohighlight">\(\boldsymbol{D} \boldsymbol{X} = 0\)</span>.</p>
<p>The variance is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \boldsymbol{C}\operatorname{Var}\left( \boldsymbol{y}  \right) \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \boldsymbol{C} \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  + (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{D} ^\top + \boldsymbol{D} \boldsymbol{X} \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^\top  + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right) + \sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \\
  \end{align}\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \in \mathrm{PSD}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right) \succeq \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)
  \]</div>
<p>The equality holds iff <span class="math notranslate nohighlight">\(\boldsymbol{D} ^\top \boldsymbol{D} = 0\)</span>, which implies that <span class="math notranslate nohighlight">\(\operatorname{tr}\left( \boldsymbol{D} \boldsymbol{D} ^\top \right) = 0\)</span>, then <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{D} \right\Vert _F^2 = 0\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{D} = 0\)</span>, i.e. <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta} } = \hat{\boldsymbol{\beta} }\)</span>. Therefore, BLUE is unique.</p>
</div>
<p>Moreover,</p>
<ul class="simple">
<li><p>If error term is normally distributed, then OLS is most efficient among all consistent estimators (not just linear ones).</p></li>
<li><p>When the distribution of error term is non-normal, other estimators may have lower variance than OLS such as least absolute deviation (median regression).</p></li>
</ul>
</div>
<div class="section" id="consistency">
<h4>Consistency<a class="headerlink" href="#consistency" title="Permalink to this headline">¶</a></h4>
<p>The OLS and consistent,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}_{OLS} \stackrel{P}{\rightarrow} \boldsymbol{\beta}
\]</div>
<p>since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{plim} \hat{\boldsymbol{\beta}}
&amp;= \operatorname{plim} \left( \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) \\
&amp;= \boldsymbol{\beta} + \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \underbrace{\operatorname{plim} \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) }_{=0 \text{ by CLM} }\\
&amp;= \boldsymbol{\beta} \\
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="large-sample-distribution">
<h4>Large Sample Distribution<a class="headerlink" href="#large-sample-distribution" title="Permalink to this headline">¶</a></h4>
<p>If we assume <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n(\boldsymbol{0} , \boldsymbol{I} _n)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} )
\]</div>
<p>Hence, the distribution of the coefficients estimator is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y}   \\
&amp;\sim  N(\boldsymbol{\beta} , (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} \operatorname{Var}\left( \boldsymbol{y}  \right)) \boldsymbol{X} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;\sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} ) \\
\end{aligned}\end{split}\]</div>
<p>The assumption may fail when the response variable <span class="math notranslate nohighlight">\(y\)</span> is</p>
<ul class="simple">
<li><p>right skewed, e.g. wages, savings</p></li>
<li><p>non-negative, e.g. counts, arrests</p></li>
</ul>
<p>When the normality assumption of the error term fails, the OLS estimator is <strong>asymptotically</strong> normal,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
<p>Therefore, in a large sample, even if the normality assumption fails, we can still do hypothesis testing which assumes normality.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}  - \boldsymbol{\beta} = \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}   \right) ^{-1} \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right)
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{A} =  \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}\)</span>. The limit variance is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{plim}\left[ \sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta} ) \cdot \sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta} )^\top \right]
&amp;= \operatorname{plim} \left[ \boldsymbol{A} ^{-1} \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{\varepsilon} \boldsymbol{\varepsilon} ^\top \boldsymbol{X}  \right) \boldsymbol{A} ^{-1}   \right] \\
&amp;=  \boldsymbol{A} ^{-1} \left( \frac{1}{n} \boldsymbol{X} ^\top \operatorname{plim} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon} ^\top \right)  \boldsymbol{X}  \right) \boldsymbol{A} ^{-1} \\
&amp;=  \boldsymbol{A} ^{-1} \left( \frac{\sigma^2 }{n} \boldsymbol{X} ^\top    \boldsymbol{X}  \right)    \boldsymbol{A} ^{-1} \\
&amp;=  \sigma^2  \boldsymbol{A} ^{-1}\boldsymbol{A} \boldsymbol{A} ^{-1} \\
&amp;= \sigma^2 \boldsymbol{A} ^{-1}\\
\end{aligned}\end{split}\]</div>
<p>where we used the fact that <span class="math notranslate nohighlight">\(\operatorname{plim} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon} ^\top \right) = \sigma^2 \boldsymbol{I} _n\)</span>.</p>
<p>Moreover, by the consistence of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\operatorname{plim}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )  = 0
\]</div>
<p>Therefore, the limit distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\sqrt{n}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{0} , \sigma^2 \boldsymbol{A} ^{-1}  )
\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
</div>
</div>
</div>
<div class="section" id="residuals-and-error-variance">
<h3>Residuals and Error Variance<a class="headerlink" href="#residuals-and-error-variance" title="Permalink to this headline">¶</a></h3>
<div class="section" id="residuals">
<h4>Residuals<a class="headerlink" href="#residuals" title="Permalink to this headline">¶</a></h4>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The residual is defined as the difference between the true response value <span class="math notranslate nohighlight">\(y\)</span> and our fitted response value <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat\varepsilon_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\]</div>
<p>It is an estimate of the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
</dd>
</dl>
<p><strong>Properties</strong></p>
<ol class="simple">
<li><p>The sum of the residual is zero: <span class="math notranslate nohighlight">\(\sum_i \hat{\varepsilon}_i = 0\)</span></p></li>
<li><p>The sum of the product of residual and any covariate is zero, or they are “uncorrelated”: <span class="math notranslate nohighlight">\(\sum_i x_{ij} \hat{\varepsilon}_i = 0\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>The sum of squared residuals: <span class="math notranslate nohighlight">\(\left\| \boldsymbol{\hat{\varepsilon}}  \right\|^2   = \left\| \boldsymbol{y} - \boldsymbol{H} \boldsymbol{y}   \right\|^2  = \boldsymbol{y} ^\top (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y}\)</span></p></li>
</ol>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Recall the normal equation</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top (\boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta} }) = \boldsymbol{0}
\]</div>
<p>We obtain</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
<p>Since the first column of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> , we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_i \hat{\varepsilon}_i  
&amp;= \sum_i(y_i - \hat{y}_i)  \\
&amp;= \sum_i(y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta} }_i)  \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>For other columns <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_j ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
<p>The 3rd equality holds since <span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{H}\)</span> is a projection matrix if <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> is a projection matrix, i.e.,</p>
<div class="math notranslate nohighlight">
\[
(\boldsymbol{I} - \boldsymbol{H}) (\boldsymbol{I} - \boldsymbol{H} ) = \boldsymbol{I} -\boldsymbol{H}
\]</div>
</div>
</div>
<div class="section" id="estimation-of-error-variance">
<h4>Estimation of Error Variance<a class="headerlink" href="#estimation-of-error-variance" title="Permalink to this headline">¶</a></h4>
<p>In the estimation section we mentioned the estimator for the error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2}=\frac{\|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}\|^{2}}{n-p}
\]</div>
<p>This is because</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2 \sim \sigma^2\chi ^2 _{n-p}  \\\end{split}\\\Rightarrow  \quad \sigma^2 =\operatorname{E}\left( \frac{\|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}\|^{2}}{n-p} \right)
\end{aligned}\end{align} \]</div>
<p>and we used the method of moment estimator. The derivation of the above expectation is a little involved.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} = [\boldsymbol{U} _ \boldsymbol{X} , \boldsymbol{U} _\bot]\)</span> be an orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^{n\times n}\)</span> where</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} _ \boldsymbol{X}  = [\boldsymbol{u} _1, \ldots, \boldsymbol{u} _p]\)</span> is an orthogonal basis of the column space (image) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, denoted <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X} )\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} _ \bot  = [\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _n]\)</span> is an orthogonal basis of the orthogonal complement of the column space (kernel) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, , denoted <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X} ) ^\bot\)</span>.</p></li>
</ul>
<p>Recall</p>
<div class="math notranslate nohighlight">
\[ \hat{\boldsymbol{\varepsilon}}  = \boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X} ) ^\bot\]</div>
<p>which is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2
&amp;= \left\| \boldsymbol{P} _{\boldsymbol{U} _\bot} \boldsymbol{y}   \right\|  \\
&amp;= \left\| \boldsymbol{U} _\bot \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top (\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon})    \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top  \boldsymbol{\varepsilon}    \right\|^2  \quad \because \boldsymbol{U} _\bot ^\top\boldsymbol{X} = \boldsymbol{0} \\
\end{aligned}\end{split}\]</div>
<p>Note that assuming <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N(\boldsymbol{0} , \sigma^2 \boldsymbol{I} )\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{U} _\bot ^\top \boldsymbol{\varepsilon}
&amp;\sim N_{n-p}(\boldsymbol{0} , \boldsymbol{U} _\bot ^\top \sigma^2 \boldsymbol{I}_n \boldsymbol{U} _\bot) \\
&amp;\sim N_{n-p}(\boldsymbol{0} , \sigma^2 \boldsymbol{I}_{n-p}) \\
\end{aligned}\end{split}\]</div>
<p>and hence the sum of squared normal variables follows</p>
<div class="math notranslate nohighlight">
\[
\left\| \boldsymbol{U} _\bot ^\top \boldsymbol{\varepsilon} \right\|  ^2 \sim \sigma^2 \chi ^2 _{n-p}  
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2 \sim \sigma^2 \chi ^2 _{n-p}  
\]</div>
<p>The first moment is</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \left\| \hat{\boldsymbol{\varepsilon}} \right\|^2    \right) = \sigma^2 (n-p)
\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[
\sigma^2 = \frac{\operatorname{E}\left( \left\| \hat{\boldsymbol{\varepsilon}} \right\|^2\right)}{n-p}
\]</div>
<p>Therefore, the method of moment estimator for <span class="math notranslate nohighlight">\(\sigma^2\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\| \hat{\boldsymbol{\varepsilon}}  \right\|^2  }{n-p}
\]</div>
<p>which is unbiased.</p>
</div>
<p>Can we find <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\sigma}^2  \right)\)</span> like we did for <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta}}  \right)\)</span>? No, unless we assume higher order moments of <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
</div>
</div>
<div class="section" id="independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
<span id="lm-independent-beta-sigma"></span><h3>Independence of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span><a class="headerlink" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2" title="Permalink to this headline">¶</a></h3>
<p>To prove the independence between the coefficients estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span> and the error variance estiamtor <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>, we need the Lemma below.</p>
<dl class="simple myst">
<dt>Lemma</dt><dd><p>Suppose a random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> follows multivariate normal distribution <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_m(\boldsymbol{\mu} , \sigma^2 I_m)\)</span> and <span class="math notranslate nohighlight">\(S, T\)</span> are orthogonal subspaces of <span class="math notranslate nohighlight">\(\mathbb{R} ^m\)</span>, then the two projected random vectors are independent</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
\boldsymbol{P}_S (\boldsymbol{y}) \perp\!\!\!\perp  \boldsymbol{P}_T (\boldsymbol{y})
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{z} \sim N(\boldsymbol{0} , \boldsymbol{I} _m)\)</span> be a standard multivariate normal random vector, and <span class="math notranslate nohighlight">\(\boldsymbol{U} = [\boldsymbol{U} _S, \boldsymbol{U} _T]\)</span> be orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^n\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;&amp;\boldsymbol{U} ^\top \boldsymbol{z} &amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \\
&amp;\Rightarrow&amp; \quad \left[\begin{array}{l}
\boldsymbol{U}_S ^\top \boldsymbol{z} \\
\boldsymbol{U}_T ^\top \boldsymbol{z} \\
\end{array}\right]&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \\
&amp;\Rightarrow&amp; \quad \boldsymbol{U} ^\top _S \boldsymbol{z}  &amp;\perp\!\!\!\perp \boldsymbol{U} ^\top _T \boldsymbol{z}  \\
&amp;\Rightarrow&amp; \quad  f(\boldsymbol{U} ^\top _S \boldsymbol{z})  &amp;\perp\!\!\!\perp f(\boldsymbol{U} ^\top _T \boldsymbol{z})  \\
\end{aligned}\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{\mu} + \sigma \boldsymbol{z}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{P} _S(\boldsymbol{y} ) = \boldsymbol{U} _S \boldsymbol{U} _S ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) \perp\!\!\!\perp \boldsymbol{U} _T \boldsymbol{U} _T ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) = \boldsymbol{P} _T(\boldsymbol{y} )
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a function of <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X}) } (\boldsymbol{y})\)</span> since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{P}_{\operatorname{im}(\boldsymbol{X} ) } (\boldsymbol{y})  \\
\end{aligned}\end{split}\]</div>
<p>and note that <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is a function of <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X} )^\bot } (\boldsymbol{y})\)</span> since</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2}=\frac{\|\hat{\boldsymbol{\varepsilon}} \|^{2}}{n-p} =
\frac{\left\| \boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X})^\bot } (\boldsymbol{y}) \right\| ^2  }{n-p}
\]</div>
<p>Therefore, by the lemma, they are independent. As a result, we can then perform <span class="math notranslate nohighlight">\(t\)</span>-test of the coefficients.</p>
</div>
<div class="section" id="sum-of-squares">
<h3>Sum of Squares<a class="headerlink" href="#sum-of-squares" title="Permalink to this headline">¶</a></h3>
<p>We can think of each observation as being made up of an explained part, and an unexplained part.</p>
<ul class="simple">
<li><p>Total sum of squares: <span class="math notranslate nohighlight">\(TSS = \sum\left(y_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Explained sum of squares: <span class="math notranslate nohighlight">\(ESS = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Residual sum of squares: <span class="math notranslate nohighlight">\(RSS = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
<div class="section" id="decomposition-of-tss">
<span id="lm-tss-identity"></span><h4>Decomposition of TSS<a class="headerlink" href="#decomposition-of-tss" title="Permalink to this headline">¶</a></h4>
<p>We have the decomposition identity</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
TSS
&amp;=\sum\left(y_{i}-\bar{y}\right)^{2} \\
&amp;=\sum\left[\left(y_{i}-\hat{y}_{i}\right)+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum\left[\hat{\varepsilon}_{i}+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum \hat{\varepsilon}_{i}^{2}+2 \sum \hat{\varepsilon}_{i}\left(\hat{y}_{i}-\bar{y}\right)+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2} \\
&amp;= RSS + 2  \sum \hat{\varepsilon}_{i}\left(\hat{\beta}_0 + \hat{\beta}_1 x_{i}-\bar{y}\right)+ ESS \\
&amp;= RSS + ESS
\end{align}\end{split}\]</div>
<p>where use the fact that <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i x_i = 0\)</span> shown [above].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some courses use the letters <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(E\)</span> to denote the opposite quantity in statistics courses.</p>
<ul class="simple">
<li><p>Sum of squares due to regression: <span class="math notranslate nohighlight">\(SSR = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Sum of squared errors: <span class="math notranslate nohighlight">\(SSE = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
</div>
<p>From linear algebra’s perspective, the identity is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y} - \bar{y} \boldsymbol{1} _n  \right\Vert ^2 = \left\Vert \boldsymbol{y} - \hat{\boldsymbol{y} }  \right\Vert ^2 + \left\Vert \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n\right\Vert ^2
\]</div>
<p>which holds because the LHS vector <span class="math notranslate nohighlight">\(\boldsymbol{y} - \bar{y}\boldsymbol{1} _n\)</span> is the the sum of the two RHS vectors, and the two vectors are orthogonal</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{y}  - \bar{y} \boldsymbol{1} _n = (\boldsymbol{y} - \hat{\boldsymbol{y} }) &amp;+ (\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n) \\
\boldsymbol{y} - \hat{\boldsymbol{y} } &amp;\perp \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n
\end{aligned}\end{split}\]</div>
<p>More specifically, they are orthogonal because</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} - \hat{\boldsymbol{y} } \in \operatorname{col}(\boldsymbol{X} )^\perp \quad  \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X} )
\]</div>
<p>since <span class="math notranslate nohighlight">\(\boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X})\)</span>, if an intercept term is included in the model.</p>
<p>drawing [here]</p>
</div>
<div class="section" id="non-increasing-rss">
<span id="lm-rss-nonincreasing"></span><h4>Non-increasing RSS<a class="headerlink" href="#non-increasing-rss" title="Permalink to this headline">¶</a></h4>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is equivalent to say <a class="reference internal" href="#lm-rsquared"><span class="std std-ref"><span class="math notranslate nohighlight">\(R\)</span>-squared</span></a> is always increasing or unchanged, if an intercept term in included in the model.</p>
</div>
<p>Given a data set, when we add an new explanatory variable into a regression model, <span class="math notranslate nohighlight">\(RSS\)</span> is non-increasing.</p>
<p>Since we are comparing two nested minimization problems</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\text{Problem 1 / Full model / with } X_{p}  \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} _{(p+1)\times 1} \right\Vert ^2   = \min \ RSS_1 \\
&amp;\text{Problem 2 / Reduced model / without } X_{p} \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} _{(p+1)\times 1} \right\Vert ^2  = \min \ RSS_2 \\
&amp;&amp;\text{s.t.}  &amp;\ \beta_{p} = 0
\end{aligned}\end{split}\]</div>
<p>Due to the constraint in Problem 2, the minimum value of the Problem 1 should be no larger than the minimum value of the Problem 2, i.e. <span class="math notranslate nohighlight">\(RSS_1^* \le RSS_2^*\)</span> , When will they be equal?</p>
<ul>
<li><p>From projection’s perspective, they are equal iff the additional orthogonal basis vector of the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> introduced by the new column <span class="math notranslate nohighlight">\(X_p\)</span> is orthogonal to the response vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. See the derivation of <a class="reference internal" href="#lm-f-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(F\)</span>-test</span></a> for details. Note that this is different from <span class="math notranslate nohighlight">\(\boldsymbol{x} _p ^\top \boldsymbol{y} =0\)</span>. The example below shows reduction in RSS even if <span class="math notranslate nohighlight">\(\boldsymbol{x} _p ^\top \boldsymbol{y} =0\)</span>.</p></li>
<li><p>From optimization’s perspective, they are equal iff <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span> in Problem 1’s solution. When will <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span>? No clear condition.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>’s are orthogonal such that <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} = I_{p}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{x}_{p} ^\top \boldsymbol{y} = 0 \Leftrightarrow \hat{\beta}_{p}=0
    \]</div>
</li>
<li><p>Note that in general, <span class="math notranslate nohighlight">\(\not\Leftarrow\)</span>. An simple example can be a data set of two points <span class="math notranslate nohighlight">\((1,0), (1,1)\)</span>. The fitted line is <span class="math notranslate nohighlight">\(y=0.5\)</span>.</p></li>
<li><p>Also, in general, <span class="math notranslate nohighlight">\(\not\Rightarrow\)</span>. The example below shows <span class="math notranslate nohighlight">\(\hat{\beta}_{2} \ne 0\)</span> even if <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top _p \boldsymbol{y} =0\)</span></p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>

<span class="c1"># reduced model</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>

<span class="c1"># full model</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="value-of-estimated-coefficients">
<h3>Value of Estimated Coefficients<a class="headerlink" href="#value-of-estimated-coefficients" title="Permalink to this headline">¶</a></h3>
<p><span class="math notranslate nohighlight">\(\beta_j\)</span> is the expected change in the value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if the value of the covariate <span class="math notranslate nohighlight">\(x_j\)</span> increases by 1, holding other covariates fixed.</p>
<p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the expected value of the response variable <span class="math notranslate nohighlight">\(y\)</span> if all covariates have values of zero.</p>
<p>If the response is in log format, i.e. <span class="math notranslate nohighlight">\(\log(Y)\)</span>, then the <span class="math notranslate nohighlight">\(\beta_j\)</span> can be interpreted as the percentage change in <span class="math notranslate nohighlight">\(Y\)</span> associated with one unit increase of <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Linear regression models only reveal linear associations between the response variable and the independent variables. But association does not imply causation. Simple example: in SLR, regress <span class="math notranslate nohighlight">\(X\)</span> over <span class="math notranslate nohighlight">\(Y\)</span>, the coefficient has same sign and significance??, but causation cannot be reversed.</p>
<p>Only when the data is from a randomized controlled trial, correlation will imply causation.</p>
</div>
</div>
<div class="section" id="partialling-out-explanation-for-mlr">
<h3>Partialling Out Explanation for MLR<a class="headerlink" href="#partialling-out-explanation-for-mlr" title="Permalink to this headline">¶</a></h3>
<p>We can interpret the coefficients in multiple linear regression from “partialling out” perspective.</p>
<p>When <span class="math notranslate nohighlight">\(p=3\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}
\]</div>
<p>We can obtain <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> by the following three steps</p>
<ol>
<li><p>regress <span class="math notranslate nohighlight">\(x_1\)</span> over <span class="math notranslate nohighlight">\(x_2\)</span> and obtain</p>
<div class="math notranslate nohighlight">
\[\hat{x}_{1}=\hat{\gamma}_{0}+\hat{\gamma}_{1} x_{2}\]</div>
</li>
<li><p>compute the residuals <span class="math notranslate nohighlight">\(\hat{u}_{1}\)</span> in the above regression</p>
<div class="math notranslate nohighlight">
\[
     \hat{u}_{i} = x_{1i} - \hat{x}_{1i}
     \]</div>
</li>
<li><p>regress <span class="math notranslate nohighlight">\(y\)</span> on the the residuals <span class="math notranslate nohighlight">\(\hat{u}_{1}\)</span>, and the estimated coefficient equals the required coefficient.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \hat{y}
     &amp;=\hat{\alpha}_{0}+\hat{\alpha}_{1} \hat{u} \\
     \hat{\alpha}_{1}
     &amp;= \frac{\sum (\hat{u}_i - \bar{\hat{u}}_i)(y_i - \bar{y})}{\sum (\hat{u}_i - \bar{\hat{u}}_i)^2} \\
     &amp;= \frac{\sum \hat{u}_{i}y_i}{\sum \hat{u}_{i}^2} \qquad \because \bar{\hat{u}}_i = 0\\
     &amp;\overset{\text{claimed}}{=} \hat{\beta}_1
     \end{align}\end{split}\]</div>
</li>
</ol>
<p>In this approach, <span class="math notranslate nohighlight">\(\hat{u}\)</span> is interpreted as the part in <span class="math notranslate nohighlight">\(x_1\)</span> that cannot be predicted by <span class="math notranslate nohighlight">\(x_2\)</span>, or is uncorrelated with <span class="math notranslate nohighlight">\(x_2\)</span>. We then regress <span class="math notranslate nohighlight">\(y\)</span> on <span class="math notranslate nohighlight">\(\hat{u}\)</span>, to get the effect of <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(y\)</span> after <span class="math notranslate nohighlight">\(x_2\)</span> has been “partialled out”.</p>
</div>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>In this section we talk about hypothesis testing and confidence intervals for <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \boldsymbol{\beta}\)</span> and other quantities. All these methods assume normality of the error terms <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)\)</span> unless otherwise specified. As a result,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1})
\]</div>
<p>since <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is an affine transformation of the error terms <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span></p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}  = \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}
\]</div>
<div class="section" id="t-test-of-boldsymbol-v-top-boldsymbol-beta">
<span id="lm-t-test"></span><h3><span class="math notranslate nohighlight">\(t\)</span>-test of <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \boldsymbol{\beta}\)</span><a class="headerlink" href="#t-test-of-boldsymbol-v-top-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<p>We can use <span class="math notranslate nohighlight">\(t\)</span>-test to conduct a hypothesis testing on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, which has a general form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H_0
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} = c \\
H_1
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} \ne c (\text{two-sided} )\\
\end{aligned}\end{split}\]</div>
<p>Usually <span class="math notranslate nohighlight">\(c=0\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(c=0, \boldsymbol{v} = \boldsymbol{e} _j\)</span> then this is equivalent to test <span class="math notranslate nohighlight">\(\beta_j=0\)</span>, i.e. the variable <span class="math notranslate nohighlight">\(X_i\)</span> has no effect on <span class="math notranslate nohighlight">\(Y\)</span> given all other variabels.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(c=0, v_i=1, v_j=-1\)</span> and <span class="math notranslate nohighlight">\(v_k=0, k\ne i, j\)</span> then this is equivalent to test <span class="math notranslate nohighlight">\(\beta_i = \beta_j\)</span>, i.e. the two variables <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> has the same effect on <span class="math notranslate nohighlight">\(Y\)</span> given all other variables.</p></li>
</ul>
<p>First, we need to find the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}\)</span>. Recall that</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}_{\text{null}} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1})
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}  \sim N(\boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} , \sigma^2 \boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} )
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim N(0, 1)
\]</div>
<p>Also recall that the RSS has the distribution</p>
<div class="math notranslate nohighlight">
\[
(n-p)\frac{\hat{\sigma}^2}{\sigma^2 } \sim \chi ^2 _{n-p}  
\]</div>
<p>and the two quantities <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\((n-p)\frac{\hat{\sigma}^2}{\sigma^2 }\)</span> are <a class="reference internal" href="#lm-independent-beta-sigma"><span class="std std-ref">independent</span></a>. Therefore, with a standard normal and a Chi-squared that are independent, we can construct a <span class="math notranslate nohighlight">\(t\)</span>-test statistic</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} / \sqrt{\frac{(n-p)\hat{\sigma}^2 }{\sigma^2 } / (n-p)} \sim t_{n-p}
\]</div>
<p>i.e.,</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\hat{\sigma}\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim t_{n-p}
\]</div>
<p>The RHS changes from <span class="math notranslate nohighlight">\(N(0,1)\)</span> to <span class="math notranslate nohighlight">\(t_{n-p}\)</span> because we are estiamteing <span class="math notranslate nohighlight">\(\sigma\)</span> by <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p>
<p>In particular, when <span class="math notranslate nohighlight">\(p=2\)</span>, to test <span class="math notranslate nohighlight">\(\beta_1 = c\)</span>, we use</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_1 - c}{\hat{\sigma}/ \sqrt{\operatorname{Var}\left( X_1 \right)}}  \sim t_{n-2}
\]</div>
<p>Another way to conduct a test without normality assumption is to use permutation test. For instance, to test <span class="math notranslate nohighlight">\(\beta_2=0\)</span>, we fix <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and sample the same <span class="math notranslate nohighlight">\(n\)</span> values of <span class="math notranslate nohighlight">\(x_2\)</span> from the column of <span class="math notranslate nohighlight">\(X_2\)</span>, and compute the <span class="math notranslate nohighlight">\(t\)</span> statistic. Repeat the permutation for multiple times and compute the percentage that</p>
<div class="math notranslate nohighlight">
\[
\left\vert t_{\text{perm} } \right\vert &gt;  \left\vert t_{\text{original} }  \right\vert
\]</div>
<p>which is the <span class="math notranslate nohighlight">\(p\)</span>-value.</p>
<div class="dropdown note admonition">
<p class="admonition-title"> Social science’s trick to test <span class="math notranslate nohighlight">\(\beta_1 = \beta_2\)</span></p>
<p>Some social science courses introduce a trick to test <span class="math notranslate nohighlight">\(\beta_1 = \beta_2\)</span> by rearranging the explanatory variables. For instance, if our model is,</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i
\]</div>
<p>Then they define <span class="math notranslate nohighlight">\(\gamma = \beta_1 - \beta_2\)</span> and <span class="math notranslate nohighlight">\(x_{i3} = x_{i1} + x_{i2}\)</span>, rearrange RHS,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
Y_i
&amp;= \beta_0 + (\gamma + \beta_2) x_{i1} + \beta_2 x_{i2} + \varepsilon_i \\
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 (x_{i1} + x_{i2}) + \varepsilon_i \\
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 x_{i3} + \varepsilon_i
\end{aligned}\end{split}\]</div>
<p>Finally, they run the regression of the last line and check the <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(\gamma\)</span>. Other parts of the model (<span class="math notranslate nohighlight">\(R\)</span>-squared, <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(\beta_0\)</span>, etc) remain the same.</p>
</div>
</div>
<div class="section" id="confidence-interval-for-boldsymbol-v-top-boldsymbol-beta">
<h3>Confidence Interval for <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \boldsymbol{\beta}\)</span><a class="headerlink" href="#confidence-interval-for-boldsymbol-v-top-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<p>Following the analysis above, we can find the <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence interval for a scalar <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \boldsymbol{\beta}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{v} }
\]</div>
<p>In particular,</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{v} = \boldsymbol{e}_j\)</span>, then this is the confidence interval for a coefficient <span class="math notranslate nohighlight">\(\boldsymbol{\beta} _j\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{v} = \boldsymbol{x}_i\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> is in the data set, then this is the confidence interval for in-sample fitting of <span class="math notranslate nohighlight">\(y_i\)</span>. We are making prediction at the mean value <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y} _i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span>. If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> is not in the design matrix, then we are doing out-of-sample prediction.</p></li>
</ul>
</div>
<div class="section" id="prediction-interval-for-y-new">
<h3>Prediction Interval for <span class="math notranslate nohighlight">\(y_{new}\)</span><a class="headerlink" href="#prediction-interval-for-y-new" title="Permalink to this headline">¶</a></h3>
<p>For a new <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, the new response is</p>
<div class="math notranslate nohighlight">
\[
y _{new} = \boldsymbol{x} ^\top \boldsymbol{\beta} + \boldsymbol{\varepsilon} _{new}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} _{new} \perp\!\!\!\perp \hat{\boldsymbol{\beta}} , \hat{\sigma}\)</span> since the RHS are from training set.</p>
<p>The prediction is</p>
<div class="math notranslate nohighlight">
\[
\hat{y} _{new} = \boldsymbol{x} ^\top \hat{\boldsymbol{\beta}}
\]</div>
<p>Thus, the prediction error is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y _{new} - \hat{y}_{new}
&amp;= \boldsymbol{\varepsilon} _{new} + \boldsymbol{x} ^\top (\boldsymbol{\beta} - \hat{\boldsymbol{\beta}} )\\
&amp;\sim N \left( \boldsymbol{0} , \sigma^2 (1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} ) \right) \\
\end{aligned}\end{split}\]</div>
<p>Hence, the <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence prediction interval for a new response value <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span> at an out-of-sample <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} }
\]</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Width of an interval</p>
<p>When we are building confidence interval for <span class="math notranslate nohighlight">\(\boldsymbol{y} _i\)</span> or prediction interval for <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span>, the width depends on the magnitude of <span class="math notranslate nohighlight">\(n\)</span> the choice of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p>
<p>As <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>, we have <span class="math notranslate nohighlight">\(\boldsymbol{a} ^\top
(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{a}\rightarrow 0\)</span> for all <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span>, hence the</p>
<ul class="simple">
<li><p>CI for <span class="math notranslate nohighlight">\(\boldsymbol{y} _i\)</span>: <span class="math notranslate nohighlight">\(\operatorname{se}  \rightarrow 0, \operatorname{width} \rightarrow 0\)</span></p></li>
<li><p>PI for <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span>: <span class="math notranslate nohighlight">\(\operatorname{se}  \rightarrow \hat{\sigma}, \operatorname{width} \rightarrow 2 \times t_{n-p}^{(1-\alpha/2)} \hat{\sigma}\)</span></p></li>
</ul>
<p>The width also depends on the choice of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is aligned with a large eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}\)</span> is small. This is because larger eigenvectors indicate a direction of large variation in the data set, and hence it has more distinguishability.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is aligned with a small eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}\)</span> is large. This is because smaller eigenvectors indicate a direction of small variation in the data set, and hence it has less distinguishability and more uncertainty.</p></li>
</ul>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/pca-pc-ellipsoids.png"><img alt="" src="../_images/pca-pc-ellipsoids.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Illustration of eigenvectors in bivariate Gaussian [Fung 2018]</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="confidence-region-for-boldsymbol-beta">
<h3>Confidence Region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span><a class="headerlink" href="#confidence-region-for-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>To test <span class="math notranslate nohighlight">\(\boldsymbol{\beta}=\boldsymbol{0}\)</span>, see <a class="reference internal" href="#lm-f-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(F\)</span>-test</span></a></p>
</div>
<p>If we want to draw conclusions to multiple coefficients <span class="math notranslate nohighlight">\(\beta_1, \beta_2, \ldots\)</span> simultaneously, we need a confidence region, and consider the multiple testing issue.</p>
<p>To find a <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, one attemp is to use a cuboid, whose <span class="math notranslate nohighlight">\(j\)</span>-th side length equals to the <span class="math notranslate nohighlight">\((1-\alpha/p)-%\)</span> confidence interval for <span class="math notranslate nohighlight">\(\beta_j\)</span>. Namely, the confidence region is</p>
<div class="math notranslate nohighlight">
\[
\left[ (1-\alpha/p) \text{ C.I. for } \beta_0 \right] \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_1 \right] \times \ldots \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_{p-1} \right]
\]</div>
<p>In this way, we ensure the overall confidence of the confidence region is at least <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{P}\left( \text{every $\beta$ is in its C.I.}  \right) \ge 1-\alpha
\]</div>
<p>A more natural approach is using an ellipsoid. Recall that</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
<p>Hence a pivot quantity for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> can be constructed as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;&amp;\frac{(\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{\sigma^2 }
&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _p)  \\
&amp;\Rightarrow&amp; \ \frac{1}{\sigma^2} \left\| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right\|^2    &amp;\sim \chi ^2 _p \\
&amp;\Rightarrow&amp; \ \frac{\frac{1}{\sigma^2} \left\| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right\|^2/p}{\frac{(n-p)\hat{\sigma}^2}{\sigma^2 }/(n-p)}   &amp;\sim F_{p, n-p}\\
&amp;\Rightarrow&amp; \ \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{p \hat{\sigma}^2}   &amp;\sim F_{p, n-p}\\
\end{aligned}\end{split}\]</div>
<p>Therefore, we can obtain an <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> from this distribution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\beta} \in \left\{ \boldsymbol{v} \in \mathbb{R} ^p: \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{v} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{v} )}{p \hat{\sigma}^2}   \le F_{p, n-p}^{(1-\alpha)} \right\}
\]</div>
<p>which is an ellipsoid centered at <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, scaled by <span class="math notranslate nohighlight">\(\frac{1}{p \hat{\sigma}}\)</span>, rotated by <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X})\)</span>.</p>
<p>In general, for matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R} ^{p \times k}, \operatorname{rank}\left( \boldsymbol{A}  \right) = k\)</span>, the confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{A} ^\top \boldsymbol{\beta}\)</span> can be found in a similar way</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A} ^\top \hat{\boldsymbol{\beta}}
&amp;\sim N(\boldsymbol{A} ^\top \boldsymbol{\beta} , \sigma^2 \boldsymbol{A} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}  \boldsymbol{A} ) \\
\Rightarrow \quad\ldots &amp;\sim F_{k, n-p} \\
\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="model-selection">
<h2>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this headline">¶</a></h2>
<div class="section" id="r-squared">
<span id="lm-rsquared"></span><h3><span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>Assuming the <a class="reference internal" href="#lm-tss-identity"><span class="std std-ref">decomposition identity</span></a> of <span class="math notranslate nohighlight">\(TSS\)</span> holds, we can define <span class="math notranslate nohighlight">\(R\)</span>-squared.</p>
<p>Defintion<br />
<span class="math notranslate nohighlight">\(R\)</span>-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
<div class="math notranslate nohighlight">
\[
  R^2 = \frac{SSR}{SST}  = 1 - \frac{SSE}{SST} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
  \]</div>
<p><strong>Properties</strong></p>
<ol>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared can never decrease when an additional explanatory variable is added to the model.</p>
<p>As long as <span class="math notranslate nohighlight">\(Cov(Y, X_j) \ne 0\)</span>, then <span class="math notranslate nohighlight">\(X_j\)</span> has some explanatory power to <span class="math notranslate nohighlight">\(Y\)</span>, and thus <span class="math notranslate nohighlight">\(RSS\)</span> decreases, See the <a class="reference internal" href="#lm-rss-nonincreasing"><span class="std std-ref">section</span></a> of <span class="math notranslate nohighlight">\(RSS\)</span> for details. As a result, <span class="math notranslate nohighlight">\(R\)</span>-squared  is not a good measure for model selection, which can cause overfitting.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared equals the squared correlation coefficient between the actual value of the response and the fitted value <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( Y, \hat{Y} \right)^2\)</span>.</p>
<p>In particular, in simple linear regression, <span class="math notranslate nohighlight">\(R^2 = \rho_{X,Y}^2\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>By the definition of correlation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \operatorname{Corr}\left( y, \hat{y} \right)^2
    &amp;= \frac{\operatorname{Cov}\left( y, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Cov}\left( \hat{y} + \hat{\varepsilon}, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Cov}\left( \hat{y} , \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Var}\left( \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Var}\left( \hat{y} \right)}{\operatorname{Var}\left( y \right)} \\
    &amp;= \frac{SSR}{SST} \\
    &amp;= R^2 \\
    \end{align}\end{split}\]</div>
<p>The third equality holds since</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Cov}\left( \hat{\varepsilon}, \hat{y} \right) = \operatorname{Cov}\left( \hat{\varepsilon}, \sum_j x_j \hat{\beta}_j  \right) = \sum_j \hat{\beta}_j \operatorname{Cov}\left( \hat{\varepsilon},  x_j \right) = 0
    \]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, since <span class="math notranslate nohighlight">\(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    R^2 = \operatorname{Corr}\left(y, \hat{y} \right)^2 = \operatorname{Corr}\left(y, x \right)^2
    \]</div>
</div>
</li>
</ol>
<p>When there is <strong>no</strong> intercept, then <span class="math notranslate nohighlight">\(\bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)\)</span> and hence <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)\)</span>. The decomposition identity may not hold. Thus, the value of <span class="math notranslate nohighlight">\(R\)</span>-squared may no longer be in <span class="math notranslate nohighlight">\([0,1]\)</span>, and its interpretation is no longer valid. What actually happen to the value <span class="math notranslate nohighlight">\(R\)</span>-squared depends on whether we define it using <span class="math notranslate nohighlight">\(TSS\)</span> with <span class="math notranslate nohighlight">\(RSS\)</span> or <span class="math notranslate nohighlight">\(ESS\)</span>.</p>
<p>If we define <span class="math notranslate nohighlight">\(R^2 = \frac{ESS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{ESS} = \left\Vert \hat{\boldsymbol{y} } - \bar{y}\boldsymbol{1} _n \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(ESS &gt; TSS\)</span>, i.e., <span class="math notranslate nohighlight">\(R^2 &gt; 1\)</span>.</p>
<p>On the other hand, if we define <span class="math notranslate nohighlight">\(R^2 = 1 - \frac{RSS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{RSS} = \left\Vert \hat{\boldsymbol{y} } -  \boldsymbol{y} \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(RSS &gt; TSS\)</span>, i.e. <span class="math notranslate nohighlight">\(R^2 &lt; 0\)</span>.</p>
</div>
<div class="section" id="adjusted-r-squared">
<h3>Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline">¶</a></h3>
<p>Due to the non-decrease property of <span class="math notranslate nohighlight">\(R\)</span>-squared, we define adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared which is a better measure of goodness of fitting.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared, denoted by <span class="math notranslate nohighlight">\(\bar{R}^2\)</span>, is defined as</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
  \bar{R}^2 = 1-\frac{RSS / (n-p)}{ TSS / (n-1)}
  \]</div>
<p>Properties</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{R}^2\)</span> can increase or decrease. When a new variable is included, <span class="math notranslate nohighlight">\(RSS\)</span> decreases, but <span class="math notranslate nohighlight">\((n-p)\)</span> also decreases.</p>
<ul>
<li><p>Relation to <span class="math notranslate nohighlight">\(R\)</span>-squared is</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
\bar{R}^2 = 1-\frac{n-1}{ n-p}(1 - R^2) &lt; R^2
\]</div>
<ul>
<li><p>Relation to estimated variance of random error and variance of response</p>
<div class="math notranslate nohighlight">
\[
    \bar{R}^2 = 1-\frac{\hat{\sigma}^2}{\operatorname{Var}\left( y \right)}
    \]</div>
</li>
<li><p>Can be negative when</p>
<div class="math notranslate nohighlight">
\[
    R^2 &lt; \frac{p-1}{n-p}
    \]</div>
<p>If <span class="math notranslate nohighlight">\(p &gt; \frac{n+1}{2}\)</span> then the above inequality always hold, and adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared is always negative.</p>
</li>
</ul>
</div>
<div class="section" id="f-test">
<span id="lm-f-test"></span><h3><span class="math notranslate nohighlight">\(F\)</span>-test<a class="headerlink" href="#f-test" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">Nested</p>
<p>It is called nested since the reduced model is a special case of the full model with</p>
<div class="math notranslate nohighlight">
\[
\beta_{p-k}=\ldots= \beta_{p-1} =0
\]</div>
</div>
<p>To compare two nested models</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Full model: } Y &amp;\sim \left\{ X_j, j=1, \ldots, p-1 \right\} \\
\text{Reduced model: } Y &amp;\sim \left\{ X_j, j=1, \ldots, p-k-1 \right\}
\end{aligned}\end{split}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">Interpretation of <span class="math notranslate nohighlight">\(F\)</span>-test</p>
<p>Given it’s form, we can interpret the numerator as an average reduction in <span class="math notranslate nohighlight">\(RSS\)</span> by adding the <span class="math notranslate nohighlight">\(k\)</span> explanatory variables. Since the denominator is fixed, if average reduction is large enough, then we reject the null hypothesis that their coefficients are 0.</p>
</div>
<p>We can use the <span class="math notranslate nohighlight">\(F\)</span>-test. The test statistic is</p>
<div class="math notranslate nohighlight">
\[
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full}}/(n-p)} \sim F_{k, n-p}
\]</div>
<p>which can be computed by <span class="math notranslate nohighlight">\(R^2\)</span> since <span class="math notranslate nohighlight">\(TSS\)</span> are the same for the two models</p>
<div class="math notranslate nohighlight">
\[
F = \frac{(R^2 _{\text{full}} - R^2 _{\text{reduced}})/k}{(1 - R^2 _{\text{full}})/(n-p)}
\]</div>
<p>In particular,</p>
<ul>
<li><p>When <span class="math notranslate nohighlight">\(k=p-1\)</span>, we are comparing a full model vs. intercept only, i.e.,</p>
<div class="math notranslate nohighlight">
\[
    \beta_1 = \ldots = \beta_p-1 = 0
    \]</div>
<p>In this case,</p>
<div class="math notranslate nohighlight">
\[
    RSS_{\text{reduced}} = \left\| \boldsymbol{y} - \bar{y} \boldsymbol{1} _n \right\|  ^2 = TSS
    \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
    F = \frac{(TSS - RSS_{\text{full}})/(p-1)}{RSS_{\text{full}}/(n-p)}  = \frac{R^2 _{\text{full}}/k}{(1 - R^2 _{\text{full}})/(n-p)}
    \]</div>
</li>
<li><p>When <span class="math notranslate nohighlight">\(k=1\)</span>, we are testing <span class="math notranslate nohighlight">\(\beta_{p-1} = 0\)</span>. In this case, the <span class="math notranslate nohighlight">\(F\)</span>-test is equivalent to the <span class="math notranslate nohighlight">\(t\)</span>-test. The two test statistics have the relation <span class="math notranslate nohighlight">\(F_{1, n-p}=t^2_{n-p}\)</span>.</p></li>
</ul>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>We need to find the distribution of <span class="math notranslate nohighlight">\(RSS_{\text{reduced} }\)</span> and <span class="math notranslate nohighlight">\(RSS_{\text{full}}\)</span> and then construct a pivot quantity.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> be an orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^n\)</span> with three orthogonal parts</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{U}  = [\underbrace{\boldsymbol{u} _1, \ldots, \boldsymbol{u} _{p-k}} _{\boldsymbol{U} _1}, \underbrace{\boldsymbol{u} _{p-k+1}, \ldots, \boldsymbol{u} _{p}} _{\boldsymbol{U} _2}, \underbrace{\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _{n}} _{\boldsymbol{U} _3}]
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{U} ^\top \boldsymbol{y} = \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{y}  \\
\boldsymbol{U} _2 ^\top  \boldsymbol{y}  \\
\boldsymbol{U} _3 ^\top  \boldsymbol{y}  \\
\end{array}\right]
\sim N_n \left( \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\boldsymbol{U} _2 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\boldsymbol{U} _3 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\end{array}\right] , \sigma^2 \boldsymbol{I} _n \right)
\end{split}\]</div>
<p>Thus, we have pairwise independences among <span class="math notranslate nohighlight">\(\boldsymbol{U}_1 ^\top \boldsymbol{y} , \boldsymbol{U} _2 ^\top \boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{U} _3 ^\top \boldsymbol{y}\)</span>.</p>
<p>Moreover, by the property of multivariate normal, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _k \\
\left\| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}   \\
\end{aligned}\end{split}\]</div>
<p>The RSSs have the relations</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
RSS_{\text{full} }
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _1 \boldsymbol{U} _2) ^\bot } \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _3} \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{U} ^\top _3 \boldsymbol{y}  \right\| ^2 \\
RSS_{\text{reduced} }
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _1) ^\bot } \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}([\boldsymbol{U} _2 \boldsymbol{U} _3])} \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \left[ \boldsymbol{U}_2, \boldsymbol{U}_3 \right] ^\top \boldsymbol{y}  \right\| ^2   \\
&amp;= \left\| \boldsymbol{U} ^\top _2  \boldsymbol{y}   \right\| ^2 +  \left\| \boldsymbol{U} ^\top _3 \boldsymbol{y}   \right\| ^2
\end{aligned}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
RSS_{\text{reduced} } - RSS_{\text{full} } = \left\| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _k \\
RSS_{\text{full} } =  \left\| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}  \\
\end{aligned}\end{split}\]</div>
<p>Therefore, we have the pivot quantity</p>
<div class="math notranslate nohighlight">
\[
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full} }/(n-p)}  \sim F_{k, n-p}
\]</div>
</div>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>A <span class="math notranslate nohighlight">\(F\)</span>-test on <span class="math notranslate nohighlight">\(\beta_1=\beta_2=0\)</span> is difference from two univariate <span class="math notranslate nohighlight">\(t\)</span>-tests <span class="math notranslate nohighlight">\(\beta_1=0, \beta_2=0\)</span>. A group of <span class="math notranslate nohighlight">\(t\)</span>-tests may be misleading if the regressors are highly correlated.</p>
</div>
</div>
<div class="section" id="anova">
<h3>ANOVA<a class="headerlink" href="#anova" title="Permalink to this headline">¶</a></h3>
<p>The Analysis Of Variance, popularly known as the ANOVA, can be used in cases where there are more than two groups.</p>
<p>TBD</p>
</div>
<div class="section" id="stepwise">
<h3>Stepwise<a class="headerlink" href="#stepwise" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
</div>
<div class="section" id="special-cases">
<h2>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">¶</a></h2>
<p>No models are perfect. In this section we introduce what happen when our model is misspecified or when some assumptions fail.</p>
<div class="section" id="omit-a-variable">
<span id="lm-omit-variable"></span><h3>Omit a Variable<a class="headerlink" href="#omit-a-variable" title="Permalink to this headline">¶</a></h3>
<p>Suppose the true model is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}_{n \times p} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  
\]</div>
<p>And we omit one explanatory variable <span class="math notranslate nohighlight">\(X_j\)</span>. Thus, our new design matrix has size <span class="math notranslate nohighlight">\(n \times (p-1)\)</span>, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{X}_{-j}\)</span>. Without loss of generality, let it be in the last column of the original design matrix, i.e. <span class="math notranslate nohighlight">\(\boldsymbol{X} = \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\)</span>. The new estimated coefficients vector is denoted by <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span>. The coefficient for <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in the true model is denoted by <span class="math notranslate nohighlight">\(\beta_j\)</span>, and the vector of coefficients for other explanatory variables is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{\beta} _{-j}\)</span>. Hence, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} ^\top = \left[ \boldsymbol{\beta} _{-j} \quad \beta_j \right] ^\top\)</span>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Though the common focus is on bias, omitting a variable probably decreases variance. See the relevant section <a class="reference internal" href="#lm-include-variable"><span class="std std-ref">below</span></a>, or the variance expression <a class="reference internal" href="#lm-inference-variance"><span class="std std-ref">above</span></a>.</p>
</div>
<p><em>Question: Is <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span> unbised for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{-j}\)</span>?</em></p>
<p><em>Answer: No. Omitting a relevant variable increases bias. There is a deterministic identity for the bias.</em></p>
<p>We will see the meaning of “relevant” later.</p>
<p>We first find the expression of the new estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \hat{\boldsymbol{\beta} }_{-j}
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{y} \\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left\{ \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\left[\begin{array}{l}
\boldsymbol{\beta} _{-j}  \\
\beta _j
\end{array}\right] + \boldsymbol{\varepsilon}  \right\}\\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left( \boldsymbol{X} _{-j} \boldsymbol{\beta} _{-j} +  \boldsymbol{x}_j \beta _j + \boldsymbol{\varepsilon}  \right) \\
&amp;=  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \right]\left(  \boldsymbol{x}_j \beta _j+ \boldsymbol{\varepsilon}  \right)\\
\end{align}\end{split}\]</div>
<p>The expectation, therefore, is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{E}\left( \hat{\boldsymbol{\beta} }_{-j} \right) =  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \right]\beta _j\\
\end{split}\]</div>
<p>What is <span class="math notranslate nohighlight">\(\left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span>? You may recognize this form. It is actually the vector of estimated coefficients when we regress the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> on all other explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>. Let it be <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_{(p-1) \times 1}\)</span>.</p>
<p>Therefore, we have, for the <span class="math notranslate nohighlight">\(k\)</span>-th explanatory variable in the new model,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta} _{-j,k} \right) = \beta_{k} + \alpha_k \beta_j
\]</div>
<p>So the bias is <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>. The sign can be positive or negative.</p>
<p>This identity can be converted to the following diagram. The explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is associated with the response <span class="math notranslate nohighlight">\(Y\)</span> in two ways. First is directly by itself with strength is <span class="math notranslate nohighlight">\(\beta_k\)</span>, and second is through the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span>, with a “compound” strength <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_k \quad \overset{\quad \beta_{k} \quad }{\longrightarrow} \quad Y
\]</div>
<div class="math notranslate nohighlight">
\[
\alpha_k \searrow \qquad \nearrow \beta_j
\]</div>
<div class="math notranslate nohighlight">
\[
X_j
\]</div>
<p>When will the bias be zero?</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\alpha_k = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the concerned explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{x}_k = 0\)</span> in the design matrix.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\beta_j = 0\)</span>, that is, the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> and the response <span class="math notranslate nohighlight">\(Y\)</span> is uncorrelated, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{y} = 0\)</span>.</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The takeaway here is that we should include all relevant omitted factors to reduce bias. But in practice, we can never know what all relevant factors are, and rarely can we measure all relevant factors.</p>
</div>
<p>That’s how we define “relevant”.</p>
<p>What is the relation between the sample estimates? The relation has a similar form.</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta }_{-j,k} =  \hat{\beta}_k + \hat{\alpha}_k\hat{\beta}_j
\]</div>
<p>Proof: TBD. Need linear algebra about inverse.</p>
<p>Verify:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">b0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 + x3 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">lmo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">ro</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lmx</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in x3 ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">rx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmx</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reconstruction difference of b0, b1, b2 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">eefce4fb76de</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;sklearn&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="include-a-variable">
<span id="lm-include-variable"></span><h3>Include a Variable<a class="headerlink" href="#include-a-variable" title="Permalink to this headline">¶</a></h3>
<p>What if we add a new variable <span class="math notranslate nohighlight">\(X_j\)</span>? What will happen to the existing estimator <span class="math notranslate nohighlight">\(\hat\beta_k\)</span>?</p>
<p>Increase</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}\left(\hat{\beta}_{k}\right)=\sigma^{2} \frac{1}{1-R_k^{2}} \frac{1}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}}\]</div>
<p>if <span class="math notranslate nohighlight">\(R_{k}^2\)</span> increases. When will <span class="math notranslate nohighlight">\(R^2_{k}\)</span> be unchanged? When the new variable <span class="math notranslate nohighlight">\(X_j\)</span> has no explanatory power to <span class="math notranslate nohighlight">\(X_k\)</span>. See the <a class="reference internal" href="#lm-rss-nonincreasing"><span class="std std-ref">section</span></a>.</p>
<p>In terms of bias, if we say the model with <span class="math notranslate nohighlight">\(X_p\)</span> is “true”, then <span class="math notranslate nohighlight">\(\operatorname{E}\left( \hat{\beta}_k \right)\)</span> is probably closer to <span class="math notranslate nohighlight">\(\beta_k\)</span> according to the equation described in the above <a class="reference internal" href="#lm-omit-variable"><span class="std std-ref">section</span></a>.</p>
</div>
<div class="section" id="multicollinearity">
<h3>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Definition (Multicollinearity)<br />
Multicollinearity measure the extent of pairwise correlation of variables in the design matrix.</p>
<div class="margin sidebar">
<p class="sidebar-title">Multicollinearity in computation</p>
<p>From numerical algebra’s perspective, the extent of correlation of variables in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> determines the condition number of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. As the correlation increases, its inverse becomes unstable. When perfect linear relation exists, then <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span> is not of full rank, and thus no inverse exists.</p>
</div>
<p>Definition (Perfect multicollinearity)<br />
A set of variables is perfectly multicollinear if a variable does not vary, or if there is an exact linear relationship between a set of variables:</p>
<div class="math notranslate nohighlight">
\[
X_{j}=\delta_{0}+\delta_{1} X_{1}+\cdots+\delta_{j-1} X_{j-1}+\delta_{i+1} X_{i+1}+\cdots+\delta_{k} X_{k}
\]</div>
<p>As long as the variables in the design matrix are not uncorrelated, then multicollinearity exists.</p>
<div class="section" id="diagnosis">
<h4>Diagnosis<a class="headerlink" href="#diagnosis" title="Permalink to this headline">¶</a></h4>
<p>Some common symptoms include</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F\)</span>-test is significant, <span class="math notranslate nohighlight">\(R^2\)</span> is good, but <span class="math notranslate nohighlight">\(t\)</span>-test is not significant.</p></li>
<li><p>Large magnitude of <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span></p></li>
<li><p>Large standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta_j)\)</span></p></li>
</ul>
<p>We can measure the extent of multicollinearity by <strong>variance inflation factor</strong> (VIF) for each explanatory variable.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{VIF}_j = \frac{1}{1-R_j^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R_j^2\)</span> is the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables excluding <span class="math notranslate nohighlight">\(X_j\)</span>. The value of <span class="math notranslate nohighlight">\(\operatorname{VIF}_j\)</span> can be interpreted as: the standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta)\)</span> is <span class="math notranslate nohighlight">\(\sqrt{\operatorname{VIF}_j}\)</span> times larger than it would have been without multicollinearity.</p>
<p>A second way of measurement is the <strong>condition number</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. If it is greater than <span class="math notranslate nohighlight">\(30\)</span>, then we can conclude that the multicollinearity problem cannot be ignored.</p>
<div class="math notranslate nohighlight">
\[
\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) = \sqrt{\frac{\lambda_1 (\boldsymbol{X} ^\top \boldsymbol{X} )}{\lambda_p (\boldsymbol{X} ^\top \boldsymbol{X} )} }
\]</div>
<p>Finally, <strong>correlation matrix</strong> can also be used to measure multicollinearity since it is closely related to the condition number <span class="math notranslate nohighlight">\(\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)\)</span>.</p>
</div>
<div class="section" id="consequences">
<h4>Consequences<a class="headerlink" href="#consequences" title="Permalink to this headline">¶</a></h4>
<ol>
<li><p>It inflates <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_j \right)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \operatorname{Var}\left( \hat{\beta}_j \right)
     &amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2}  \\
     &amp;=  \sigma^2 \frac{\operatorname{VIF}_j}{\operatorname{Var}\left( X_j \right)}  
     \end{align}\end{split}\]</div>
<p>When perfect multicollinearity exists, the variance goes to infinity since <span class="math notranslate nohighlight">\(R^2_{j} = 1\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>-tests fail to reveal significant predictors, due to 1.</p></li>
<li><p>Estimated coefficients are sensitive to randomness in <span class="math notranslate nohighlight">\(Y\)</span>, i.e. unreliable. If you run the experiment again, the coefficients can change dramatically, which is measured by <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( X_1, X_2 \right)\)</span> is large, then we expect to have large <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right), \operatorname{Var}\left( \hat{\beta}_2 \right), \operatorname{Var}\left( \hat{\beta}_1, \hat{\beta}_2 \right)\)</span>, but <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)\)</span> can be small. This means we cannot distinguish the effect of <span class="math notranslate nohighlight">\(X_1 + X_2\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> is from <span class="math notranslate nohighlight">\(X_1\)</span> or <span class="math notranslate nohighlight">\(X_2\)</span>, i.e. <strong>non-identifiable</strong>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
<em>Proof</em><div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">By the fact that, for symmetric positive definite matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} \boldsymbol{b} = \sum \lambda_i b_i ^2
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \boldsymbol{a} ^\top \boldsymbol{S} ^{-1}  \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} ^{-1}  \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} ^{-1}  \boldsymbol{b} = \sum \frac{1}{\lambda_i}  b_i ^2
 \]</div>
<p class="card-text">we have:</p>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx 0
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 - \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx \infty
 \]</div>
<p class="card-text">If</p>
<div class="math notranslate nohighlight">
\[
 \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
<p class="card-text">then</p>
<div class="math notranslate nohighlight">
\[
 \operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
 \]</div>
</div>
</details></li>
</ol>
</div>
<div class="section" id="implications">
<h4>Implications<a class="headerlink" href="#implications" title="Permalink to this headline">¶</a></h4>
<p>If <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> show high correlation, then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1\)</span> may be a proxy of <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1 - X_2\)</span> may just be noise.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_2\)</span> is removed, <span class="math notranslate nohighlight">\(X_1\)</span> may still be good for prediction.</p></li>
</ol>
</div>
</div>
<div class="section" id="heteroscedasticity">
<h3>Heteroscedasticity<a class="headerlink" href="#heteroscedasticity" title="Permalink to this headline">¶</a></h3>
<p>TBD</p>
</div>
<div class="section" id="categorical-x">
<h3>Categorical <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#categorical-x" title="Permalink to this headline">¶</a></h3>
<p>dummy variables <span class="math notranslate nohighlight">\(X_ij\)</span></p>
<p>when <span class="math notranslate nohighlight">\(c = 2\)</span>,</p>
<p>interpretation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>: difference in means between the group with <span class="math notranslate nohighlight">\(X=1\)</span> and <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span>: mean of the group with <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
</ul>
<p>TBD</p>
<p>https://www.1point3acres.com/bbs/thread-703302-1-1.html</p>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Slope vs Correlation</p>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, we can see from the solution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
    \hat{\beta}_{1} &amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
    \end{align}\]</div>
<p>that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \hat{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
    &amp;= r_{X,Y} \frac{s_Y}{s_X}
    \end{align}\end{split}\]</div>
<p>Thus, the slope has the same sign with the correlation <span class="math notranslate nohighlight">\(r_{X,Y}\)</span>, and equals to the correlation times a ratio of the sample standard deviations of the dependent variable over the independent variable.</p>
<p>Once can see that the magnitude of <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> increases with the magnitude of <span class="math notranslate nohighlight">\(\rho_{X,Y}\)</span> and <span class="math notranslate nohighlight">\(s_Y\)</span>, and decreases with <span class="math notranslate nohighlight">\(s_X\)</span>, holding others fixed.</p>
</li>
<li><p>Fitted Line Passes Sample Mean</p>
<p>Since <span class="math notranslate nohighlight">\(\hat{\beta}_{0} =\bar{y}-\hat{\beta}_{1} \bar{x}\)</span>, we have <span class="math notranslate nohighlight">\(\bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \bar{x}\)</span>, i.e. the regression line always goes through the mean <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span> of the sample.</p>
<p>This also hold for multiple regression, by the first order condition w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</li>
<li><p>Non-zero Mean of Error Term</p>
<p><em>What if the mean of the error term is not zero?</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>If <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon \right) = \mu_\varepsilon \ne 0\)</span>, we can just denote <span class="math notranslate nohighlight">\(\varepsilon = \mu_\varepsilon + v\)</span>, where <span class="math notranslate nohighlight">\(v\)</span> is a new error term with zero mean. Our model becomes</p>
<div class="math notranslate nohighlight">
\[
    y_i = (\beta_0 + \mu_\varepsilon) + \beta_1 x_1 + v
    \]</div>
<p>where <span class="math notranslate nohighlight">\((\beta_0 + \mu_\varepsilon)\)</span> is the new intercept. We can still apply the methods above to conduct estimation and inference.</p>
</div>
</li>
<li><p>No Intercept</p>
<p><em>Assume the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> in the model <span class="math notranslate nohighlight">\(y=\beta_0 + \beta_1 x + \varepsilon\)</span> is zero. Find the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span>, denoted <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span>. Find its mean, variance, and compare them with those of the OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span> when there is an intercept term.</em></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>If there is no intercept, consider a simple case</p>
<div class="math notranslate nohighlight">
\[
    y_i = \beta x_i + \varepsilon_i
    \]</div>
<p>Then by minimizing sum of squared errors</p>
<div class="math notranslate nohighlight">
\[
    \min \sum_i (y_i - \beta x_i)^2
    \]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
    -2 \sum_i (y_i - \beta x_i) x_i = 0
    \]</div>
<p>and hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde{\beta}
    &amp;= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \\
    &amp;= \frac{\sum_i x_i (\beta x_i + \varepsilon_i)}{\sum_i x_i^2}\\
    &amp;= \beta + \frac{\sum x_i \varepsilon_i}{\sum_i x_i^2}
    \end{align}\end{split}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> is still an unbiased estimator of <span class="math notranslate nohighlight">\(\beta\)</span>, while its variance is smaller than the variance calculated assuming the intercept is non-zero.</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left( \tilde{\beta} \right) = \frac{\sigma^2}{\sum x_i^2} \le  \frac{\sigma^2}{\sum (x_i - \bar{x})^2} = \operatorname{Var}\left( \hat{\beta}  \right)
    \]</div>
<p>Hence, if the intercept is known to be zero, better use <span class="math notranslate nohighlight">\(\tilde\beta\)</span> instead of <span class="math notranslate nohighlight">\(\hat\beta\)</span>, since the standard error of the <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is smaller, and both are unbiased.</p>
<p>If the true model has a non-zero intercept, then <span class="math notranslate nohighlight">\(\tilde\beta\)</span> is biased for <span class="math notranslate nohighlight">\(\beta\)</span>, but it has a smaller variance, which brings a tradeoff of bias vs variance.</p>
</div>
</li>
<li><p>Transformation of Variables</p>
<p>[insert] summary table.</p>
<p>First, we take simple linear regression as an example.</p>
<p>If <span class="math notranslate nohighlight">\(X ^\prime = aX + b\)</span>, then the new slope estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
    &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y, aX + b  \right)}{\widehat{\operatorname{Var}}\left( aX+b \right)}  \\
    &amp;= \frac{a\widehat{\operatorname{Cov}}\left( Y, X \right)}{a^2\widehat{\operatorname{Var}}\left( X \right)}  \\
    &amp;= \frac{1}{a} \hat\beta_1 \\
    \end{align}\end{split}\]</div>
<p>and the new intercept is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde\beta_0
    &amp;= \bar{y} - \tilde\beta_1 \bar{x} ^\prime \\
    &amp;= \bar{y} - \hat\beta_1 \frac{1}{a}  (a\bar{x}+b) \\
    &amp;= \hat\beta_0 - \hat\beta_1 \frac{b}{a} \\
    \end{align}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(Y ^\prime = cY + d\)</span> then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde{\beta}_1 &amp;= \frac{\widehat{\operatorname{Cov}}\left( Y ^\prime, X ^\prime \right)}{\widehat{\operatorname{Var}}\left( X ^\prime \right)}  \\
    &amp;= \frac{\widehat{\operatorname{Cov}}\left( cY+d, X  \right)}{\widehat{\operatorname{Var}}\left( X \right)}  \\
    &amp;= \frac{c\widehat{\operatorname{Cov}}\left( Y, X \right)}{c\widehat{\operatorname{Var}}\left( X \right)}  \\
    &amp;= c \hat\beta_1 \\
    \end{align}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \tilde\beta_0
    &amp;= \bar{y}^\prime - \tilde\beta_1 \bar{x} \\
    &amp;= (c\bar{y}+d) - c\hat\beta_1 \bar{x} \\
    &amp;= c\hat\beta_0 + d\\
    \end{align}\end{split}\]</div>
<p>Can the conclusions be extended to multiple regression?</p>
<p>TBD.</p>
</li>
<li><p>Exchange <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p></li>
</ol>
<p>TBD.</p>
<ol>
<li><p>Covariance, <span class="math notranslate nohighlight">\(R\)</span>-squared, and <span class="math notranslate nohighlight">\(\beta_j\)</span></p>
<p>In multiple regression, if <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y, X_j \right) = 0\)</span> then <span class="math notranslate nohighlight">\(\beta_j= 0\)</span>?</p>
<p>Is it possible that <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( X_j, X_k \right) \ne 0, \operatorname{Cov}\left( Y, X_k \right) \ne 0\)</span> but <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y, X_j \right) = 0\)</span>?</p>
<p>TBD.</p>
</li>
<li><p>Increase Estimation Precision</p>
<p>TBD.</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{j}\)</span>, and hence decrease the variance.</p></li>
</ul>
</li>
<li><p>Partialling Out in General Cases</p></li>
</ol>
<p>TBD.</p>
<ol>
<li><p>Causal?</p>
<p>313.qz1.q2</p>
<p>TBD.</p>
</li>
<li><p>Add/Remove a Variable/Observation</p>
<p>TBD</p>
<p>Table summary.</p>
<p>Rows: E(b), Var(b), RSS, TSS, R^2</p>
</li>
<li><p>To compare the effects of two variable <span class="math notranslate nohighlight">\(X_j, X_k\)</span>, can we say they have the same effect since the confidence interval of <span class="math notranslate nohighlight">\(\beta_j, \beta_k\)</span> overlaps?</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Solution</em></p>
<p>No, since</p>
<ul class="simple">
<li><p>the two coefficients are probably correlated <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \boldsymbol{\beta} _j, \beta_k \right) \ne 0\)</span></p></li>
<li><p>even if they are not correlated, we still need to find a pivot quantity for <span class="math notranslate nohighlight">\(\theta = \beta_j - \beta_k\)</span> and conduct a hypothesis testing on <span class="math notranslate nohighlight">\(\theta=0\)</span>. See the <a class="reference internal" href="#lm-t-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(t\)</span>-test section</span></a>.</p></li>
</ul>
</div>
</li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-regression.html" title="previous page">Regression</a>
    <a class='right-next' id="next-link" href="../32-classification/00-classification.html" title="next page">Classification</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>