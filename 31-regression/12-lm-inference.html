
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Regression - Inference &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression - Extension" href="13-lm-extension.html" />
    <link rel="prev" title="Linear Models - Estimation" href="11-lm-estimation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-lm-extension.html">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-topic-models.html">
     Topic Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/12-lm-inference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/12-lm-inference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/12-lm-inference.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/12-lm-inference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coefficients-boldsymbol-beta">
   Coefficients
   <span class="math notranslate nohighlight">
    \(\boldsymbol{\beta}\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unbiasedness">
     Unbiasedness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#efficiency-blue">
     Efficiency (BLUE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consistency">
     Consistency
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#large-sample-distribution">
     Large Sample Distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#residuals-and-error-variance">
   Residuals and Error Variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals-hat-boldsymbol-varepsilon">
     Residuals
     <span class="math notranslate nohighlight">
      \(\hat{\boldsymbol{\varepsilon}}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation-of-error-variance">
     Estimation of Error Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
     Independence of
     <span class="math notranslate nohighlight">
      \(\hat{\boldsymbol{\beta}}\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\hat{\sigma}^2\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sum-of-squares">
   Sum of Squares
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition-of-tss">
     Decomposition of TSS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-increasing-rss">
     Non-increasing RSS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r-2-and-adjusted-r-2">
   <span class="math notranslate nohighlight">
    \(R^2\)
   </span>
   and Adjusted
   <span class="math notranslate nohighlight">
    \(R^2\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjusted-r-squared">
     Adjusted
     <span class="math notranslate nohighlight">
      \(R\)
     </span>
     -squared
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t-test">
   <span class="math notranslate nohighlight">
    \(t\)
   </span>
   -test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#f-test">
   <span class="math notranslate nohighlight">
    \(F\)
   </span>
   -test
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-two-nested-models">
     Compare Two Nested Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-two-groups-of-data">
     Compare Two Groups of Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-region-for-boldsymbol-beta">
     Confidence Region for
     <span class="math notranslate nohighlight">
      \(\boldsymbol{\beta}\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-interval-for-y-new">
   Prediction Interval for
   <span class="math notranslate nohighlight">
    \(y_{new}\)
   </span>
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-inference">
<h1>Linear Regression - Inference<a class="headerlink" href="#linear-regression-inference" title="Permalink to this headline">¶</a></h1>
<p>We first describe the properties of OLS estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and the corresponding residuals <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\varepsilon} }\)</span>. Then we introduce sum of squares, <span class="math notranslate nohighlight">\(R\)</span>-squared, hypothesis testing and confidence intervals. All these methods assume normality of the error terms <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)\)</span> unless otherwise specified.</p>
<div class="section" id="coefficients-boldsymbol-beta">
<h2>Coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span><a class="headerlink" href="#coefficients-boldsymbol-beta" title="Permalink to this headline">¶</a></h2>
<div class="section" id="unbiasedness">
<h3>Unbiasedness<a class="headerlink" href="#unbiasedness" title="Permalink to this headline">¶</a></h3>
<p>The OLS estimators are unbiased since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{E}\left( \hat{\boldsymbol{\beta} } \right) &amp;= \operatorname{E}\left( (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \operatorname{E}\left( \boldsymbol{y} \right) \\
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \boldsymbol{X} \boldsymbol{\beta} \\
&amp;= \boldsymbol{\beta}
\end{align}\end{split}\]</div>
<p>when <span class="math notranslate nohighlight">\(p=2\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{\beta}_{1}
&amp;=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\end{align}\end{split}\]</div>
<p>To prove unbiasedness, using the fact that for any constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_i (x_i - \bar{x})c = 0
\]</div>
<p>Then, the numerator becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
&amp;=\sum\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+u_{i}\right) \\
&amp;=\sum\left(x_{i}-\bar{x}\right) \beta_{0}+\sum\left(x_{i}-\bar{x}\right) \beta_{1} x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{0} \sum\left(x_{i}-\bar{x}\right)+\beta_{1} \sum\left(x_{i}-\bar{x}\right) x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
&amp;=\beta_{1} \sum\left(x_{i}-\bar{x}\right)^2 +\sum\left(x_{i}-\bar{x}\right) u_{i} \\
\end{align}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\hat{\beta}_{1}=\beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}}
\end{equation}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \hat{\beta}_1 \right) = \beta_1
\]</div>
</div>
<div class="section" id="variance">
<span id="lm-inference-variance"></span><h3>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h3>
<p>The variance (covariance matrix) of the coefficients is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \boldsymbol{\beta}  \right) &amp;= \operatorname{Var}\left(  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right)  \\
&amp;=   (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \operatorname{Var}\left( \boldsymbol{y}  \right)  \boldsymbol{X}  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;= \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\\
\end{align}\end{split}\]</div>
<div class="admonition">
<p class="admonition-title"> What is the <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_j \right)\)</span>?</p>
<p>More specifically, for the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient estimator <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, its variance is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_j \right)
&amp;= \sigma^2 \left[ (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \right]_{[j,j]} \\
&amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2} \\
&amp;= \sigma^2 \frac{TSS_j}{RSS_j} \frac{1}{TSS_j} \\
&amp;= \sigma^2 \frac{1}{\sum_i(\hat{x}_{ij} - x_{ij})} \\
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(R_j^2\)</span>, <span class="math notranslate nohighlight">\(RSS_j\)</span>, <span class="math notranslate nohighlight">\(TSS_j\)</span>, and <span class="math notranslate nohighlight">\(\hat{x}_{ij}\)</span> are the corresponding representatives when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables.</p>
<p>Note that the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regressing <span class="math notranslate nohighlight">\(X_1\)</span> to an constant intercept is 0. So we have the particular result below.</p>
</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X} )^\top\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c}
\left(\boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1}
=\frac{1}{\sum_{i=1}^{n} \left(x_{i}-\bar{x}\right)^{2}}\left[\begin{array}{cc}
\bar{x^2} &amp; - \bar{x} \\
- \bar{x} &amp; 1
\end{array}\right]
\end{array}
\end{split}\]</div>
<p>the variance of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( \hat{\beta}_1 \right)
&amp;= \operatorname{Var}\left( \beta_{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}} \right)\\
&amp;= \frac{\operatorname{Var}\left( \sum\left(x_{i}-\bar{x}\right) u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sum\left(x_{i}-\bar{x}\right)^2 \operatorname{Var}\left( u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \sigma^2 \frac{\sum\left(x_{i}-\bar{x}\right)^2 }{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\\
&amp;= \frac{\sigma^2}{\sum_{i=1}^n \left(x_{i}-\bar{x}\right)^{2}}\\
\end{align}\end{split}\]</div>
<p>We conclude that</p>
<ul class="simple">
<li><p>The larger the error variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the <span class="math notranslate nohighlight">\(x_i\)</span>, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between <span class="math notranslate nohighlight">\(X_j\)</span> and other covariates (e.g. by orthogonal design) can decreases <span class="math notranslate nohighlight">\(R^2_{j}\)</span>, and hence decrease the variance.</p></li>
</ul>
<p>A problem is that the error <span class="math notranslate nohighlight">\(\sigma^2\)</span> variance is <strong>unknown</strong>. In practice, we can estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> by its unbiased estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2=\frac{\sum_i (x_i - \bar{x})}{n-2}\)</span> (to be shown [link]), and substitute it into <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span>. Since the error variance <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is estimated, the slope variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right)\)</span> is estimated too, and hence the square root is called standard error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, instead of standard deviation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{se}\left(\hat{\beta}_{1}\right)
&amp;= \sqrt{\widehat{\operatorname{Var}}\left( \hat{\beta}_1 \right)}\\
&amp;= \frac{\hat{\sigma}}{\sqrt{\sum \left(x_{i}-\bar{x}\right)^{2}}}
\end{align}\end{split}\]</div>
</div>
<div class="section" id="efficiency-blue">
<h3>Efficiency (BLUE)<a class="headerlink" href="#efficiency-blue" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Theorem (Gauss–Markov)</dt><dd><p>The ordinary least squares (OLS) estimator has the <strong>lowest</strong> sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. In abbreviation, the OLS estimator is BLUE: Best (lowest variance) Linear Unbiased Estimator.</p>
</dd>
</dl>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \boldsymbol{C} \boldsymbol{y}\)</span> be another linear estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. We can write <span class="math notranslate nohighlight">\(\boldsymbol{C} = \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \boldsymbol{X} ^\top + \boldsymbol{D}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{D} \ne \boldsymbol{0}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{E}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \operatorname{E}\left( \boldsymbol{C} \boldsymbol{y}   \right)\\
  &amp;= \boldsymbol{C} \operatorname{E}\left( \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  \right)\\
  &amp;= \boldsymbol{\beta} + \boldsymbol{D} \boldsymbol{X} \boldsymbol{\beta} \\
  \end{align}\end{split}\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}}\)</span> is unbiased iff <span class="math notranslate nohighlight">\(\boldsymbol{D} \boldsymbol{X} = 0\)</span>.</p>
<p>The variance is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right)
  &amp;= \boldsymbol{C}\operatorname{Var}\left( \boldsymbol{y}  \right) \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \boldsymbol{C} \boldsymbol{C} ^\top \\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  + (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{D} ^\top + \boldsymbol{D} \boldsymbol{X} \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^\top  + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\\
  &amp;= \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right) + \sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \\
  \end{align}\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \in \mathrm{PSD}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right) \succeq \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)
  \]</div>
<p>The equality holds iff <span class="math notranslate nohighlight">\(\boldsymbol{D} ^\top \boldsymbol{D} = 0\)</span>, which implies that <span class="math notranslate nohighlight">\(\operatorname{tr}\left( \boldsymbol{D} \boldsymbol{D} ^\top \right) = 0\)</span>, then <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{D} \right\Vert _F^2 = 0\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{D} = 0\)</span>, i.e. <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta} } = \hat{\boldsymbol{\beta} }\)</span>. Therefore, BLUE is unique.</p>
</div>
<p>Moreover,</p>
<ul class="simple">
<li><p>If error term is normally distributed, then OLS is most efficient among all consistent estimators (not just linear ones).</p></li>
<li><p>When the distribution of error term is non-normal, other estimators may have lower variance than OLS such as least absolute deviation (median regression).</p></li>
</ul>
</div>
<div class="section" id="consistency">
<h3>Consistency<a class="headerlink" href="#consistency" title="Permalink to this headline">¶</a></h3>
<p>The OLS and consistent,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}_{OLS} \stackrel{P}{\rightarrow} \boldsymbol{\beta}
\]</div>
<p>since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{plim} \hat{\boldsymbol{\beta}}
&amp;= \operatorname{plim} \left( \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) \\
&amp;= \boldsymbol{\beta} + \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \underbrace{\operatorname{plim} \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) }_{=0 \text{ by CLM} }\\
&amp;= \boldsymbol{\beta} \\
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="large-sample-distribution">
<h3>Large Sample Distribution<a class="headerlink" href="#large-sample-distribution" title="Permalink to this headline">¶</a></h3>
<p>If we assume <span class="math notranslate nohighlight">\(\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)\)</span>, or <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N_n(\boldsymbol{0} , \boldsymbol{I} _n)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} )
\]</div>
<p>Hence, the distribution of the coefficients estimator is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y}   \\
&amp;\sim  N(\boldsymbol{\beta} , (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} \operatorname{Var}\left( \boldsymbol{y}  \right)) \boldsymbol{X} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \\
&amp;\sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} ) \\
\end{aligned}\end{split}\]</div>
<p>The assumption may fail when the response variable <span class="math notranslate nohighlight">\(y\)</span> is</p>
<ul class="simple">
<li><p>right skewed, e.g. wages, savings</p></li>
<li><p>non-negative, e.g. counts, arrests</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>As discussed at the beginning, the data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is treated as fixed by statisticians while as random by econometricians. As a result, the derivation of the asymptotic distribution differs in the two domains.</p>
</div>
<p>When the normality assumption of the error term fails, the OLS estimator is <strong>asymptotically</strong> normal,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
<p>Therefore, in a large sample, even if the normality assumption fails, we can still do hypothesis testing which assumes normality.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Sketch of derivation</em></p>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[
\sqrt{n}\left( \hat{\boldsymbol{\beta}}  - \boldsymbol{\beta}  \right)= \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}   \right) ^{-1} \left( \frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right)
\]</div>
<p>By CLT, the second term</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \overset{\mathcal{D}}{\longrightarrow} N \left( \boldsymbol{0}, \frac{\sigma^2 }{n} \boldsymbol{X} ^\top \boldsymbol{X}  \right)
\]</div>
<p>By Slusky’s Theorem, the product</p>
<div class="math notranslate nohighlight">
\[
\left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}   \right) ^{-1} \left( \frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) \overset{\mathcal{D}}{\longrightarrow} N \left( \boldsymbol{0}, \sigma^2 \left( \frac{1}{n}  \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  \right)
\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
</div>
</div>
</div>
<div class="section" id="residuals-and-error-variance">
<h2>Residuals and Error Variance<a class="headerlink" href="#residuals-and-error-variance" title="Permalink to this headline">¶</a></h2>
<div class="section" id="residuals-hat-boldsymbol-varepsilon">
<h3>Residuals <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\varepsilon}}\)</span><a class="headerlink" href="#residuals-hat-boldsymbol-varepsilon" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The residual is defined as the difference between the true response value <span class="math notranslate nohighlight">\(y\)</span> and our fitted response value <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat\varepsilon_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}\]</div>
<p>It is an estimate of the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
</dd>
</dl>
<p><strong>Properties</strong></p>
<ol class="simple">
<li><p>The sum of the residual is zero: <span class="math notranslate nohighlight">\(\sum_i \hat{\varepsilon}_i = 0\)</span></p></li>
<li><p>The sum of the product of residual and any covariate is zero, or they are “uncorrelated”: <span class="math notranslate nohighlight">\(\sum_i x_{ij} \hat{\varepsilon}_i = 0\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>The sum of squared residuals: <span class="math notranslate nohighlight">\(\left\| \boldsymbol{\hat{\varepsilon}}  \right\|^2   = \left\| \boldsymbol{y} - \boldsymbol{H} \boldsymbol{y}   \right\|^2  = \boldsymbol{y} ^\top (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y}\)</span></p></li>
</ol>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Recall the normal equation</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top (\boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta} }) = \boldsymbol{0}
\]</div>
<p>We obtain</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X} ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
<p>Since the first column of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> , we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_i \hat{\varepsilon}_i  
&amp;= \sum_i(y_i - \hat{y}_i)  \\
&amp;= \sum_i(y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta} }_i)  \\
&amp;= 0
\end{align}\end{split}\]</div>
<p>For other columns <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_j ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
\]</div>
<p>The 3rd equality holds since <span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{H}\)</span> is a projection matrix if <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> is a projection matrix, i.e.,</p>
<div class="math notranslate nohighlight">
\[
(\boldsymbol{I} - \boldsymbol{H}) (\boldsymbol{I} - \boldsymbol{H} ) = \boldsymbol{I} -\boldsymbol{H}
\]</div>
</div>
</div>
<div class="section" id="estimation-of-error-variance">
<h3>Estimation of Error Variance<a class="headerlink" href="#estimation-of-error-variance" title="Permalink to this headline">¶</a></h3>
<p>In the estimation section we mentioned the estimator for the error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2}=\frac{\|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}\|^{2}}{n-p}
\]</div>
<p>This is because</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2 \sim \sigma^2\chi ^2 _{n-p}  \\\end{split}\\\Rightarrow  \quad \sigma^2 =\operatorname{E}\left( \frac{\|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}\|^{2}}{n-p} \right)
\end{aligned}\end{align} \]</div>
<p>and we used the method of moment estimator. The derivation of the above expectation is a little involved.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} = [\boldsymbol{U} _ \boldsymbol{X} , \boldsymbol{U} _\bot]\)</span> be an orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^{n\times n}\)</span> where</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} _ \boldsymbol{X}  = [\boldsymbol{u} _1, \ldots, \boldsymbol{u} _p]\)</span> is an orthogonal basis of the column space (image) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, denoted <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X} )\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} _ \bot  = [\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _n]\)</span> is an orthogonal basis of the orthogonal complement of the column space (kernel) of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, , denoted <span class="math notranslate nohighlight">\(\operatorname{col}(\boldsymbol{X} ) ^\bot\)</span>.</p></li>
</ul>
<p>Recall</p>
<div class="math notranslate nohighlight">
\[ \hat{\boldsymbol{\varepsilon}}  = \boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X} ) ^\bot\]</div>
<p>which is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2
&amp;= \left\| \boldsymbol{P} _{\boldsymbol{U} _\bot} \boldsymbol{y}   \right\|  \\
&amp;= \left\| \boldsymbol{U} _\bot \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top (\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon})    \right\|^2  \\
&amp;= \left\| \boldsymbol{U} _\bot ^\top  \boldsymbol{\varepsilon}    \right\|^2  \quad \because \boldsymbol{U} _\bot ^\top\boldsymbol{X} = \boldsymbol{0} \\
\end{aligned}\end{split}\]</div>
<p>Note that assuming <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N(\boldsymbol{0} , \sigma^2 \boldsymbol{I} )\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{U} _\bot ^\top \boldsymbol{\varepsilon}
&amp;\sim N_{n-p}(\boldsymbol{0} , \boldsymbol{U} _\bot ^\top \sigma^2 \boldsymbol{I}_n \boldsymbol{U} _\bot) \\
&amp;\sim N_{n-p}(\boldsymbol{0} , \sigma^2 \boldsymbol{I}_{n-p}) \\
\end{aligned}\end{split}\]</div>
<p>and hence the sum of squared normal variables follows</p>
<div class="math notranslate nohighlight">
\[
\left\| \boldsymbol{U} _\bot ^\top \boldsymbol{\varepsilon} \right\|  ^2 \sim \sigma^2 \chi ^2 _{n-p}  
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\left\| \hat{\boldsymbol{\varepsilon}}  \right\|  ^2 \sim \sigma^2 \chi ^2 _{n-p}  
\]</div>
<p>The first moment is</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \left\| \hat{\boldsymbol{\varepsilon}} \right\|^2    \right) = \sigma^2 (n-p)
\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[
\sigma^2 = \frac{\operatorname{E}\left( \left\| \hat{\boldsymbol{\varepsilon}} \right\|^2\right)}{n-p}
\]</div>
<p>Therefore, the method of moment estimator for <span class="math notranslate nohighlight">\(\sigma^2\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{\left\| \hat{\boldsymbol{\varepsilon}}  \right\|^2  }{n-p}
\]</div>
<p>which is unbiased.</p>
</div>
<p>Can we find <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\sigma}^2  \right)\)</span> like we did for <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta}}  \right)\)</span>? No, unless we assume higher order moments of <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
</div>
<div class="section" id="independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
<span id="lm-independent-beta-sigma"></span><h3>Independence of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span><a class="headerlink" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2" title="Permalink to this headline">¶</a></h3>
<p>To prove the independence between the coefficients estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta} }\)</span> and the error variance estiamtor <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>, we need the Lemma below.</p>
<dl class="simple myst">
<dt>Lemma</dt><dd><p>Suppose a random vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> follows multivariate normal distribution <span class="math notranslate nohighlight">\(\boldsymbol{y} \sim N_m(\boldsymbol{\mu} , \sigma^2 I_m)\)</span> and <span class="math notranslate nohighlight">\(S, T\)</span> are orthogonal subspaces of <span class="math notranslate nohighlight">\(\mathbb{R} ^m\)</span>, then the two projected random vectors are independent</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
\boldsymbol{P}_S (\boldsymbol{y}) \perp\!\!\!\perp  \boldsymbol{P}_T (\boldsymbol{y})
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a function of <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X}) } (\boldsymbol{y})\)</span> since</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{P}_{\operatorname{im}(\boldsymbol{X} ) } (\boldsymbol{y})  \\
\end{aligned}\end{split}\]</div>
<p>and note that <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is a function of <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X} )^\bot } (\boldsymbol{y})\)</span> since</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^{2}=\frac{\|\hat{\boldsymbol{\varepsilon}} \|^{2}}{n-p} =
\frac{\left\| \boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X})^\bot } (\boldsymbol{y}) \right\| ^2  }{n-p}
\]</div>
<div class="margin sidebar">
<p class="sidebar-title">Why we need this independency?</p>
<p>This independency is used to construct a pivot quantity to <a class="reference internal" href="#lm-t-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(t\)</span>-test</span></a> a hypothesis on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<p>Since <span class="math notranslate nohighlight">\(\operatorname{im}(\boldsymbol{X})\)</span> and <span class="math notranslate nohighlight">\(\operatorname{im}(\boldsymbol{X})^\bot\)</span> are orthogonal subspaces of <span class="math notranslate nohighlight">\(\mathbb{R} ^p\)</span>, by the lemma, <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X}) } (\boldsymbol{y})\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X})^\bot } (\boldsymbol{y})\)</span> are independent. Hence, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> are independent.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof of Lemma</em></p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{z} \sim N(\boldsymbol{0} , \boldsymbol{I} _m)\)</span> be a standard multivariate normal random vector, and <span class="math notranslate nohighlight">\(\boldsymbol{U} = [\boldsymbol{U} _S, \boldsymbol{U} _T]\)</span> be orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^n\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;&amp;\boldsymbol{U} ^\top \boldsymbol{z} &amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \\
&amp;\Rightarrow&amp; \quad \left[\begin{array}{l}
\boldsymbol{U}_S ^\top \boldsymbol{z} \\
\boldsymbol{U}_T ^\top \boldsymbol{z} \\
\end{array}\right]&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \\
&amp;\Rightarrow&amp; \quad \boldsymbol{U} ^\top _S \boldsymbol{z}  &amp;\perp\!\!\!\perp \boldsymbol{U} ^\top _T \boldsymbol{z}  \\
&amp;\Rightarrow&amp; \quad  f(\boldsymbol{U} ^\top _S \boldsymbol{z})  &amp;\perp\!\!\!\perp f(\boldsymbol{U} ^\top _T \boldsymbol{z})  \\
\end{aligned}\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{\mu} + \sigma \boldsymbol{z}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{P} _S(\boldsymbol{y} ) = \boldsymbol{U} _S \boldsymbol{U} _S ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) \perp\!\!\!\perp \boldsymbol{U} _T \boldsymbol{U} _T ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) = \boldsymbol{P} _T(\boldsymbol{y} )
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
</div>
<div class="section" id="sum-of-squares">
<h2>Sum of Squares<a class="headerlink" href="#sum-of-squares" title="Permalink to this headline">¶</a></h2>
<p>We can think of each observation as being made up of an explained part, and an unexplained part.</p>
<ul class="simple">
<li><p>Total sum of squares: <span class="math notranslate nohighlight">\(TSS = \sum\left(y_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Explained sum of squares: <span class="math notranslate nohighlight">\(ESS = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Residual sum of squares: <span class="math notranslate nohighlight">\(RSS = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
<div class="section" id="decomposition-of-tss">
<span id="lm-tss-identity"></span><h3>Decomposition of TSS<a class="headerlink" href="#decomposition-of-tss" title="Permalink to this headline">¶</a></h3>
<p>We have the decomposition identity</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
TSS
&amp;=\sum\left(y_{i}-\bar{y}\right)^{2} \\
&amp;=\sum\left[\left(y_{i}-\hat{y}_{i}\right)+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum\left[\hat{\varepsilon}_{i}+\left(\hat{y}_{i}-\bar{y}\right)\right]^{2} \\
&amp;=\sum \hat{\varepsilon}_{i}^{2}+2 \sum \hat{\varepsilon}_{i}\left(\hat{y}_{i}-\bar{y}\right)+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2} \\
&amp;= RSS + 2  \sum \hat{\varepsilon}_{i}\left(\hat{\beta}_0 + \hat{\beta}_1 x_{i}-\bar{y}\right)+ ESS \\
&amp;= RSS + ESS
\end{align}\end{split}\]</div>
<p>where use the fact that <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_i \varepsilon_i x_i = 0\)</span> shown [above].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some courses use the letters <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(E\)</span> to denote the opposite quantity in statistics courses.</p>
<ul class="simple">
<li><p>Sum of squares due to regression: <span class="math notranslate nohighlight">\(SSR = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}\)</span></p></li>
<li><p>Sum of squared errors: <span class="math notranslate nohighlight">\(SSE = \sum (y_i - \hat{y}_i)^2\)</span></p></li>
</ul>
</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Decomposition due to orthogonality</p>
<p>In vector form, the decomposition identity is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\left\Vert \boldsymbol{y} - \bar{y} \boldsymbol{1} _n  \right\Vert ^2 = \left\Vert \boldsymbol{y} - \hat{\boldsymbol{y} }  \right\Vert ^2 + \left\Vert \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n\right\Vert ^2
\]</div>
<p>From linear algebra’s perspective, it holds because the LHS vector <span class="math notranslate nohighlight">\(\boldsymbol{y} - \bar{y}\boldsymbol{1} _n\)</span> is the the sum of the two RHS vectors, and the two vectors are orthogonal</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{y}  - \bar{y} \boldsymbol{1} _n = (\boldsymbol{y} - \hat{\boldsymbol{y} }) &amp;+ (\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n) \\
\boldsymbol{y} - \hat{\boldsymbol{y} } &amp;\perp \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n
\end{aligned}\end{split}\]</div>
<p>More specifically, they are orthogonal because</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} - \hat{\boldsymbol{y} } \in \operatorname{col}(\boldsymbol{X} )^\perp \quad  \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X} )
\]</div>
<p>since <span class="math notranslate nohighlight">\(\boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X})\)</span>, if an intercept term is included in the model.</p>
</div>
</div>
<div class="section" id="non-increasing-rss">
<span id="lm-rss-nonincreasing"></span><h3>Non-increasing RSS<a class="headerlink" href="#non-increasing-rss" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is equivalent to say <a class="reference internal" href="#lm-rsquared"><span class="std std-ref"><span class="math notranslate nohighlight">\(R\)</span>-squared</span></a> is always increasing or unchanged, if an intercept term in included in the model.</p>
</div>
<p>Given a data set, when we add an new explanatory variable into a regression model, <span class="math notranslate nohighlight">\(RSS\)</span> is non-increasing.</p>
<p>Since we are comparing two nested minimization problems</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\text{Problem 1 / Full model / with } X_{p}  \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} _{(p+1)\times 1} \right\Vert ^2   = \min \ RSS_1 \\
&amp;\text{Problem 2 / Reduced model / without } X_{p} \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} _{(p+1)\times 1} \right\Vert ^2  = \min \ RSS_2 \\
&amp;&amp;\text{s.t.}  &amp;\ \beta_{p} = 0
\end{aligned}\end{split}\]</div>
<p>Due to the constraint in Problem 2, the minimum value of the Problem 1 should be no larger than the minimum value of the Problem 2, i.e. <span class="math notranslate nohighlight">\(RSS_1^* \le RSS_2^*\)</span> , When will they be equal?</p>
<ul>
<li><p>From projection’s perspective, they are equal iff the additional orthogonal basis vector of the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> introduced by the new column <span class="math notranslate nohighlight">\(X_p\)</span> is orthogonal to the response vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. See the derivation of <a class="reference internal" href="#lm-f-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(F\)</span>-test</span></a> for details. Note that this is different from <span class="math notranslate nohighlight">\(\boldsymbol{x} _p ^\top \boldsymbol{y} =0\)</span>. The example below shows reduction in RSS even if <span class="math notranslate nohighlight">\(\boldsymbol{x} _p ^\top \boldsymbol{y} =0\)</span>.</p></li>
<li><p>From optimization’s perspective, they are equal iff <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span> in Problem 1’s solution. When will <span class="math notranslate nohighlight">\(\hat{\beta}_{p}=0\)</span>? No clear condition.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>’s are orthogonal such that <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X} = I_{p}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{x}_{p} ^\top \boldsymbol{y} = 0 \Leftrightarrow \hat{\beta}_{p}=0
    \]</div>
</li>
<li><p>Note that in general, <span class="math notranslate nohighlight">\(\not\Leftarrow\)</span>. An simple example can be a data set of two points <span class="math notranslate nohighlight">\((1,0), (1,1)\)</span>. The fitted line is <span class="math notranslate nohighlight">\(y=0.5\)</span>.</p></li>
<li><p>Also, in general, <span class="math notranslate nohighlight">\(\not\Rightarrow\)</span>. The example below shows <span class="math notranslate nohighlight">\(\hat{\beta}_{2} \ne 0\)</span> even if <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top _p \boldsymbol{y} =0\)</span></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>

<span class="c1"># reduced model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reduced model ---&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients: &quot;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RSS: &quot;</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

<span class="c1"># full model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">full model ---&quot;</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x2 and y are uncorrelated, dot product :&quot;</span><span class="p">,</span> <span class="n">x2</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients: &quot;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RSS: &quot;</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>reduced model ---
coefficients:  [0.5        0.64285714]
RSS:  [0.07142857]

full model ---
x2 and y are uncorrelated, dot product : [[0]]
coefficients:  [ 0.44444444  0.66666667 -0.11111111]
RSS:  [2.76101317e-30]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="r-2-and-adjusted-r-2">
<h2><span class="math notranslate nohighlight">\(R^2\)</span> and Adjusted <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#r-2-and-adjusted-r-2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="r-squared">
<span id="lm-rsquared"></span><h3><span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>Assuming the <a class="reference internal" href="#lm-tss-identity"><span class="std std-ref">decomposition identity</span></a> of <span class="math notranslate nohighlight">\(TSS\)</span> holds, we can define <span class="math notranslate nohighlight">\(R\)</span>-squared.</p>
<dl class="simple myst">
<dt>Definition (<span class="math notranslate nohighlight">\(R\)</span>-squared)</dt><dd><p><span class="math notranslate nohighlight">\(R\)</span>-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{SSR}{SST}  = 1 - \frac{SSE}{SST} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]</div>
<p><strong>Properties</strong></p>
<ol>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared can never decrease when an additional explanatory variable is added to the model.</p>
<p>As long as <span class="math notranslate nohighlight">\(Cov(Y, X_j) \ne 0\)</span>, then <span class="math notranslate nohighlight">\(X_j\)</span> has some explanatory power to <span class="math notranslate nohighlight">\(Y\)</span>, and thus <span class="math notranslate nohighlight">\(RSS\)</span> decreases, See the <a class="reference internal" href="#lm-rss-nonincreasing"><span class="std std-ref">section</span></a> of <span class="math notranslate nohighlight">\(RSS\)</span> for details. As a result, <span class="math notranslate nohighlight">\(R\)</span>-squared  is not a good measure for model selection, which can cause overfitting.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(R\)</span>-squared equals the squared correlation coefficient between the actual value of the response and the fitted value <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( Y, \hat{Y} \right)^2\)</span>.</p>
<p>In particular, in simple linear regression, <span class="math notranslate nohighlight">\(R^2 = \rho_{X,Y}^2\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>By the definition of correlation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \operatorname{Corr}\left( y, \hat{y} \right)^2
    &amp;= \frac{\operatorname{Cov}\left( y, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Cov}\left( \hat{y} + \hat{\varepsilon}, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Cov}\left( \hat{y} , \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Var}\left( \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \\
    &amp;= \frac{\operatorname{Var}\left( \hat{y} \right)}{\operatorname{Var}\left( y \right)} \\
    &amp;= \frac{SSR}{SST} \\
    &amp;= R^2 \\
    \end{align}\end{split}\]</div>
<p>The third equality holds since</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Cov}\left( \hat{\varepsilon}, \hat{y} \right) = \operatorname{Cov}\left( \hat{\varepsilon}, \sum_j x_j \hat{\beta}_j  \right) = \sum_j \hat{\beta}_j \operatorname{Cov}\left( \hat{\varepsilon},  x_j \right) = 0
    \]</div>
<p>When <span class="math notranslate nohighlight">\(p=2\)</span>, since <span class="math notranslate nohighlight">\(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    R^2 = \operatorname{Corr}\left(y, \hat{y} \right)^2 = \operatorname{Corr}\left(y, x \right)^2
    \]</div>
</div>
</li>
</ol>
<div class="dropdown note admonition">
<p class="admonition-title"> What is <span class="math notranslate nohighlight">\(R^2\)</span> if there is no intercept?</p>
<p>When there is <strong>no</strong> intercept, then <span class="math notranslate nohighlight">\(\bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)\)</span> and hence <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)\)</span>. The decomposition identity may not hold. Thus, the value of <span class="math notranslate nohighlight">\(R\)</span>-squared may no longer be in <span class="math notranslate nohighlight">\([0,1]\)</span>, and its interpretation is no longer valid. What actually happen to the value <span class="math notranslate nohighlight">\(R\)</span>-squared depends on whether we define it using <span class="math notranslate nohighlight">\(TSS\)</span> with <span class="math notranslate nohighlight">\(RSS\)</span> or <span class="math notranslate nohighlight">\(ESS\)</span>.</p>
<p>If we define <span class="math notranslate nohighlight">\(R^2 = \frac{ESS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{ESS} = \left\Vert \hat{\boldsymbol{y} } - \bar{y}\boldsymbol{1} _n \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(ESS &gt; TSS\)</span>, i.e., <span class="math notranslate nohighlight">\(R^2 &gt; 1\)</span>.</p>
<p>On the other hand, if we define <span class="math notranslate nohighlight">\(R^2 = 1 - \frac{RSS}{TSS}\)</span>, then when</p>
<div class="math notranslate nohighlight">
\[
\sqrt{RSS} = \left\Vert \hat{\boldsymbol{y} } -  \boldsymbol{y} \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
\]</div>
<p>we will have <span class="math notranslate nohighlight">\(RSS &gt; TSS\)</span>, i.e. <span class="math notranslate nohighlight">\(R^2 &lt; 0\)</span>.</p>
</div>
</div>
<div class="section" id="adjusted-r-squared">
<h3>Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline">¶</a></h3>
<p>Due to the non-decrease property of <span class="math notranslate nohighlight">\(R\)</span>-squared, we define adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared which is a better measure of goodness of fitting.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared, denoted by <span class="math notranslate nohighlight">\(\bar{R}^2\)</span>, is defined as</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
  \bar{R}^2 = 1-\frac{RSS / (n-p)}{ TSS / (n-1)}
  \]</div>
<p>Properties</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{R}^2\)</span> can increase or decrease. When a new variable is included, <span class="math notranslate nohighlight">\(RSS\)</span> decreases, but <span class="math notranslate nohighlight">\((n-p)\)</span> also decreases.</p>
<ul>
<li><p>Relation to <span class="math notranslate nohighlight">\(R\)</span>-squared is</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
\bar{R}^2 = 1-\frac{n-1}{ n-p}(1 - R^2) &lt; R^2
\]</div>
<ul>
<li><p>Relation to estimated variance of random error and variance of response</p>
<div class="math notranslate nohighlight">
\[
    \bar{R}^2 = 1-\frac{\hat{\sigma}^2}{\operatorname{Var}\left( y \right)}
    \]</div>
</li>
<li><p>Can be negative when</p>
<div class="math notranslate nohighlight">
\[
    R^2 &lt; \frac{p-1}{n-p}
    \]</div>
<p>If <span class="math notranslate nohighlight">\(p &gt; \frac{n+1}{2}\)</span> then the above inequality always hold, and adjusted <span class="math notranslate nohighlight">\(R\)</span>-squared is always negative.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="t-test">
<span id="lm-t-test"></span><h2><span class="math notranslate nohighlight">\(t\)</span>-test<a class="headerlink" href="#t-test" title="Permalink to this headline">¶</a></h2>
<p>We can use <span class="math notranslate nohighlight">\(t\)</span>-test to conduct a hypothesis testing on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, which has a general form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H_0
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} = c \\
H_1
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} \ne c (\text{two-sided} )\\
\end{aligned}\end{split}\]</div>
<p>Usually <span class="math notranslate nohighlight">\(c=0\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(c=0, \boldsymbol{v} = \boldsymbol{e} _j\)</span> then this is equivalent to test <span class="math notranslate nohighlight">\(\beta_j=0\)</span>, i.e. the variable <span class="math notranslate nohighlight">\(X_i\)</span> has no effect on <span class="math notranslate nohighlight">\(Y\)</span> given all other variabels.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(c=0, v_i=1, v_j=-1\)</span> and <span class="math notranslate nohighlight">\(v_k=0, k\ne i, j\)</span> then this is equivalent to test <span class="math notranslate nohighlight">\(\beta_i = \beta_j\)</span>, i.e. the two variables <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> has the same effect on <span class="math notranslate nohighlight">\(Y\)</span> given all other variables.</p></li>
</ul>
<p>The test statistic is</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\hat{\sigma}\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim t_{n-p}
\]</div>
<p>In particular, when <span class="math notranslate nohighlight">\(p=2\)</span>, to test <span class="math notranslate nohighlight">\(\beta_1 = c\)</span>, we use</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\beta}_1 - c}{\hat{\sigma}/ \sqrt{\operatorname{Var}\left( X_1 \right)}}  \sim t_{n-2}
\]</div>
<p>Following this analysis, we can find the <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence interval for a scalar <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \boldsymbol{\beta}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{v} }
\]</div>
<p>In particular,</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{v} = \boldsymbol{e}_j\)</span>, then this is the confidence interval for a coefficient <span class="math notranslate nohighlight">\(beta _j\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{v} = \boldsymbol{x}_i\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> is in the data set, then this is the confidence interval for in-sample fitting of <span class="math notranslate nohighlight">\(y_i\)</span>. We are making in-sample fitting at the mean value <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{y} _i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}\)</span>. If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> is not in the design matrix, then we are doing out-of-sample prediction, and a <a class="reference internal" href="#lm-prediction-interval"><span class="std std-ref">prediction interval</span></a> should be used instead.</p></li>
</ul>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>First, we need to find the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}\)</span>. Recall that</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}_{\text{null}} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1})
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}  \sim N(\boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} , \sigma^2 \boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} )
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim N(0, 1)
\]</div>
<p>Also recall that the RSS has the distribution</p>
<div class="math notranslate nohighlight">
\[
(n-p)\frac{\hat{\sigma}^2}{\sigma^2 } \sim \chi ^2 _{n-p}  
\]</div>
<p>and the two quantities <span class="math notranslate nohighlight">\(\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}\)</span> and <span class="math notranslate nohighlight">\((n-p)\frac{\hat{\sigma}^2}{\sigma^2 }\)</span> are <a class="reference internal" href="#lm-independent-beta-sigma"><span class="std std-ref">independent</span></a>. Therefore, with a standard normal and a Chi-squared that are independent, we can construct a <span class="math notranslate nohighlight">\(t\)</span>-test statistic</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} / \sqrt{\frac{(n-p)\hat{\sigma}^2 }{\sigma^2 } / (n-p)} \sim t_{n-p}
\]</div>
<p>i.e.,</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\hat{\sigma}\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim t_{n-p}
\]</div>
<p>Note that, compared with</p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim N(0, 1)
\]</div>
<p>the RHS changes from <span class="math notranslate nohighlight">\(N(0,1)\)</span> to <span class="math notranslate nohighlight">\(t_{n-p}\)</span> because we are estiamteing <span class="math notranslate nohighlight">\(\sigma\)</span> by <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>. Similar to the case of univariate mean test.</p>
</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Permutation test</p>
<p>Another way to conduct a test without normality assumption is to use permutation test. For instance, to test <span class="math notranslate nohighlight">\(\beta_2=0\)</span>, we fix <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and sample the same <span class="math notranslate nohighlight">\(n\)</span> values of <span class="math notranslate nohighlight">\(x_2\)</span> from the column of <span class="math notranslate nohighlight">\(X_2\)</span>, and compute the <span class="math notranslate nohighlight">\(t\)</span> statistic. Repeat the permutation for multiple times and compute the percentage that</p>
<div class="math notranslate nohighlight">
\[
\left\vert t_{\text{perm} } \right\vert &gt;  \left\vert t_{\text{original} }  \right\vert
\]</div>
<p>which is the <span class="math notranslate nohighlight">\(p\)</span>-value.</p>
</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Social science’s trick to test <span class="math notranslate nohighlight">\(\beta_1 = \beta_2\)</span></p>
<p>Some social science courses introduce a trick to test <span class="math notranslate nohighlight">\(\beta_1 = \beta_2\)</span> by rearranging the explanatory variables. For instance, if our model is,</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i
\]</div>
<p>Then they define <span class="math notranslate nohighlight">\(\gamma = \beta_1 - \beta_2\)</span> and <span class="math notranslate nohighlight">\(x_{i3} = x_{i1} + x_{i2}\)</span>, rearrange RHS,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
Y_i
&amp;= \beta_0 + (\gamma + \beta_2) x_{i1} + \beta_2 x_{i2} + \varepsilon_i \\
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 (x_{i1} + x_{i2}) + \varepsilon_i \\
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 x_{i3} + \varepsilon_i
\end{aligned}\end{split}\]</div>
<p>Finally, they run the regression of the last line and check the <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(\gamma\)</span>. Other parts of the model (<span class="math notranslate nohighlight">\(R\)</span>-squared, <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(\beta_0\)</span>, etc) remain the same.</p>
</div>
</div>
<div class="section" id="f-test">
<span id="lm-f-test"></span><h2><span class="math notranslate nohighlight">\(F\)</span>-test<a class="headerlink" href="#f-test" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compare-two-nested-models">
<h3>Compare Two Nested Models<a class="headerlink" href="#compare-two-nested-models" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">Nested</p>
<p>It is called nested since the reduced model is a special case of the full model with</p>
<div class="math notranslate nohighlight">
\[
\beta_{p-k}=\ldots= \beta_{p-1} =0
\]</div>
</div>
<p><span class="math notranslate nohighlight">\(F\)</span>-test can be used to compare two nested models</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Full model: } Y &amp;\sim \left\{ X_j, j=1, \ldots, p-k-1, p-k, \ldots, p-1 \right\} \\
\text{Reduced model: } Y &amp;\sim \left\{ X_j, j=1, \ldots, p-k-1 \right\}
\end{aligned}\end{split}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">Interpretation of <span class="math notranslate nohighlight">\(F\)</span>-test</p>
<p>Given it’s form, we can interpret the numerator as an average reduction in <span class="math notranslate nohighlight">\(RSS\)</span> by adding the <span class="math notranslate nohighlight">\(k\)</span> explanatory variables. Since the denominator is fixed, if the average reduction is large enough, then we reject the null hypothesis that the <span class="math notranslate nohighlight">\(k\)</span> coefficients are 0.</p>
</div>
<p>The test statistic is</p>
<div class="math notranslate nohighlight">
\[
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full}}/(n-p)} \sim F_{k, n-p}
\]</div>
<p>which can also be computed by <span class="math notranslate nohighlight">\(R^2\)</span> since <span class="math notranslate nohighlight">\(TSS\)</span> are the same for the two models</p>
<div class="math notranslate nohighlight">
\[
F = \frac{(R^2 _{\text{full}} - R^2 _{\text{reduced}})/k}{(1 - R^2 _{\text{full}})/(n-p)}
\]</div>
<p>In particular,</p>
<ul>
<li><p>When <span class="math notranslate nohighlight">\(k=p-1\)</span>, we are comparing a full model vs. intercept only, i.e.,</p>
<div class="math notranslate nohighlight">
\[
    \beta_1 = \ldots = \beta_{p-1} = 0
    \]</div>
<p>In this case,</p>
<div class="math notranslate nohighlight">
\[
    RSS_{\text{reduced}} = \left\| \boldsymbol{y} - \bar{y} \boldsymbol{1} _n \right\|  ^2 = TSS
    \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
    F = \frac{(TSS - RSS_{\text{full}})/(p-1)}{RSS_{\text{full}}/(n-p)}  = \frac{R^2 _{\text{full}}/k}{(1 - R^2 _{\text{full}})/(n-p)}
    \]</div>
</li>
<li><p>When <span class="math notranslate nohighlight">\(k=1\)</span>, we are testing <span class="math notranslate nohighlight">\(\beta_{p-1} = 0\)</span>. In this case, the <span class="math notranslate nohighlight">\(F\)</span>-test is equivalent to the <span class="math notranslate nohighlight">\(t\)</span>-test. The two test statistics have the relation <span class="math notranslate nohighlight">\(F_{1, n-p}=t^2_{n-p}\)</span>. Note that the alternative hypothesis in both tests are two-sided, and <span class="math notranslate nohighlight">\(t\)</span>-test is two-sided while <span class="math notranslate nohighlight">\(F\)</span>-test is one-sided.</p></li>
</ul>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Derivation</em></p>
<p>We need to find the distribution of <span class="math notranslate nohighlight">\(RSS_{\text{reduced} }\)</span> and <span class="math notranslate nohighlight">\(RSS_{\text{full}}\)</span> and then construct a pivot quantity.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> be an orthogonal basis of <span class="math notranslate nohighlight">\(\mathbb{R} ^n\)</span> with three orthogonal parts</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{U}  = [\underbrace{\boldsymbol{u} _1, \ldots, \boldsymbol{u} _{p-k}} _{\boldsymbol{U} _1}, \underbrace{\boldsymbol{u} _{p-k+1}, \ldots, \boldsymbol{u} _{p}} _{\boldsymbol{U} _2}, \underbrace{\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _{n}} _{\boldsymbol{U} _3}]
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{U} ^\top \boldsymbol{y} = \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{y}  \\
\boldsymbol{U} _2 ^\top  \boldsymbol{y}  \\
\boldsymbol{U} _3 ^\top  \boldsymbol{y}  \\
\end{array}\right]
\sim N_n \left( \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\boldsymbol{U} _2 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\boldsymbol{U} _3 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \\
\end{array}\right] , \sigma^2 \boldsymbol{I} _n \right)
\end{split}\]</div>
<p>Thus, we have pairwise independences among <span class="math notranslate nohighlight">\(\boldsymbol{U}_1 ^\top \boldsymbol{y} , \boldsymbol{U} _2 ^\top \boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{U} _3 ^\top \boldsymbol{y}\)</span>.</p>
<p>Moreover, by the property of multivariate normal, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _k \\
\left\| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}   \\
\end{aligned}\end{split}\]</div>
<p>The RSSs have the relations</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
RSS_{\text{full} }
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _1 \boldsymbol{U} _2) ^\bot } \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _3} \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{U} ^\top _3 \boldsymbol{y}  \right\| ^2 \\
RSS_{\text{reduced} }
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _1) ^\bot } \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \boldsymbol{P}_{\operatorname{im}([\boldsymbol{U} _2 \boldsymbol{U} _3])} \boldsymbol{y}  \right\| ^2 \\
&amp;= \left\| \left[ \boldsymbol{U}_2, \boldsymbol{U}_3 \right] ^\top \boldsymbol{y}  \right\| ^2   \\
&amp;= \left\| \boldsymbol{U} ^\top _2  \boldsymbol{y}   \right\| ^2 +  \left\| \boldsymbol{U} ^\top _3 \boldsymbol{y}   \right\| ^2
\end{aligned}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
RSS_{\text{reduced} } - RSS_{\text{full} } = \left\| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _k \\
RSS_{\text{full} } =  \left\| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right\|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}  \\
\end{aligned}\end{split}\]</div>
<p>Therefore, we have the pivot quantity</p>
<div class="math notranslate nohighlight">
\[
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full} }/(n-p)}  \sim F_{k, n-p}
\]</div>
</div>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>A <span class="math notranslate nohighlight">\(F\)</span>-test on <span class="math notranslate nohighlight">\(\beta_1=\beta_2=0\)</span> is difference from two univariate <span class="math notranslate nohighlight">\(t\)</span>-tests <span class="math notranslate nohighlight">\(\beta_1=0, \beta_2=0\)</span>. A group of <span class="math notranslate nohighlight">\(t\)</span>-tests may be misleading if the regressors are highly correlated.</p>
</div>
</div>
<div class="section" id="compare-two-groups-of-data">
<h3>Compare Two Groups of Data<a class="headerlink" href="#compare-two-groups-of-data" title="Permalink to this headline">¶</a></h3>
<p>If the data set contains data from two groups, then the regression line can be different. For instance,</p>
<ul class="simple">
<li><p>In time series analysis, there may be a structural break at a period which can be assumed to be known a priori (for instance, a major historical event such as a war).</p></li>
<li><p>In program evaluation, the explanatory variables may have different impacts on different subgroups of the population.</p></li>
</ul>
<div class="figure align-default" id="lm-chow-test">
<a class="reference internal image-reference" href="../_images/lm-chow-test.png"><img alt="" src="../_images/lm-chow-test.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Illustration of two groups of data <a class="reference external" href="https://en.wikipedia.org/wiki/Chow_test">[Wikipedia]</a></span><a class="headerlink" href="#lm-chow-test" title="Permalink to this image">¶</a></p>
</div>
<p>Testing whether a regression function is different for one group <span class="math notranslate nohighlight">\((i=1,\ldots,m)\)</span> versus another <span class="math notranslate nohighlight">\((i=m+1,\ldots,n)\)</span>, we can use <span class="math notranslate nohighlight">\(F\)</span>-test. The full model is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y_{i}
&amp;= \left( \beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\ldots+\beta_{k} x_{k i} \right) \\
&amp;\ + \left( \gamma_{0}+\gamma_{1} (d_i x_{1 i})+\gamma_{2} (d_i x_{2 i})+\ldots+\gamma_{k} (d_i x_{k i}) \right) \\
&amp;\ +\varepsilon_{i}
\end{aligned}\end{split}\]</div>
<p>where dummy variable <span class="math notranslate nohighlight">\(d_i = 1\)</span> if <span class="math notranslate nohighlight">\(i=m+1, \ldots, n\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>The null and alternative hypotheses are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H_{0}:&amp;\ \gamma_{0}=0, \gamma_{1}=0, \ldots, \gamma_{k}=0 \\
H_{1}:&amp;\ \text{otherwise}
\end{aligned}\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-test statistic is</p>
<div class="math notranslate nohighlight">
\[
F=\frac{\left(RSS_{\text{reduced}}-RSS_{\text{full}}\right) /(k+1)}{RSS_{\text{full}} / (n-2(k+1))}
\]</div>
<div class="margin sidebar">
<p class="sidebar-title">Question</p>
<p>Can you show why the fact holds?</p>
</div>
<p>Equivalently, there is a method called <strong>Chow test</strong>. It uses the fact that</p>
<div class="math notranslate nohighlight">
\[
RSS_{\text{full}} = RSS_{1} + RSS_{2}  
\]</div>
<p>where <span class="math notranslate nohighlight">\(RSS_1\)</span> and <span class="math notranslate nohighlight">\(RSS_2\)</span> are the RSS obtained when we run the following regression model on two groups of data respectively,</p>
<div class="math notranslate nohighlight">
\[
y_{i} = \beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\ldots+\beta_{k} x_{k i} +\varepsilon_{i}
\]</div>
<p>Hence, there is no need to create dummy variables <span class="math notranslate nohighlight">\(d_i\)</span>. There are only three steps:</p>
<ol class="simple">
<li><p>Run the regression with group 1’s data to obtain <span class="math notranslate nohighlight">\(RSS_1\)</span></p></li>
<li><p>Run the regression with group 2’s data to obtain <span class="math notranslate nohighlight">\(RSS_2\)</span></p></li>
<li><p>Run the regression with all data to obtain <span class="math notranslate nohighlight">\(RSS_{\text{reduced}}\)</span></p></li>
</ol>
<p>And then use the <span class="math notranslate nohighlight">\(F\)</span>-test to test the hypothesis.</p>
</div>
<div class="section" id="confidence-region-for-boldsymbol-beta">
<h3>Confidence Region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span><a class="headerlink" href="#confidence-region-for-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>To test <span class="math notranslate nohighlight">\(\boldsymbol{\beta}=\boldsymbol{0}\)</span>, see <a class="reference internal" href="#lm-f-test"><span class="std std-ref"><span class="math notranslate nohighlight">\(F\)</span>-test</span></a></p>
</div>
<p>If we want to draw conclusions to multiple coefficients <span class="math notranslate nohighlight">\(\beta_1, \beta_2, \ldots\)</span> simultaneously, we need a confidence region, and consider the multiple testing issue.</p>
<p>To find a <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, one attemp is to use a cuboid, whose <span class="math notranslate nohighlight">\(j\)</span>-th side length equals to the <span class="math notranslate nohighlight">\((1-\alpha/p)-%\)</span> confidence interval for <span class="math notranslate nohighlight">\(\beta_j\)</span>. Namely, the confidence region is</p>
<div class="math notranslate nohighlight">
\[
\left[ (1-\alpha/p) \text{ C.I. for } \beta_0 \right] \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_1 \right] \times \ldots \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_{p-1} \right]
\]</div>
<p>In this way, we ensure the overall confidence of the confidence region is at least <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{P}\left( \text{every $\beta$ is in its C.I.}  \right) \ge 1-\alpha
\]</div>
<p>A more natural approach is using an ellipsoid. Recall that</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
\]</div>
<p>Hence a pivot quantity for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> can be constructed as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;&amp;\frac{(\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{\sigma^2 }
&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _p)  \\
&amp;\Rightarrow&amp; \ \frac{1}{\sigma^2} \left\| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right\|^2    &amp;\sim \chi ^2 _p \\
&amp;\Rightarrow&amp; \ \frac{\frac{1}{\sigma^2} \left\| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right\|^2/p}{\frac{(n-p)\hat{\sigma}^2}{\sigma^2 }/(n-p)}   &amp;\sim F_{p, n-p}\\
&amp;\Rightarrow&amp; \ \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{p \hat{\sigma}^2}   &amp;\sim F_{p, n-p}\\
\end{aligned}\end{split}\]</div>
<p>Therefore, we can obtain an <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> from this distribution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\beta} \in \left\{ \boldsymbol{v} \in \mathbb{R} ^p: \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{v} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{v} )}{p \hat{\sigma}^2}   \le F_{p, n-p}^{(1-\alpha)} \right\}
\]</div>
<p>which is an ellipsoid centered at <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, scaled by <span class="math notranslate nohighlight">\(\frac{1}{p \hat{\sigma}}\)</span>, rotated by <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X})\)</span>.</p>
<p>In general, for matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R} ^{p \times k}, \operatorname{rank}\left( \boldsymbol{A}  \right) = k\)</span>, the confidence region for <span class="math notranslate nohighlight">\(\boldsymbol{A} ^\top \boldsymbol{\beta}\)</span> can be found in a similar way</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A} ^\top \hat{\boldsymbol{\beta}}
&amp;\sim N(\boldsymbol{A} ^\top \boldsymbol{\beta} , \sigma^2 \boldsymbol{A} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}  \boldsymbol{A} ) \\
\Rightarrow \quad\ldots &amp;\sim F_{k, n-p} \\
\end{aligned}\end{split}\]</div>
</div>
</div>
<div class="section" id="prediction-interval-for-y-new">
<span id="lm-prediction-interval"></span><h2>Prediction Interval for <span class="math notranslate nohighlight">\(y_{new}\)</span><a class="headerlink" href="#prediction-interval-for-y-new" title="Permalink to this headline">¶</a></h2>
<p>For a new <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, the new response is</p>
<div class="math notranslate nohighlight">
\[
y _{new} = \boldsymbol{x} ^\top \boldsymbol{\beta} + \boldsymbol{\varepsilon} _{new}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} _{new} \perp\!\!\!\perp \hat{\boldsymbol{\beta}} , \hat{\sigma}\)</span> since the RHS are from training set.</p>
<p>The prediction is</p>
<div class="math notranslate nohighlight">
\[
\hat{y} _{new} = \boldsymbol{x} ^\top \hat{\boldsymbol{\beta}}
\]</div>
<p>Thus, the prediction error is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y _{new} - \hat{y}_{new}
&amp;= \boldsymbol{\varepsilon} _{new} + \boldsymbol{x} ^\top (\boldsymbol{\beta} - \hat{\boldsymbol{\beta}} )\\
&amp;\sim N \left( \boldsymbol{0} , \sigma^2 (1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} ) \right) \\
\end{aligned}\end{split}\]</div>
<p>Hence, the <span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence prediction interval for a new response value <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span> at an out-of-sample <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} }
\]</div>
<p>There is an additional “<span class="math notranslate nohighlight">\(1\)</span>” in the squared root, compared to the confidence interval for an in-sample fitting,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{ \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} }
\]</div>
<div class="dropdown note admonition">
<p class="admonition-title"> Width of an interval</p>
<p>When we are building confidence interval for <span class="math notranslate nohighlight">\(\boldsymbol{y} _i\)</span> or prediction interval for <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span>, the width depends on the magnitude of <span class="math notranslate nohighlight">\(n\)</span> the choice of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p>
<p>As <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>, we have <span class="math notranslate nohighlight">\(\boldsymbol{a} ^\top
(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{a}\rightarrow 0\)</span> for all <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span>, hence the</p>
<ul class="simple">
<li><p>CI for <span class="math notranslate nohighlight">\(\boldsymbol{y} _i\)</span>: <span class="math notranslate nohighlight">\(\operatorname{se}  \rightarrow 0, \operatorname{width} \rightarrow 0\)</span></p></li>
<li><p>PI for <span class="math notranslate nohighlight">\(\boldsymbol{y} _{new}\)</span>: <span class="math notranslate nohighlight">\(\operatorname{se}  \rightarrow \hat{\sigma}, \operatorname{width} \rightarrow 2 \times t_{n-p}^{(1-\alpha/2)} \hat{\sigma}\)</span></p></li>
</ul>
<p>The width also depends on the choice of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is aligned with a large eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}\)</span> is small. This is because larger eigenvectors indicate a direction of large variation in the data set, and hence it has more distinguishability.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is aligned with a small eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}\)</span> is large. This is because smaller eigenvectors indicate a direction of small variation in the data set, and hence it has less distinguishability and more uncertainty.</p></li>
</ul>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/pca-pc-ellipsoids.png"><img alt="" src="../_images/pca-pc-ellipsoids.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Illustration of eigenvectors in bivariate Gaussian [Fung 2018]</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="11-lm-estimation.html" title="previous page">Linear Models - Estimation</a>
    <a class='right-next' id="next-link" href="13-lm-extension.html" title="next page">Linear Regression - Extension</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>