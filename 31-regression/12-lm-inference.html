
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Regression - Inference &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression - Extension" href="13-lm-extension.html" />
    <link rel="prev" title="Linear Models - Estimation" href="11-lm-estimation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-lm-extension.html">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-t-SNE.html">
     SNE and $t$-SNE
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     K-means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/12-lm-inference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/12-lm-inference.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coefficients-boldsymbol-beta">
   Coefficients $\boldsymbol{\beta}$
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unbiasedness">
     Unbiasedness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#efficiency-blue">
     Efficiency (BLUE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consistency">
     Consistency
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#large-sample-distribution">
     Large Sample Distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#residuals-and-error-variance">
   Residuals and Error Variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals-hat-boldsymbol-varepsilon">
     Residuals $\hat{\boldsymbol{\varepsilon}}$
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation-of-error-variance">
     Estimation of Error Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
     Independence of $\hat{\boldsymbol{\beta}}$ and $\hat{\sigma}^2$
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sum-of-squares">
   Sum of Squares
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition-of-tss">
     Decomposition of TSS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-increasing-rss">
     Non-increasing RSS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r-2-and-adjusted-r-2">
   $R^2$ and Adjusted $R^2$
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-squared">
     $R$-squared
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjusted-r-squared">
     Adjusted $R$-squared
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t-test">
   $t$-test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#f-test">
   $F$-test
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-two-nested-models">
     Compare Two Nested Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-two-groups-of-data">
     Compare Two Groups of Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-region-for-boldsymbol-beta">
     Confidence Region for $\boldsymbol{\beta}$
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-interval-for-y-new">
   Prediction Interval for $y_{new}$
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-inference">
<h1>Linear Regression - Inference<a class="headerlink" href="#linear-regression-inference" title="Permalink to this headline">¶</a></h1>
<p>We first describe the properties of OLS estimator $\hat{\boldsymbol{\beta}}$ and the corresponding residuals $\hat{\boldsymbol{\varepsilon} }$. Then we introduce sum of squares, $R$-squared, hypothesis testing and confidence intervals. All these methods assume normality of the error terms $\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)$ unless otherwise specified.</p>
<div class="section" id="coefficients-boldsymbol-beta">
<h2>Coefficients $\boldsymbol{\beta}$<a class="headerlink" href="#coefficients-boldsymbol-beta" title="Permalink to this headline">¶</a></h2>
<div class="section" id="unbiasedness">
<h3>Unbiasedness<a class="headerlink" href="#unbiasedness" title="Permalink to this headline">¶</a></h3>
<p>The OLS estimators are unbiased since</p>
<p>$$\begin{align}
\operatorname{E}\left( \hat{\boldsymbol{\beta} } \right) &amp;= \operatorname{E}\left( (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right) \
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \operatorname{E}\left( \boldsymbol{y} \right) \
&amp;=  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top  \boldsymbol{X} \boldsymbol{\beta} \
&amp;= \boldsymbol{\beta}
\end{align}$$</p>
<p>when $p=2$,</p>
<p>$$\begin{align}
\hat{\beta}<em>{1}
&amp;=\frac{\sum</em>{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \
\end{align}$$</p>
<p>To prove unbiasedness, using the fact that for any constant $c$,</p>
<p>$$
\sum_i (x_i - \bar{x})c = 0
$$</p>
<p>Then, the numerator becomes</p>
<p>$$\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
&amp;=\sum\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+u_{i}\right) \
&amp;=\sum\left(x_{i}-\bar{x}\right) \beta_{0}+\sum\left(x_{i}-\bar{x}\right) \beta_{1} x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \
&amp;=\beta_{0} \sum\left(x_{i}-\bar{x}\right)+\beta_{1} \sum\left(x_{i}-\bar{x}\right) x_{i} +\sum\left(x_{i}-\bar{x}\right) u_{i} \
&amp;=\beta_{1} \sum\left(x_{i}-\bar{x}\right)^2 +\sum\left(x_{i}-\bar{x}\right) u_{i} \
\end{align}$$</p>
<p>Hence</p>
<p>$$
\begin{equation}
\hat{\beta}<em>{1}=\beta</em>{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}}
\end{equation}
$$</p>
<p>and</p>
<p>$$
\operatorname{E}\left( \hat{\beta}_1 \right) = \beta_1
$$</p>
</div>
<div class="section" id="variance">
<span id="lm-inference-variance"></span><h3>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h3>
<p>The variance (covariance matrix) of the coefficients is</p>
<p>$$\begin{align}
\operatorname{Var}\left( \boldsymbol{\beta}  \right) &amp;= \operatorname{Var}\left(  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top  \boldsymbol{y}  \right)  \
&amp;=   (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X}^\top \operatorname{Var}\left( \boldsymbol{y}  \right)  \boldsymbol{X}  (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \
&amp;= \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\
\end{align}$$</p>
<p>:::{admonition} What is the $(j,j)$-th entry $\operatorname{Var}\left( \hat{\beta}_j \right)$?</p>
<p>More specifically, for the $j$-th coefficient estimator $\hat{\beta}_j$, its variance is,</p>
<p>$$\begin{align}
\operatorname{Var}\left( \hat{\beta}<em>j \right)
&amp;= \sigma^2 \left[ (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \right]</em>{[j,j]} \
&amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}<em>j)^2} \
&amp;= \sigma^2 \frac{TSS_j}{RSS_j} \frac{1}{TSS_j} \
&amp;= \sigma^2 \frac{1}{\sum_i(\hat{x}</em>{ij} - x_{ij})} \
\end{align}$$</p>
<p>where $R_j^2$, $RSS_j$, $TSS_j$, and $\hat{x}_{ij}$ are the corresponding representatives when we regress $X_j$ over all other explanatory variables.</p>
<p>Note that the value of $R^2$ when we regressing $X_1$ to an constant intercept is 0. So we have the particular result below.
:::</p>
<p>When $p=2$, the inverse $(\boldsymbol{X} ^\top \boldsymbol{X} )^\top$ is</p>
<p>$$
\begin{array}{c}
\left(\boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1}
=\frac{1}{\sum_{i=1}^{n} \left(x_{i}-\bar{x}\right)^{2}}\left[\begin{array}{cc}
\bar{x^2} &amp; - \bar{x} \</p>
<ul class="simple">
<li><p>\bar{x} &amp; 1
\end{array}\right]
\end{array}
$$</p></li>
</ul>
<p>the variance of $\hat{\beta}_1$ is</p>
<p>$$\begin{align}
\operatorname{Var}\left( \hat{\beta}<em>1 \right)
&amp;= \operatorname{Var}\left( \beta</em>{1}+\frac{\sum\left(x_{i}-\bar{x}\right) u_{i}}{\sum \left(x_{i}-\bar{x}\right)^{2}} \right)\
&amp;= \frac{\operatorname{Var}\left( \sum\left(x_{i}-\bar{x}\right) u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\
&amp;= \frac{\sum\left(x_{i}-\bar{x}\right)^2 \operatorname{Var}\left( u_{i} \right)}{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\
&amp;= \sigma^2 \frac{\sum\left(x_{i}-\bar{x}\right)^2 }{\left[ \sum \left(x_{i}-\bar{x}\right)^{2} \right]^2}\
&amp;= \frac{\sigma^2}{\sum_{i=1}^n \left(x_{i}-\bar{x}\right)^{2}}\
\end{align}$$</p>
<p>We conclude that</p>
<ul class="simple">
<li><p>The larger the error variance, $\sigma^2$, the larger the variance of the coefficient estimates.</p></li>
<li><p>The larger the variability in the $x_i$, the smaller the variance.</p></li>
<li><p>A larger sample size should decrease the variance.</p></li>
<li><p>In multiple regression, reduce the relation between $X_j$ and other covariates (e.g. by orthogonal design) can decreases $R^2_{j}$, and hence decrease the variance.</p></li>
</ul>
<p>A problem is that the error $\sigma^2$ variance is <strong>unknown</strong>. In practice, we can estimate $\sigma^2$ by its unbiased estimator $\hat{\sigma}^2=\frac{\sum_i (x_i - \bar{x})}{n-2}$ (to be shown [link]), and substitute it into $\operatorname{Var}\left( \hat{\beta}_1 \right)$. Since the error variance $\hat{\sigma}^2$ is estimated, the slope variance $\operatorname{Var}\left( \hat{\beta}_1 \right)$ is estimated too, and hence the square root is called standard error of $\hat{\beta}$, instead of standard deviation.</p>
<p>$$\begin{align}
\operatorname{se}\left(\hat{\beta}_{1}\right)
&amp;= \sqrt{\widehat{\operatorname{Var}}\left( \hat{\beta}<em>1 \right)}\
&amp;= \frac{\hat{\sigma}}{\sqrt{\sum \left(x</em>{i}-\bar{x}\right)^{2}}}
\end{align}$$</p>
</div>
<div class="section" id="efficiency-blue">
<h3>Efficiency (BLUE)<a class="headerlink" href="#efficiency-blue" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Theorem (Gauss–Markov)</dt><dd><p>The ordinary least squares (OLS) estimator has the <strong>lowest</strong> sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. In abbreviation, the OLS estimator is BLUE: Best (lowest variance) Linear Unbiased Estimator.</p>
</dd>
</dl>
<p>:::{admonition,dropdown,seealso} <em>Proof</em></p>
<p>Let $\tilde{\boldsymbol{\beta}} = \boldsymbol{C} \boldsymbol{y}$ be another linear estimator of $\boldsymbol{\beta}$. We can write $\boldsymbol{C} = \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \boldsymbol{X} ^\top + \boldsymbol{D}$ where $\boldsymbol{D} \ne \boldsymbol{0}$. Then</p>
<p>$$\begin{align}
\operatorname{E}\left( \tilde{\boldsymbol{\beta} } \right)
&amp;= \operatorname{E}\left( \boldsymbol{C} \boldsymbol{y}   \right)\
&amp;= \boldsymbol{C} \operatorname{E}\left( \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  \right)\
&amp;= \boldsymbol{\beta} + \boldsymbol{D} \boldsymbol{X} \boldsymbol{\beta} \
\end{align}$$</p>
<p>Hence, $\tilde{\boldsymbol{\beta}}$ is unbiased iff $\boldsymbol{D} \boldsymbol{X} = 0$.</p>
<p>The variance is</p>
<p>$$\begin{align}
\operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right)
&amp;= \boldsymbol{C}\operatorname{Var}\left( \boldsymbol{y}  \right) \boldsymbol{C} ^\top \
&amp;= \sigma^2 \boldsymbol{C} \boldsymbol{C} ^\top \
&amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  + (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{D} ^\top + \boldsymbol{D} \boldsymbol{X} \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^\top  + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\
&amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} + \boldsymbol{D} \boldsymbol{D} ^\top  \right]\
&amp;= \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right) + \sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \
\end{align}$$</p>
<p>Since $\sigma^2 \boldsymbol{D} \boldsymbol{D} ^\top \in \mathrm{PSD}$, we have</p>
<p>$$
\operatorname{Var}\left( \tilde{\boldsymbol{\beta} } \right) \succeq \operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)
$$</p>
<p>The equality holds iff $\boldsymbol{D} ^\top \boldsymbol{D} = 0$, which implies that $\operatorname{tr}\left( \boldsymbol{D} \boldsymbol{D} ^\top \right) = 0$, then $\left\Vert \boldsymbol{D} \right\Vert _F^2 = 0$, then $\boldsymbol{D} = 0$, i.e. $\tilde{\boldsymbol{\beta} } = \hat{\boldsymbol{\beta} }$. Therefore, BLUE is unique.
:::</p>
<p>Moreover,</p>
<ul class="simple">
<li><p>If error term is normally distributed, then OLS is most efficient among all consistent estimators (not just linear ones).</p></li>
<li><p>When the distribution of error term is non-normal, other estimators may have lower variance than OLS such as least absolute deviation (median regression).</p></li>
</ul>
</div>
<div class="section" id="consistency">
<h3>Consistency<a class="headerlink" href="#consistency" title="Permalink to this headline">¶</a></h3>
<p>The OLS and consistent,</p>
<p>$$
\hat{\boldsymbol{\beta}}_{OLS} \stackrel{P}{\rightarrow} \boldsymbol{\beta}
$$</p>
<p>since</p>
<p>$$\begin{aligned}
\operatorname{plim} \hat{\boldsymbol{\beta}}
&amp;= \operatorname{plim} \left( \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) \
&amp;= \boldsymbol{\beta} + \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X} \right)^{-1} \underbrace{\operatorname{plim} \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) }_{=0 \text{ by CLM} }\
&amp;= \boldsymbol{\beta} \
\end{aligned}$$</p>
</div>
<div class="section" id="large-sample-distribution">
<h3>Large Sample Distribution<a class="headerlink" href="#large-sample-distribution" title="Permalink to this headline">¶</a></h3>
<p>If we assume $\varepsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)$, or $\boldsymbol{\varepsilon} \sim N_n(\boldsymbol{0} , \boldsymbol{I} _n)$, then</p>
<p>$$
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta} , \sigma^2 \boldsymbol{I} )
$$</p>
<p>Hence, the distribution of the coefficients estimator is</p>
<p>$$\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y}   \
&amp;\sim  N(\boldsymbol{\beta} , (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} \operatorname{Var}\left( \boldsymbol{y}  \right)) \boldsymbol{X} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \
&amp;\sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} ) \
\end{aligned}$$</p>
<p>The assumption may fail when the response variable $y$ is</p>
<ul class="simple">
<li><p>right skewed, e.g. wages, savings</p></li>
<li><p>non-negative, e.g. counts, arrests</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>As discussed at the beginning, the data matrix $\boldsymbol{X}$ is treated as fixed by statisticians while as random by econometricians. As a result, the derivation of the asymptotic distribution differs in the two domains.</p>
</div>
<p>When the normality assumption of the error term fails, the OLS estimator is <strong>asymptotically</strong> normal,</p>
<p>$$
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
$$</p>
<p>Therefore, in a large sample, even if the normality assumption fails, we can still do hypothesis testing which assumes normality.</p>
<p>:::{admonition,dropdown,seealso} <em>Sketch of derivation</em></p>
<p>Note that</p>
<p>$$
\sqrt{n}\left( \hat{\boldsymbol{\beta}}  - \boldsymbol{\beta}  \right)= \left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}   \right) ^{-1} \left( \frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right)
$$</p>
<p>By CLT, the second term</p>
<p>$$
\frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \overset{\mathcal{D}}{\longrightarrow} N \left( \boldsymbol{0}, \frac{\sigma^2 }{n} \boldsymbol{X} ^\top \boldsymbol{X}  \right)
$$</p>
<p>By Slusky’s Theorem, the product</p>
<p>$$
\left( \frac{1}{n} \boldsymbol{X} ^\top \boldsymbol{X}   \right) ^{-1} \left( \frac{1}{\sqrt{n}} \boldsymbol{X} ^\top \boldsymbol{\varepsilon}  \right) \overset{\mathcal{D}}{\longrightarrow} N \left( \boldsymbol{0}, \sigma^2 \left( \frac{1}{n}  \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}  \right)
$$</p>
<p>or equivalently,</p>
<p>$$
\hat{\boldsymbol{\beta}} \overset{\mathcal{D}}{\rightarrow} N(\boldsymbol{\beta},\sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
$$
:::</p>
</div>
</div>
<div class="section" id="residuals-and-error-variance">
<h2>Residuals and Error Variance<a class="headerlink" href="#residuals-and-error-variance" title="Permalink to this headline">¶</a></h2>
<div class="section" id="residuals-hat-boldsymbol-varepsilon">
<h3>Residuals $\hat{\boldsymbol{\varepsilon}}$<a class="headerlink" href="#residuals-hat-boldsymbol-varepsilon" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The residual is defined as the difference between the true response value $y$ and our fitted response value $\hat{y}$.</p>
<p>$$\hat\varepsilon_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta}}$$</p>
<p>It is an estimate of the error term $\varepsilon_i$.</p>
</dd>
</dl>
<p><strong>Properties</strong></p>
<ol class="simple">
<li><p>The sum of the residual is zero: $\sum_i \hat{\varepsilon}_i = 0$</p></li>
<li><p>The sum of the product of residual and any covariate is zero, or they are “uncorrelated”: $\sum_i x_{ij} \hat{\varepsilon}_i = 0$ for all $j$.</p></li>
<li><p>The sum of squared residuals: $\left| \boldsymbol{\hat{\varepsilon}}  \right|^2   = \left| \boldsymbol{y} - \boldsymbol{H} \boldsymbol{y}   \right|^2  = \boldsymbol{y} ^\top (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y}$</p></li>
</ol>
<p>:::{admonition,dropdown,seealso} <em>Proof</em>
Recall the normal equation</p>
<p>$$
\boldsymbol{X} ^\top (\boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta} }) = \boldsymbol{0}
$$</p>
<p>We obtain</p>
<p>$$
\boldsymbol{X} ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
$$</p>
<p>Since the first column of $\boldsymbol{X}$ is $\boldsymbol{1}$ , we have</p>
<p>$$\begin{align}
\sum_i \hat{\varepsilon}_i<br />
&amp;= \sum_i(y_i - \hat{y}_i)  \
&amp;= \sum_i(y_i - \boldsymbol{x}_i ^\top \hat{\boldsymbol{\beta} }_i)  \
&amp;= 0
\end{align}$$</p>
<p>For other columns $\boldsymbol{x}_j$ in $\boldsymbol{X}$, we have</p>
<p>$$
\boldsymbol{x}_j ^\top \boldsymbol{\hat{\varepsilon}}  = \boldsymbol{0}
$$</p>
<p>The 3rd equality holds since $\boldsymbol{I} - \boldsymbol{H}$ is a projection matrix if $\boldsymbol{H}$ is a projection matrix, i.e.,</p>
<p>$$
(\boldsymbol{I} - \boldsymbol{H}) (\boldsymbol{I} - \boldsymbol{H} ) = \boldsymbol{I} -\boldsymbol{H}
$$</p>
<p>:::</p>
</div>
<div class="section" id="estimation-of-error-variance">
<h3>Estimation of Error Variance<a class="headerlink" href="#estimation-of-error-variance" title="Permalink to this headline">¶</a></h3>
<p>In the estimation section we mentioned the estimator for the error variance $\sigma^2$ is</p>
<p>$$
\hat{\sigma}^{2}=\frac{|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}|^{2}}{n-p}
$$</p>
<p>This is because</p>
<p>$$
\left| \hat{\boldsymbol{\varepsilon}}  \right|  ^2 \sim \sigma^2\chi ^2 _{n-p}  \</p>
<p>\Rightarrow  \quad \sigma^2 =\operatorname{E}\left( \frac{|\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}|^{2}}{n-p} \right)
$$</p>
<p>and we used the method of moment estimator. The derivation of the above expectation is a little involved.</p>
<p>:::{admonition,dropdown,seealso} <em>Derivation</em></p>
<p>Let</p>
<ul class="simple">
<li><p>$\boldsymbol{U} = [\boldsymbol{U} _ \boldsymbol{X} , \boldsymbol{U} _\bot]$ be an orthogonal basis of $\mathbb{R} ^{n\times n}$ where</p></li>
<li><p>$\boldsymbol{U} _ \boldsymbol{X}  = [\boldsymbol{u} _1, \ldots, \boldsymbol{u} _p]$ is an orthogonal basis of the column space (image) of $\boldsymbol{X}$, denoted $\operatorname{col}(\boldsymbol{X} )$</p></li>
<li><p>$\boldsymbol{U} _ \bot  = [\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _n]$ is an orthogonal basis of the orthogonal complement of the column space (kernel) of $\boldsymbol{X}$, , denoted $\operatorname{col}(\boldsymbol{X} ) ^\bot$.</p></li>
</ul>
<p>Recall</p>
<p>$$ \hat{\boldsymbol{\varepsilon}}  = \boldsymbol{y} - \hat{\boldsymbol{y}} = (\boldsymbol{I} - \boldsymbol{H} ) \boldsymbol{y} \in \operatorname{col}(\boldsymbol{X} ) ^\bot$$</p>
<p>which is</p>
<p>$$\begin{aligned}
\left| \hat{\boldsymbol{\varepsilon}}  \right|  ^2
&amp;= \left| \boldsymbol{P} _{\boldsymbol{U} _\bot} \boldsymbol{y}   \right|  \
&amp;= \left| \boldsymbol{U} _\bot \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right|^2  \
&amp;= \left| \boldsymbol{U} _\bot ^\top \boldsymbol{y}  \right|^2  \
&amp;= \left| \boldsymbol{U} _\bot ^\top (\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon})    \right|^2  \
&amp;= \left| \boldsymbol{U} _\bot ^\top  \boldsymbol{\varepsilon}    \right|^2  \quad \because \boldsymbol{U} _\bot ^\top\boldsymbol{X} = \boldsymbol{0} \
\end{aligned}$$</p>
<p>Note that assuming $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0} , \sigma^2 \boldsymbol{I} )$, we have</p>
<p>$$\begin{aligned}
\boldsymbol{U} <em>\bot ^\top \boldsymbol{\varepsilon}
&amp;\sim N</em>{n-p}(\boldsymbol{0} , \boldsymbol{U} _\bot ^\top \sigma^2 \boldsymbol{I}<em>n \boldsymbol{U} <em>\bot) \
&amp;\sim N</em>{n-p}(\boldsymbol{0} , \sigma^2 \boldsymbol{I}</em>{n-p}) \
\end{aligned}$$</p>
<p>and hence the sum of squared normal variables follows</p>
<p>$$
\left| \boldsymbol{U} _\bot ^\top \boldsymbol{\varepsilon} \right|  ^2 \sim \sigma^2 \chi ^2 _{n-p}<br />
$$</p>
<p>Thus,</p>
<p>$$
\left| \hat{\boldsymbol{\varepsilon}}  \right|  ^2 \sim \sigma^2 \chi ^2 _{n-p}<br />
$$</p>
<p>The first moment is</p>
<p>$$
\operatorname{E}\left( \left| \hat{\boldsymbol{\varepsilon}} \right|^2    \right) = \sigma^2 (n-p)
$$</p>
<p>or equivalently</p>
<p>$$
\sigma^2 = \frac{\operatorname{E}\left( \left| \hat{\boldsymbol{\varepsilon}} \right|^2\right)}{n-p}
$$</p>
<p>Therefore, the method of moment estimator for $\sigma^2$ is</p>
<p>$$
\hat{\sigma}^2 = \frac{\left| \hat{\boldsymbol{\varepsilon}}  \right|^2  }{n-p}
$$</p>
<p>which is unbiased.</p>
<p>:::</p>
<p>Can we find $\operatorname{Var}\left( \hat{\sigma}^2  \right)$ like we did for $\operatorname{Var}\left( \hat{\boldsymbol{\beta}}  \right)$? No, unless we assume higher order moments of $\varepsilon_i$.</p>
</div>
<div class="section" id="independence-of-hat-boldsymbol-beta-and-hat-sigma-2">
<span id="lm-independent-beta-sigma"></span><h3>Independence of $\hat{\boldsymbol{\beta}}$ and $\hat{\sigma}^2$<a class="headerlink" href="#independence-of-hat-boldsymbol-beta-and-hat-sigma-2" title="Permalink to this headline">¶</a></h3>
<p>To prove the independence between the coefficients estimator $\hat{\boldsymbol{\beta} }$ and the error variance estiamtor $\hat{\sigma}^2$, we need the Lemma below.</p>
<dl class="simple myst">
<dt>Lemma</dt><dd><p>Suppose a random vector $\boldsymbol{y}$ follows multivariate normal distribution $\boldsymbol{y} \sim N_m(\boldsymbol{\mu} , \sigma^2 I_m)$ and $S, T$ are orthogonal subspaces of $\mathbb{R} ^m$, then the two projected random vectors are independent</p>
</dd>
</dl>
<p>$$
\boldsymbol{P}_S (\boldsymbol{y}) \perp!!!\perp  \boldsymbol{P}_T (\boldsymbol{y})
$$</p>
<p>Note that $\hat{\boldsymbol{\beta}}$ is a function of $\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X}) } (\boldsymbol{y})$ since</p>
<p>$$\begin{aligned}
\hat{\boldsymbol{\beta}}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{P}_{\operatorname{im}(\boldsymbol{X} ) } (\boldsymbol{y})  \
\end{aligned}$$</p>
<p>and note that $\hat{\sigma}^2$ is a function of $\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X} )^\bot } (\boldsymbol{y})$ since</p>
<p>$$
\hat{\sigma}^{2}=\frac{|\hat{\boldsymbol{\varepsilon}} |^{2}}{n-p} =
\frac{\left| \boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X})^\bot } (\boldsymbol{y}) \right| ^2  }{n-p}
$$</p>
<div class="margin sidebar">
<p class="sidebar-title">Why we need this independency?</p>
<p>This independency is used to construct a pivot quantity to <a class="reference internal" href="#lm-t-test"><span class="std std-ref">$t$-test</span></a> a hypothesis on $\boldsymbol{\beta}$.</p>
</div>
<p>Since $\operatorname{im}(\boldsymbol{X})$ and $\operatorname{im}(\boldsymbol{X})^\bot$ are orthogonal subspaces of $\mathbb{R} ^p$, by the lemma, $\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X}) } (\boldsymbol{y})$ and $\boldsymbol{P}  _{\operatorname{im}(\boldsymbol{X})^\bot } (\boldsymbol{y})$ are independent. Hence, $\hat{\boldsymbol{\beta}}$ and $\hat{\sigma}^2$ are independent.</p>
<p>:::{admonition,dropdown,seealso} <em>Proof of Lemma</em></p>
<p>Let $\boldsymbol{z} \sim N(\boldsymbol{0} , \boldsymbol{I} _m)$ be a standard multivariate normal random vector, and $\boldsymbol{U} = [\boldsymbol{U} _S, \boldsymbol{U} _T]$ be orthogonal basis of $\mathbb{R} ^n$. Then</p>
<p>$$\begin{aligned}
&amp;&amp;\boldsymbol{U} ^\top \boldsymbol{z} &amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \
&amp;\Rightarrow&amp; \quad \left[\begin{array}{l}
\boldsymbol{U}_S ^\top \boldsymbol{z} \
\boldsymbol{U}_T ^\top \boldsymbol{z} \
\end{array}\right]&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} _m) \
&amp;\Rightarrow&amp; \quad \boldsymbol{U} ^\top _S \boldsymbol{z}  &amp;\perp!!!\perp \boldsymbol{U} ^\top _T \boldsymbol{z}  \
&amp;\Rightarrow&amp; \quad  f(\boldsymbol{U} ^\top _S \boldsymbol{z})  &amp;\perp!!!\perp f(\boldsymbol{U} ^\top _T \boldsymbol{z})  \
\end{aligned}$$</p>
<p>Let $\boldsymbol{y} = \boldsymbol{\mu} + \sigma \boldsymbol{z}$, then</p>
<p>$$
\boldsymbol{P} _S(\boldsymbol{y} ) = \boldsymbol{U} _S \boldsymbol{U} _S ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) \perp!!!\perp \boldsymbol{U} _T \boldsymbol{U} _T ^\top (\boldsymbol{\mu} + \sigma \boldsymbol{z} ) = \boldsymbol{P} _T(\boldsymbol{y} )
$$</p>
<p>$\square$</p>
<p>:::</p>
</div>
</div>
<div class="section" id="sum-of-squares">
<h2>Sum of Squares<a class="headerlink" href="#sum-of-squares" title="Permalink to this headline">¶</a></h2>
<p>We can think of each observation as being made up of an explained part, and an unexplained part.</p>
<ul class="simple">
<li><p>Total sum of squares: $TSS = \sum\left(y_{i}-\bar{y}\right)^{2}$</p></li>
<li><p>Explained sum of squares: $ESS = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}$</p></li>
<li><p>Residual sum of squares: $RSS = \sum (y_i - \hat{y}_i)^2$</p></li>
</ul>
<div class="section" id="decomposition-of-tss">
<span id="lm-tss-identity"></span><h3>Decomposition of TSS<a class="headerlink" href="#decomposition-of-tss" title="Permalink to this headline">¶</a></h3>
<p>We have the decomposition identity</p>
<p>$$\begin{align}
TSS
&amp;=\sum\left(y_{i}-\bar{y}\right)^{2} \
&amp;=\sum\left[\left(y_{i}-\hat{y}<em>{i}\right)+\left(\hat{y}</em>{i}-\bar{y}\right)\right]^{2} \
&amp;=\sum\left[\hat{\varepsilon}<em>{i}+\left(\hat{y}</em>{i}-\bar{y}\right)\right]^{2} \
&amp;=\sum \hat{\varepsilon}<em>{i}^{2}+2 \sum \hat{\varepsilon}</em>{i}\left(\hat{y}<em>{i}-\bar{y}\right)+\sum\left(\hat{y}</em>{i}-\bar{y}\right)^{2} \
&amp;= RSS + 2  \sum \hat{\varepsilon}_{i}\left(\hat{\beta}_0 + \hat{\beta}<em>1 x</em>{i}-\bar{y}\right)+ ESS \
&amp;= RSS + ESS
\end{align}$$</p>
<p>where use the fact that $\sum_i \varepsilon_i = 0$ and $\sum_i \varepsilon_i x_i = 0$ shown [above].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some courses use the letters $R$ and $E$ to denote the opposite quantity in statistics courses.</p>
<ul class="simple">
<li><p>Sum of squares due to regression: $SSR = \sum\left(\hat{y}_{i}-\bar{y}\right)^{2}$</p></li>
<li><p>Sum of squared errors: $SSE = \sum (y_i - \hat{y}_i)^2$</p></li>
</ul>
</div>
<p>:::{admonition,dropdown,note} Decomposition due to orthogonality</p>
<p>In vector form, the decomposition identity is equivalent to</p>
<p>$$
\left\Vert \boldsymbol{y} - \bar{y} \boldsymbol{1} _n  \right\Vert ^2 = \left\Vert \boldsymbol{y} - \hat{\boldsymbol{y} }  \right\Vert ^2 + \left\Vert \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n\right\Vert ^2
$$</p>
<p>From linear algebra’s perspective, it holds because the LHS vector $\boldsymbol{y} - \bar{y}\boldsymbol{1} _n$ is the the sum of the two RHS vectors, and the two vectors are orthogonal</p>
<p>$$\begin{aligned}
\boldsymbol{y}  - \bar{y} \boldsymbol{1} _n = (\boldsymbol{y} - \hat{\boldsymbol{y} }) &amp;+ (\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n) \
\boldsymbol{y} - \hat{\boldsymbol{y} } &amp;\perp \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n
\end{aligned}$$</p>
<p>More specifically, they are orthogonal because</p>
<p>$$
\boldsymbol{y} - \hat{\boldsymbol{y} } \in \operatorname{col}(\boldsymbol{X} )^\perp \quad  \hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X} )
$$</p>
<p>since $\boldsymbol{1} _n \in \operatorname{col}(\boldsymbol{X})$, if an intercept term is included in the model.</p>
<p>:::</p>
</div>
<div class="section" id="non-increasing-rss">
<span id="lm-rss-nonincreasing"></span><h3>Non-increasing RSS<a class="headerlink" href="#non-increasing-rss" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is equivalent to say <a class="reference internal" href="#lm-rsquared"><span class="std std-ref">$R$-squared</span></a> is always increasing or unchanged, if an intercept term in included in the model.</p>
</div>
<p>Given a data set, when we add an new explanatory variable into a regression model, $RSS$ is non-increasing.</p>
<p>Since we are comparing two nested minimization problems</p>
<p>$$\begin{aligned}
&amp;\text{Problem 1 / Full model / with } X_{p}  \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} <em>{(p+1)\times 1} \right\Vert ^2   = \min \ RSS_1 \
&amp;\text{Problem 2 / Reduced model / without } X</em>{p} \ &amp;\min &amp;\ \left\Vert  \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} <em>{(p+1)\times 1} \right\Vert ^2  = \min \ RSS_2 \
&amp;&amp;\text{s.t.}  &amp;\ \beta</em>{p} = 0
\end{aligned}$$</p>
<p>Due to the constraint in Problem 2, the minimum value of the Problem 1 should be no larger than the minimum value of the Problem 2, i.e. $RSS_1^* \le RSS_2^*$ , When will they be equal?</p>
<ul>
<li><p>From projection’s perspective, they are equal iff the additional orthogonal basis vector of the design matrix $\boldsymbol{X}$ introduced by the new column $X_p$ is orthogonal to the response vector $\boldsymbol{y}$. See the derivation of <a class="reference internal" href="#lm-f-test"><span class="std std-ref">$F$-test</span></a> for details. Note that this is different from $\boldsymbol{x} _p ^\top \boldsymbol{y} =0$. The example below shows reduction in RSS even if $\boldsymbol{x} _p ^\top \boldsymbol{y} =0$.</p></li>
<li><p>From optimization’s perspective, they are equal iff $\hat{\beta}<em>{p}=0$ in Problem 1’s solution. When will $\hat{\beta}</em>{p}=0$? No clear condition.</p>
<ul>
<li><p>If $\boldsymbol{x}<em>i$’s are orthogonal such that $\boldsymbol{X} ^\top \boldsymbol{X} = I</em>{p}$, then</p>
<p>$$
\boldsymbol{x}<em>{p} ^\top \boldsymbol{y} = 0 \Leftrightarrow \hat{\beta}</em>{p}=0
$$</p>
</li>
<li><p>Note that in general, $\not\Leftarrow$. An simple example can be a data set of two points $(1,0), (1,1)$. The fitted line is $y=0.5$.</p></li>
<li><p>Also, in general, $\not\Rightarrow$. The example below shows $\hat{\beta}_{2} \ne 0$ even if $\boldsymbol{x} ^\top _p \boldsymbol{y} =0$</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>

 <span class="n">reduced</span> <span class="n">model</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>

 <span class="n">full</span> <span class="n">model</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">XXinv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="r-2-and-adjusted-r-2">
<h2>$R^2$ and Adjusted $R^2$<a class="headerlink" href="#r-2-and-adjusted-r-2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="r-squared">
<span id="lm-rsquared"></span><h3>$R$-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>Assuming the <a class="reference internal" href="#lm-tss-identity"><span class="std std-ref">decomposition identity</span></a> of $TSS$ holds, we can define $R$-squared.</p>
<dl class="simple myst">
<dt>Definition ($R$-squared)</dt><dd><p>$R$-squared is a statistical measure that represents the <strong>proportion of the variance</strong> for a dependent variable that’s <strong>explained</strong> by an independent variable or variables in a regression model.</p>
</dd>
</dl>
<p>$$
R^2 = \frac{SSR}{SST}  = 1 - \frac{SSE}{SST} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$</p>
<p><strong>Properties</strong></p>
<ol>
<li><p>$R$-squared can never decrease when an additional explanatory variable is added to the model.</p>
<p>As long as $Cov(Y, X_j) \ne 0$, then $X_j$ has some explanatory power to $Y$, and thus $RSS$ decreases, See the <a class="reference internal" href="#lm-rss-nonincreasing"><span class="std std-ref">section</span></a> of $RSS$ for details. As a result, $R$-squared  is not a good measure for model selection, which can cause overfitting.</p>
</li>
<li><p>$R$-squared equals the squared correlation coefficient between the actual value of the response and the fitted value $\operatorname{Corr}\left( Y, \hat{Y} \right)^2$.</p>
<p>In particular, in simple linear regression, $R^2 = \rho_{X,Y}^2$.</p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em>
By the definition of correlation,</p>
<p>$$\begin{align}
\operatorname{Corr}\left( y, \hat{y} \right)^2
&amp;= \frac{\operatorname{Cov}\left( y, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \
&amp;= \frac{\operatorname{Cov}\left( \hat{y} + \hat{\varepsilon}, \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \
&amp;= \frac{\operatorname{Cov}\left( \hat{y} , \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \
&amp;= \frac{\operatorname{Var}\left( \hat{y} \right)^2}{\operatorname{Var}\left( y \right)\operatorname{Var}\left( \hat{y} \right)} \
&amp;= \frac{\operatorname{Var}\left( \hat{y} \right)}{\operatorname{Var}\left( y \right)} \
&amp;= \frac{SSR}{SST} \
&amp;= R^2 \
\end{align}$$</p>
<p>The third equality holds since</p>
<p>$$
\operatorname{Cov}\left( \hat{\varepsilon}, \hat{y} \right) = \operatorname{Cov}\left( \hat{\varepsilon}, \sum_j x_j \hat{\beta}_j  \right) = \sum_j \hat{\beta}_j \operatorname{Cov}\left( \hat{\varepsilon},  x_j \right) = 0
$$</p>
<p>When $p=2$, since $\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x$, we have</p>
<p>$$
R^2 = \operatorname{Corr}\left(y, \hat{y} \right)^2 = \operatorname{Corr}\left(y, x \right)^2
$$
:::</p>
</li>
</ol>
<p>:::{admonition,dropdown,note} What is $R^2$ if there is no intercept?</p>
<p>When there is <strong>no</strong> intercept, then $\bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)$ and hence $\hat{\boldsymbol{y} } - \bar{y} \boldsymbol{1} _n \notin \operatorname{col}(X)$. The decomposition identity may not hold. Thus, the value of $R$-squared may no longer be in $[0,1]$, and its interpretation is no longer valid. What actually happen to the value $R$-squared depends on whether we define it using $TSS$ with $RSS$ or $ESS$.</p>
<p>If we define $R^2 = \frac{ESS}{TSS}$, then when</p>
<p>$$
\sqrt{ESS} = \left\Vert \hat{\boldsymbol{y} } - \bar{y}\boldsymbol{1} _n \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
$$</p>
<p>we will have $ESS &gt; TSS$, i.e., $R^2 &gt; 1$.</p>
<p>On the other hand, if we define $R^2 = 1 - \frac{RSS}{TSS}$, then when</p>
<p>$$
\sqrt{RSS} = \left\Vert \hat{\boldsymbol{y} } -  \boldsymbol{y} \right\Vert&gt; \left\Vert \boldsymbol{y} - \bar{y}\boldsymbol{1} _n \right\Vert = \sqrt{TSS}
$$</p>
<p>we will have $RSS &gt; TSS$, i.e. $R^2 &lt; 0$.</p>
<p>:::</p>
</div>
<div class="section" id="adjusted-r-squared">
<h3>Adjusted $R$-squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline">¶</a></h3>
<p>Due to the non-decrease property of $R$-squared, we define adjusted $R$-squared which is a better measure of goodness of fitting.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>Adjusted $R$-squared, denoted by $\bar{R}^2$, is defined as</p>
</dd>
</dl>
<p>$$
\bar{R}^2 = 1-\frac{RSS / (n-p)}{ TSS / (n-1)}
$$</p>
<p>Properties</p>
<ul class="simple">
<li><p>$\bar{R}^2$ can increase or decrease. When a new variable is included, $RSS$ decreases, but $(n-p)$ also decreases.</p>
<ul>
<li><p>Relation to $R$-squared is</p></li>
</ul>
</li>
</ul>
<p>$$
\bar{R}^2 = 1-\frac{n-1}{ n-p}(1 - R^2) &lt; R^2
$$</p>
<ul>
<li><p>Relation to estimated variance of random error and variance of response</p>
<p>$$
\bar{R}^2 = 1-\frac{\hat{\sigma}^2}{\operatorname{Var}\left( y \right)}
$$</p>
</li>
<li><p>Can be negative when</p>
<p>$$
R^2 &lt; \frac{p-1}{n-p}
$$</p>
<p>If $p &gt; \frac{n+1}{2}$ then the above inequality always hold, and adjusted $R$-squared is always negative.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="t-test">
<span id="lm-t-test"></span><h2>$t$-test<a class="headerlink" href="#t-test" title="Permalink to this headline">¶</a></h2>
<p>We can use $t$-test to conduct a hypothesis testing on $\boldsymbol{\beta}$, which has a general form</p>
<p>$$\begin{aligned}
H_0
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}<em>{\text{null}} = c \
H_1
&amp;: \boldsymbol{v} ^\top \boldsymbol{\beta}</em>{\text{null}} \ne c (\text{two-sided} )\
\end{aligned}$$</p>
<p>Usually $c=0$.</p>
<ul class="simple">
<li><p>If $c=0, \boldsymbol{v} = \boldsymbol{e} _j$ then this is equivalent to test $\beta_j=0$, i.e. the variable $X_i$ has no effect on $Y$ given all other variabels.</p></li>
<li><p>If $c=0, v_i=1, v_j=-1$ and $v_k=0, k\ne i, j$ then this is equivalent to test $\beta_i = \beta_j$, i.e. the two variables $X_i$ and $X_j$ has the same effect on $Y$ given all other variables.</p></li>
</ul>
<p>The test statistic is</p>
<p>$$
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}<em>{\text{null}}}{\hat{\sigma}\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim t</em>{n-p}
$$</p>
<p>In particular, when $p=2$, to test $\beta_1 = c$, we use</p>
<p>$$
\frac{\hat{\beta}<em>1 - c}{\hat{\sigma}/ \sqrt{\operatorname{Var}\left( X_1 \right)}}  \sim t</em>{n-2}
$$</p>
<p>Following this analysis, we can find the $(1-\alpha)%$ confidence interval for a scalar $\boldsymbol{v} ^\top \boldsymbol{\beta}$ as</p>
<p>$$
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{v} }
$$</p>
<p>In particular,</p>
<ul class="simple">
<li><p>If $\boldsymbol{v} = \boldsymbol{e}_j$, then this is the confidence interval for a coefficient $beta _j$.</p></li>
<li><p>If $\boldsymbol{v} = \boldsymbol{x}_i$ where $\boldsymbol{x}_i$ is in the data set, then this is the confidence interval for in-sample fitting of $y_i$. We are making in-sample fitting at the mean value $\operatorname{E}\left( \boldsymbol{y} _i \right) = \boldsymbol{x}_i ^\top \boldsymbol{\beta}$. If $\boldsymbol{x}_i$ is not in the design matrix, then we are doing out-of-sample prediction, and a <a class="reference internal" href="#lm-prediction-interval"><span class="std std-ref">prediction interval</span></a> should be used instead.</p></li>
</ul>
<p>:::{admonition,dropdown,seealso} <em>Derivation</em></p>
<p>First, we need to find the distribution of $\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}$. Recall that</p>
<p>$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}_{\text{null}} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1})
$$</p>
<p>Hence,</p>
<p>$$
\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}  \sim N(\boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}} , \sigma^2 \boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} )
$$</p>
<p>or</p>
<p>$$
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim N(0, 1)
$$</p>
<p>Also recall that the RSS has the distribution</p>
<p>$$
(n-p)\frac{\hat{\sigma}^2}{\sigma^2 } \sim \chi ^2 _{n-p}<br />
$$</p>
<p>and the two quantities $\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}}$ and $(n-p)\frac{\hat{\sigma}^2}{\sigma^2 }$ are <a class="reference internal" href="#lm-independent-beta-sigma"><span class="std std-ref">independent</span></a>. Therefore, with a standard normal and a Chi-squared that are independent, we can construct a $t$-test statistic</p>
<p>$$
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}<em>{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} / \sqrt{\frac{(n-p)\hat{\sigma}^2 }{\sigma^2 } / (n-p)} \sim t</em>{n-p}
$$</p>
<p>i.e.,</p>
<p>$$
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}<em>{\text{null}}}{\hat{\sigma}\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim t</em>{n-p}
$$</p>
<p>Note that, compared with</p>
<p>$$
\frac{\boldsymbol{v} ^\top \hat{\boldsymbol{\beta}} - \boldsymbol{v} ^\top \boldsymbol{\beta}_{\text{null}}}{\sigma\sqrt{\boldsymbol{v} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{v} }} \sim N(0, 1)
$$</p>
<p>the RHS changes from $N(0,1)$ to $t_{n-p}$ because we are estiamteing $\sigma$ by $\hat{\sigma}$. Similar to the case of univariate mean test.</p>
<p>:::</p>
<p>:::{admonition,dropdown,note} Permutation test</p>
<p>Another way to conduct a test without normality assumption is to use permutation test. For instance, to test $\beta_2=0$, we fix $y$ and $x_1$, and sample the same $n$ values of $x_2$ from the column of $X_2$, and compute the $t$ statistic. Repeat the permutation for multiple times and compute the percentage that</p>
<p>$$
\left\vert t_{\text{perm} } \right\vert &gt;  \left\vert t_{\text{original} }  \right\vert
$$</p>
<p>which is the $p$-value.</p>
<p>:::</p>
<p>:::{admonition,dropdown,note} Social science’s trick to test $\beta_1 = \beta_2$</p>
<p>Some social science courses introduce a trick to test $\beta_1 = \beta_2$ by rearranging the explanatory variables. For instance, if our model is,</p>
<p>$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i
$$</p>
<p>Then they define $\gamma = \beta_1 - \beta_2$ and $x_{i3} = x_{i1} + x_{i2}$, rearrange RHS,</p>
<p>$$\begin{aligned}
Y_i
&amp;= \beta_0 + (\gamma + \beta_2) x_{i1} + \beta_2 x_{i2} + \varepsilon_i \
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 (x_{i1} + x_{i2}) + \varepsilon_i \
&amp;= \beta_0 + \gamma x_{i1} + \beta_2 x_{i3} + \varepsilon_i
\end{aligned}$$</p>
<p>Finally, they run the regression of the last line and check the $p$-value of $\gamma$. Other parts of the model ($R$-squared, $p$-value of $\beta_0$, etc) remain the same.</p>
<p>:::</p>
</div>
<div class="section" id="f-test">
<span id="lm-f-test"></span><h2>$F$-test<a class="headerlink" href="#f-test" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compare-two-nested-models">
<h3>Compare Two Nested Models<a class="headerlink" href="#compare-two-nested-models" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">Nested</p>
<p>It is called nested since the reduced model is a special case of the full model with</p>
<p>$$
\beta_{p-k}=\ldots= \beta_{p-1} =0
$$</p>
</div>
<p>$F$-test can be used to compare two nested models</p>
<p>$$\begin{aligned}
\text{Full model: } Y &amp;\sim \left{ X_j, j=1, \ldots, p-1 \right} \
\text{Reduced model: } Y &amp;\sim \left{ X_j, j=1, \ldots, p-k-1 \right}
\end{aligned}$$</p>
<div class="margin sidebar">
<p class="sidebar-title">Interpretation of $F$-test</p>
<p>Given it’s form, we can interpret the numerator as an average reduction in $RSS$ by adding the $k$ explanatory variables. Since the denominator is fixed, if average reduction is large enough, then we reject the null hypothesis that their coefficients are 0.</p>
</div>
<p>The test statistic is</p>
<p>$$
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full}}/(n-p)} \sim F_{k, n-p}
$$</p>
<p>which can also be computed by $R^2$ since $TSS$ are the same for the two models</p>
<p>$$
F = \frac{(R^2 _{\text{full}} - R^2 _{\text{reduced}})/k}{(1 - R^2 _{\text{full}})/(n-p)}
$$</p>
<p>In particular,</p>
<ul>
<li><p>When $k=p-1$, we are comparing a full model vs. intercept only, i.e.,</p>
<p>$$
\beta_1 = \ldots = \beta_p-1 = 0
$$</p>
<p>In this case,</p>
<p>$$
RSS_{\text{reduced}} = \left| \boldsymbol{y} - \bar{y} \boldsymbol{1} _n \right|  ^2 = TSS
$$</p>
<p>and</p>
<p>$$
F = \frac{(TSS - RSS_{\text{full}})/(p-1)}{RSS_{\text{full}}/(n-p)}  = \frac{R^2 _{\text{full}}/k}{(1 - R^2 _{\text{full}})/(n-p)}
$$</p>
</li>
<li><p>When $k=1$, we are testing $\beta_{p-1} = 0$. In this case, the $F$-test is equivalent to the $t$-test. The two test statistics have the relation $F_{1, n-p}=t^2_{n-p}$. Note that the alternative hypothesis in both tests are two-sided, and $t$-test is two-sided while $F$-test is one-sided.</p></li>
</ul>
<p>:::{admonition,dropdown,seealso} <em>Derivation</em></p>
<p>We need to find the distribution of $RSS_{\text{reduced} }$ and $RSS_{\text{full}}$ and then construct a pivot quantity.</p>
<p>Let $\boldsymbol{U}$ be an orthogonal basis of $\mathbb{R} ^n$ with three orthogonal parts</p>
<p>$$
\boldsymbol{U}  = [\underbrace{\boldsymbol{u} _1, \ldots, \boldsymbol{u} _{p-k}} _{\boldsymbol{U} _1}, \underbrace{\boldsymbol{u} _{p-k+1}, \ldots, \boldsymbol{u} _{p}} _{\boldsymbol{U} _2}, \underbrace{\boldsymbol{u} _{p+1}, \ldots, \boldsymbol{u} _{n}} _{\boldsymbol{U} _3}]
$$</p>
<p>Then</p>
<p>$$
\boldsymbol{U} ^\top \boldsymbol{y} = \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{y}  \
\boldsymbol{U} _2 ^\top  \boldsymbol{y}  \
\boldsymbol{U} _3 ^\top  \boldsymbol{y}  \
\end{array}\right]
\sim N_n \left( \left[\begin{array}{l}
\boldsymbol{U} _1 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \
\boldsymbol{U} _2 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \
\boldsymbol{U} _3 ^\top  \boldsymbol{X} \boldsymbol{\beta}   \
\end{array}\right] , \sigma^2 \boldsymbol{I} _n \right)
$$</p>
<p>Thus, we have pairwise independences among $\boldsymbol{U}_1 ^\top \boldsymbol{y} , \boldsymbol{U} _2 ^\top \boldsymbol{y}$ and $\boldsymbol{U} _3 ^\top \boldsymbol{y}$.</p>
<p>Moreover, by the property of multivariate normal, we have</p>
<p>$$\begin{aligned}
\left| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right|  ^2
&amp;\sim \sigma^2  \chi ^2 _k \
\left| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}   \
\end{aligned}$$</p>
<p>The RSSs have the relations</p>
<p>$$\begin{aligned}
RSS_{\text{full} }
&amp;= \left| \boldsymbol{P}_{\operatorname{im}(\boldsymbol{U} _1 \boldsymbol{U} <em>2) ^\bot } \boldsymbol{y}  \right| ^2 \
&amp;= \left| \boldsymbol{P}</em>{\operatorname{im}(\boldsymbol{U} <em>3} \boldsymbol{y}  \right| ^2 \
&amp;= \left| \boldsymbol{U} ^\top <em>3 \boldsymbol{y}  \right| ^2 \
RSS</em>{\text{reduced} }
&amp;= \left| \boldsymbol{P}</em>{\operatorname{im}(\boldsymbol{U} <em>1) ^\bot } \boldsymbol{y}  \right| ^2 \
&amp;= \left| \boldsymbol{P}</em>{\operatorname{im}([\boldsymbol{U} _2 \boldsymbol{U} _3])} \boldsymbol{y}  \right| ^2 \
&amp;= \left| \left[ \boldsymbol{U}_2, \boldsymbol{U}_3 \right] ^\top \boldsymbol{y}  \right| ^2   \
&amp;= \left| \boldsymbol{U} ^\top _2  \boldsymbol{y}   \right| ^2 +  \left| \boldsymbol{U} ^\top _3 \boldsymbol{y}   \right| ^2
\end{aligned}$$</p>
<p>Hence</p>
<p>$$\begin{aligned}
RSS_{\text{reduced} } - RSS_{\text{full} } = \left| \boldsymbol{U} _2 ^\top \boldsymbol{y}  \right|  ^2
&amp;\sim \sigma^2  \chi ^2 <em>k \
RSS</em>{\text{full} } =  \left| \boldsymbol{U} _3 ^\top \boldsymbol{y}  \right|  ^2
&amp;\sim \sigma^2  \chi ^2 _{n-p}  \
\end{aligned}$$</p>
<p>Therefore, we have the pivot quantity</p>
<p>$$
\frac{(RSS_{\text{reduced} } - RSS_{\text{full} })/k}{RSS_{\text{full} }/(n-p)}  \sim F_{k, n-p}
$$</p>
<p>:::</p>
<p>:::{admonition,warning} Warning</p>
<p>A $F$-test on $\beta_1=\beta_2=0$ is difference from two univariate $t$-tests $\beta_1=0, \beta_2=0$. A group of $t$-tests may be misleading if the regressors are highly correlated.</p>
<p>:::</p>
</div>
<div class="section" id="compare-two-groups-of-data">
<h3>Compare Two Groups of Data<a class="headerlink" href="#compare-two-groups-of-data" title="Permalink to this headline">¶</a></h3>
<p>If the data set contains data from two groups, then the regression line can be different. For instance,</p>
<ul class="simple">
<li><p>In time series analysis, there may be a structural break at a period which can be assumed to be known a priori (for instance, a major historical event such as a war).</p></li>
<li><p>In program evaluation, the explanatory variables may have different impacts on different subgroups of the population.</p></li>
</ul>
<p>:::{figure} lm-chow-test
<img src="../imgs/lm-chow-test.png" width = "80%" alt=""/></p>
<p>Illustration of two groups of data <a class="reference external" href="https://en.wikipedia.org/wiki/Chow_test">[Wikipedia]</a>
:::</p>
<p>Testing whether a regression function is different for one group $(i=1,\ldots,m)$ versus another $(i=m+1,\ldots,n)$, we can use $F$-test. The full model is</p>
<p>$$\begin{aligned}
y_{i}
&amp;= \left( \beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\ldots+\beta_{k} x_{k i} \right) \
&amp;\ + \left( \gamma_{0}+\gamma_{1} (d_i x_{1 i})+\gamma_{2} (d_i x_{2 i})+\ldots+\gamma_{k} (d_i x_{k i}) \right) \
&amp;\ +\varepsilon_{i}
\end{aligned}$$</p>
<p>where dummy variable $d_i = 1$ if $i=m+1, \ldots, n$ and $0$ otherwise.</p>
<p>The null and alternative hypotheses are</p>
<p>$$\begin{aligned}
H_{0}:&amp;\ \gamma_{0}=0, \gamma_{1}=0, \ldots, \gamma_{k}=0 \
H_{1}:&amp;\ \text{otherwise}
\end{aligned}$$</p>
<p>The $F$-test statistic is</p>
<p>$$
F=\frac{\left(RSS_{\text{reduced}}-RSS_{\text{full}}\right) /(k+1)}{RSS_{\text{full}} / (n-2(k+1))}
$$</p>
<div class="margin sidebar">
<p class="sidebar-title">Question</p>
<p>Can you show why the fact holds?</p>
</div>
<p>Equivalently, there is a method called <strong>Chow test</strong>. It uses the fact that</p>
<p>$$
RSS_{\text{full}} = RSS_{1} + RSS_{2}<br />
$$</p>
<p>where $RSS_1$ and $RSS_2$ are the RSS obtained when we run the following regression model on two groups of data respectively,</p>
<p>$$
y_{i} = \beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\ldots+\beta_{k} x_{k i} +\varepsilon_{i}
$$</p>
<p>Hence, there is no need to create dummy variables $d_i$. There are only three steps:</p>
<ol class="simple">
<li><p>Run the regression with group 1’s data to obtain $RSS_1$</p></li>
<li><p>Run the regression with group 2’s data to obtain $RSS_2$</p></li>
<li><p>Run the regression with all data to obtain $RSS_{\text{reduced}}$</p></li>
</ol>
<p>And then use the $F$-test to test the hypothesis.</p>
</div>
<div class="section" id="confidence-region-for-boldsymbol-beta">
<h3>Confidence Region for $\boldsymbol{\beta}$<a class="headerlink" href="#confidence-region-for-boldsymbol-beta" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>To test $\boldsymbol{\beta}=\boldsymbol{0}$, see <a class="reference internal" href="#lm-f-test"><span class="std std-ref">$F$-test</span></a></p>
</div>
<p>If we want to draw conclusions to multiple coefficients $\beta_1, \beta_2, \ldots$ simultaneously, we need a confidence region, and consider the multiple testing issue.</p>
<p>To find a $(1-\alpha)%$ confidence region for $\boldsymbol{\beta}$, one attemp is to use a cuboid, whose $j$-th side length equals to the $(1-\alpha/p)-%$ confidence interval for $\beta_j$. Namely, the confidence region is</p>
<p>$$
\left[ (1-\alpha/p) \text{ C.I. for } \beta_0 \right] \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_1 \right] \times \ldots \times \left[ (1-\alpha/p) \text{ C.I. for } \beta_{p-1} \right]
$$</p>
<p>In this way, we ensure the overall confidence of the confidence region is at least $(1-\alpha)%$.</p>
<p>$$
\operatorname{P}\left( \text{every $\beta$ is in its C.I.}  \right) \ge 1-\alpha
$$</p>
<p>A more natural approach is using an ellipsoid. Recall that</p>
<p>$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta} , \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} )
$$</p>
<p>Hence a pivot quantity for $\boldsymbol{\beta}$ can be constructed as follows,</p>
<p>$$\begin{aligned}
&amp;&amp;\frac{(\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{\sigma^2 }
&amp;\sim N(\boldsymbol{0} , \boldsymbol{I} <em>p)  \
&amp;\Rightarrow&amp; \ \frac{1}{\sigma^2} \left| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right|^2    &amp;\sim \chi ^2 <em>p \
&amp;\Rightarrow&amp; \ \frac{\frac{1}{\sigma^2} \left| (\boldsymbol{X} ^\top \boldsymbol{X} )^{1/2}(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} ) \right|^2/p}{\frac{(n-p)\hat{\sigma}^2}{\sigma^2 }/(n-p)}   &amp;\sim F</em>{p, n-p}\
&amp;\Rightarrow&amp; \ \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{\beta} )}{p \hat{\sigma}^2}   &amp;\sim F</em>{p, n-p}\
\end{aligned}$$</p>
<p>Therefore, we can obtain an $(1-\alpha)%$ confidence region for $\boldsymbol{\beta}$ from this distribution</p>
<p>$$
\boldsymbol{\beta} \in \left{ \boldsymbol{v} \in \mathbb{R} ^p: \frac{ (\hat{\boldsymbol{\beta}} -\boldsymbol{v} )^\top (\boldsymbol{X} ^\top \boldsymbol{X} )(\hat{\boldsymbol{\beta}} -\boldsymbol{v} )}{p \hat{\sigma}^2}   \le F_{p, n-p}^{(1-\alpha)} \right}
$$</p>
<p>which is an ellipsoid centered at $\hat{\boldsymbol{\beta}}$, scaled by $\frac{1}{p \hat{\sigma}}$, rotated by $(\boldsymbol{X} ^\top \boldsymbol{X})$.</p>
<p>In general, for matrix $\boldsymbol{A} \in \mathbb{R} ^{p \times k}, \operatorname{rank}\left( \boldsymbol{A}  \right) = k$, the confidence region for $\boldsymbol{A} ^\top \boldsymbol{\beta}$ can be found in a similar way</p>
<p>$$\begin{aligned}
\boldsymbol{A} ^\top \hat{\boldsymbol{\beta}}
&amp;\sim N(\boldsymbol{A} ^\top \boldsymbol{\beta} , \sigma^2 \boldsymbol{A} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}  \boldsymbol{A} ) \
\Rightarrow \quad\ldots &amp;\sim F_{k, n-p} \
\end{aligned}$$</p>
</div>
</div>
<div class="section" id="prediction-interval-for-y-new">
<span id="lm-prediction-interval"></span><h2>Prediction Interval for $y_{new}$<a class="headerlink" href="#prediction-interval-for-y-new" title="Permalink to this headline">¶</a></h2>
<p>For a new $\boldsymbol{x}$, the new response is</p>
<p>$$
y _{new} = \boldsymbol{x} ^\top \boldsymbol{\beta} + \boldsymbol{\varepsilon} _{new}
$$</p>
<p>where $\boldsymbol{\varepsilon} _{new} \perp!!!\perp \hat{\boldsymbol{\beta}} , \hat{\sigma}$ since the RHS are from training set.</p>
<p>The prediction is</p>
<p>$$
\hat{y} _{new} = \boldsymbol{x} ^\top \hat{\boldsymbol{\beta}}
$$</p>
<p>Thus, the prediction error is</p>
<p>$$\begin{aligned}
y <em>{new} - \hat{y}</em>{new}
&amp;= \boldsymbol{\varepsilon} _{new} + \boldsymbol{x} ^\top (\boldsymbol{\beta} - \hat{\boldsymbol{\beta}} )\
&amp;\sim N \left( \boldsymbol{0} , \sigma^2 (1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} ) \right) \
\end{aligned}$$</p>
<p>Hence, the $(1-\alpha)%$ confidence prediction interval for a new response value $\boldsymbol{y} _{new}$ at an out-of-sample $\boldsymbol{x}$ is</p>
<p>$$
\boldsymbol{x} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{1 + \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} }
$$</p>
<p>There is an additional “$1$” in the squared root, compared to the confidence interval for an in-sample fitting,</p>
<p>$$
\boldsymbol{x} ^\top \hat{\boldsymbol{\beta}} \pm t_{n-p}^{(1-\alpha/2)}\cdot \hat{\sigma} \sqrt{ \boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x} }
$$</p>
<p>::::{admonition,dropdown,note} Width of an interval</p>
<p>When we are building confidence interval for $\boldsymbol{y} _i$ or prediction interval for $\boldsymbol{y} _{new}$, the width depends on the magnitude of $n$ the choice of $\boldsymbol{x}$.</p>
<p>As $n \rightarrow \infty$, we have $\boldsymbol{a} ^\top
(\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{a}\rightarrow 0$ for all $\boldsymbol{a}$, hence the</p>
<ul class="simple">
<li><p>CI for $\boldsymbol{y} _i$: $\operatorname{se}  \rightarrow 0, \operatorname{width} \rightarrow 0$</p></li>
<li><p>PI for $\boldsymbol{y} <em>{new}$: $\operatorname{se}  \rightarrow \hat{\sigma}, \operatorname{width} \rightarrow 2 \times t</em>{n-p}^{(1-\alpha/2)} \hat{\sigma}$</p></li>
</ul>
<p>The width also depends on the choice of $\boldsymbol{x}$.</p>
<ul class="simple">
<li><p>If $\boldsymbol{x}$ is aligned with a large eigenvector of $\boldsymbol{X} ^\top \boldsymbol{X}$, then $\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}$ is small. This is because larger eigenvectors indicate a direction of large variation in the data set, and hence it has more distinguishability.</p></li>
<li><p>If $\boldsymbol{x}$ is aligned with a small eigenvector of $\boldsymbol{X} ^\top \boldsymbol{X}$, then $\boldsymbol{x} ^\top (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{x}$ is large. This is because smaller eigenvectors indicate a direction of small variation in the data set, and hence it has less distinguishability and more uncertainty.</p></li>
</ul>
<p>:::{figure}
<img src="../imgs/pca-pc-ellipsoids.png" width = "70%" alt=""/></p>
<p>Illustration of eigenvectors in bivariate Gaussian [Fung 2018]
:::</p>
<p>::::</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="11-lm-estimation.html" title="previous page">Linear Models - Estimation</a>
    <a class='right-next' id="next-link" href="13-lm-extension.html" title="next page">Linear Regression - Extension</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>