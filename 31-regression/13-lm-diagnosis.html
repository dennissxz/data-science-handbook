
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Models - Diagnosis &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Models - Advanced Topics" href="14-lm-advanced.html" />
    <link rel="prev" title="Linear Models - Inference" href="12-lm-inference.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/test.html">
     Test
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-regression.html">
   Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/31-regression/13-lm-diagnosis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/31-regression/13-lm-diagnosis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F31-regression/13-lm-diagnosis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/31-regression/13-lm-diagnosis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-observations">
   Special Observations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outliers">
     Outliers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#leverage-points">
     Leverage Points
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influential-points">
     Influential Points
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#omitting-a-variable">
   Omitting a Variable
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-a-variable">
   Adding a Variable
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance-usually-increases">
     Variance usually Increases
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standard-error-uncertain">
     Standard Error Uncertain
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#over-controlling">
     Over Controlling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multicollinearity">
   Multicollinearity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagnosis">
     Diagnosis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consequences">
     Consequences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implications">
     Implications
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#heteroskedasticity">
   Heteroskedasticity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Diagnosis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plot">
       Plot
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#breusch-pagan-test">
       Breusch-Pagan Test
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#white-test">
       White Test
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#correction-by-robust-error">
     Correction by Robust Error
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alt-model-weighted-least-squares">
     Alt Model: Weighted Least Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation">
     Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inference">
       Inference
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#find-weights">
       Find Weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measurement-error">
   Measurement Error
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#of-y">
     Of
     <span class="math notranslate nohighlight">
      \(Y\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#of-x">
     Of
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution">
     Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#missing-values">
   Missing Values
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#completely-at-random">
     Completely At Random
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#depends-on-y-endogenous">
     Depends on
     <span class="math notranslate nohighlight">
      \(Y\)
     </span>
     (Endogenous)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#depends-on-x-exogenous">
     Depends on
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
     (Exogenous)
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-models-diagnosis">
<h1>Linear Models - Diagnosis<a class="headerlink" href="#linear-models-diagnosis" title="Permalink to this headline">¶</a></h1>
<p>No models are perfect. In this section we introduce what happen when our model is misspecified or when some assumptions fail. We will introduce how to diagnose these problems, and corresponding remedies and alternative models, e.g. Lasso, ridge regression, etc.</p>
<div class="section" id="special-observations">
<h2>Special Observations<a class="headerlink" href="#special-observations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="outliers">
<h3>Outliers<a class="headerlink" href="#outliers" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Outliers)</dt><dd><p>There are many ways to define outliers. Observations <span class="math notranslate nohighlight">\(i\)</span> is an outlier</p>
<ul class="simple">
<li><p>univariate measure: if <span class="math notranslate nohighlight">\(\left\vert x_{ji} - \mu_j \right\vert\)</span> is larger than some threshold, say some multiples of standard deviations of <span class="math notranslate nohighlight">\(X_j\)</span>, then the <span class="math notranslate nohighlight">\(j\)</span>-th value of observation <span class="math notranslate nohighlight">\(i\)</span> is an outlier w.r.t. other observations of variable <span class="math notranslate nohighlight">\(X_j\)</span>.</p></li>
<li><p>multivariate measure: if <span class="math notranslate nohighlight">\(\left\| \boldsymbol{x}_i -\boldsymbol{\mu}  \right\|^2\)</span> is larger than some threshold, then <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> an outlier w.r.t. other data points in <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p></li>
<li><p>if its studentized residual <span class="math notranslate nohighlight">\(\left\vert t_i \right\vert\)</span> is larger than some threshold, say <span class="math notranslate nohighlight">\(t_{n-p}^{(\alpha/2n)}\)</span></p></li>
</ul>
</dd>
<dt>Solution</dt><dd><ul class="simple">
<li><p>If outlier is a mistake (typo) you can drop it (or correct it)</p></li>
<li><p>If outlier is valid but unusual, look for robustness – does dropping it change answer?</p></li>
<li><p>If it does change answer, report both versions – and argue for the approach you think more appropriate</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="leverage-points">
<h3>Leverage Points<a class="headerlink" href="#leverage-points" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Unlike outliers which can be a univariate measure, leverage looks at the whole data vector <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> w.r.t. all data <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
</div>
<dl class="simple myst">
<dt>Definition (Leverages and leverage points)</dt><dd><ul class="simple">
<li><p>Leverage (score) of an observation <span class="math notranslate nohighlight">\(i\)</span> is defined as <span class="math notranslate nohighlight">\(h_i = \boldsymbol{x}_i ^\top (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1} \boldsymbol{x}_i\)</span>. It is the <span class="math notranslate nohighlight">\(i\)</span>-th diagonal entry of the projection matrix <span class="math notranslate nohighlight">\(\boldsymbol{H} = \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top = \boldsymbol{P}_{\operatorname{im} (\boldsymbol{X} )}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(h_i\)</span> is large than some threshold, say <span class="math notranslate nohighlight">\(2p/n\)</span>, then we say <span class="math notranslate nohighlight">\(i\)</span> is a <strong>leverage point</strong>.</p></li>
</ul>
</dd>
</dl>
<div class="figure align-default" id="lr-leverage">
<a class="reference internal image-reference" href="../_images/lr-leverage.png"><img alt="" src="../_images/lr-leverage.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 56 </span><span class="caption-text">Leverage scores [Zhang+ 2018]</span><a class="headerlink" href="#lr-leverage" title="Permalink to this image">¶</a></p>
</div>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p>In particular, we have <span class="math notranslate nohighlight">\(0&lt;h_i&lt;1\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n h_i = p\)</span>, or <span class="math notranslate nohighlight">\(\bar{h} = p/n\)</span></p></li>
<li><p>Recall that <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\varepsilon} } \right) = \operatorname{Var}\left( \boldsymbol{y} - \hat{\boldsymbol{y}} \right)= \sigma^2 (\boldsymbol{I} - \boldsymbol{H} )\)</span>, so <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\varepsilon}_i \right) = \sigma^2 (1-h_i)\)</span></p></li>
</ul>
</dd>
<dt>Definition (Standardized residuals)</dt><dd><p>Standardized residuals is defined as</p>
<div class="math notranslate nohighlight">
\[
  r_i = \ \frac{\hat{\varepsilon}_i}{\sigma\sqrt{1- h_i}}
  \]</div>
<p>It is standardized since <span class="math notranslate nohighlight">\(\operatorname{Var}\left( r_i \right) = 1\)</span>. But in practice, we don’t know <span class="math notranslate nohighlight">\(\sigma\)</span>. So we plug in its estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p>
</dd>
</dl>
</div>
<div class="section" id="influential-points">
<h3>Influential Points<a class="headerlink" href="#influential-points" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Influential points)</dt><dd><p>Influential points are data points such that if we remove it, the model changes substantially. It can be quantified by Cook’s distance</p>
<div class="math notranslate nohighlight">
\[
  D_i = \frac{r_i ^2}{p} \frac{h_i}{1-h_i}  
  \]</div>
<p>where <span class="math notranslate nohighlight">\(r_i\)</span> is standardized residual.</p>
</dd>
</dl>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<ul class="simple">
<li><p>A influential point can be close to the fitted line. If we drop it, then there seems no linear relation in the remaining data cloud.</p></li>
<li><p>Two or more influential points near each other can mask each other’s influence in a leave-one-out regression. That is, if remove any one of them, then the regression results do not change substantially, but if we remove both of them, then the results change substantially.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="omitting-a-variable">
<span id="lm-omit-variable"></span><h2>Omitting a Variable<a class="headerlink" href="#omitting-a-variable" title="Permalink to this headline">¶</a></h2>
<p>Suppose the true model is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}_{n \times p} \boldsymbol{\beta} + \boldsymbol{\varepsilon}  
\]</div>
<p>And we omit one explanatory variable <span class="math notranslate nohighlight">\(X_j\)</span>. Thus, our new design matrix has size <span class="math notranslate nohighlight">\(n \times (p-1)\)</span>, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{X}_{-j}\)</span>. Without loss of generality, let it be in the last column of the original design matrix, i.e. <span class="math notranslate nohighlight">\(\boldsymbol{X} = \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\)</span>. The new estimated coefficients vector is denoted by <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{\beta}}_{-j}\)</span>. The coefficient for <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span> in the true model is denoted by <span class="math notranslate nohighlight">\(\beta_j\)</span>, and the vector of coefficients for other explanatory variables is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{\beta} _{-j}\)</span>. Hence, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} ^\top = \left[ \boldsymbol{\beta} _{-j} \quad \beta_j \right] ^\top\)</span>.</p>
<p>We first find the expression of the new estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{-j}\)</span> of the regression where the design matrix is <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \widetilde{\boldsymbol{\beta} }_{-j}
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{y} \\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left\{ \left[ \boldsymbol{X} _{-j} \quad \boldsymbol{x}_j \right]\left[\begin{array}{l}
\boldsymbol{\beta} _{-j}  \\
\beta _j
\end{array}\right] + \boldsymbol{\varepsilon}  \right\}\\
&amp;= \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \left( \boldsymbol{X} _{-j} \boldsymbol{\beta} _{-j} +  \boldsymbol{x}_j \beta _j + \boldsymbol{\varepsilon}  \right) \\
&amp;=  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \right]\left(  \boldsymbol{x}_j \beta _j+ \boldsymbol{\varepsilon}  \right)\\
\end{align}\end{split}\]</div>
<p>The expectation, therefore, is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{E}\left( \widetilde{\boldsymbol{\beta} }_{-j} \right) =  \boldsymbol{\beta} _{-j} + \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X}  _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \right]\beta _j\\
\end{split}\]</div>
<p>What is <span class="math notranslate nohighlight">\(\left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span>? You may recognize this form. It is actually the vector of estimated coefficients when we regress the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span> on all other explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>. Let it be <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_{(p-1) \times 1}\)</span>. Note that <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> is not random, since <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is fixed.</p>
<p>Therefore, we have, for the <span class="math notranslate nohighlight">\(k\)</span>-th explanatory variable in the new model,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{E}\left( \widetilde{\beta}_{k} \right) = \beta_{k} + \alpha_k \beta_j
\]</div>
<p>So the bias is <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>. The sign can be positive or negative.</p>
<p>This identity can be illustrated by the following diagram. The explanatory variable <span class="math notranslate nohighlight">\(X_k\)</span> is associated with the response <span class="math notranslate nohighlight">\(Y\)</span> in two ways. The first way is directly by itself with strength is <span class="math notranslate nohighlight">\(\beta_k\)</span>, and the second is through the omitted variable <span class="math notranslate nohighlight">\(X_j\)</span>, with a “compound” strength <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_k \quad \overset{\quad \beta_{k} \quad }{\longrightarrow} \quad Y
\]</div>
<div class="math notranslate nohighlight">
\[
\alpha_k \searrow \qquad \nearrow \beta_j
\]</div>
<div class="math notranslate nohighlight">
\[
X_j
\]</div>
<p>When will the bias be zero?</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\alpha_k = 0\)</span>, which can be checked by running a regression <span class="math notranslate nohighlight">\(X_j\)</span> over other all other covariates. Note that this does NOT implies <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span> are uncorrelated in the design matrix: <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{x}_k = 0\)</span>, see related <a class="reference internal" href="12-lm-inference.html#lm-rss-nonincreasing"><span class="std std-ref">discussion</span></a>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\beta_j = 0\)</span> in the full model, but this is never known. Note that this does NOT mean <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are uncorrelated in the data set: <span class="math notranslate nohighlight">\(\boldsymbol{x}_j ^\top \boldsymbol{y} = 0\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> is random.</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The takeaway here is that we should include all relevant omitted factors to reduce bias. But in practice, we can never know what all relevant factors are, and rarely can we measure all relevant factors.</p>
</div>
<p>What is the relation between the sample estimates? The relation has a similar form.</p>
<div class="math notranslate nohighlight">
\[
\widetilde{\beta}_{k} =  \hat{\beta}_k + \alpha_k\hat{\beta}_j
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Need linear algebra about inverse. We want to show</p>
<div class="math notranslate nohighlight">
\[
\left( \boldsymbol{X} ^{\top} _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y}  = \left[ \left( \boldsymbol{X} ^{\top}  \boldsymbol{X}  \right) ^{-1} \boldsymbol{X} ^{\top}  \boldsymbol{y}  \right]_{[:k]} + \left( \boldsymbol{X} ^{\top} _{-j} \boldsymbol{X} _{-j} \right) ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{x}_j \hat{\beta}_j
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{X} ^{\top} _{-j} \boldsymbol{X} _{-j}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{b} = \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j\)</span>. By some block matrix inverse formula,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
RHS - LHS &amp;= \left[\begin{array}{cc}
\boldsymbol{A} ^{-1} +\frac{1}{k} \boldsymbol{A} ^{-1}\boldsymbol{b} \boldsymbol{b} ^{\top}  \boldsymbol{A} ^{-1} &amp; -\frac{1}{k} \boldsymbol{A} ^{-1} \boldsymbol{b}  \\
\cdot &amp; \cdot
\end{array}\right] \left[\begin{array}{cc}
\boldsymbol{X} ^{\top} _{-j}\boldsymbol{y}  \\
\boldsymbol{x}_j ^{\top} \boldsymbol{y}
\end{array}\right] + \boldsymbol{A} ^{-1} \boldsymbol{b} \hat{\beta}_j - \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y}  \\
&amp;= \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j}\boldsymbol{y}  + \frac{1}{k} \boldsymbol{A} ^{-1}\boldsymbol{b} \boldsymbol{b} ^{\top}  \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y} -\frac{1}{k} \boldsymbol{A} ^{-1} \boldsymbol{b} \boldsymbol{x} _j ^{\top} \boldsymbol{y} + \boldsymbol{A} ^{-1} \boldsymbol{b} \hat{\beta}_j - \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y}  \\
&amp;= \boldsymbol{A} ^{-1} \boldsymbol{b} \left( \frac{1}{k} \left( \boldsymbol{b} ^{\top} \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y} - \boldsymbol{x} ^{\top} _j \boldsymbol{y} \right) - \hat{\beta}_j \right) \\
&amp;= \boldsymbol{A} ^{-1} \boldsymbol{b} \left( \hat{\beta}_j - \hat{\beta}_j \right) \\
&amp;= \boldsymbol{0}  \\
\end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>For derivation of <span class="math notranslate nohighlight">\(\hat{\beta}_j = \frac{1}{k} \left( \boldsymbol{b} ^{\top} \boldsymbol{A} ^{-1} \boldsymbol{X} ^{\top} _{-j} \boldsymbol{y} - \boldsymbol{x} ^{\top} _j \boldsymbol{y} \right)\)</span> see <a class="reference internal" href="11-lm-estimation.html#lm-partialling-out"><span class="std std-ref">partialling out</span></a> section.</p>
</div>
<p>Verify:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">b0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 + x3 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">lmo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in y ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">ro</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lmx</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficients in x3 ~ x1 + x2 :&quot;</span><span class="p">,</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="n">rx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">lmx</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reconstruction difference of b0, b1, b2 :&quot;</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">lmx</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lmo</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>coefficients in y ~ x1 + x2 + x3 : [[1.00084833 0.99870347 2.00371906 2.99677315]]
coefficients in y ~ x1 + x2 : [[1.06039402 1.02647372 3.47368318]]
coefficients in x3 ~ x1 + x2 : [[0.01986993 0.00926672 0.49051565]]
reconstruction difference of b0, b1, b2 : [-1.11022302e-15 -1.77635684e-15 -2.66453526e-15]
</pre></div>
</div>
</div>
</div>
<p>To sum up, omitting a variable <span class="math notranslate nohighlight">\(X_j\)</span> from the true model introduces bias of the remaining estimates of size <span class="math notranslate nohighlight">\(\alpha_k \beta_j\)</span>. The magnitude is 0 if</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha_k = 0\)</span>: the variable <span class="math notranslate nohighlight">\(X_k\)</span> has no explanatory power to <span class="math notranslate nohighlight">\(X_j\)</span> given other covariates</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_j = 0\)</span>: the variable <span class="math notranslate nohighlight">\(X_j\)</span> has no effect in the data generating process of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
</div>
<div class="section" id="adding-a-variable">
<span id="lm-add-variable"></span><h2>Adding a Variable<a class="headerlink" href="#adding-a-variable" title="Permalink to this headline">¶</a></h2>
<p>If we add a new variable <span class="math notranslate nohighlight">\(X_j\)</span></p>
<ul class="simple">
<li><p>What will happen to the variance and standard error of an existing estimator <span class="math notranslate nohighlight">\(\hat\beta_k\)</span>? Assuming homoskedasticity.</p></li>
<li><p>How will the interpretation and test change? Over-controlling.</p></li>
</ul>
<div class="section" id="variance-usually-increases">
<h3>Variance usually Increases<a class="headerlink" href="#variance-usually-increases" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\hat{\beta}_k\)</span> be the estimate obtained by using full design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, let <span class="math notranslate nohighlight">\(\hat{\beta}_k ^{(-j)}\)</span> be the estimate obtained by using the design matrix excluding the last column <span class="math notranslate nohighlight">\(\boldsymbol{x}_j\)</span>, denoted <span class="math notranslate nohighlight">\(\boldsymbol{X} _{-j}\)</span>.</p>
<p>We want to compare</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{Var}\left( \hat{\beta}_k \right)&amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} \right] _{kk}\\
\operatorname{Var}\left( \hat{\beta}_k ^{(-j)} \right)&amp;= \sigma^2 \left[ \left( \boldsymbol{X}_{-j} ^\top \boldsymbol{X}_{-j}  \right) ^{-1}  \right]_{kk}\\
\end{aligned}\end{split}\]</div>
<p>By the formula of matrix inverse, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1}
&amp;= \left[\begin{array}{cc}
\left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} - \boldsymbol{A} \right) ^{-1} &amp; \cdot \\
\cdot &amp; \cdot
\end{array}\right]\\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{A} = \frac{1}{\left\| \boldsymbol{x}_j  \right\|^2}   \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j \boldsymbol{x}_j ^\top \boldsymbol{X} _{-j} \succeq 0\)</span>.</p>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{Var}\left( \hat{\beta}_k \right)&amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) ^{-1} \right] _{kk}\\
&amp;= \sigma^2 \left[ \left( \boldsymbol{X} ^\top _{-j} \boldsymbol{X} _{-j} - \boldsymbol{A} \right) ^{-1}  \right] _{kk}\\
&amp;\ge \sigma^2 \left[ \left( \boldsymbol{X}_{-j} ^\top \boldsymbol{X}_{-j}  \right) ^{-1} \right] _{kk}\\
&amp;= \operatorname{Var}\left( \hat{\beta}_k ^{(-j)} \right)
\end{aligned}\end{split}\]</div>
<p>with equality iff <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{0} \Leftrightarrow \boldsymbol{X} ^\top _{-j} \boldsymbol{x}_j = \boldsymbol{0}\)</span>.</p>
<p>In conclusion, unless the new variable <span class="math notranslate nohighlight">\(X_j\)</span> is uncorrelated with all existing variable, the variance of existing <span class="math notranslate nohighlight">\(\hat{\beta}_k\)</span> will increase.</p>
</div>
<div class="section" id="standard-error-uncertain">
<h3>Standard Error Uncertain<a class="headerlink" href="#standard-error-uncertain" title="Permalink to this headline">¶</a></h3>
<p>Recall that the estimated variance of <span class="math notranslate nohighlight">\(\hat{\beta}_k\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\widehat{\operatorname{Var}}\left(\hat{\beta}_{k}\right) &amp;= \frac{1}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}}  \frac{\hat{\sigma}^{2}}{1-R_k^{2}} \\
&amp;= \frac{1}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}} \frac{1}{1-R_k^{2}}  \frac{RSS}{n-p} \\
&amp;= \frac{\sum_{i}\left(y_{i}-\bar{y}\right)^{2}/(n-1)}{\sum_{i}\left(x_{i k}-\bar{x}_{k}\right)^{2}} \frac{1- \operatorname{adj}R^2}{1-R_k^{2}}  \\
\end{aligned}\end{split}\]</div>
<p>In short,</p>
<ul class="simple">
<li><p>Adding a variable <span class="math notranslate nohighlight">\(X_j\)</span> increases <span class="math notranslate nohighlight">\(R_k^2\)</span> unless <span class="math notranslate nohighlight">\(X_j\)</span> has no explanatory power to <span class="math notranslate nohighlight">\(X_k\)</span> given other covariates. Hence the factor <span class="math notranslate nohighlight">\(\frac{1}{1-R_k^{2}}\)</span> usually increases.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_j\)</span> has good explanatory power to <span class="math notranslate nohighlight">\(Y\)</span> given other covariates and hence leads to a large reduction in <span class="math notranslate nohighlight">\(RSS\)</span>, then the term <span class="math notranslate nohighlight">\(\frac{RSS}{n-p}\)</span> decreases (though <span class="math notranslate nohighlight">\(n-p\)</span> decreases too).</p></li>
</ul>
<p>Hence, the overall effect is uncertain. To sum up,</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(X_j\)</span> has SMALL explanatory power to <span class="math notranslate nohighlight">\(X_k\)</span> given other covariates, and has GOOD explanatory power to <span class="math notranslate nohighlight">\(Y\)</span> given other covariates, say, providing a new relevant dimension in <span class="math notranslate nohighlight">\(Y\)</span> and is nearly orthogonal to existing dimensions, then the standard error <strong>decreases</strong>. This is the optimal new variable we want to add.</p>
<p>Example: add a variable <span class="math notranslate nohighlight">\(PriorScore\)</span> to the regression <span class="math notranslate nohighlight">\(NewScore \sim Treatment\)</span> can reduce the standard error of <span class="math notranslate nohighlight">\(\hat{\beta}_{Treatment}\)</span> if the treatment assignment is random.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(X_j\)</span> has GOOD explanatory power to <span class="math notranslate nohighlight">\(X_k\)</span> given other covariates, and has SMALL explanatory power to <span class="math notranslate nohighlight">\(Y\)</span> given other covariates, say, nearly a linear combination of the existing covariates, then the standard error <strong>increases</strong>. This is the kind of new variable we want to avoid.</p>
<p>Hence, if our model is already a good (or close to true) model, adding more covariates increases standard error. This gives a stopping criteria when adding more covariates.</p>
</li>
</ul>
</div>
<div class="section" id="over-controlling">
<h3>Over Controlling<a class="headerlink" href="#over-controlling" title="Permalink to this headline">¶</a></h3>
<p>Aka. over-specification.</p>
<p>Over controlling occurs if you include variables that modify the ceteris paribus interpretation so that the test <strong>no longer</strong> test the hypothesis of interest. To avoid this,</p>
<ul class="simple">
<li><p>do not include multiple measures of the same economic concept</p></li>
<li><p>do not include intermediate outcomes or alternative forms of the dependent variables.</p></li>
</ul>
<p>Examples:</p>
<ul>
<li><p>To evaluate a new course curriculum, consider two models</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \text{Model 1: } \text{score} _i &amp;= \beta_0 + \beta_1 \text{In_new_program}_i  + u_i\\
  \text{Model 2: } \text{score} _i &amp;= \beta_0 + \beta_1 \text{In_new_program}_i  + \beta_2 \text{hours}_i + u_i\\
  \end{aligned}\end{split}\]</div>
<p>Studied hours is an intermediate outcome. The ceteris paribus interpretation of <span class="math notranslate nohighlight">\(\beta_1\)</span> changed, so that it no longer tests the hypothesis of interest.</p>
</li>
<li><p>To evaluate the effect of tax rate change on GDP, consider two models</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \text{Model 1: } \text{GDP} _i &amp;= \beta_0 + \beta_1 \text{Tax_rate}_i  + u_i\\
  \text{Model 2: } \text{GDP} _i &amp;= \beta_0 + \beta_1 \text{Tax_rate}_i  + \beta_2 \text{No_of_corporations}_i + u_i\\
  \end{aligned}\end{split}\]</div>
<p>The number of corporations is an intermediate outcome.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="multicollinearity">
<span id="lm-multicollinearity"></span><h2>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h2>
<p>Definition (Multicollinearity)<br />
Multicollinearity measure the extent of pairwise correlation of variables in the design matrix.</p>
<div class="margin sidebar">
<p class="sidebar-title">Multicollinearity in computation</p>
<p>From numerical algebra’s perspective, the extent of correlation of variables in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> determines the condition number of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. As the correlation increases, its inverse becomes unstable. When perfect linear relation exists, then <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span> is not of full rank, and thus no inverse exists.</p>
</div>
<p>Definition (Perfect multicollinearity)<br />
A set of variables is perfectly multicollinear if a variable does not vary, or if there is an exact linear relationship between a set of variables:</p>
<div class="math notranslate nohighlight">
\[
X_{j}=\delta_{0}+\delta_{1} X_{1}+\cdots+\delta_{j-1} X_{j-1}+\delta_{i+1} X_{i+1}+\cdots+\delta_{k} X_{k}
\]</div>
<p>As long as the variables in the design matrix are not uncorrelated, then multicollinearity exists.</p>
<div class="section" id="diagnosis">
<h3>Diagnosis<a class="headerlink" href="#diagnosis" title="Permalink to this headline">¶</a></h3>
<p>Some common symptoms include</p>
<ul class="simple">
<li><p>Large standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\beta_j)\)</span></p></li>
<li><p>Overall <span class="math notranslate nohighlight">\(F\)</span>-test is significant, <span class="math notranslate nohighlight">\(R^2\)</span> is good, but individual <span class="math notranslate nohighlight">\(t\)</span>-tests are not significant due to large standard errors.</p></li>
</ul>
<p>We can measure the extent of multicollinearity by <strong>variance inflation factor</strong> (VIF) for each explanatory variable.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{VIF}_j = \frac{1}{1-R_j^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R_j^2\)</span> is the value of <span class="math notranslate nohighlight">\(R^2\)</span> when we regress <span class="math notranslate nohighlight">\(X_j\)</span> over all other explanatory variables excluding <span class="math notranslate nohighlight">\(X_j\)</span>. The value of <span class="math notranslate nohighlight">\(\operatorname{VIF}_j\)</span> can be interpreted as: the standard error <span class="math notranslate nohighlight">\(\operatorname{se}(\hat{\beta})\)</span> is <span class="math notranslate nohighlight">\(\sqrt{\operatorname{VIF}_j}\)</span> times larger than it would have been without multicollinearity.</p>
<p>A second way of measurement is the <strong>condition number</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>. If it is greater than <span class="math notranslate nohighlight">\(30\)</span>, then we can conclude that the multicollinearity problem cannot be ignored.</p>
<div class="math notranslate nohighlight">
\[
\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X}  \right) = \sqrt{\frac{\lambda_1 (\boldsymbol{X} ^\top \boldsymbol{X} )}{\lambda_p (\boldsymbol{X} ^\top \boldsymbol{X} )} }
\]</div>
<p>Finally, <strong>correlation matrix</strong> can also be used to measure multicollinearity since it is closely related to the condition number <span class="math notranslate nohighlight">\(\kappa_2 \left( \boldsymbol{X} ^\top \boldsymbol{X} \right)\)</span>.</p>
</div>
<div class="section" id="consequences">
<h3>Consequences<a class="headerlink" href="#consequences" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>It inflates <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_j \right)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \operatorname{Var}\left( \hat{\beta}_j \right)
     &amp;= \sigma^2 \frac{1}{1- R^2_{j}} \frac{1}{\sum_i (x_{ij} - \bar{x}_j)^2}  \\
     &amp;=  \sigma^2 \frac{\operatorname{VIF}_j}{\operatorname{Var}\left( X_j \right)}  
     \end{align}\end{split}\]</div>
<p>When perfect multicollinearity exists, the variance goes to infinity since <span class="math notranslate nohighlight">\(R^2_{j} = 1\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>-tests fail to reveal significant predictors, due to 1.</p></li>
<li><p>Estimated coefficients are sensitive to randomness in <span class="math notranslate nohighlight">\(Y\)</span>, i.e. unreliable. If you run the experiment again, the coefficients can change dramatically, which is measured by <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\boldsymbol{\beta} } \right)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\operatorname{Corr}\left( X_1, X_2 \right)\)</span> is large, then we expect to have large <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 \right), \operatorname{Var}\left( \hat{\beta}_2 \right), \operatorname{Var}\left( \hat{\beta}_1, \hat{\beta}_2 \right)\)</span>, but <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)\)</span> can be small. This means we cannot distinguish the effect of <span class="math notranslate nohighlight">\(X_1 + X_2\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> is from <span class="math notranslate nohighlight">\(X_1\)</span> or <span class="math notranslate nohighlight">\(X_2\)</span>, i.e. <strong>non-identifiable</strong>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>By the fact that, for symmetric positive definite matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{a} ^\top \boldsymbol{S} \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} \boldsymbol{b} = \sum \lambda_i b_i ^2
    \]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{a} ^\top \boldsymbol{S} ^{-1}  \boldsymbol{a}  = \boldsymbol{a} \boldsymbol{U} \boldsymbol{\Lambda} ^{-1}  \boldsymbol{U} ^\top \boldsymbol{a} = \boldsymbol{b} ^\top \boldsymbol{\Lambda} ^{-1}  \boldsymbol{b} = \sum \frac{1}{\lambda_i}  b_i ^2
    \]</div>
<p>we have:</p>
<p>If</p>
<div class="math notranslate nohighlight">
\[
    \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 - \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx 0
    \]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left( \hat{\beta}_1 - \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 - \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 - \boldsymbol{e} _2   \right) \approx \infty
    \]</div>
<p>If</p>
<div class="math notranslate nohighlight">
\[
    \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right) ^\top \left( \boldsymbol{x}_1 + \boldsymbol{x}_2 \right)  = \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \boldsymbol{X} ^\top \boldsymbol{X} \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
    \]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left( \hat{\beta}_1 + \hat{\beta}_2 \right)  = \sigma^2  \left( \boldsymbol{e}_1 + \boldsymbol{e}_2   \right) ^\top \left( \boldsymbol{X} ^\top \boldsymbol{X} \right) ^{-1}  \left( \boldsymbol{e}_1 + \boldsymbol{e} _2   \right) \approx \text{constant}
    \]</div>
</div>
</li>
</ol>
</div>
<div class="section" id="implications">
<h3>Implications<a class="headerlink" href="#implications" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> show high correlation, then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1\)</span> may be a proxy of <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1 - X_2\)</span> may just be noise.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_2\)</span> is removed, <span class="math notranslate nohighlight">\(X_1\)</span> may still be good for prediction.</p></li>
</ol>
</div>
</div>
<div class="section" id="heteroskedasticity">
<h2>Heteroskedasticity<a class="headerlink" href="#heteroskedasticity" title="Permalink to this headline">¶</a></h2>
<p>When <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \varepsilon _i \right)\)</span> is not a constant, we say heteroskedasticity of error variance exists. This may happen because data attributes, or due to transformation of <span class="math notranslate nohighlight">\(y\)</span>, e.g. <span class="math notranslate nohighlight">\(\log(y)\)</span>. We can diagnose its existence by plots or tests.</p>
<p>If it exists, OLS is still <strong>unbiased</strong> &amp; <strong>consistent</strong>. But the formula <span class="math notranslate nohighlight">\(\operatorname{Var}_{OLS}\left( \hat{\boldsymbol{\beta}}  \right) = \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\)</span> which uses homoskedastic assumption is incorrect now. There are problems in <strong>testing</strong>.</p>
<ul class="simple">
<li><p>To fix that for testing purpose, we can use robust standard error.</p></li>
<li><p>To get more precise estimate and correct standard errors, we can try alternative models that produce homoskedastic errors.</p></li>
</ul>
<div class="section" id="id1">
<h3>Diagnosis<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="section" id="plot">
<h4>Plot<a class="headerlink" href="#plot" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="lm-residual-plot">
<a class="reference internal image-reference" href="../_images/lm-residual-plot.png"><img alt="" src="../_images/lm-residual-plot.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 57 </span><span class="caption-text">Homoskedastic (left, right) and Heteroskedasticity (middle)</span><a class="headerlink" href="#lm-residual-plot" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="breusch-pagan-test">
<h4>Breusch-Pagan Test<a class="headerlink" href="#breusch-pagan-test" title="Permalink to this headline">¶</a></h4>
<p>Idea: under the null hypothesis of homoskedasticity, the covariate <span class="math notranslate nohighlight">\(X_1, X-2, \ldots, X_p\)</span> should have no explanatory power on the error <span class="math notranslate nohighlight">\(\varepsilon\)</span>. That is, in the regression <span class="math notranslate nohighlight">\(\varepsilon \sim X_1 \ldots X_p\)</span>, the <span class="math notranslate nohighlight">\(F\)</span>-test should be insignificant.</p>
<p>Steps are:</p>
<ol>
<li><p>Run the original regression</p>
<div class="math notranslate nohighlight">
\[Y \sim X_1 + X_2 + \ldots X_j\]</div>
<p>Obtain residuals <span class="math notranslate nohighlight">\(\hat{\varepsilon}\)</span>.</p>
</li>
<li><p>Run an auxiliary regression</p>
<div class="math notranslate nohighlight">
\[\hat{\varepsilon}^2 \sim X_1 + X_2 + \ldots X_j\]</div>
<p>Obtain <span class="math notranslate nohighlight">\(TSS\)</span> and <span class="math notranslate nohighlight">\(RSS\)</span>.</p>
</li>
<li><p>Compute the Lagrange multiplier test statistic</p>
<div class="math notranslate nohighlight">
\[LM = \frac{1}{2}(TSS - RSS)\]</div>
<p>Under the null hypothesis of homoskedasticity, it follows <span class="math notranslate nohighlight">\(\chi^2_{p-1}\)</span></p>
</li>
</ol>
</div>
<div class="section" id="white-test">
<h4>White Test<a class="headerlink" href="#white-test" title="Permalink to this headline">¶</a></h4>
<p>The Breusch-Pagan test will detect any linear forms of heteroskedasticity, while the White test allows for nonlinearities by using squares and crossproducts of all the covariates. White test also tests model misspecification.</p>
<p>Steps are:</p>
<ol>
<li><p>Run the original regression</p>
<div class="math notranslate nohighlight">
\[Y \sim X_1 + X_2 + \ldots X_j\]</div>
<p>Obtain residuals <span class="math notranslate nohighlight">\(\hat{\varepsilon}\)</span>.</p>
</li>
<li><p>Run an auxiliary regression</p>
<div class="math notranslate nohighlight">
\[\hat{\varepsilon}^2 \sim \left\{ X_j \right\} + \left\{ X_j^2 \right\} + \left\{ X_j X_k \right\}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\left\{ X_j \right\}\)</span> stands for all first-order terms, <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\left\{ X_j^2 \right\}\)</span> stands for all second-order terms, <span class="math notranslate nohighlight">\(X_1^2, X_2^2, \ldots, X_p^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\left\{ X_j X_k \right\}\)</span> stands for all interaction terms (aka cross-products)</p></li>
</ul>
<p>Obtain <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
</li>
<li><p>Compute the Lagrange multiplier test statistic</p>
<div class="math notranslate nohighlight">
\[LM=n R^2\]</div>
<p>Under the null hypothesis of homoskedasticity, it follows <span class="math notranslate nohighlight">\(\chi^2 _{q-1}\)</span> where <span class="math notranslate nohighlight">\(q\)</span> is the number of parameters in the auxiliary regression model.</p>
</li>
</ol>
<p>The logic of the test is as follows. [<a class="reference external" href="https://en.wikipedia.org/wiki/White_test">Wikipedia</a>]</p>
<ul class="simple">
<li><p>First, the squared residuals from the original model serve as a proxy for the variance of the error term at each observation. (The error term is assumed to have a mean of zero, and the variance of a zero-mean random variable is just the expectation of its square.) The independent variables in the auxiliary regression account for the possibility that the error variance depends on the values of the original regressors in some way (linear or quadratic).</p></li>
<li><p>If the error term in the original model is in fact homoskedastic (has a constant variance) then the coefficients in the auxiliary regression (besides the constant) should be statistically indistinguishable from zero and the R2 should be “small”. Conversely, a “large” R2 (scaled by the sample size so that it follows the chi-squared distribution) counts against the hypothesis of homoskedasticity.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>In cases where the White test statistic is statistically significant, heteroskedasticity may not necessarily be the cause; instead the problem could be a specification error.</p>
</div>
</div>
</div>
<div class="section" id="correction-by-robust-error">
<h3>Correction by Robust Error<a class="headerlink" href="#correction-by-robust-error" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="alt-model-weighted-least-squares">
<h3>Alt Model: Weighted Least Squares<a class="headerlink" href="#alt-model-weighted-least-squares" title="Permalink to this headline">¶</a></h3>
<p>We can use weighted least squares as an alternative model. Assume <span class="math notranslate nohighlight">\(Var(\varepsilon_i) = \sigma_i\)</span>, then we can scale the error <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> by <span class="math notranslate nohighlight">\(\frac{1}{\sigma_i}\)</span>. The new error term has the same unit variance <span class="math notranslate nohighlight">\(Var(\frac{\varepsilon_i}{\sigma_i} ) = 1\)</span>.</p>
<p>The optimization problem is therefore</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} _{WLS} = \arg\min \sum_{i} (y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} )^2 / \sigma_i ^2
\]</div>
<p>We will talk about how to find <span class="math notranslate nohighlight">\(\sigma_i\)</span> later.</p>
</div>
<div class="section" id="estimation">
<h3>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h3>
<p>To find the solution, we introduce an <span class="math notranslate nohighlight">\(n\times n\)</span> diagonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, whose diagonal entries are the scaling factor <span class="math notranslate nohighlight">\(w_{ii} = \frac{1}{\sigma^2 _i}\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}} _{WLS}
&amp;= \arg\min\ \sum_{i} (y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} )^2 / \sigma_i ^2\\
&amp;= \arg\min\ (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )^\top \boldsymbol{W} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} )\\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X}  ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{y}\\
\end{aligned}\end{split}\]</div>
<p>In particular,</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\boldsymbol{W} = c \boldsymbol{I}\)</span> then <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} _{WLS} = \hat{\boldsymbol{\beta}} _{OLS}\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(\boldsymbol{V} = c \boldsymbol{W}\)</span> then <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} _{WLS(\boldsymbol{V})} = \hat{\boldsymbol{\beta}} _{WLS(\boldsymbol{W})}\)</span>, i.e. the solution is invariant to spherical scaling of <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>.</p></li>
</ul>
<p>From its form <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} _{WLS} = (\boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X}  ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{y}\)</span>, we can see that there is an equivalent formulation:</p>
<ol class="simple">
<li><p>Scale the data and response by error standard deviation <span class="math notranslate nohighlight">\((\widetilde{\boldsymbol{x}}_i , \widetilde{y}_i) = \left( \frac{\boldsymbol{x}_i  }{\sigma_i}  , \frac{y_i}{\sigma_i} \right)\)</span>. in matrix form, <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{X}} = \boldsymbol{X} \boldsymbol{W} ^{1/2}, \widetilde{\boldsymbol{y} } = \boldsymbol{W} ^{1/2}\boldsymbol{y}\)</span></p></li>
<li><p>Run OLS with <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{X} }\)</span> and <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{y} }\)</span></p></li>
</ol>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>Like in OLS, we want to find the distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} _{WLS}\)</span>. Note that <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim N(\boldsymbol{0} , \boldsymbol{W} ^{-1})\)</span>, so</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{\boldsymbol{\beta}} _{WLS}
&amp;= (\boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X} ) ^{-1} \boldsymbol{X} ^\top \boldsymbol{W} (\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon})\\
&amp;= \boldsymbol{\beta}  + \left( \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X}  \right) ^{-1} \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{\varepsilon} \\
&amp;\sim N \left( \boldsymbol{\beta} , \left( \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X}  \right) ^{-1}  \right)\\
\end{aligned}\end{split}\]</div>
<p>In particular, if <span class="math notranslate nohighlight">\(\boldsymbol{W} = \frac{1}{\sigma^2 } \boldsymbol{I}\)</span> as in OLS (homogeneity), then the <span class="math notranslate nohighlight">\(\left( \boldsymbol{X} ^\top \boldsymbol{W} \boldsymbol{X}  \right) ^{-1} = \sigma^2 (\boldsymbol{X} ^\top \boldsymbol{X} ) ^{-1}\)</span> as in OLS.</p>
</div>
<div class="section" id="find-weights">
<h4>Find Weights<a class="headerlink" href="#find-weights" title="Permalink to this headline">¶</a></h4>
<p>The question is, how to find <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> in advance? Some rules of thumbs include</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i\)</span> is a <strong>sum</strong> from a sample of size <span class="math notranslate nohighlight">\(n_i\)</span>, then <span class="math notranslate nohighlight">\(\sigma^2 _i \propto n_i\)</span>, then we can set weights <span class="math notranslate nohighlight">\(w_{ii} = \frac{1}{n_i}\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(y_i\)</span> is an <strong>average</strong> from a sample of size <span class="math notranslate nohighlight">\(n_i\)</span>, then <span class="math notranslate nohighlight">\(\sigma^2 _i \propto \frac{1}{n_i}\)</span>, then we can set weights <span class="math notranslate nohighlight">\(w_{ii} = n_i\)</span></p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} _{WLS}\)</span> is invariant to spherical scaling of <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, so we don’t need to know the exact value of <span class="math notranslate nohighlight">\(\sigma^2_i\)</span>, we just need to know their relative ratio.</p>
</div>
<p>Besides, we can find weights from data adaptively.</p>
<ul>
<li><p>If measurements are in blocks of size <span class="math notranslate nohighlight">\(n_1, n_2, \ldots, n_m\)</span> such that <span class="math notranslate nohighlight">\(n = n_1+ \ldots + n_m\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 _i\)</span> is constant within each block but differ across blocks, then the error variance matrix looks like</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \operatorname{Var}\left( \boldsymbol{\varepsilon}  \right) = \left[\begin{array}{cccc}
    c_1 \boldsymbol{I}_{n_1} &amp; 0 &amp; \ldots &amp; 0 \\
     0 &amp; c_2 \boldsymbol{I} _{n_2} &amp; \ldots &amp; 0 \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \ldots &amp; \ldots &amp; c_m \boldsymbol{I}  _{n_m}
    \end{array}\right]
    \end{split}\]</div>
<p>Therefore, each block of observations satisfy the homogeneity assumption. We can run OLS to estimate <span class="math notranslate nohighlight">\(\hat{\sigma}^2_k\)</span> in each block <span class="math notranslate nohighlight">\(k\)</span>, and set <span class="math notranslate nohighlight">\(c_k = \frac{1}{\hat{\sigma}_k ^2}\)</span>.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\sigma^2 _{i}\)</span> varies with some explanatory variable, say <span class="math notranslate nohighlight">\(X_{ij}\)</span>, (which can be known in advance or revealed by diagnostic plot of <span class="math notranslate nohighlight">\(\hat{\sigma}_i\)</span> over <span class="math notranslate nohighlight">\(x_{ij}\)</span>), then we can formulate there relation as some function.</p>
<ul>
<li><p>If we assume linear relation, then we can follow the steps</p>
<ol class="simple">
<li><p>run original OLS and obtain <span class="math notranslate nohighlight">\(\hat{\varepsilon}\)</span></p></li>
<li><p>regress <span class="math notranslate nohighlight">\(\ln (\hat{\varepsilon}^2)\)</span> over all explanatory varibles, call the fitted value by <span class="math notranslate nohighlight">\(\hat{u}\)</span></p></li>
<li><p>set <span class="math notranslate nohighlight">\(w_{ii} = 1/\exp(\hat{u})\)</span>.</p></li>
</ol>
</li>
<li><p>For other relations, for instance, we can assume <span class="math notranslate nohighlight">\(\sigma_i = r_0 + \left\vert x_{ij} \right\vert ^{r_1}\)</span>. Then, we can estimate <span class="math notranslate nohighlight">\(r_0\)</span> and <span class="math notranslate nohighlight">\(r_1\)</span> jointly with <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> by maximum likelihood</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
      \boldsymbol{\beta} , r_0, r_1
      &amp;= \arg\max \left\{ \prod_i \frac{1}{\sqrt{2 \pi \sigma^2 }} \exp
      \left( - \frac{ \left( y_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} \right)^2 }{2 \sigma^2 } \right)   \right\}\\
      &amp;= \arg \min \left\{ \sum_i \frac{(\boldsymbol{y}_i - \boldsymbol{x}_i ^\top \boldsymbol{\beta} )^2}{2(r_0 + \left\vert x_{ij} \right\vert ^ {r_1})^2}  + \log \left( r_0 + \left\vert x_{ij} \right\vert ^ {r_1} \right) \right\} \\
      \end{aligned}\end{split}\]</div>
</li>
</ul>
</li>
</ul>
<p>and use the estimated</p>
</div>
</div>
</div>
<div class="section" id="measurement-error">
<h2>Measurement Error<a class="headerlink" href="#measurement-error" title="Permalink to this headline">¶</a></h2>
<div class="section" id="of-y">
<h3>Of <span class="math notranslate nohighlight">\(Y\)</span><a class="headerlink" href="#of-y" title="Permalink to this headline">¶</a></h3>
<p>Suppose due to some measurement error <span class="math notranslate nohighlight">\(w\)</span>, we obtain <span class="math notranslate nohighlight">\(\widetilde{Y} = Y + w\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{True response } Y&amp;=\beta_{0}+\beta_{1} X_{1}+\ldots+\beta_{k} X_{k}+\varepsilon \\
\text{Observe } \widetilde{Y} &amp;=Y +w\\
\widetilde{Y} &amp;=\beta_{0}+\beta_{1} X_{1}+\ldots+\beta_{k} X_{k}+ \underbrace{\varepsilon}_{\text{other variables' effect} } +  \underbrace{w}_{\text{measurement error}}
\end{aligned}
\end{split}\]</div>
<p>Then, the new estimates <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{\beta} }\)</span> and the inference are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\widetilde{\boldsymbol{\beta}}
&amp;= \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top (\boldsymbol{\varepsilon} + \boldsymbol{w} ) \\
\operatorname{E}\left( \widetilde{\boldsymbol{\beta}}  \right)&amp;= \boldsymbol{\beta} + (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \operatorname{E}( \boldsymbol{w} ) \\
\operatorname{Var}\left( \widetilde{\boldsymbol{\beta}}  \right)&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \operatorname{Var}( \boldsymbol{\varepsilon} + \boldsymbol{w} ) \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \\
&amp;= (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top (\sigma^2 _\varepsilon \boldsymbol{I} + \sigma^2 _w \boldsymbol{I}) \boldsymbol{X} (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \text{ (assumed)} \\
&amp;= (\sigma^2 _\varepsilon + \sigma^2 _w) (\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \\
\end{aligned}\end{split}\]</div>
<ul class="simple">
<li><p>In terms of unbiasedness,  <span class="math notranslate nohighlight">\((\boldsymbol{X} ^\top \boldsymbol{X} )^{-1} \boldsymbol{X} ^\top \operatorname{E}( \boldsymbol{w} )\)</span> can be viewed as the estimates by regressing <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{w}  \right)\)</span> over <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\operatorname{E}\left(\boldsymbol{w}  \right) = \boldsymbol{0}\)</span>, then <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{\beta} }\)</span> is unbiased.</p></li>
<li><p>Else, bias may appear</p></li>
</ul>
</li>
<li><p>In terms of variances, assume <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> are independent, and assume <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \boldsymbol{w}  \right) = \sigma^2 _w \boldsymbol{I}\)</span>, then the variance increases, so the standard error increases.</p></li>
</ul>
</div>
<div class="section" id="of-x">
<h3>Of <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#of-x" title="Permalink to this headline">¶</a></h3>
<p>Suppose the true values of explanatory variables is <span class="math notranslate nohighlight">\(x_{ij}\)</span>. Due to some measurement error <span class="math notranslate nohighlight">\(w_{ij}\)</span>, we collect <span class="math notranslate nohighlight">\(\tilde{x}_{ij} = x_{ij} + w_{ij}\)</span>. Now we want to analyze its effect on our OLS estimates.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Erros satisfying these assumptions are called classical errors</p>
</div>
<p>Assume</p>
<ul class="simple">
<li><p>common zero mean <span class="math notranslate nohighlight">\(\operatorname{E}\left( w_{ij} \right) = 0\)</span></p></li>
<li><p>common variance <span class="math notranslate nohighlight">\(\operatorname{Var}\left( w_{ij} \right) = \sigma^2_w\)</span></p></li>
<li><p>pairwisely uncorrelated <span class="math notranslate nohighlight">\(\operatorname{Cov} (w_{ij}, w_{k\ell}) = 0\)</span></p></li>
<li><p>uncorrelated with <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span></p></li>
</ul>
<p>In matrix form, we can write an error matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}_{n \times p} = [\boldsymbol{w}_1 \ \ldots \ \boldsymbol{w}_p ]\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{w}_j = [w_{1j}, w_{2j}, \ldots, w_{nj}]^\top\)</span>. Then the assumption becomes</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{W}  \right) = \boldsymbol{0}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{W} ^\top \boldsymbol{W}  \right) = n \sigma^2_w \boldsymbol{I} _p\)</span></p></li>
</ul>
<p>By CLM and LLN, as <span class="math notranslate nohighlight">\(n\rightarrow \infty\)</span>, we have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W} ^\top \boldsymbol{v} \rightarrow \boldsymbol{0}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W} ^\top \boldsymbol{W} \rightarrow n\sigma^2_w I_n\)</span></p></li>
</ul>
<p>The data matrix we collect is</p>
<div class="math notranslate nohighlight">
\[\widetilde{\boldsymbol{X} } = \boldsymbol{X} + \boldsymbol{W}\]</div>
<p>We can find the OLS estimates as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\widetilde{\boldsymbol{\beta} }
&amp;= \left( \widetilde{\boldsymbol{X} }^\top \widetilde{\boldsymbol{X} }  \right) ^{-1} \widetilde{\boldsymbol{X} } ^\top \boldsymbol{y} \\
&amp;= \left( \boldsymbol{X} ^\top \boldsymbol{X} + \boldsymbol{X} ^\top \boldsymbol{W}  + \boldsymbol{W} ^\top \boldsymbol{X} + \boldsymbol{W} ^\top \boldsymbol{W} \right) ^{-1}  \left( \boldsymbol{X} ^\top y + \boldsymbol{W} ^\top \boldsymbol{y}  \right)\\
&amp;\rightarrow \left( \boldsymbol{X} ^\top \boldsymbol{X} + n \sigma^2_w \boldsymbol{I} _p \right) ^{-1} \boldsymbol{X} ^\top \boldsymbol{y} \\
\end{aligned}\end{split}\]</div>
<p>which is like the form of ridge regression.</p>
<p>As <span class="math notranslate nohighlight">\(\sigma^2_w\)</span> increases,</p>
<ul class="simple">
<li><p>the effect amounts to shrinkage in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, i.e. <span class="math notranslate nohighlight">\(\left\vert \beta_j \right\vert\)</span> decreases, which is called <strong>attenuation bias</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{Corr}\left( X_j, Y \right)\)</span> decreases</p></li>
</ul>
<p>In particular, in SLR,</p>
<div class="math notranslate nohighlight">
\[
\tilde{\beta}_1 \rightarrow \beta_1 \cdot \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n\sigma^2_w  + \sum_{i=1}^n (x_i - \bar{x})^2}
\]</div>
</div>
<div class="section" id="solution">
<h3>Solution<a class="headerlink" href="#solution" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Use better measures</p></li>
<li><p>Can correct estimates accordingly, if we know how the distribution of measurement error <span class="math notranslate nohighlight">\(w\)</span> given <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p></li>
<li><p>Use instrumental variables</p></li>
</ul>
</div>
</div>
<div class="section" id="missing-values">
<span id="lm-missing-values"></span><h2>Missing Values<a class="headerlink" href="#missing-values" title="Permalink to this headline">¶</a></h2>
<p>For more details about missing, refer to <a class="reference internal" href="../30-ml-basics/11-data-issues.html#missing-values"><span class="std std-ref">missing values</span></a>.</p>
<p>Suppose the true model is <span class="math notranslate nohighlight">\(\boldsymbol{Y}  = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>. What happen if some <span class="math notranslate nohighlight">\(Y\)</span> or <span class="math notranslate nohighlight">\(X\)</span> are not observed?</p>
<div class="section" id="completely-at-random">
<h3>Completely At Random<a class="headerlink" href="#completely-at-random" title="Permalink to this headline">¶</a></h3>
<p>If missing is completely at random, then we have a smaller sample. This will increase the standard errors of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> estimators (lower precision), but it does not cause bias.</p>
</div>
<div class="section" id="depends-on-y-endogenous">
<h3>Depends on <span class="math notranslate nohighlight">\(Y\)</span> (Endogenous)<a class="headerlink" href="#depends-on-y-endogenous" title="Permalink to this headline">¶</a></h3>
<p>If missing depends on <span class="math notranslate nohighlight">\(Y\)</span>, Consider the following cases.</p>
<ol>
<li><p>If some upper value <span class="math notranslate nohighlight">\(y_i &gt; c\)</span> is missing, then for the remaining observations with <span class="math notranslate nohighlight">\(\boldsymbol{x}_i ^\top \boldsymbol{\beta} = c\)</span>, their error must be <span class="math notranslate nohighlight">\(\varepsilon &lt; 0\)</span>, to ensure <span class="math notranslate nohighlight">\(y_i &lt; c\)</span> to be selected. Consequently, the assumption that <span class="math notranslate nohighlight">\(\operatorname{E}\left( \varepsilon_i \right) = 0\)</span> no longer hold for such <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Hence, the estimates <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> will be <strong>biased</strong> since <span class="math notranslate nohighlight">\(\operatorname{E}\left( \boldsymbol{\varepsilon}  \right) \ne 0\)</span>.</p>
<p>The analysis is the same for the case of lower value missing <span class="math notranslate nohighlight">\(y_i &lt; c\)</span>.</p>
</li>
<li><p>If some middle value <span class="math notranslate nohighlight">\(\left\vert y_i -\mu _y\right\vert &lt; c\)</span> is missing, will the estimates be unbiased?</p>
<p>This depends. Consider an extreme case that <span class="math notranslate nohighlight">\(Y = 10X + \varepsilon\)</span> and we only have three kinds of covariates values <span class="math notranslate nohighlight">\(X= 1,2,3\)</span>, so there are three data clouds of equal sizes (assumed). Then dropping <span class="math notranslate nohighlight">\(\left\vert Y -20 \right\vert &lt; 1\)</span> and using the remaining two data clouds to run OLS, we will still get <span class="math notranslate nohighlight">\(E(\tilde{\beta}_1)=1\)</span>, which is unbiased.</p>
<p>On the other hand, this selection criteria can be viewed as a combination of an upper dropping and a lower dropping, so bias is also possible under certain cases.</p>
</li>
</ol>
</div>
<div class="section" id="depends-on-x-exogenous">
<h3>Depends on <span class="math notranslate nohighlight">\(X\)</span> (Exogenous)<a class="headerlink" href="#depends-on-x-exogenous" title="Permalink to this headline">¶</a></h3>
<p>If missing depends on <span class="math notranslate nohighlight">\(X\)</span>, then we miss part of the data.</p>
<div class="margin sidebar">
<p class="sidebar-title">Non-linear true model</p>
<p>If the true underlying model is not linear, then the estimates may or may not change. One example is <span class="math notranslate nohighlight">\(Y = X^2 + \varepsilon\)</span> where <span class="math notranslate nohighlight">\(X \sim U(-1,1)\)</span>, where we will obtain <span class="math notranslate nohighlight">\(E(\hat{\beta_1}) = 0\)</span>, and after removing <span class="math notranslate nohighlight">\(X&lt;0\)</span>, we will obtain <span class="math notranslate nohighlight">\(\operatorname{E}\left( \widetilde{\beta_1} \right) &gt; 0\)</span>.</p>
</div>
<ul>
<li><p>In terms of unbiasedness, we only observe some parts of the hyperplane, with the same local slope as the overall hyperplane. Hence, the estimates are still <strong>unbiased</strong>.</p></li>
<li><p>In terms of variance of estimates, recall that <span class="math notranslate nohighlight">\(\operatorname{Var}\left( \beta_j \right) = \sigma^{2} \frac{1}{1-R_{j}^{2}} \frac{1}{\sum_{i}\left(x_{i j}-\bar{x}_{j}\right)^{2}}\)</span>, then</p>
<ul class="simple">
<li><p>data TSS <span class="math notranslate nohighlight">\(\sum_{i}\left(x_{i j}-\bar{x}_{\cdot j}\right)^{2}\)</span> can only decrease (think about one-way ANOVA)</p></li>
<li><p><span class="math notranslate nohighlight">\(R_j^2\)</span> can increase or decrease (think about leaky ReLU shape relation between <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span>)</p></li>
</ul>
<p>Hence the how <span class="math notranslate nohighlight">\(\operatorname{Var} (\beta_j)\)</span> change is uncertain.</p>
</li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./31-regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="12-lm-inference.html" title="previous page">Linear Models - Inference</a>
    <a class='right-next' id="next-link" href="14-lm-advanced.html" title="next page">Linear Models - Advanced Topics</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
    
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-150740237-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>