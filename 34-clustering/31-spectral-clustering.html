
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Spectral Clustering &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gaussian Mixtures" href="41-gaussian-mixtures.html" />
    <link rel="prev" title="Agglomerative Methods" href="13-agglomerative-methods.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-clustering.html">
   Clustering
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/51-graph-rep-learning.html">
     Graph Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/34-clustering/31-spectral-clustering.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F34-clustering/31-spectral-clustering.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-graphs">
   Similarity Graphs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adjacency-matrix-based">
   Adjacency Matrix-based
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#property">
     Property
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bisection">
     Bisection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-and-cons">
     Pros and Cons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laplacian-matrix-based">
   Laplacian Matrix-based
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Property
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Bisection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Pros and Cons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation">
   Computation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weighted-case">
   Weighted Case
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#objectives">
     Objectives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bisection-min-cut">
     Bisection Min Cut
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bisection-normalized-cut">
     Bisection Normalized Cut
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-clusters">
     More Clusters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications">
   Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-to-k-means">
     Comparison to
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-segmentation">
     Image Segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-separation">
     Speech Separation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-block-models">
   Stochastic Block Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adjacency-matrix">
     Adjacency Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analysis">
     Analysis
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="spectral-clustering">
<h1>Spectral Clustering<a class="headerlink" href="#spectral-clustering" title="Permalink to this headline">¶</a></h1>
<p>Spectral clustering methods use a similarity graph (as in graph-based dimensionality reduction) to represents the data points, then apply spectral (eigenvector-based) methods on certain graph matrices associated with the connectivity of <span class="math notranslate nohighlight">\(G\)</span>, to divide the graph into connected sub-graphs.</p>
<p>Similar to graph-based representation learning, there are three steps for spectral clustering. Given a data set.</p>
<ol class="simple">
<li><p>Define a similarity measure</p></li>
<li><p>Construct a similarity graph, and obtain</p>
<ul class="simple">
<li><p>Adjacency matrix</p></li>
<li><p>Laplacian matrix</p></li>
<li><p>…</p></li>
</ul>
</li>
<li><p>Run graph cut algorithm on graph matrices with some objectives.</p></li>
</ol>
<div class="figure align-default" id="spectral-clustering-ep-1">
<a class="reference internal image-reference" href="../_images/spectral-clustering-ep-1.png"><img alt="" src="../_images/spectral-clustering-ep-1.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 99 </span><span class="caption-text">Divide the graph into sub-graphs [D. Sontag]</span><a class="headerlink" href="#spectral-clustering-ep-1" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="similarity-graphs">
<h2>Similarity Graphs<a class="headerlink" href="#similarity-graphs" title="Permalink to this headline">¶</a></h2>
<p>Formally, we have data points <span class="math notranslate nohighlight">\(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}\)</span>, Consider some similarity measures <span class="math notranslate nohighlight">\(s_{i j}\)</span> or dissimilarity measures <span class="math notranslate nohighlight">\(d_{ij}\)</span>. We create a graph <span class="math notranslate nohighlight">\(G=(V,E)\)</span>, add one node <span class="math notranslate nohighlight">\(v_i\)</span> for each data point <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>, and unweighted or weighted edges by some criterions.</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\varepsilon\)</span>-neighborhood graph</strong>: add edge <span class="math notranslate nohighlight">\((v_i, v_j)\)</span> if <span class="math notranslate nohighlight">\(d_{ij} \le \varepsilon\)</span></p>
<ul>
<li><p>Advantages: Geometrically motivated, the relationship is naturally symmetric.</p></li>
<li><p>Disadvantages: Often leads to graphs with several connected components, difficult to choose <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(k\)</span>-NN</strong>: add edge <span class="math notranslate nohighlight">\((v_i,v_j)\)</span> if <span class="math notranslate nohighlight">\(v_j\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-NN of <span class="math notranslate nohighlight">\(v_i\)</span> <strong>or</strong> vice versa, according to <span class="math notranslate nohighlight">\(d_{ij}\)</span></p>
<ul>
<li><p>Advantages: Easier to choose; does not tend to lead to disconnected graphs.</p></li>
<li><p>Disadvantages: Less geometrically intuitive.</p></li>
</ul>
</li>
<li><p><strong>Mutual <span class="math notranslate nohighlight">\(k\)</span>-NN</strong>: add edge <span class="math notranslate nohighlight">\((v_i,v_j)\)</span> if <span class="math notranslate nohighlight">\(v_i\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-NN of <span class="math notranslate nohighlight">\(v_j\)</span> <strong>and</strong> vice versa. It works well for sub-graphs with different densities.</p></li>
<li><p><strong>Fully connected</strong>: add edge <span class="math notranslate nohighlight">\((v_i,v_j)\)</span> with weight <span class="math notranslate nohighlight">\(w_{ij} = s_{ij}\)</span> or some function of <span class="math notranslate nohighlight">\(s_{ij}\)</span>.</p></li>
</ul>
<div class="figure align-default" id="spectral-clustering-types">
<a class="reference internal image-reference" href="../_images/spectral-clustering-types.png"><img alt="" src="../_images/spectral-clustering-types.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 100 </span><span class="caption-text">Comparison of three types of graphs [von Luxburg]</span><a class="headerlink" href="#spectral-clustering-types" title="Permalink to this image">¶</a></p>
</div>
<p>We mainly consider bisection (<span class="math notranslate nohighlight">\(K=2\)</span>) and unweighted case.</p>
</div>
<div class="section" id="adjacency-matrix-based">
<h2>Adjacency Matrix-based<a class="headerlink" href="#adjacency-matrix-based" title="Permalink to this headline">¶</a></h2>
<p>Recall that an adjacency matrix contains binary entries of connection relation between any two nodes. Sometimes we can extend it to be the matrix of edge weights (similarities).</p>
<ul class="simple">
<li><p><strong>Degree</strong> (considering weights) <span class="math notranslate nohighlight">\(\operatorname{deg} (i)=\sum_{j} a_{i j} = \operatorname{RowSum}_i (A)\)</span></p></li>
<li><p><strong>Volume</strong> of a set <span class="math notranslate nohighlight">\(S\)</span> of nodes <span class="math notranslate nohighlight">\(\operatorname{vol}(S)=\sum_{i \in S} \operatorname{deg} (i)\)</span>.</p></li>
</ul>
<div class="section" id="property">
<h3>Property<a class="headerlink" href="#property" title="Permalink to this headline">¶</a></h3>
<p>Let the eigen-pairs of an binary adjacency matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> be <span class="math notranslate nohighlight">\((\lambda_i, \boldsymbol{v}_i)\)</span>, where <span class="math notranslate nohighlight">\(\lambda_1 \le \ldots \le \lambda_{N_v}\)</span> (not necessarily distinct).</p>
<dl class="simple myst">
<dt>Fact (Spectrum of graph adjacency matrices)</dt><dd><p>In the case of a graph <span class="math notranslate nohighlight">\(G\)</span> consisting of two <span class="math notranslate nohighlight">\(d\)</span>-regular graphs joined to each other by just a handful of vertices,</p>
<ul class="simple">
<li><p>the two largest eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2\)</span> will be roughly equal to <span class="math notranslate nohighlight">\(d\)</span>, and the remaining eigenvalues will be of only <span class="math notranslate nohighlight">\(\mathcal{O} (d^{1/2})\)</span> in magnitude. Hence, there is a gap in the spectrum of eigenvalues, namely ‘spectral gap’.</p></li>
<li><p>the second eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> are expected to have large positive entires on vertices of one <span class="math notranslate nohighlight">\(d\)</span>-‘regular’ graphs, and large negative entires on the vertices of the other. For details, see <a class="reference internal" href="#stochastic-block-models"><span class="std std-ref">stochastic block models</span></a>.</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="bisection">
<h3>Bisection<a class="headerlink" href="#bisection" title="Permalink to this headline">¶</a></h3>
<p>Using this fact, to find two clusters in the data set, we can compute eigenvalues and eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, then find the largest positive and largest negative entries in the 2nd eigenvector. Their respective neighbors are declared to be two clusters.</p>
<p>For instance, in the plots below, We see that</p>
<ul class="simple">
<li><p>The first two eigenvalues are fairly distinct from the rest, indicating the possible presence of two sub-graphs.</p></li>
<li><p>The first eigenvector appears to suggest a separation of the 1st and 34th actors, and some of their nearest neighbors, from the rest of the actors.</p></li>
<li><p>The second eigenvector in turn provides evidence that these two actors, and certain of their neighbors, should themselves be separated.</p></li>
</ul>
<div class="figure align-default" id="spectral-clustering-ex1">
<a class="reference internal image-reference" href="../_images/spectral-clustering-ex1.png"><img alt="" src="../_images/spectral-clustering-ex1.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 101 </span><span class="caption-text">Spectral analysis of the karate club network. Left: <span class="math notranslate nohighlight">\(\left\vert \lambda_i \right\vert\)</span>. Right: <span class="math notranslate nohighlight">\(\boldsymbol{v} _1, \boldsymbol{v} _2\)</span>, colored by subgroups. [Kolaczyk 2009]</span><a class="headerlink" href="#spectral-clustering-ex1" title="Permalink to this image">¶</a></p>
</div>
<p>For unkonwn <span class="math notranslate nohighlight">\(k\)</span>, we can</p>
<ul class="simple">
<li><p>Look for a spectral gap to determine <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>Run <span class="math notranslate nohighlight">\(k\)</span>-means clustering using the first <span class="math notranslate nohighlight">\(k\)</span> eigenvectors to determine assignment</p></li>
</ul>
</div>
<div class="section" id="pros-and-cons">
<h3>Pros and Cons<a class="headerlink" href="#pros-and-cons" title="Permalink to this headline">¶</a></h3>
<p>Cons</p>
<ul class="simple">
<li><p>In reality, graphs are far from regular, so this method does not work well</p></li>
<li><p>The partitions found through spectral analysis will tend to be ordered and separated by vertex degree, since the eigenvalues will mirror quite closely the underlying degree distribution. Normalizing the adjacency matrix to have unit row sums is a commonly proposed solution.</p></li>
</ul>
<p>For an analysis of this method, i.e. how well the discretization of empirical <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> gives the correct binary label, see the <a class="reference internal" href="#stochastic-block-models"><span class="std std-ref">section</span></a>.</p>
</div>
</div>
<div class="section" id="laplacian-matrix-based">
<h2>Laplacian Matrix-based<a class="headerlink" href="#laplacian-matrix-based" title="Permalink to this headline">¶</a></h2>
<p>Recall that the graph Laplacian matrix is defined as <span class="math notranslate nohighlight">\(\boldsymbol{L} = \boldsymbol{D} -\boldsymbol{A}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span> is diagonal matrix of degrees.</p>
<p>Let the eigen-pairs of <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span> be <span class="math notranslate nohighlight">\((\lambda_i, \boldsymbol{v}_i)\)</span>, where <span class="math notranslate nohighlight">\(\lambda_1 \le \ldots \le \lambda_{N_v}\)</span> (not necessarily distinct).</p>
<div class="section" id="id1">
<h3>Property<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Fact (Spectrum of graph Laplacian matrices)</dt><dd><p>A graph <span class="math notranslate nohighlight">\(G\)</span> will consist of <span class="math notranslate nohighlight">\(K\)</span> connected components if and only <span class="math notranslate nohighlight">\(\lambda_1 = \ldots = \lambda_K = 0\)</span> and <span class="math notranslate nohighlight">\(\lambda_{K+1} &gt; 0\)</span>.</p>
</dd>
</dl>
<p>Therefore, if we suspect a graph <span class="math notranslate nohighlight">\(G\)</span> to consist of nearly <span class="math notranslate nohighlight">\(K=2\)</span> components, then we expect <span class="math notranslate nohighlight">\(\lambda_2\)</span> to be close to zero.</p>
<dl class="simple myst">
<dt>Definitions</dt><dd><ul>
<li><p>The <strong>ratio</strong> of the cut defined by <span class="math notranslate nohighlight">\((S, \bar{S})\)</span> is the ratio between the number of across-part edges and the number of vertices in the smaller component.</p>
<div class="math notranslate nohighlight">
\[\phi(S, \bar{S}) = \frac{\left\vert E(S, \bar{S}) \right\vert}{ \left\vert S \right\vert}\]</div>
</li>
<li><p>The <strong>isoperimetric number</strong> of a graph <span class="math notranslate nohighlight">\(G\)</span> is defined as the smallest ratio of all cuts.</p>
<div class="math notranslate nohighlight">
\[\phi(G)=\min _{S \subset V:|S| \leq N_{v} / 2} \phi(S, \bar{S})\]</div>
</li>
</ul>
</dd>
</dl>
<p>The ratio is a natural quantity to minimize in seeking a good bisection of <span class="math notranslate nohighlight">\(G\)</span>. Unfortunately, this minimization problem to find <span class="math notranslate nohighlight">\(S\)</span> for <span class="math notranslate nohighlight">\(\phi(G)\)</span>, aka <strong>ratio cut</strong>, is NP-hard. But the isoperimetric number is closely related to <span class="math notranslate nohighlight">\(\lambda_2\)</span></p>
<dl class="simple myst">
<dt>Cheeger’s Inequality</dt><dd><p>The isoperimetric number <span class="math notranslate nohighlight">\(\phi(G)\)</span> is bounded by <span class="math notranslate nohighlight">\(\lambda_2\)</span> as</p>
<div class="math notranslate nohighlight">
\[
  \frac{\lambda_{2}}{2} \leq \phi(G) \leq \sqrt{\lambda_{2}}\left(2 \operatorname{deg} _{\max }-\lambda_{2}\right)
  \]</div>
</dd>
</dl>
<p>Thus, <span class="math notranslate nohighlight">\(\phi(G)\)</span> will be small when <span class="math notranslate nohighlight">\(\lambda_2\)</span> is small and vice versa.</p>
</div>
<div class="section" id="id2">
<h3>Bisection<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Fiedler [SAND 144] associate <span class="math notranslate nohighlight">\(\lambda_2\)</span> with the connectivity of a graph. We partition vertices according to the sign of their entires in <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
S_F=\left\{u \in V: \boldsymbol{v} _{2}[u] \geq 0\right\} \quad \text { and } \quad \bar{S}_F=\left\{u \in V: \boldsymbol{v} _{2}[u]&lt;0\right\}
\]</div>
<ul class="simple">
<li><p>The eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> is hence often called the Fiedler vector</p></li>
<li><p>The eigenvalue <span class="math notranslate nohighlight">\(\lambda_2\)</span> is often called the Fiedler value, which is also the algebraic connectivity of the graph</p></li>
</ul>
<p>This method is often called <strong>spectral bisection</strong>. It can be shown that</p>
<div class="math notranslate nohighlight">
\[
\phi(G) \leq \phi(S_F, \bar{S}_F) \leq \frac{\phi^{2}(G)}{\operatorname{deg} _{\max }} \leq \lambda_{2}
\]</div>
<p>Fiedler bisection can be viewed as a computationally efficient approximation to finding a best cut achieving <span class="math notranslate nohighlight">\(\phi(G)\)</span>. In the example below, we see that is gives a good result, classifying all but the 3rd actor correctly.</p>
<div class="figure align-default" id="spectral-clustering-ex2">
<a class="reference internal image-reference" href="../_images/spectral-clustering-ex2.png"><img alt="" src="../_images/spectral-clustering-ex2.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 102 </span><span class="caption-text">Fiedler vector <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> of the karate club graph. Color and shape indicate subgroups. [Kolaczyk 2009]</span><a class="headerlink" href="#spectral-clustering-ex2" title="Permalink to this image">¶</a></p>
</div>
<p>For <span class="math notranslate nohighlight">\(K \ge 3\)</span>, apply bisection recursively.</p>
</div>
<div class="section" id="id3">
<h3>Pros and Cons<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Pros</p>
<ul class="simple">
<li><p>Work well on bounded-degree planar graphs and certain mesh graphs [SAND 366]</p></li>
</ul>
</div>
</div>
<div class="section" id="computation">
<h2>Computation<a class="headerlink" href="#computation" title="Permalink to this headline">¶</a></h2>
<p>For both of the two spectral partitioning methods above, the computational
overhead is in principle determined by the cost of computing the spectral decomposition of an <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span>, which takes <span class="math notranslate nohighlight">\(\mathcal{O} (N_v^2)\)</span> time. However, realistically,</p>
<ul class="simple">
<li><p>only a small subset of extreme eigenvalues and eigenvectors are needed.</p></li>
<li><p>the matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span> will typically be quite sparse in practice.</p></li>
</ul>
<p>Lanczos algorithm can efficiently solve such settings. If the graph between <span class="math notranslate nohighlight">\(\lambda_2\)</span> and <span class="math notranslate nohighlight">\(\lambda_3\)</span> is sufficiently large (nearly <span class="math notranslate nohighlight">\(K=2\)</span>), the spectral bisection takes <span class="math notranslate nohighlight">\(\mathcal{O} (\frac{1}{\lambda_3 - \lambda_2} N_e)\)</span>, i.e. almost linear.</p>
</div>
<div class="section" id="weighted-case">
<h2>Weighted Case<a class="headerlink" href="#weighted-case" title="Permalink to this headline">¶</a></h2>
<p>Now we consider the weighted case from optimization point of view, where weights <span class="math notranslate nohighlight">\(w_{ij} = s_{ij}\)</span>.</p>
<p>We define the (unnormalized) graph Laplacian  as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{L} = \boldsymbol{D} - \boldsymbol{W}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> is the similarity matrix of <span class="math notranslate nohighlight">\(s_{ij}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span> is the diagonal matrix of <span class="math notranslate nohighlight">\(\boldsymbol{W} \boldsymbol{1}\)</span></p></li>
</ul>
<p>The volume of a set <span class="math notranslate nohighlight">\(S\)</span> of vertices extends to <span class="math notranslate nohighlight">\(\operatorname{vol}(S)= \sum_{i \in S} d_i =  \sum_{i \in S} \left( \sum_{j \in N(i)} w_{ij} \right)\)</span>.</p>
<div class="section" id="objectives">
<h3>Objectives<a class="headerlink" href="#objectives" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Value of a cut)</dt><dd><p>A cut is a partition of the graph into two sub-graphs <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\bar{S}\)</span>. The value of a cut is defined as the sum of total edge weights between the two subgraphs</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[W(S, \bar{S})=\sum_{i \in A, j \in B} w_{i j}\]</div>
<p>To partition the graph into <span class="math notranslate nohighlight">\(K\)</span> subgraphs, we would like to minimize the sum of cut values between <strong>each</strong> subgraph <span class="math notranslate nohighlight">\(A_i\)</span> and the rest of the graph:</p>
<div class="math notranslate nohighlight">
\[
\underset{S_{1}, \ldots, S_{K}}{\operatorname{min}} \frac{1}{2} \sum_{k=1}^{K} W\left(S_{k}, \bar{S}_{k}\right)
\]</div>
<div class="figure align-default" id="spectral-clustering-cuts">
<a class="reference internal image-reference" href="../_images/spectral-clustering-cuts.png"><img alt="" src="../_images/spectral-clustering-cuts.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 103 </span><span class="caption-text">A graph cut with <span class="math notranslate nohighlight">\(W(S,\bar{S})=0.3\)</span> [Hamad &amp; Biela]</span><a class="headerlink" href="#spectral-clustering-cuts" title="Permalink to this image">¶</a></p>
</div>
<p>The algorithm with the above objective function is called <strong>Min Cut</strong>, which favors <strong>isolated</strong> nodes.</p>
<p>Other methods with modified/normalized objectives include:</p>
<ul class="simple">
<li><p><strong>Ratio cuts</strong> <span class="math notranslate nohighlight">\((\operatorname{RatioCut} )\)</span> normalizes by cardinality: <span class="math notranslate nohighlight">\(\frac{1}{2} \sum_{k=1}^{K} \frac{W\left(S_{k}, \bar{S}_{k}\right)}{\left|S_{k}\right|}\)</span></p></li>
<li><p><strong>Normalized cuts</strong> <span class="math notranslate nohighlight">\((\operatorname{Ncut})\)</span> normalizes by volume: <span class="math notranslate nohighlight">\(\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(S_{i}, \bar{S}_{i}\right)}{\operatorname{vol}\left(S_{i}\right)}\)</span>.</p></li>
</ul>
</div>
<div class="section" id="bisection-min-cut">
<h3>Bisection Min Cut<a class="headerlink" href="#bisection-min-cut" title="Permalink to this headline">¶</a></h3>
<p>Recall that the bisection Min-cut objective is</p>
<div class="math notranslate nohighlight">
\[\min_S\ W\left(S, \bar{S}\right)\]</div>
<p>We define a indicator vector <span class="math notranslate nohighlight">\(\boldsymbol{c} \in\{-1,1\}^{n}\)</span>, where <span class="math notranslate nohighlight">\(c_i = 1\)</span> means data point <span class="math notranslate nohighlight">\(i\)</span> is in cluster/subgraph <span class="math notranslate nohighlight">\(S\)</span>; otherwise cluster <span class="math notranslate nohighlight">\(\bar{S}\)</span>. Note that</p>
<div class="math notranslate nohighlight">
\[
W\left(S, \bar{S}\right) = \sum_{i \in S, j \in \bar{S}} w_{i j} = \frac{1}{4}\sum_{ij} w_{ij}(c_i - c_j)^2 = \frac{1}{4} \boldsymbol{c} ^{\top} \boldsymbol{L} \boldsymbol{c}
\]</div>
<p>The objective can be formulated as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\min_{\boldsymbol{c}} &amp;&amp;&amp; \boldsymbol{c} ^{\top} \boldsymbol{L} \boldsymbol{c}\\
\mathrm{s.t.}
&amp;&amp;&amp; \boldsymbol{c} \in\{-1,1\}^{n} \\
\end{aligned}\end{split}\]</div>
<p>A relaxation of this problem is to solve for a continuous <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span> vector instead <span class="math notranslate nohighlight">\(\boldsymbol{c} \in \mathbb{R} ^{n}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\min_{\boldsymbol{c}} &amp;&amp; \boldsymbol{c} ^{\top} \boldsymbol{L} \boldsymbol{c} &amp;\\
\mathrm{s.t.}
&amp;&amp; \boldsymbol{c} &amp;\in \mathbb{R} ^n\\
&amp;&amp; \boldsymbol{c} ^{\top} \boldsymbol{c} &amp;= n \\
\end{aligned}\end{split}\]</div>
<p>If we don’t add the <span class="math notranslate nohighlight">\(\boldsymbol{c} ^{\top} \boldsymbol{c} = n\)</span> constraint then a trivial solution is <span class="math notranslate nohighlight">\(\boldsymbol{c} = \boldsymbol{0}\)</span>.  The solution is given by the eigenvector of the eigenproblem</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{L} \boldsymbol{c}=\lambda \boldsymbol{c}
\]</div>
<p>The first eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span> is all ones (all data in a single cluster, which is meaningless). We take the 2nd eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> as the real-valued solution. There are several ways to decide the binary assignment</p>
<ul class="simple">
<li><p>take 0 or the median value as the splitting point or,</p></li>
<li><p>search for the splitting point such that the resulting partition has the best objective value</p></li>
</ul>
<p>Actually, this problem can be solved exactly. See the <a class="reference internal" href="../25-graph-related/31-maximum-flow.html#max-flow"><span class="std std-ref">max flow</span></a> section.</p>
</div>
<div class="section" id="bisection-normalized-cut">
<span id="ncut"></span><h3>Bisection Normalized Cut<a class="headerlink" href="#bisection-normalized-cut" title="Permalink to this headline">¶</a></h3>
<p>[<a class="reference external" href="https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf">Shi &amp; Malik 1999</a>]</p>
<p>Recall that the objective of the bisection normalized cut is</p>
<div class="math notranslate nohighlight">
\[\min_S\ \left( \frac{W\left(S, \bar{S}\right)}{\operatorname{vol}\left(S\right)} + \frac{W\left(\bar{S}, S\right)}{\operatorname{vol}\left(\bar{S}\right)}  \right)\]</div>
<p>Let the <span class="math notranslate nohighlight">\(\boldsymbol{c} \in\{-1,1\}^{n}\)</span> be the assignment vector. Define <span class="math notranslate nohighlight">\(\boldsymbol{y} = (\boldsymbol{1}  + \boldsymbol{x} ) - b (\boldsymbol{1} - \boldsymbol{x} )\)</span> where <span class="math notranslate nohighlight">\(b = \frac{\operatorname{vol}(S) }{\operatorname{vol}(\bar{S})}\)</span> such that <span class="math notranslate nohighlight">\(y_i = 2\)</span> if <span class="math notranslate nohighlight">\(x_i=1\)</span>, and <span class="math notranslate nohighlight">\(y_i = -2b\)</span> and <span class="math notranslate nohighlight">\(x_i = -1\)</span>. It can be shown that finding <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span> is equivalent to solve the following optimization problem for <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\min _{\boldsymbol{y} } &amp;&amp; \frac{\boldsymbol{y} ^{\top} \boldsymbol{L} \boldsymbol{y} }{\boldsymbol{y} ^{\top} \boldsymbol{D} \boldsymbol{y} }  &amp; &amp;&amp;\\
\mathrm{s.t.}
&amp;&amp; \boldsymbol{y} &amp;\in \left\{ 1, -b \right\} ^n &amp;&amp;\\
&amp;&amp; \boldsymbol{y} ^{\top} \boldsymbol{D} \boldsymbol{1}  &amp;= \boldsymbol{0} &amp;&amp; \\
\end{aligned}\end{split}\]</div>
<p>where the constraint <span class="math notranslate nohighlight">\(\boldsymbol{y} ^{\top} \boldsymbol{D} \boldsymbol{1} = \boldsymbol{0}\)</span> comes from the condition of the assignment vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. However, solving for discrete combinatorial values is hard. The optimization problem is relaxed to solve for a continuous <span class="math notranslate nohighlight">\(\boldsymbol{y} \in \mathbb{R} ^n\)</span> vector instead. The solution <span class="math notranslate nohighlight">\(\boldsymbol{y} ^*\)</span> is given by the 2nd smallest eigenvector of the generalized eigenproblem (see the paper eq. 6-9)</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{L} \boldsymbol{y}=\lambda \boldsymbol{D}  \boldsymbol{y}
\]</div>
<p>That is, <span class="math notranslate nohighlight">\(\boldsymbol{y} ^*\)</span> is the 2nd smallest eigenvector of random-walk Laplacian <span class="math notranslate nohighlight">\(\boldsymbol{L} ^{\mathrm{rw}} = \boldsymbol{D} ^{-1} \boldsymbol{L}\)</span>. Moreover, if we set <span class="math notranslate nohighlight">\(\boldsymbol{z} ^* = \boldsymbol{D} ^{1/2} \boldsymbol{y}^*\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{z} ^*\)</span> is the second smallest eigenvector of symmetric normalized Laplacian <span class="math notranslate nohighlight">\(\boldsymbol{L} ^\mathrm{sym}= \boldsymbol{D} ^{-1/2} \boldsymbol{L} \boldsymbol{D} ^{-1/2}\)</span>. See properties of graph <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html#graph-laplacian"><span class="std std-ref">Laplacians</span></a>.</p>
<p>We then find a splitting point to decide assignment with the methods introduced in Min-cut.</p>
<div class="figure align-default" id="spectral-clustering-egvector">
<a class="reference internal image-reference" href="../_images/spectral-clustering-egvector.png"><img alt="" src="../_images/spectral-clustering-egvector.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 104 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\operatorname{Ncut}\)</span> for a data set of <span class="math notranslate nohighlight">\(40\)</span> points.</span><a class="headerlink" href="#spectral-clustering-egvector" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="more-clusters">
<h3>More Clusters<a class="headerlink" href="#more-clusters" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Option 1: Recursively apply the 2-cluster algorithm</p></li>
<li><p>Option 2: For <span class="math notranslate nohighlight">\(K\)</span> clusters, treat the smallest <span class="math notranslate nohighlight">\(K-1\)</span> eigenvectors (excluding the last one) as a reduced-dimensionality representation of the data, and cluster these eigenvectors. The task will be easier if the values in the eigenvectors are close to discrete, like the above case.</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title">Relation to Representation Learning</p>
<p>Spectral clustering is like representation learning, but push the representation to be discrete. We can then apply some simple clustering examples, like <span class="math notranslate nohighlight">\(k\)</span>-means.</p>
</div>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="comparison-to-k-means">
<h3>Comparison to <span class="math notranslate nohighlight">\(k\)</span>-means<a class="headerlink" href="#comparison-to-k-means" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="spectral-clustering-vs-k-means">
<a class="reference internal image-reference" href="../_images/spectral-clustering-vs-k-means.png"><img alt="" src="../_images/spectral-clustering-vs-k-means.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 105 </span><span class="caption-text"><span class="math notranslate nohighlight">\(k\)</span>-means vs spectral clustering on double rings data set [Ng, Jordan, &amp; Weiss]</span><a class="headerlink" href="#spectral-clustering-vs-k-means" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="image-segmentation">
<h3>Image Segmentation<a class="headerlink" href="#image-segmentation" title="Permalink to this headline">¶</a></h3>
<p>For an image, we can view each pixel <span class="math notranslate nohighlight">\(i\)</span> as a graph vertex, and the similarity depends on pixel content (color intensity) <span class="math notranslate nohighlight">\(f_i\)</span> and pixel location <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
w_{i j}= \left\{\begin{array}{ll}
\exp \left\{ -\left(\frac{1}{2 \pi \sigma^{2}}\|f_i-f_j\|^{2}+\frac{1}{2 \pi \tau^{2}}\|x_i- x_j\|^{2}\right) \right\}, &amp; \text { if } \left\vert x_i-x_j \right\vert&lt; \varepsilon \\
0, &amp; \text { otherwise }
\end{array}\right.
\end{split}\]</div>
<div class="figure align-default" id="spectral-clustering-img-seg">
<a class="reference internal image-reference" href="../_images/spectral-clustering-img-seg.png"><img alt="" src="../_images/spectral-clustering-img-seg.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 106 </span><span class="caption-text">Spectral clustering for image segmentation [Shi &amp; Malik]</span><a class="headerlink" href="#spectral-clustering-img-seg" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="spectral-clustering-img-seg-2">
<a class="reference internal image-reference" href="../_images/spectral-clustering-img-seg-2.png"><img alt="" src="../_images/spectral-clustering-img-seg-2.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 107 </span><span class="caption-text">Spectral clustering for image segmentation [Arbel ́aez et al.]</span><a class="headerlink" href="#spectral-clustering-img-seg-2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="speech-separation">
<h3>Speech Separation<a class="headerlink" href="#speech-separation" title="Permalink to this headline">¶</a></h3>
<p>Speech separation (into speakers): Similar to image segmentation, where a “pixel” is a cell of a spectrogram. Darkness corresponds to amount of frequency at that time.</p>
<div class="figure align-default" id="spectral-clustering-speech-sep">
<a class="reference internal image-reference" href="../_images/spectral-clustering-speech-sep.png"><img alt="" src="../_images/spectral-clustering-speech-sep.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 108 </span><span class="caption-text">Spectral clustering for speech separation [Bach &amp; Jordan]</span><a class="headerlink" href="#spectral-clustering-speech-sep" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="stochastic-block-models">
<span id="id4"></span><h2>Stochastic Block Models<a class="headerlink" href="#stochastic-block-models" title="Permalink to this headline">¶</a></h2>
<p>Aka planted partition model.</p>
<div class="section" id="adjacency-matrix">
<h3>Adjacency Matrix<a class="headerlink" href="#adjacency-matrix" title="Permalink to this headline">¶</a></h3>
<p>Consider a perfect case: a graph consisting of <span class="math notranslate nohighlight">\(k\)</span> clusters of equal size <span class="math notranslate nohighlight">\(\frac{n}{k}\)</span>, each cluster is complete and there are no across-cluster edges, then the spectral decomposition of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{A} = \left[\begin{array}{ccc}
\boldsymbol{1} \boldsymbol{1} ^{\top}  &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \boldsymbol{1} \boldsymbol{1} ^{\top}
\end{array}\right],
\quad \boldsymbol{U} = \frac{1}{\sqrt{n/k}}  \left[\begin{array}{ccc}
\boldsymbol{1}  &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \boldsymbol{1}
\end{array}\right] \boldsymbol{Q}, \quad \boldsymbol{\Lambda} = \frac{n}{k} \boldsymbol{I}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> are a vector of size <span class="math notranslate nohighlight">\(\frac{n}{k}\)</span>. <span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span> is an orthogonal transform (does not change distance between a pair of embeddings).</p>
<p>In practice, <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is not that perfect. How imperfect <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> can be?</p>
<p>Consider a stochastic block model (SBM), <span class="math notranslate nohighlight">\(k=2\)</span> clusters (each cluster has size <span class="math notranslate nohighlight">\(\frac{n}{2}\)</span>), and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{P} (A_{ij} = 1) = \left\{\begin{array}{ll}
p, &amp; \text { if $i, j$ in same cluster}  \\
q, &amp; \text { if $i, j$ in different cluster}  \\
\end{array}\right.
\end{split}\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Conventionally <span class="math notranslate nohighlight">\(A_{ii}=0\)</span>, but when <span class="math notranslate nohighlight">\(n\)</span> is large this does not affect the analysis much.</p>
</div>
<p>Usually <span class="math notranslate nohighlight">\(p &gt;q\)</span>. Then the expectation of the random adjacency matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E} [\boldsymbol{A}] = \left[\begin{array}{cc}
p \boldsymbol{1} \boldsymbol{1} ^{\top}  &amp; q \boldsymbol{1} \boldsymbol{1} ^{\top}  \\
q \boldsymbol{1} \boldsymbol{1} ^{\top}  &amp; p \boldsymbol{1} \boldsymbol{1} ^{\top}  \\
\end{array}\right] = \frac{p+q}{2} \boldsymbol{1}_n \boldsymbol{1}_n ^{\top} +  \frac{p-q}{2} \left[\begin{array}{cc}
\boldsymbol{1}   \\
-\boldsymbol{1}  
\end{array}\right] [\boldsymbol{1} ^{\top} \ -\boldsymbol{1} ^{\top}]
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{A}]\)</span> has rank 2 and two non-zero eigenvalues</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\lambda_1 &amp;= \frac{p+q}{2}n &amp; \boldsymbol{v} _1 &amp;= \frac{1}{\sqrt{n}} \boldsymbol{1} _n \\
\lambda_2 &amp;= \frac{p-q}{2}n &amp; \boldsymbol{v} _2 &amp;= \frac{1}{\sqrt{n}}\left[\begin{array}{cc}
\boldsymbol{1}   \\
-\boldsymbol{1}  
\end{array}\right] \\
\end{aligned}\end{split}\]</div>
<p>Hence, to give labels for 2-clustering, we can discretize <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> of <span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{A}]\)</span> according to the signs of its entries. However, we only observe <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> rather than <span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{A}]\)</span>. Is the second eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{\hat{v}} _2\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> a good estimator for <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> of <span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{A}]\)</span>?</p>
</div>
<div class="section" id="analysis">
<h3>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h3>
<p>Suppose we know the parameters <span class="math notranslate nohighlight">\(p, q\)</span>. Now we analyze the discretization performance by quantify some ‘distance’ between <span class="math notranslate nohighlight">\(\boldsymbol{v} _2\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\hat{v}}_2\)</span>.</p>
<p>First, we can write <span class="math notranslate nohighlight">\(\boldsymbol{A} = \mathbb{E} [\boldsymbol{A} ] + (\boldsymbol{A} - \mathbb{E} [\boldsymbol{A} ] )\)</span> where the second term is noise. If noise <span class="math notranslate nohighlight">\(=0\)</span>, then the second eigenvectors of observed <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is that of <span class="math notranslate nohighlight">\(\mathbb{E} [\boldsymbol{A}]\)</span>, which is <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}} \left[\begin{array}{cc}
\boldsymbol{1}   \\
-\boldsymbol{1}  
\end{array}\right]\)</span>, whose its discretization perfectly reveals the label.</p>
<p>But the second eigenvector is hard to analyze (since its computation depends on the 1st eigenvector, which is also random). We introduce an equivalent analysis: compute the first eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{A} - \frac{p+q}{2} \boldsymbol{1}_n \boldsymbol{1}_n ^{\top}\)</span>, denoted <span class="math notranslate nohighlight">\(\boldsymbol{\hat{u}}\)</span>. And assign the label according to the sign of the entries in <span class="math notranslate nohighlight">\(\boldsymbol{\hat{u}}\)</span>. Some interpretation</p>
<ul class="simple">
<li><p>avoid computing the first eigenvector which is not informative</p></li>
<li><p>approximately equivalent to compute the top eigenvector of the ‘centered’ version of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{C} \boldsymbol{A} \boldsymbol{C}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{C} = \boldsymbol{I} - \frac{1}{n}\boldsymbol{1} \boldsymbol{1} ^{\top}\)</span>.</p></li>
</ul>
<p>We expect <span class="math notranslate nohighlight">\(\boldsymbol{\hat{u}} \approx \frac{1}{\sqrt{n}} \left[\begin{array}{cc}
\boldsymbol{1}   \\
-\boldsymbol{1}  
\end{array}\right]\)</span>. To analyze the error, let</p>
<ul class="simple">
<li><p>truth: <span class="math notranslate nohighlight">\(\boldsymbol{M} = \mathbb{E} [\boldsymbol{A}]  - \frac{p+q}{2} \boldsymbol{1}_n \boldsymbol{1}_n ^{\top} = \frac{p-q}{2} \left[\begin{array}{cc}
\boldsymbol{1}   \\
-\boldsymbol{1}  
\end{array}\right] [\boldsymbol{1} ^{\top} \ -\boldsymbol{1} ^{\top}]\)</span></p></li>
<li><p>observed: <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{M}} = \boldsymbol{A} - \frac{p+q}{2} \boldsymbol{1}_n \boldsymbol{1}_n ^{\top}\)</span>.</p></li>
<li><p>perturbation: <span class="math notranslate nohighlight">\(\boldsymbol{H} =  \widehat{\boldsymbol{M}} - \boldsymbol{M} = \boldsymbol{A} - \mathbb{E} [\boldsymbol{A} ]\)</span>.</p></li>
</ul>
<p>By applying <a class="reference internal" href="../11-math/21-linear-algebra.html#davis-kahan"><span class="std std-ref">Davis-Kahan theorem</span></a> to SBM and use the distance measure defined there, let <span class="math notranslate nohighlight">\(r=1\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\operatorname{dist}(\hat{\boldsymbol{u}}, \boldsymbol{u} ) = \left\| \hat{\boldsymbol{u}} \hat{\boldsymbol{u}}^{\top} - \boldsymbol{u}  \boldsymbol{u} ^{\top}\right\|_2 \le \frac{\left\| \boldsymbol{A} - \mathbb{E} [\boldsymbol{A} ] \right\|  }{\frac{p-q}{2} n - 0 - \left\| \boldsymbol{A} - \mathbb{E} [\boldsymbol{A}]  \right\|  }
\]</div>
<p>Hence if <span class="math notranslate nohighlight">\(p \gg q\)</span>, then the error is low. Can we quantify <span class="math notranslate nohighlight">\(\left\| \boldsymbol{A} - \mathbb{E} [\boldsymbol{A} ] \right\|\)</span>? By <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html#bernstein-inequality"><span class="std std-ref">Bernstein inequality</span></a>, for <span class="math notranslate nohighlight">\(p &gt; q \ge \frac{b \log n}{n}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\left\| \boldsymbol{A} - \mathbb{E} [\boldsymbol{A} ] \right\|  }{\frac{p-q}{2} n - \left\| \boldsymbol{A} - \mathbb{E} [\boldsymbol{A}]  \right\|} \le \mathcal{O} \left( \frac{\sqrt{np \log n}}{(p-q)n}  \right)
\]</div>
<p>Therefore, when <span class="math notranslate nohighlight">\((p-q)n \gg \sqrt{n p \log n}\)</span>, discretizing <span class="math notranslate nohighlight">\(\boldsymbol{\hat{u}}\)</span> approximately recover <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>.</p>
<div class="note dropdown admonition">
<p class="admonition-title"> On distance measure</p>
<p>If <span class="math notranslate nohighlight">\(\operatorname{dist}(\boldsymbol{u} , \hat{\boldsymbol{u} }) =\left\| \hat{\boldsymbol{u}} \hat{\boldsymbol{u}}^{\top} - \boldsymbol{u}  \boldsymbol{u} ^{\top}\right\|_2\)</span> is small, then <span class="math notranslate nohighlight">\(\left\vert \langle \boldsymbol{u} , \hat{\boldsymbol{u}} \rangle \right\vert\)</span> is large, but some entires of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{u}}\)</span> might have opposite sign as <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>. Hence, it is better to use other distance measure, e.g. <span class="math notranslate nohighlight">\(\left\| \cdot \right\| _\infty\)</span>, as developed by Abbe, Fan, Wang [2020].</p>
</div>
<div class="note dropdown admonition">
<p class="admonition-title"> On lower bound of <span class="math notranslate nohighlight">\(p, q\)</span></p>
<p>The lower bound <span class="math notranslate nohighlight">\(\frac{b\log n}{n}\)</span> is to <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html#er-random-graph"><span class="std std-ref">ensure</span></a> the graph is connected, since most algorithms applied only to relatively dense graphs [<a class="reference external" href="https://arxiv.org/pdf/1202.1499.pdf">link</a> pg.3]. Also see <a class="reference external" href="https://arxiv.org/pdf/1311.4115.pdf">here</a> and <a class="reference external" href="https://arxiv.org/pdf/1502.06775.pdf">here</a>.</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(p = \frac{a \log n}{n} , q = \frac{b \log n}{n}\)</span>, (i.e., sparse regime), then <span class="math notranslate nohighlight">\(d_i = \mathcal{O} (\log n)\)</span> (‘constant’ degree). Need <span class="math notranslate nohighlight">\(a-b \gg \sqrt{a}\)</span> for the algorithm to successfully detect cluster.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(p = \frac{a}{n}, q=\frac{b}{n}\)</span> and <span class="math notranslate nohighlight">\(a - b \ge 2 \sqrt{a + b}\)</span>, then it is possible to detect cluster that is correlated with true cluster, otherwise impossible. [Mossel, Newman, Sly 2015]</p></li>
</ul>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./34-clustering"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="13-agglomerative-methods.html" title="previous page">Agglomerative Methods</a>
    <a class='right-next' id="next-link" href="41-gaussian-mixtures.html" title="next page">Gaussian Mixtures</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>