# Variational Autoencoders

More or less simultaneously proposed by Kingma & Welling (2013) and Rezende et al. (2014)

VAE is a type of generative model for a vector of random variables $\boldsymbol{x}$ assumed to be generated from a set of latent variables $\boldsymbol{z}$. Assuming both $\boldsymbol{x}$ and $\boldsymbol{z}$ are continuous, the unconditional distribution of $\boldsymbol{x}$ can be written as

$$
p(\boldsymbol{x})=\int_{\boldsymbol{z}} p(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{z}) d \boldsymbol{z}
$$


Initially used for generation, VAE has also been successful for representation learning. It is popular, fast, and relatively easy to train.

VAE have similar structure with AE. But we add probabilistic assumptions of the VAE representation and optimize the VAE objective via variational optimization.

## Objective

Ideally, the encoder and decoder should learn the two conditional distributions (from the joint distribution of $(x,z)$) parameterized by $\theta$.

:::{figure} vae-ideal-step
<img src="../imgs/vae-ideal-step.png" width = "30%" alt=""/>

Ideal case of VAE [Durr 2016]
:::

But $p(\boldsymbol{z} \mid \boldsymbol{x} )$ can be very costly to estimate. As a result, we approximate it with another neural function $q(\boldsymbol{z} \mid \boldsymbol{x} )$, which is the decoder.

Assume w.l.o.g. that latent variable $\boldsymbol{z}$ is 1-dimensional, $\boldsymbol{x}$ is 2-dimensional. The VAE structure can be shown as

:::{figure} vae-networks
<img src="../imgs/vae-networks.png" width = "80%" alt=""/>

VAE structure [Durr 2016]
:::

- $q(\boldsymbol{x}  \mid \boldsymbol{z} )$ is the distribution learned by the encoder part.
- $p(\boldsymbol{x} \mid \boldsymbol{z})$ is the distribution learned by the decoder part.

```{margin}
We often assume $q(\boldsymbol{z})$ is Gaussian.
```

- The orange nodes in the middle and at the right are the parameters in the distribution $q(\boldsymbol{z})$ and $p(\boldsymbol{x})$ respectively.


After the model is trained, we have

- a learned representation described by $q(\boldsymbol{z})$. To output a single value, we can use the mean, or mode.

- a generator $p(\boldsymbol{x} \mid \boldsymbol{z} )$. To generate $\boldsymbol{x}$ (e.g. an image) given $\boldsymbol{z}$, we can use the decoder network.

:::{figure}
<img src="../imgs/vae-decoder-generation.png" width = "80%" alt=""/>

Use VAE to generate a handwritten digit [Durr 2016]
:::

There is a theoretical foundation of generation.

Theorem (Validation of VAE)
: Any $d$-dimensional distribution can be generated by taking $d$ normally distributed variables and mapping them through some appropriate (possibly very complicated) function.

Of course, the function can be a neural network.


## Training

Directly Maximizing the likelihood can be challenging. Instead, we maximize the lower bound of the likelihood.

By some formula from conditional probability and [information theory](../30-ml-basics/03-information-theory), we have


$$
\begin{aligned}
L &=\log p(x) \\
\text { multiply by 1, }\quad  &=\sum_{z} q(z \mid x) \log p(x) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{p(z \mid x)}\right) \\
\text { multiply by 1, } \quad&=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)} \frac{q(z \mid x)}{p(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right)+\sum_{z} q(z \mid x) \log \left(\frac{q(z \mid x)}{p(z \mid x)}\right) \\
&=L^{v} + KL \left[ q(z \mid x), p(z \mid x) \right] \\
& \geq L^{v}
\end{aligned}
$$

If we can well approximate $p(z \mid x)$ by $q(z \mid x)$, then the KL divergence is small, and we can maximizing $L$ by maximizing $L^v$.

```{margin}
The lower abound $L^v$ is also known as evidence lower bound, or ELBO.
```
The lower bound $L^v$ can be further arranged to

$$
\begin{aligned}
L^{v} &=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(x \mid z) p(z)}{q(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z)}{q(z \mid x)}\right)+\sum_{z} q(z \mid x) \log p(x \mid z)\\
&=-KL \left[ q(z \mid x), p(z) \right]+E_{q(z \mid x)}\left[ \log p(x \mid z) \right] \\
\text { for } x_{i} \ldots &=-KL \left[ q\left(z \mid x_{i}\right), p(z) \right]+E_{q\left(z \mid x_{i}\right)}\left[ \log p\left(x_{i} \mid z\right) \right]
\end{aligned}
$$

We then maximizing the last line.

- The first term is the negative KL divergence between the posterior and the prior (often Gaussian). We want to minimize the distance. It can be viewed as a regularizer.

    When $p(z) = N (0, 1)$ and $q(z|x)$ is also Gaussian, this KL divergence has a closed form

    $$
    -KL \left[ q\left(z \mid x_{i}\right), p(z) \right]=\frac{1}{2} \sum_{j=1}^{J} 1+\log \left(\sigma_{z_{i, j}}^{2}\right)-\mu_{z_{i, j}}^{2}-\sigma_{z_{i, j}}^{2}
    $$

- The second term can be seen as a reconstruction loss, which equals $\log(1)$ if $\boldsymbol{x}_i$ is perfectly reconstructed from $\boldsymbol{z}$. In training, the expectation is estimated by sampling $B$ samples from the decoder network $q(z\mid x_i)$

    $$
    E_{q(z \mid x)}\left[ \log p(x \mid z) \right] = \frac{1}{B} \sum_{l=1}^{B}\log p\left(x_{i} \mid z_{i, l}\right)
    $$

    If $p(x \mid z)$ is Gaussian and $B=1$ (often used), then it is just a least squares loss

    $$
    \log p\left(x_{i} \mid z_{i}\right)=\sum_{j=1}^{D} \frac{1}{2} \log \sigma_{x_{j}}^{2}+\frac{\left(x_{i, j}-\mu_{x_{j}}\right)^{2}}{2 \sigma_{x_{j}}^{2}}
    $$
