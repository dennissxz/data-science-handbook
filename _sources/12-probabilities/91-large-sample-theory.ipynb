{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handled-hacker",
   "metadata": {},
   "source": [
    "# Large Sample Theory\n",
    "\n",
    "\n",
    "## Law of Large Numbers\n",
    "\n",
    "The Law of Large Numbers states that, as the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n",
    "\n",
    "\n",
    "### Weak Law of Large Numbers\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ be independently and identically distributed random variables with mean $\\mu$ and variance $\\sigma^{2}<\\infty$.\n",
    "Then for every $\\epsilon>0$,\n",
    "\n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}\\mathrm{P}\\left(\\left|\\bar{X}_{n}-\\mu\\right|<\\epsilon\\right)=1\n",
    "$$\n",
    "\n",
    "i.e. the sample mean converge in probability to the theoretical mean $\\mu$,\n",
    "\n",
    "$$\n",
    "\\bar{X}_{n}\\overset{\\mathcal{P}}{\\rightarrow}\\mu \\quad \\text{as}\\ n\\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "It leaves open the possibility that $\\left|\\overline{X}_{n}-\\mu \\right|>\\epsilon$  happens an **infinite** number of times,\n",
    "\n",
    "To prove it, by Chebychev's Inequality,\n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(\\left|\\bar{X}_{n}-\\mu\\right|\\geq\\epsilon\\right)\n",
    "=\\mathrm{P}\\left(\\left(\\bar{X}_{n}-\\mu\\right)^{2}\\geq\\epsilon^{2}\\right)\n",
    "\\leq\\frac{\\mathrm{E}\\left(\\bar{X}_{n}-\\mu\\right)^{2}}{\\epsilon^{2}}\n",
    "=\\frac{\\operatorname{Var}\\bar{X}_{\\mathfrak{H}}}{\\epsilon^{2}}=\\frac{\\sigma^{2}}{n\\epsilon^{2}} \\rightarrow 0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Strong Law of Large Numbers\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ be independently and identically distributed random variables with mean $\\mu$ and variance $\\sigma^{2}<\\infty$.\n",
    "Then for every $\\epsilon>0$,\n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(\\lim_{n\\rightarrow\\infty}\\left|\\bar{X}_{n}-\\mu\\right|<\\epsilon\\right)=1\n",
    "$$\n",
    "\n",
    "i.e. the sample mean converge to the theoretical mean $\\mu$ almost surely,\n",
    "\n",
    "$$\n",
    "\\bar{X}_{n}\\overset{a.s.}{\\rightarrow}\\mu \\quad \\text{as} \\ n\\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "- Converge almost surely is a stronger condition than converge in probability.\n",
    "- In some cases, the strong law does not hold, but the weak law does.\n",
    "```\n",
    "\n",
    "## Central Limit Theorem\n",
    "\n",
    "The Central Limit Theorem, in probability theory, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a \"bell curve\") even if the original variables themselves are not normally distributed.\n",
    "\n",
    "There are many versions of CLT with various problem settings. Here we introduce Lindeberg–Lévy CLT.\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ be a sequence of independently and identically distributed random variables such that\n",
    "$\\mathrm{E}\\left( X_{i} \\right)=\\mu$, $\\mathrm{Var}\\left( X_{i} \\right)=\\sigma^{2}>0$. Let $G_{n}(x)$ denote\n",
    "the CDF of $\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}$,\n",
    "then\n",
    "\n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}G_{n}(x)=\\Phi(x)\n",
    "$$\n",
    "\n",
    "i.e., the normalized sample mean converge in distribution to a standard normal random variable,\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\overset{\\mathcal{D}}{\\rightarrow}N(0,1)\n",
    "$$\n",
    "\n",
    "The Central Limit Theorem implies that we can obtain a normal distribution from a uniform random variable generator. Let  $X\\sim U(0,1)$, then $\\mu = \\mathrm{E}\\left( X \\right) =\\frac{1}{2}$, $\\sigma = \\sqrt{\\mathrm{Var}\\left(X \\right)} = \\sqrt{\\frac{1}{12}}$. Hence,\n",
    "\n",
    "$$\n",
    "Y_n=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\overset{\\mathcal{D}}{\\rightarrow}N(0,1)\n",
    "$$\n",
    "\n",
    "Implementation with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "n = 10000 # sample size\n",
    "m = 10000 # number of samples\n",
    "means = np.random.rand(m, n).mean(axis=1)\n",
    "std_means = np.sqrt(n) * (means - 0.5) / np.sqrt(1/12)\n",
    "\n",
    "points = np.linspace(-5, 5, 100)\n",
    "true = norm.pdf(points)\n",
    "\n",
    "plt.hist(std_means, bins='auto', density=True, label='sampling from uniform')\n",
    "plt.plot(points, true, label='true standard normal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-triple",
   "metadata": {},
   "source": [
    "In general, one can then sample from any normal distribution $N(a,b^2)$ by the transformation $Z = bY_n+a$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11,
   100,
   117
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}