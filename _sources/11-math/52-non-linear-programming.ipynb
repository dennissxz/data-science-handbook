{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f3d31f",
   "metadata": {},
   "source": [
    "# Non-linear Programming\n",
    "\n",
    ":::{figure} opt-hierarchy\n",
    "<img src=\"../imgs/opt-hierarchy.png\" width = \"30%\" alt=\"\"/>\n",
    "\n",
    "A hierarchy of convex optimization problems. (LP: linear program, QP: quadratic program, SOCP second-order cone program, SDP: semidefinite program, CP: cone program.) [[Wikipedia](https://en.wikipedia.org/wiki/Convex_optimization)]\n",
    ":::\n",
    "\n",
    "\n",
    "**TBC**\n",
    "\n",
    "Primal and Dual short [intro](https://zhuanlan.zhihu.com/p/46944722).\n",
    "\n",
    "Lagrange multiplier:\n",
    "\n",
    "-   geometric [motivation](https://www.youtube.com/watch?v=yuqB-d5MjZA&t=16s&ab_channel=KhanAcademy) of the method: align tangency of the objective function and the constraints.\n",
    "-   [formulation](https://www.youtube.com/watch?v=hQ4UNu1P2kw&t=311s&ab_channel=KhanAcademy) of Lagrangean $\\mathcal{L}$: combining all equations to $\\nabla\\mathcal{L} = 0$.\n",
    "-   [interpretation](https://www.youtube.com/watch?v=m-G3K2GPmEQ&t=185s&ab_channel=KhanAcademy) and [proof](https://www.youtube.com/watch?v=b9B2FZ5cqbM&ab_channel=KhanAcademy) of the Lagrange multiplier $\\lambda$ as $\\frac{\\partial f}{\\partial c}$, e.g. if budget change, how much will revenue change?\n",
    "\n",
    "(rayleigh-quotient)=\n",
    "## Rayleigh Quotients\n",
    "\n",
    "Consider the following constrained optimization:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max_{\\boldsymbol{x}} && \\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}  & \\\\\n",
    "\\mathrm{s.t.}\n",
    "&& \\left\\| \\boldsymbol{x}  \\right\\|^2 &= 1  \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "An equivalent unconstrained problem is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max_{\\boldsymbol{x} \\ne \\boldsymbol{0}} && \\frac{\\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x} }{\\boldsymbol{x} ^{\\top} \\boldsymbol{x} }  & \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "which makes the objective function invariant to scaling of $\\boldsymbol{x}$. How do we solve this?\n",
    "\n",
    "Definition (Quadratic forms)  \n",
    "Let $\\boldsymbol{A}$ be a symmetric real matrix. A quadratic form corresponding to $\\boldsymbol{A}$ is a function $Q: \\mathbb{R} ^n \\rightarrow \\mathbb{R}$ with\n",
    "\n",
    "$$\n",
    "  Q_{\\boldsymbol{A}}(\\boldsymbol{x}) = \\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}\n",
    "  $$\n",
    "\n",
    "A quadratic form is can be written as a polynomial with terms all of second order\n",
    "\n",
    "$$\n",
    "  \\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}  = \\sum_{i, j=1}^n a_{ij} x_i x_j\n",
    "  $$\n",
    "\n",
    "Definition (Rayleigh quotient)  \n",
    "-   For a fixed symmetric matrix $\\boldsymbol{A}$, the normalized quadratic form $\\frac{\\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x} ^{\\top} \\boldsymbol{x} }$ is called a Rayleigh quotient.\n",
    "    -   In addition, given a positive definite matrix $\\boldsymbol{B}$ of the same size, the quantity $\\frac{\\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x} }{\\boldsymbol{x} ^{\\top} \\boldsymbol{B} \\boldsymbol{x} }$ is called a generalized Rayleigh quotient.\n",
    "\n",
    "Applications  \n",
    "-   PCA: $\\max _{\\boldsymbol{v} \\neq 0} \\frac{\\boldsymbol{v}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{v}}{\\boldsymbol{v}^{\\top} \\boldsymbol{v}}$ where $\\boldsymbol{\\Sigma}$ is a covariance matrix\n",
    "    -   LDA: $\\max _{\\boldsymbol{v} \\neq 0} \\frac{\\boldsymbol{v}^{\\top} \\boldsymbol{S}_{b} \\boldsymbol{v}}{\\boldsymbol{v}^{\\top} \\boldsymbol{S}_{w} \\boldsymbol{v}}$ where $\\boldsymbol{S} _b$ is a between-class scatter matrix, and $\\boldsymbol{S} _w$ is a within-class scatter matrix\n",
    "    -   Spectral clustering (relaxed Ncut): $\\max _{\\boldsymbol{v} \\neq \\boldsymbol{0}} \\frac{\\boldsymbol{v}^{\\top} \\boldsymbol{L} \\boldsymbol{v}}{\\boldsymbol{v}^{\\top} \\boldsymbol{D} \\boldsymbol{v}} \\quad {s.t.} \\boldsymbol{v} ^{\\top} \\boldsymbol{D} \\boldsymbol{1} = 0$ where $\\boldsymbol{L}$ is graph Laplacian and $\\boldsymbol{D}$ is degree matrix.\n",
    "\n",
    "Theorem (Range of Rayleigh quotients)  \n",
    "For any symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R} {n \\times n}$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\max _{\\boldsymbol{x} \\in \\mathbb{R}^{n}: \\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{\\top} \\boldsymbol{x}} &=\\lambda_{\\max } \\\\\n",
    "  \\min _{\\boldsymbol{x} \\in \\mathbb{R}^{n}: \\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{\\top} \\boldsymbol{x}} &=\\lambda_{\\min }\n",
    "  \\end{aligned}$$\n",
    "\n",
    "That is, the largest and the smallest eigenvalues of $\\boldsymbol{A}$ gives the range for the Rayleigh quotient. The maximum and the minimum is attainted when $\\boldsymbol{x}$ is the corresponding eigenvector.\n",
    "\n",
    "In addition, if we add an orthogonal constraint that $\\boldsymbol{x}$ is orthogonal to all the $j$ largest eigenvectors, then\n",
    "\n",
    "$$\n",
    "  \\max _{\\boldsymbol{x} \\in \\mathbb{R}^{n}: \\boldsymbol{x} \\neq \\boldsymbol{0}, \\boldsymbol{x} \\perp \\boldsymbol{v} _1 \\ldots, \\boldsymbol{v} _j} \\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{\\top} \\boldsymbol{x}} =\\lambda_{j+1}\n",
    "  $$\n",
    "\n",
    "and the maximum is achieved when $\\boldsymbol{x} = \\boldsymbol{v} _{j+1}$.\n",
    "\n",
    ":::{admonition,dropdown,seealso} *Proof: Linear algebra approach*\n",
    "\n",
    "Consider EVD of $\\boldsymbol{A}$:\n",
    "\n",
    "$$\n",
    "  \\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}=\\boldsymbol{x}^{\\top}\\left(\\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{\\top}\\right) \\boldsymbol{x}=\\left(\\boldsymbol{x}^{\\top} \\boldsymbol{U}\\right) \\boldsymbol{\\Lambda}\\left(\\boldsymbol{U}^{\\top} \\boldsymbol{x}\\right)=\\boldsymbol{y}^{\\top} \\boldsymbol{\\Lambda} \\boldsymbol{y}\n",
    "  $$\n",
    "\n",
    "where $\\boldsymbol{y} = \\boldsymbol{U} ^{\\top} \\boldsymbol{x}$ is also a unit vector since $\\left\\| \\boldsymbol{y} \\right\\| ^2 = 1$. The original optimization problem becomes\n",
    "\n",
    "$$\n",
    "  \\max _{\\boldsymbol{y} \\in \\mathbb{R}^{n}:\\|\\boldsymbol{y}\\|=1} \\quad \\boldsymbol{y}^{\\top} \\underbrace{\\boldsymbol{\\Lambda}}_{\\text {diagonal }} \\boldsymbol{y}\n",
    "  $$\n",
    "\n",
    "Note that the objective and constraint can be written as a weighted sum of eigenvalues\n",
    "\n",
    "$$\n",
    "  \\boldsymbol{y}^{\\top} \\boldsymbol{\\Lambda} \\boldsymbol{y}=\\sum_{i=1}^{n} \\underbrace{\\lambda_{i}}_{\\text {fixed }} y_{i}^{2} \\quad \\text { (subject to } y_{1}^{2}+y_{2}^{2}+\\cdots+y_{n}^{2}=1)\n",
    "  $$\n",
    "\n",
    "Let $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_n$, then when $y_1^2 = 1$ and $y_2^2 = \\ldots = y_n ^2 = 0$, the objective function attains its maximum $\\boldsymbol{y} ^{\\top} \\boldsymbol{\\Lambda} \\boldsymbol{y} = \\lambda_1$. In terms of $\\boldsymbol{x}$, the maximizer is\n",
    "\n",
    "$$\n",
    "  \\boldsymbol{x} ^* = \\boldsymbol{U} \\boldsymbol{y} ^* = \\boldsymbol{U} (\\pm \\boldsymbol{e} _1) = \\pm \\boldsymbol{u}_1   \n",
    "  $$\n",
    "\n",
    "In conclusion, when $\\boldsymbol{x} = \\pm \\boldsymbol{u} _1$, i.e. the largest eigenvector, $\\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}$ attains its maximum value $\\lambda_1$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{admonition,dropdown,seealso} *Proof: Multivariable calculus approach*\n",
    "\n",
    "Alternatively, we can use the Method of Lagrange Multipliers to prove the theorem. First, we form the Lagrangian function\n",
    "\n",
    "    $$\n",
    "    L(\\boldsymbol{x}, \\lambda)=\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}-\\lambda\\left(\\|\\boldsymbol{x}\\|^{2}-1\\right)\n",
    "    $$\n",
    "\n",
    "    Differentiation gives\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial \\boldsymbol{x}} &=2 \\boldsymbol{A} \\boldsymbol{x}-\\lambda(2 \\boldsymbol{x})=0 & \\longrightarrow & \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x} \\\\\n",
    "    \\frac{\\partial L}{\\partial \\lambda} &=\\|\\boldsymbol{x}\\|^{2}-1=0 & \\longrightarrow &\\|\\boldsymbol{x}\\|^{2}=1\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    This implies that $\\boldsymbol{x}$ and $\\lambda$ must be an eigenpair of $\\boldsymbol{A}$. Moreover, for any solution $\\lambda=\\lambda_{i}, \\boldsymbol{x}=\\boldsymbol{v}_{i}$, the objective function takes the value\n",
    "\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{v}_{i}^{\\top} \\boldsymbol{A} \\boldsymbol{v}_{i}=\\boldsymbol{v}_{i}^{\\top}\\left(\\lambda_{i} \\boldsymbol{v}_{i}\\right)=\\lambda_{i}\\left\\|\\boldsymbol{v}_{i}\\right\\|^{2}=\\lambda_{i}\n",
    "    $$\n",
    "\n",
    "    Therefore, the eigenvector $\\boldsymbol{v} _1$ (corresponding to largest eigenvalue $\\lambda_1$ of $\\boldsymbol{A}$) is the global maximizer, and it yields the absolute maximum value $\\lambda_1$.\n",
    "\n",
    ":::\n",
    "\n",
    "Corollary (Generalized Rayleigh quotient problem)  \n",
    "For the generalized Rayleigh quotient $\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{\\top} \\boldsymbol{B} \\boldsymbol{x}}$, the smallest and largest values $\\lambda$ satisfy\n",
    "\n",
    "$$\n",
    "  \\boldsymbol{A v}=\\lambda \\boldsymbol{B v} \\quad \\Longleftrightarrow \\quad \\boldsymbol{B}^{-1} \\boldsymbol{A v}=\\lambda \\boldsymbol{v}\n",
    "  $$\n",
    "\n",
    "That is, the smallest/largest quotient value equals the smallest/largest eigenvalue of $(\\boldsymbol{B} ^{-1} \\boldsymbol{A})$. The left equation is called a generalized eigenvalue problem.\n",
    "\n",
    ":::{admonition,dropdown,seealso} *Proof*\n",
    "\n",
    "-   Substitution approach\n",
    "\n",
    "    Since $\\boldsymbol{B}$ is p.d., we have $\\boldsymbol{B} ^{1/2}$. Let $\\boldsymbol{y} = \\boldsymbol{B} ^{1/2}\\boldsymbol{x}$, then the denominator can be written as\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{x}^{\\top} \\boldsymbol{B} \\boldsymbol{x}=\\boldsymbol{x}^{\\top} \\boldsymbol{B}^{1 / 2} \\boldsymbol{B}^{1 / 2} \\boldsymbol{x}=\\boldsymbol{y}^{\\top} \\boldsymbol{y}\n",
    "    $$\n",
    "\n",
    "    Substitute $\\boldsymbol{x}=\\left(\\boldsymbol{B}^{1 / 2}\\right)^{-1} \\boldsymbol{y} \\stackrel{\\text { denote }}{=} \\boldsymbol{B}^{-1 / 2} \\boldsymbol{y}$ y into the numerator to rewrite it\n",
    "    in terms of the new variable $\\boldsymbol{y}$. This will convert the generalized Rayleigh\n",
    "    quotient problem back to a regular Rayleigh quotient problem, which has\n",
    "    been solved.\n",
    "\n",
    "    $$\n",
    "    \\frac{\\boldsymbol{y} \\boldsymbol{B} ^{-1/2} \\boldsymbol{A} \\boldsymbol{B} ^{-1/2} \\boldsymbol{y} }{\\boldsymbol{y} ^{\\top} \\boldsymbol{y}}\n",
    "    $$\n",
    "\n",
    "-   Lagrange multipliers:\n",
    "\n",
    "    $$\n",
    "    \\max _{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{\\top} \\boldsymbol{B} \\boldsymbol{x}}\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\max _{\\boldsymbol{x} \\in \\mathbb{R}^{n}} \\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x} \\quad \\text { subject to } \\boldsymbol{x}^{\\top} \\boldsymbol{B} \\boldsymbol{x}=1\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    L(\\boldsymbol{x}, \\lambda)=\\boldsymbol{x}^{\\top} \\boldsymbol{A} \\boldsymbol{x}-\\lambda\\left(\\boldsymbol{x}^{\\top} \\boldsymbol{B} \\boldsymbol{x}-1\\right)\n",
    "    $$\n",
    "\n",
    "    Then\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial \\boldsymbol{x}} &=2 \\boldsymbol{A} \\boldsymbol{x}-2\\lambda \\boldsymbol{B}  \\boldsymbol{x}=0 & \\longrightarrow & \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{B}\\boldsymbol{x} \\\\\n",
    "    \\frac{\\partial L}{\\partial \\lambda} &=0 & \\longrightarrow & \\boldsymbol{x} ^{\\top} \\boldsymbol{B} \\boldsymbol{x} =1\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    ":::\n",
    "\n",
    "reference: [notes](https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec4RayleighQuotient.pdf)\n",
    "\n",
    "\n",
    "## Semi-definite Programming\n",
    "\n",
    "We introduce semi-definite programming. Then use it solve max-cut problem, and analyze its performance for min-cut problem over stochastic block model.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "A linear programming problem is one in which we wish to maximize or minimize a **linear** objective function of real variables over a polytope. In semidefinite programming, we instead use **real-valued vectors** and are allowed to take the **dot product** of vectors. In general, a SDP has a form\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\min _{\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{n} \\in \\mathbb{R}_{n}}& \\sum_{i, j \\in[n]} c_{ij} \\langle \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\rangle \\\\\n",
    "\\text {s.t.} &\\sum_{i, j \\in[n]} a_{ijk}\\langle \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\rangle \\leq b_{k} \\text { for all } k\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "The array of real variables in LP is then replaced by an array of vector variables, which form a matrix variable. By using this notation, the problem can be written as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\boldsymbol{X} \\in \\mathbb{R}^{n \\times n}}\\ &\\langle \\boldsymbol{C} , \\boldsymbol{X} \\rangle\\\\\n",
    "\\text {s.t.}\\ & \\left\\langle \\boldsymbol{A} _{k}, \\boldsymbol{X} \\right\\rangle\\leq b_{k}, \\quad k=1, \\ldots, m \\\\\n",
    "& X_{ij} = \\boldsymbol{x}_i ^{\\top} \\boldsymbol{x} _j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C_{ij} = (c_{ij} + c_{ji})/2, A_{ij}^{(k)} = (a_{ijk} + a_{jik})/2$ and $\\langle \\boldsymbol{P}, \\boldsymbol{Q} \\rangle = \\operatorname{tr}\\left(\\boldsymbol{P}  ^{\\top} \\boldsymbol{Q} \\right) = \\sum_{i,j}^n p_{ij}q_{ij}$ is the [Frobenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product).\n",
    "\n",
    "Note that an $n \\times n$ matrix $\\boldsymbol{M}$ is said to be positive semidefinite if it is the Gramian matrix of some vectors (i.e. if there exist vectors $\\boldsymbol{v} _1, \\ldots, \\boldsymbol{v} _n$ such that $M_{ij} = \\langle \\boldsymbol{v} _i, \\boldsymbol{v} _j \\rangle$ for all $i,j$). Hence, the last constraint is just $\\boldsymbol{X} \\succeq \\boldsymbol{0}$. That is, the nonnegativity constraints on real variables in LP are replaced by semidefiniteness constraints on matrix variables in SDP.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\boldsymbol{X} \\succeq \\boldsymbol{0}}\\ &\\langle \\boldsymbol{C} , \\boldsymbol{X} \\rangle\\\\\n",
    "\\text {s.t.}\\ & \\left\\langle \\boldsymbol{A} _{k}, \\boldsymbol{X} \\right\\rangle\\leq b_{k}, \\quad k=1, \\ldots, m \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All linear programs can be expressed as SDPs. SDPs are in fact a special case of cone programming and can be efficiently solved by interior point methods. Given the solution $\\boldsymbol{X}^*$ to the SDP in the standard form, the vectors $\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v} _n$ can be recovered in $\\mathcal{O} (n^3)$ time, e.g. by using an incomplete Cholesky decomposition of $\\boldsymbol{X}^* = \\boldsymbol{V} ^{\\top} \\boldsymbol{V}$ where $\\boldsymbol{V} = [\\boldsymbol{v} _1, \\ldots \\boldsymbol{v} _n]$.\n",
    "\n",
    "(sdp-rq)=\n",
    "### From Rayleigh Quotient\n",
    "\n",
    "Recall that an Rayleigh quotient can be formulated as\n",
    "\n",
    "$$\n",
    "\\max\\  \\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}  \\qquad \\mathrm{s.t.}\\ \\boldsymbol{x} ^{\\top} \\boldsymbol{x} = 1\n",
    "$$\n",
    "\n",
    "Since both the objective and the constraint are scalar valued, we can re-write them using trace\n",
    "\n",
    "$$\n",
    "\\max\\  \\operatorname{tr}\\left( \\boldsymbol{x} ^{\\top} \\boldsymbol{A} \\boldsymbol{x}  \\right) \\qquad \\mathrm{s.t.}\\ \\operatorname{tr}\\left( \\boldsymbol{x} ^{\\top} \\boldsymbol{x} \\right) = 1\n",
    "$$\n",
    "\n",
    "Then, by the property of trace, we have\n",
    "\n",
    "$$\n",
    "\\max\\  \\operatorname{tr}\\left(\\boldsymbol{A} \\boldsymbol{x} \\boldsymbol{x} ^{\\top}  \\right) \\qquad \\mathrm{s.t.}\\ \\operatorname{tr}\\left( \\boldsymbol{x} \\boldsymbol{x} ^{\\top} \\right) = 1\n",
    "$$\n",
    "\n",
    "Let $\\boldsymbol{X} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top}$, then the variable $\\boldsymbol{x}$ can be replaced by a rank-1 matrix $\\boldsymbol{X} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\max\\  \\operatorname{tr}\\left(\\boldsymbol{A} \\boldsymbol{X} \\right) \\qquad \\mathrm{s.t.}\\ \\operatorname{tr}\\left( \\boldsymbol{X}  \\right) = 1, \\boldsymbol{X} = x\n",
    "\\boldsymbol{x} ^{\\top}\n",
    "$$\n",
    "\n",
    "Note that $\\boldsymbol{X} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top}$ if and only if $\\operatorname{rank}\\left( \\boldsymbol{X}  \\right)=1$ and $\\boldsymbol{X} \\succeq \\boldsymbol{0}$, hence the constraints are equivalent to.\n",
    "\n",
    "\n",
    "$$\n",
    "\\max\\  \\operatorname{tr}\\left(\\boldsymbol{A} \\boldsymbol{X} \\right) \\qquad \\mathrm{s.t.}\\ \\operatorname{tr}\\left( \\boldsymbol{X}  \\right) = 1, \\operatorname{rank}\\left( \\boldsymbol{X}  \\right)=1, \\boldsymbol{X} \\succeq 0 \\qquad (RQ)\n",
    "$$\n",
    "\n",
    "Call this problem RQ, if we drop the rank 1 constraint, then this would be a semidefinite program, where $\\operatorname{tr}\\left( \\boldsymbol{X} \\right)$ can be viewed as $\\langle \\boldsymbol{I} , \\boldsymbol{X} \\rangle = 1$.\n",
    "\n",
    "$$\n",
    "\\max\\  \\operatorname{tr}\\left(\\boldsymbol{A} \\boldsymbol{X} \\right) \\qquad \\mathrm{s.t.}\\ \\operatorname{tr}\\left( \\boldsymbol{X}  \\right) = 1, \\boldsymbol{X} \\succeq 0 \\qquad (SDP)\n",
    "$$\n",
    "\n",
    "In fact, any optimal solution $\\boldsymbol{X} _{SDP}^*$ to this SDP can always be convert to an rank-1 matrix $\\boldsymbol{X} _{RQ}^*$ with the same optimal value. Hence, solving the SDP is equivalent to solving the RQ.\n",
    "\n",
    ":::{admonition,dropdown,seealso} *Proof*\n",
    "\n",
    "In SDP, since $\\boldsymbol{X} \\succeq \\boldsymbol{0}$, let its EVD be $\\boldsymbol{X} = \\sum_{i=1}^n \\lambda_i \\boldsymbol{u}_i \\boldsymbol{u}_i ^{\\top}$, then the objective function is\n",
    "\n",
    "$$\n",
    "\\operatorname{tr}\\left( \\boldsymbol{A} \\sum_{i=1}^n \\lambda_i \\boldsymbol{u} _i \\boldsymbol{u} _i ^{\\top} \\right) = \\sum_{i=1}^n \\lambda_i \\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{u} _i \\boldsymbol{u} _i ^{\\top} \\right)\n",
    "$$\n",
    "\n",
    "Note that $1 = \\operatorname{tr}\\left( \\boldsymbol{X} \\right) = \\sum_{i=1}^n \\lambda_i$ and $\\lambda_i \\ge 0$. Thus, the objective function is a convex combination of $\\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{u} _i \\boldsymbol{u} _i ^{\\top} \\right)$. Hence, for any feasible solution $\\boldsymbol{X} _{SDP} =\\sum_{i=1}^n \\lambda_i \\boldsymbol{u}_i \\boldsymbol{u}_i ^{\\top}$, we can formulate another rank-1 feasible solution, by selecting $j = \\arg\\max_i \\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{u} _i \\boldsymbol{u} _i ^{\\top} \\right)$, and setting $\\lambda_j=1$ while $\\lambda_{-j}=0$. The rank-1 feasible solution is then $\\boldsymbol{X} _{RQ} = \\boldsymbol{u}_j \\boldsymbol{u}_j ^{\\top}$, and has a better objective value $\\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{X} _{RQ} \\right) \\ge \\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{X} _{SDP}\\right)$. Therefore, for any optimal solution $\\boldsymbol{X} _{SDP}^*$, we can find a rank-1 matrix $\\boldsymbol{X} _{RQ}^*$, such that it is still optimal, since $\\operatorname{tr}(\\boldsymbol{A} \\boldsymbol{X} _{SDP}^* ) = \\operatorname{tr}(\\boldsymbol{A} \\boldsymbol{X} _{RQ}^* )$.\n",
    "\n",
    "In this way, the optimal value is then\n",
    "\n",
    "$$\\max \\operatorname{tr}\\left( \\boldsymbol{A} \\boldsymbol{u} _j \\boldsymbol{u} _j ^{\\top} \\right) = \\max \\boldsymbol{u} _j ^{\\top} \\boldsymbol{A} \\boldsymbol{u} _j = \\lambda_{\\max}(\\boldsymbol{A})$$\n",
    "\n",
    "where the eigenvector $\\left\\| \\boldsymbol{u} \\right\\| =1$. This goes back to the formulation of Rayleigh quotient.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "### Max-cut Problem\n",
    "\n",
    "In a graph $G = (V, E)$ with edge weights $\\boldsymbol{W}$, we want to find a maximum bisection cut\n",
    "\n",
    "$$\n",
    "\\operatorname{cut} (\\boldsymbol{W}) = \\max_{\\boldsymbol{x} \\in \\left\\{ \\pm 1 \\right\\}^n} \\frac{1}{2} \\sum_{i,j=1}^n w_{ij} (1 - x_i x_j)\n",
    "$$\n",
    "\n",
    "where the product $x_i x_j$ is an indicator variable that equals 1 if two vertices $i, j$ are in the same part, and $-1$ otherwise. Hence, the summation only involves the case when $x_i x_j = -1$, and we divide it by half since $(1- x_i x_j)=2$.\n",
    "\n",
    "Denote $\\boldsymbol{X} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top} \\in \\mathbb{R} ^{n \\times n}$. Then the domain $\\boldsymbol{x} \\in \\left\\{ \\pm 1 \\right\\}^n$ is bijective to $\\Omega = \\left\\{ \\boldsymbol{X} \\in \\mathbb{R} ^{n \\times n} : \\boldsymbol{X} \\succeq 0, X_{ii}=1, \\operatorname{rank}\\left( \\boldsymbol{X} \\right) = 1 \\right\\}$. The optimization problem can then be expressed as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\operatorname{cut} (\\boldsymbol{W})\n",
    "&= \\max_{\\boldsymbol{X} \\in \\Omega} \\frac{1}{2}  \\operatorname{tr}\\left( \\boldsymbol{W} (\\boldsymbol{1} \\boldsymbol{1} ^{\\top}  - \\boldsymbol{X}) \\right) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Solving this integer optimization is NP-hard. We work with relaxation of $\\Omega$.\n",
    "\n",
    "\n",
    "### Relaxation\n",
    "\n",
    "#### Spectral Relaxation\n",
    "\n",
    "Two relaxations:\n",
    "- drop the rank 1-constraint $\\operatorname{rank}\\left( \\boldsymbol{X} \\right) = 1$\n",
    "- replace the diagonal constraint $X_{ii}=1$ by $\\operatorname{tr}\\left( \\boldsymbol{X} \\right) = n$\n",
    "\n",
    "To solve it, it is equivalent to solve\n",
    "\n",
    "$$\n",
    "\\min\\ \\operatorname{tr}\\left(\\boldsymbol{W}\\boldsymbol{X} \\right) \\qquad \\text{s.t. }\\boldsymbol{X} \\succeq \\boldsymbol{0} , \\operatorname{tr}\\left( \\boldsymbol{X} \\right) = n\n",
    "$$\n",
    "\n",
    "As proved [above](sdp-rq), this SDP can be solved by solving a Rayleigh quotient problem\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{y}}\\ \\boldsymbol{y} ^{\\top} \\boldsymbol{W} \\boldsymbol{y} \\qquad \\text{s.t.}  \\left\\| \\boldsymbol{y}  \\right\\| = \\sqrt{n}\n",
    "$$\n",
    "\n",
    "The optimal solution $\\boldsymbol{y} ^*$ is the last eigenvector of $\\boldsymbol{W}$, and the objective value is the last eigenvalue of $\\boldsymbol{W}$. The solution to the SDP problem is then $\\boldsymbol{X} ^* = \\boldsymbol{y}^* \\boldsymbol{y} ^{*\\top}$, with the same objective value. We can then round $\\boldsymbol{y} ^*$ by its sign to decide partition assignment.\n",
    "\n",
    "\n",
    "#### SDP Relaxation\n",
    "\n",
    "Only one relaxation: drop the rank-1 constraint, which is non-convex. The remaining two constraints forms a domain\n",
    "\n",
    "$$\\Omega_{SDP} = \\left\\{ \\boldsymbol{X} \\in \\mathbb{R} ^{n \\times n}:  \\boldsymbol{X} \\succeq 0, X_{ii}=1 \\right\\}$$\n",
    "\n",
    "The optimization problem is then\n",
    "\n",
    "$$\n",
    "\\operatorname{SDP} (\\boldsymbol{W}) = \\max_{\\boldsymbol{X} \\in \\Omega_{SDP}} \\frac{1}{2}  \\sum_{i,j=1}^n w_{ij} (1 - X_{ij})\n",
    "$$\n",
    "\n",
    "Note that after we drop the rank-1 constraint,\n",
    "- $\\operatorname{SDP}(\\boldsymbol{W} ) \\ge \\operatorname{cut} (\\boldsymbol{W})$.\n",
    "- The solution $\\boldsymbol{X} ^*$ to $\\operatorname{SDP} (\\boldsymbol{W})$ may not be rank-1. If it is rank-1 then the above equality holds.\n",
    "\n",
    "It remains to solve $\\operatorname{SDP} (\\boldsymbol{W})$ for $\\boldsymbol{X}$, and then find a way to obtain the binary partition assignment $\\hat{\\boldsymbol{x}} \\in \\left\\{ \\pm 1 \\right\\}^n$ from $\\boldsymbol{X}$.\n",
    "\n",
    ":::{admonition,note,dropdown} Lifting\n",
    "\n",
    "$\\Omega_{SDP}$ is equivalent to $\\left\\{\\boldsymbol{X} \\in \\mathbb{R} ^{n \\times n}: \\boldsymbol{X} = \\boldsymbol{V} ^{\\top} \\boldsymbol{V}, \\left\\| \\boldsymbol{v} _i \\right\\| ^2 = 1 \\right\\}$. In this way, we convert a scalar constraint $X_{ii}$ to a vector constraint $\\left\\| \\boldsymbol{v} _i \\right\\| =1$, aka ‘lifting’.\n",
    "\n",
    ":::\n",
    "\n",
    "### Random Rounding\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "-   Solve $\\operatorname{SDP} (\\boldsymbol{W})$ and obtain $\\hat{\\boldsymbol{X}}$\n",
    "-   Decompose $\\hat{\\boldsymbol{X}} = \\hat{\\boldsymbol{V}} ^{\\top} \\boldsymbol{\\hat{V}}$, e.g. using EVD $\\boldsymbol{\\hat{X}} ^{\\top} = \\boldsymbol{\\hat{U}} \\sqrt{\\boldsymbol{\\hat{\\Lambda}}}$, or using Cholesky. Note that $\\left\\| \\boldsymbol{\\hat{v}} _i \\right\\| =1$ always holds, due to the constraint $\\hat{X}_{ii}=1$.\n",
    "-   Sample a direction $\\boldsymbol{r}$ uniformly from $S^{p-1}$\n",
    "-   Return binary partition assignment $\\hat{\\boldsymbol{x}} = \\operatorname{sign} (\\boldsymbol{\\hat{V}} ^{\\top} \\boldsymbol{r} )$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "- Randomly sample a hyperplane in $\\mathbb{R} ^n$ characterized by vector $\\boldsymbol{r}$. If $\\hat{\\boldsymbol{v}} _i$ lies on the same side of the hyperplane with $\\boldsymbol{r}$, then set $\\hat{x}_i =1$, else $\\hat{x}_i = -1$.\n",
    "- If there indeed exists a partition $I$ and $J$ of vertices characterizedd by $\\boldsymbol{x}$, then the two groups of directions $\\boldsymbol{v} _i$’s and $\\boldsymbol{v} _j$’s should point to opposite direction since $\\boldsymbol{v} _i ^{\\top} \\boldsymbol{v} _j = x_i x_j = -1$. After random rounding, they should be well separated. Hence, if $\\hat{\\boldsymbol{v}}_i ^{\\top} \\hat{\\boldsymbol{v} }_j$ recovers $\\boldsymbol{v}_i ^{* \\top} \\boldsymbol{v}^* _j$ well enough, then $\\hat{\\boldsymbol{x}}$ well recovers $\\boldsymbol{x}^*$, the optimal max-cut in $\\operatorname{cut}(\\boldsymbol{W})$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\boldsymbol{X} ^* \\text{ to }  \\operatorname{cut} \\ & \\Leftrightarrow \\quad  \\boldsymbol{x} ^* \\text{ to } \\operatorname{cut}\\\\\n",
    "\\text{SDP relaxation}\\quad \\downarrow \\quad  &\\qquad \\qquad \\uparrow \\text{recover} \\\\\n",
    "\\boldsymbol{\\hat{X}} \\text{ to } \\operatorname{SDP} &  \\xrightarrow[\\text{rounding} ]{\\text{random} } \\hat{\\boldsymbol{x}} = \\operatorname{sign} (\\boldsymbol{\\hat{V}} ^{\\top} \\boldsymbol{r} )\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "To see how $\\boldsymbol{V}$ looks like for $\\boldsymbol{x} \\in \\left\\{ \\pm 1 \\right\\} ^n$, let $\\boldsymbol{x} = [1, 1, -1, -1, -1]$, then\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left[\\begin{array}{rrrrr}\n",
    "1 & 1 & -1 & -1 & -1 \\\\\n",
    "1 & 1 & -1 & -1 & -1 \\\\\n",
    "-1 & -1 & 1 & 1 & 1 \\\\\n",
    "-1 & -1 & 1 & 1 & 1 \\\\\n",
    "-1 & -1 & 1 & 1 & 1\n",
    "\\end{array}\\right],\\qquad \\boldsymbol{V} = \\left[ \\begin{array}{rrrrr}\n",
    "-1 & -1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "<!--\n",
    "```{code-cell} R\n",
    ":tags: [hide-input]\n",
    "x = c(rep(1, 2), rep(-1, 3))\n",
    "X = x %*% t(x)\n",
    "Lam = diag(eigen(X)$value)\n",
    "U = eigen(X)$vectors\n",
    "V = t(U %*% sqrt(Lam))\n",
    "print('X:')\n",
    "print(X)\n",
    "print('V:')\n",
    "print(round(V, 2))\n",
    "```\n",
    " -->\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "How well the algorithm does, in expectation? We define Geomans-Williams quantity, which is the expected cut value returned by the algorithm, where randomness comes from random direction $\\boldsymbol{r}$.\n",
    "\n",
    "$$\\operatorname{GW} (\\boldsymbol{W}) = \\mathbb{E}_{\\boldsymbol{r}} [f_{\\boldsymbol{W} }(\\hat{\\boldsymbol{x}}) ] = \\mathbb{E}_{\\boldsymbol{r}}  \\left[ \\frac{1}{2} \\sum_{i,j=1}^n w_{ij} (1 - \\hat{x}_i \\hat{x}_j)\\right]$$\n",
    "\n",
    "Obviously $\\operatorname{GW} (\\boldsymbol{W}) \\le \\operatorname{cut} (\\boldsymbol{W})$, since we are averaging the value of feasible solutions $\\hat{\\boldsymbol{x}}$ in $\\left\\{ \\pm 1\\right\\}^n$, and each of them is $\\le \\operatorname{cut} (\\boldsymbol{W})$. But how small can $\\operatorname{GW} (\\boldsymbol{W})$ be?\n",
    "\n",
    "It can be shown that $\\operatorname{GW}(\\boldsymbol{W}) \\ge \\alpha \\operatorname{cut}(\\boldsymbol{W})$ where $\\alpha \\approx 0.87$. That is, the random rounding algorithm return a cut value not too small than the optimal value, in expectation.\n",
    "\n",
    "::::{admonition,dropdown,seealso} *Proof*\n",
    "\n",
    "We randomly sample direction $\\boldsymbol{r}$ from unit sphere $S^{p-1}$. If $\\boldsymbol{v} _i$ and $\\boldsymbol{v} _j$ lie on different side of hyperplane characterized by $\\boldsymbol{r}$, we call such direction 'good'.\n",
    "\n",
    "In $p=2$ case, we sample from a unit circle. All good $\\boldsymbol{r}$ lie on two arcs on the circle, whose length are related to the angle between $\\boldsymbol{v} _i$ and $\\boldsymbol{v} _j$, denoted $\\theta$. The probability of sampling good equals the ratio between the total length of the two arcs and the circumference. Thus,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E} \\left[ \\frac{1}{2}(1 - \\hat{x}_i \\hat{x}_j) \\right]\n",
    "&= \\mathbb{P} (\\hat{x}_i \\hat{x}_j = -1) \\\\\n",
    "&= \\mathbb{P} (\\boldsymbol{v} _i, \\boldsymbol{v} _j \\text{ lie on different side of hyperplane}) \\\\\n",
    "&= \\frac{\\theta}{2 \\pi} \\times 2  \\\\\n",
    "&= \\frac{\\arccos (\\boldsymbol{v} _i ^{\\top} \\boldsymbol{v} _j)}{\\pi} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "In $p = 3$ case, we sample $\\boldsymbol{r}$ from a unit sphere. All good $\\boldsymbol{r}$ lie on two [spherical wedges](https://en.wikipedia.org/wiki/Spherical_wedge), with angle $\\theta$. An example is given below. The ratio between the area of each spherical wedge and the area of the sphere is $\\theta/2\\pi$.\n",
    "\n",
    ":::{figure} max-cut-gw-3d\n",
    "<img src=\"../imgs/max-cut-gw-3d.png\" width = \"70%\" alt=\"\"/>\n",
    "\n",
    "Two vectors $\\boldsymbol{v}_1, \\boldsymbol{v} _2$ (red, green) and random directions $\\boldsymbol{r}$ (blue) from unit sphere whose corresponding hyperplane separates the two vectors.\n",
    ":::\n",
    "\n",
    "Now we compare $\\operatorname{GW}(\\boldsymbol{W}) = \\sum_{i,j}^n w_{ij} \\frac{1}{\\pi}\\arccos (\\boldsymbol{v} _i ^{\\top} \\boldsymbol{v} _j)$ and $\\operatorname{SDP} (\\boldsymbol{W}) = \\sum_{i,j}^n w_{ij} \\frac{1}{2} (1 - \\boldsymbol{v} _i ^{\\top} \\boldsymbol{v} _j)$.\n",
    "\n",
    "Let’s first see two functions $f(y) = \\frac{1}{\\pi}\\arccos(y)$ and $g(y) = \\frac{1}{2}(1-y)$ for $-1 \\le y \\le 1$. Let $\\alpha = \\min_{-1 \\le y \\le 1} \\frac{f(y)}{g(y)}$, then it is easy to find $\\alpha \\approx 0.87$.\n",
    "\n",
    ":::{figure} max-cut-gw-thm\n",
    "<img src=\"../imgs/max-cut-gw-thm.png\" width = \"50%\" alt=\"\"/>\n",
    "\n",
    "Plots of $f(y)$ and $g(y)$\n",
    ":::\n",
    "\n",
    "Therefore, let $y_{ij} = \\boldsymbol{v} _i \\boldsymbol{v} _j$, since $w_{ij} \\ge 0$, we have $\\operatorname{GW} (\\boldsymbol{W}) \\ge \\alpha \\operatorname{SDP} (\\boldsymbol{W})$. Using the SDP relaxation inequality $\\operatorname{SDP} (\\boldsymbol{W}) \\ge \\operatorname{cut} (\\boldsymbol{W})$, we have $\\operatorname{GW} (\\boldsymbol{W}) \\ge \\alpha \\operatorname{cut} (\\boldsymbol{W})$.\n",
    "\n",
    "Note that we require $w_{ij} \\ge 0$.\n",
    "\n",
    "::::\n",
    "\n",
    "<!--\n",
    "```{code-cell} python\n",
    ":tags: [hide-input]\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.offline as py\n",
    "\n",
    "pio.renderers.default = \"png\"\n",
    "\n",
    "v = np.array([[1,0,3], [-1,0,3]])\n",
    "v = v/np.linalg.norm(v, 2, axis=1)[:, np.newaxis]\n",
    "n = 3000\n",
    "np.random.seed(1)\n",
    "x = np.random.normal(size=(n,3))\n",
    "x = x / np.linalg.norm(x, 2, axis=1)[:, np.newaxis]\n",
    "x = x[np.dot(x,v[0])*np.dot(x,v[1]) <0, :]\n",
    "print(f'v1 = {np.round(v[0],3)}, \\nv2 = {np.round(v[1],3)}')\n",
    "print(f'arccos(v1,v2)/pi = {np.round(np.arccos(v[0] @ v[1].T)/np.pi,3)}')\n",
    "print(f'simulated result = {np.round(len(x)/n,3)}')\n",
    "fig = px.scatter_3d(x=x[:,0], y=x[:,1], z=x[:,2], size=np.ones(len(x)), range_z=[-1,1])\n",
    "fig.add_scatter3d(x=[0, v[0,0]], y=[0, v[0,1]], z=[0, v[0,2]], name='v1')\n",
    "fig.add_scatter3d(x=[0, v[1,0]], y=[0, v[1,1]], z=[0, v[1,2]], name='v2')\n",
    "fig.show()\n",
    "``` -->\n",
    "\n",
    "How large can the SDP relaxation value $\\operatorname{SDP} (\\boldsymbol{W})$ be? **Grothendieck’s Inequality** says\n",
    "\n",
    "$$\n",
    "K \\operatorname{cut}(\\boldsymbol{W})   \\ge \\operatorname{SDP}(\\boldsymbol{W})\n",
    "$$\n",
    "\n",
    "where $K \\approx 1.7$. Hence, the SDP relaxation $\\Omega_{SDP}$ does not relax the original domain $\\Omega$ too much (otherwise we may see $\\operatorname{SDP}(\\boldsymbol{W}) \\gg \\operatorname{cut}(\\boldsymbol{W})$). Hence $\\hat{\\boldsymbol{v}}_i ^{\\top} \\boldsymbol{\\hat{v}} _j$ should recover binary $x_i^* x_j^*$ well.\n",
    "\n",
    "\n",
    "### For SBM\n",
    "\n",
    "The above inequalities applies to any problem instance $G=(V, E, \\boldsymbol{W})$. It may give too generous or useless guarantee for some particular model. Let’s see its performance in [stochastic block models](stochastic-block-models).\n",
    "\n",
    "We work with the mean-shifted matrix $\\boldsymbol{B} = 2\\boldsymbol{A} - \\boldsymbol{1} \\boldsymbol{1} ^{\\top}$, where\n",
    "\n",
    "$$\n",
    "b_{ij} = \\left\\{\\begin{array}{ll}\n",
    "1, & \\text { if } a_{ij}=1 \\\\\n",
    "-1, & \\text { if } a_{ij}=0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Essentially, $\\boldsymbol{B}$ just re-codes the connectivity in $G$ from 1/0 to 1/-1. Note that in SBM, $\\boldsymbol{B}$ is random, depending on parameters $p$ and $q$. In the perfect case, if $p=1, q=0$, then we can tell cluster label $\\boldsymbol{x}\\in \\left\\{ \\pm 1 \\right\\}^n$ directly from $\\boldsymbol{B}$, which can be expressed exactly as $\\boldsymbol{B} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top}$. In general cases, $\\boldsymbol{B}$ cannot be expressed as $\\boldsymbol{x} \\boldsymbol{x} ^{\\top}$ for some $\\boldsymbol{x} \\in \\left\\{ \\pm 1 \\right\\}^n$. We in turn want to find some $\\boldsymbol{X} = \\boldsymbol{x} \\boldsymbol{x} ^{\\top}$ that is close enough to $\\boldsymbol{B}$, and then use $\\boldsymbol{x}$ as the approximated cluster label.\n",
    "\n",
    "Similar to the max-cut case, we apply SDP relaxation that drops the rank-1 constraint to $\\boldsymbol{X}$. The SDP problem is then\n",
    "\n",
    "$$\n",
    "\\max\\ \\operatorname{tr}\\left( \\boldsymbol{B} \\boldsymbol{X} \\right) \\qquad \\text{s.t. } \\boldsymbol{X} \\succeq 0, X_{ii}=1\n",
    "$$\n",
    "\n",
    "Note $\\operatorname{tr}\\left( \\boldsymbol{B} \\boldsymbol{X} \\right) = \\langle \\boldsymbol{B} , \\boldsymbol{X} \\rangle = \\sum_{i,j}^n b_{ij} x_{ij}$. Next, we show that the solution to the above problem $\\hat{\\boldsymbol{X}}$ is exactly rank-1, even we've dropped the rank-1 constraint.\n",
    "\n",
    ":::{admonition,dropdown,seealso} *Proof*\n",
    "\n",
    "First we convert it to an equivalent minimization problem\n",
    "\n",
    "$$\\min\\ - \\langle  \\boldsymbol{B}, \\boldsymbol{X} \\rangle \\qquad \\text{s.t. } \\boldsymbol{X} \\succeq 0, X_{ii}=1$$\n",
    "\n",
    "The Lagrangean is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} (\\boldsymbol{X} ; \\boldsymbol{z} , \\boldsymbol{\\Lambda})= - \\langle \\boldsymbol{B} , \\boldsymbol{X} \\rangle - \\langle \\boldsymbol{z} , \\operatorname{diag}\\left( \\boldsymbol{X}  \\right)  - \\boldsymbol{1}\\rangle - \\langle \\boldsymbol{\\Lambda} , \\boldsymbol{X} \\rangle\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{z} \\in \\mathbb{R} ^n$ and $\\boldsymbol{\\Lambda} \\succeq \\boldsymbol{0}$ are dual variables (??). Then\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{X}} \\max _{\\boldsymbol{\\Lambda}, \\boldsymbol{z}} \\mathcal{L} (\\boldsymbol{X} ; \\boldsymbol{z} , \\boldsymbol{\\Lambda}) \\ge \\max _{\\boldsymbol{\\Lambda}, \\boldsymbol{z}}\\min_{\\boldsymbol{X}}  \\mathcal{L} (\\boldsymbol{X} ; \\boldsymbol{z} , \\boldsymbol{\\Lambda})\n",
    "$$\n",
    "\n",
    "Now we solve the RHS dual problem. For the inner minimization problem $\\min_{\\boldsymbol{X}} \\mathcal{L} (\\boldsymbol{X} ; \\boldsymbol{z} , \\boldsymbol{\\Lambda})$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial \\boldsymbol{X}} = - \\boldsymbol{B} - \\operatorname{diag}\\left( \\boldsymbol{z}  \\right) - \\boldsymbol{\\Lambda} = \\boldsymbol{0}\n",
    "$$\n",
    "\n",
    "Plug this identity to $\\mathcal{L}$ gives the RHS outer maximization problem\n",
    "\n",
    "$$\n",
    "\\max _{\\boldsymbol{\\Lambda} \\succeq \\boldsymbol{0} , \\boldsymbol{z}}\\ \\boldsymbol{z} ^{\\top} \\boldsymbol{1}\n",
    "$$\n",
    "\n",
    ":::\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}