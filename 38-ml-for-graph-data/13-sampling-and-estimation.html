
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sampling and Estimation &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Modeling" href="21-modeling.html" />
    <link rel="prev" title="Descriptive Analysis" href="11-descriptive-analysis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Normal Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-graph-rep-learning.html">
     Graph Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/38-ml-for-graph-data/13-sampling-and-estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F38-ml-for-graph-data/13-sampling-and-estimation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling">
   Sampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#induced-subgraph-sampling">
     Induced Subgraph Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#incident-subgraph-sampling">
     Incident Subgraph Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#star-sampling">
     Star Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#snowball-sampling">
     Snowball sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#link-tracing">
     Link Tracing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation">
   Estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types">
     Types
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#totals-on-vertex">
       Totals on Vertex
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#totals-on-vertex-pairs">
       Totals on Vertex Pairs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#totals-of-higher-order">
       Totals of Higher Order
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples">
     Examples
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#average-degree">
       Average Degree
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hidden-population-size">
       Hidden Population Size
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-size-via-link-tracing">
       Graph Size via Link Tracing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#issues">
     Issues
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="sampling-and-estimation">
<h1>Sampling and Estimation<a class="headerlink" href="#sampling-and-estimation" title="Permalink to this headline">¶</a></h1>
<p>Like other statistical data, we usually only observe a sample from a larger underlying graph. We introduce sampling and estimation in graphs.</p>
<div class="section" id="sampling">
<h2>Sampling<a class="headerlink" href="#sampling" title="Permalink to this headline">¶</a></h2>
<p>Graph sampling designs are somewhat distinct from typical sampling designs in non-network contexts, in that there are effectively two inter-related sets <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(E\)</span> of units being sampled. Often these designs can be characterized as having two stages:</p>
<ol class="simple">
<li><p>a selection stage, among one set (e.g. vertices)</p></li>
<li><p>an observation stage, among the other or both</p></li>
</ol>
<p>It’s also important to discuss the inclusion probabilities of a vertex and an edge in each sampling design, denoted <span class="math notranslate nohighlight">\(\pi_i\)</span> for vertex <span class="math notranslate nohighlight">\(i \in E\)</span> and <span class="math notranslate nohighlight">\(\pi_{(i, j)}\)</span> for edge <span class="math notranslate nohighlight">\((i,j) \in V^{(2)}\)</span>, where <span class="math notranslate nohighlight">\(V^{(2)}\)</span> is the set of all unordered pairs of vertices.</p>
<div class="section" id="induced-subgraph-sampling">
<h3>Induced Subgraph Sampling<a class="headerlink" href="#induced-subgraph-sampling" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>When <span class="math notranslate nohighlight">\(N_v\)</span> is large and <span class="math notranslate nohighlight">\(p \approx n/N_v\)</span> is small, Bernoulli sampling design is a quite reasonable approximation to simple random sampling without replacement in stage 1.</p>
</div>
<p>The two stages are</p>
<ol class="simple">
<li><p>Select a simple random sample of <span class="math notranslate nohighlight">\(n\)</span> vertices <span class="math notranslate nohighlight">\(V^{*}=\left\{i_{1}, \ldots, i_{n}\right\}\)</span> from <span class="math notranslate nohighlight">\(V\)</span> without replacement</p></li>
<li><p>Observe a set <span class="math notranslate nohighlight">\(S^*\)</span> of edges in their induced subgraphs: for <span class="math notranslate nohighlight">\(n(n-1)\)</span> pairs of <span class="math notranslate nohighlight">\((i, j)\)</span> for <span class="math notranslate nohighlight">\(i,j \in V^*\)</span>, check whether <span class="math notranslate nohighlight">\((i,j)\in E\)</span>.</p></li>
</ol>
<p>For instance, in social networks, we an sample a group of individuals, and then ask their relation or some measure of contact, e.g. friendship, likes or dislike.</p>
<p>The inclusion probabilities are uniformly equal to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\pi_{i}&amp;= \mathbb{P}\left( v_i \text{ is selected}  \right) \\
&amp;= \frac{n}{N_v} \\
\pi_{(i,j)}&amp;= \mathbb{P}\left( \text{both $v_i$ and $v_j$ are selected}  \right) \\
&amp;= \mathbb{P}\left( \text{$v_i$ is selected}  \right) \mathbb{P}\left( \text{$v_j$ is selected} \mid \text{$v_i$ is selected}  \right)\\
&amp;= \frac{n}{N_v} \cdot \frac{n-1}{N_v - 1} \quad \because \text{without replacement}
\end{aligned}\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(N_v\)</span> is necessary to compute these probabilities.</p>
<div class="figure align-default" id="graph-sampling-induced-incident">
<a class="reference internal image-reference" href="../_images/graph-sampling-induced-incident.png"><img alt="" src="../_images/graph-sampling-induced-incident.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 185 </span><span class="caption-text">Induced (left) and incident (right) subgraph sampling. Selected vertices/edges are shown in yellow, while observed edges/vertices are shown in orange.</span><a class="headerlink" href="#graph-sampling-induced-incident" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="incident-subgraph-sampling">
<h3>Incident Subgraph Sampling<a class="headerlink" href="#incident-subgraph-sampling" title="Permalink to this headline">¶</a></h3>
<p>Complementary to induced subgraph sampling is incident subgraph sampling.
Instead of selecting <span class="math notranslate nohighlight">\(n\)</span> vertices in the initial stage, <span class="math notranslate nohighlight">\(n\)</span> edges are selected:</p>
<ol class="simple">
<li><p>Select a simple random sample <span class="math notranslate nohighlight">\(E^*\)</span> of <span class="math notranslate nohighlight">\(n\)</span> edges from <span class="math notranslate nohighlight">\(E\)</span> without replacement</p></li>
<li><p>All vertices incident to the selected edges are then observed, thus providing <span class="math notranslate nohighlight">\(V^*\)</span>.</p></li>
</ol>
<p>For instance, we sample email correspondence from a database, and observe the sender and receiver.</p>
<p>Inclusion probabilities:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\pi_{(i, j)} &amp;= \frac{n}{N_e} \\
\pi_i&amp;= 1-\mathbb{P}(\text{no edge incident to $v_i$ is selected}) \\
&amp;=\left\{\begin{array}{ll}\frac{\binom{N_e - d_i}{n}}{\binom{N_e}{n}} &amp; \text { if } n \leq N_{e}-d_{i} \\ 1, &amp; \text { if } n&gt;N_{e}-d_{i}\end{array}\right. \\
\end{aligned}\end{split}\]</div>
<p>Hence, in incident subgraph sampling, while the edges are included in the sample graph <span class="math notranslate nohighlight">\(G^*\)</span> with equal probability, the vertices are included with unequal probabilities depending on their degrees.</p>
<p>Note that <span class="math notranslate nohighlight">\(N_e\)</span> and <span class="math notranslate nohighlight">\(d_i\)</span> are necessary to compute the inclusion probabilities. In the example of sampling email correspondence graph, this would require having access to marginal summaries of the total number of emails (say, in a given month) as well as the number of emails in which a given sender had participated.</p>
</div>
<div class="section" id="star-sampling">
<h3>Star Sampling<a class="headerlink" href="#star-sampling" title="Permalink to this headline">¶</a></h3>
<p>The first stage selects vertices like in induced subgraph sampling, but in the second stage, as its name suggests, we sample all edges incident to the selected vertices, as well as the new vertices on the other end.</p>
<ol class="simple">
<li><p>Select a simple random sample <span class="math notranslate nohighlight">\(V_0^*\)</span> from <span class="math notranslate nohighlight">\(V\)</span> without replacement</p></li>
<li><p>For each <span class="math notranslate nohighlight">\(v \in V^*\)</span>,</p>
<ul class="simple">
<li><p>observe all edges incident to <span class="math notranslate nohighlight">\(v\)</span>, yielding <span class="math notranslate nohighlight">\(E^*\)</span>.</p></li>
<li><p>also observe its neighbors, together with <span class="math notranslate nohighlight">\(V_0^*\)</span> yielding <span class="math notranslate nohighlight">\(V^*\)</span></p></li>
</ul>
</li>
</ol>
<p>More precisely, this is called labeled star sampling. In unlabeled star sampling, the resulting graph is <span class="math notranslate nohighlight">\(G^* = (V_0^*, E^*)\)</span>. In the latter case, we focus on some particular characteristics (e.g. degrees), so we don’t need the vertices on the other end.</p>
<p>For instance, in co-authorship graph, randomly sampling records of <span class="math notranslate nohighlight">\(n\)</span> authors and recording the total number of co-authors of each author would correspond to unlabeled star sampling; if not only the number but the identities of the co-authors are recorded, this would correspond to labeled star sampling.</p>
<p>The inclusion probabilities are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\pi_{(i, j)}
&amp;= \mathbb{P}\left( \text{neither $i$ nor $j$ are sampled}  \right)\\
&amp;= 1- \frac{\binom{N_v-2}{n}}{\binom{N_v}{n}} \\
\pi_ i &amp;= \frac{n}{N_v} \quad \text{unlabeled case}  \\
\pi_ i &amp;= \sum_{L \subseteq N[i]}(-1)^{|L|+1} \mathbb{P}(L) \quad \text{labeled case}  \\
\end{aligned}\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N[i]\)</span> is the union of vertex <span class="math notranslate nohighlight">\(i\)</span> and the its immediate neighbors</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}\left( L \right) = \frac{\binom{N_v - \left\vert L \right\vert}{n - \left\vert L \right\vert} }{\binom{N_v}{n} }\)</span> is the probability that <span class="math notranslate nohighlight">\(L \subseteq V_0^*\)</span>. (<span class="math notranslate nohighlight">\(n &gt; \left\vert L \right\vert\)</span>??)</p></li>
</ul>
</div>
<div class="section" id="snowball-sampling">
<h3>Snowball sampling<a class="headerlink" href="#snowball-sampling" title="Permalink to this headline">¶</a></h3>
<p>In star sampling we only look at the immediate neighborhood. We can extends it to up to the <span class="math notranslate nohighlight">\(K\)</span>-th order neighbors, which is snowball sampling. In short, a <span class="math notranslate nohighlight">\(K\)</span>-stage snowball sampling is</p>
<ol class="simple">
<li><p>select a simple random sample <span class="math notranslate nohighlight">\(V_0^*\)</span> from <span class="math notranslate nohighlight">\(V\)</span> without replacement</p></li>
<li><p>for each <span class="math notranslate nohighlight">\(k = 1, \ldots , K\)</span>, observe a <span class="math notranslate nohighlight">\(k\)</span>-th order neighbors, add them to <span class="math notranslate nohighlight">\(V^*\)</span> (excluding repeated vertices), and add their incident edges to <span class="math notranslate nohighlight">\(E^*\)</span>.</p></li>
</ol>
<p>Formally, let <span class="math notranslate nohighlight">\(N(S)\)</span> be the set of all neighbors of vertices in a set <span class="math notranslate nohighlight">\(S\)</span>. After we initialize <span class="math notranslate nohighlight">\(V_0^*\)</span>, we add vertices, for <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V_k^* = N(V_{k-1}^*)\cap \bar{V}_0^* \cap \ldots \cap \bar{V}_{k-1}^*\)</span>, called the <span class="math notranslate nohighlight">\(k\)</span>-th wave.</p></li>
</ul>
<p>A termination condition is <span class="math notranslate nohighlight">\(V_k = \emptyset\)</span>. The final graph <span class="math notranslate nohighlight">\(G^*\)</span> consists of the vertices in <span class="math notranslate nohighlight">\(V^* = V_0^* \cup V_1 ^* \cup \ldots \cup V_K^*\)</span> and their incident edges.</p>
<p>Unfortunately, although not surprisingly, inclusion probabilities for snowball sampling become increasingly intractable to calculate after the one-stage level corresponding to star sampling.</p>
<div class="figure align-default" id="graph-sampling-link-tracing">
<a class="reference internal image-reference" href="../_images/graph-sampling-link-tracing.png"><img alt="" src="../_images/graph-sampling-link-tracing.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 186 </span><span class="caption-text">Two-stage snowball sampling (left) where <span class="math notranslate nohighlight">\(V_0^*\)</span> in yellow, <span class="math notranslate nohighlight">\(V_1^*\)</span> in orange, and <span class="math notranslate nohighlight">\(V_2^*\)</span> in red. Traceroute sampling (right) for sources <span class="math notranslate nohighlight">\(\left\{ s_1, s_2 \right\}\)</span> and targets <span class="math notranslate nohighlight">\(\left\{ t_1, t_2 \right\}\)</span> in yellow, observed vertices and edges in orange.</span><a class="headerlink" href="#graph-sampling-link-tracing" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="link-tracing">
<span id="id1"></span><h3>Link Tracing<a class="headerlink" href="#link-tracing" title="Permalink to this headline">¶</a></h3>
<p>Many of the other sampling designs fall under link tracing designs: after some selection of an initial sample, some <strong>subset</strong> of the edges (‘links’) from vertices in this sample are traced to additional vertices.</p>
<p>Snowball sampling is a special case of link tracing, in that all edges are observed. Sometimes this is not feasible, for example, in sampling social contact networks, it may be that individuals are unaware of or cannot recall all of their contacts, or that they do not wish to divulge some of them.</p>
<p>We introduce <strong>traceroute</strong> sampling.</p>
<ol class="simple">
<li><p>select a random sample <span class="math notranslate nohighlight">\(S=\left\{s_{1}, \ldots, s_{n_{s}}\right\}\)</span> of vertices as sources from <span class="math notranslate nohighlight">\(V\)</span>, and another random sample <span class="math notranslate nohighlight">\(T=\left\{t_{1}, \ldots, t_{n_{t}}\right\}\)</span> of vertices as targets from <span class="math notranslate nohighlight">\(V \setminus S\)</span>.</p></li>
<li><p>For each pair <span class="math notranslate nohighlight">\((s_i, t_j) \in S \times T\)</span>, sample a <span class="math notranslate nohighlight">\(s_i\)</span>-<span class="math notranslate nohighlight">\(t_j\)</span> path. Observe all vertices and edges in the path, whose union yielding <span class="math notranslate nohighlight">\(G^* = (V^*, E^*)\)</span>.</p></li>
</ol>
<p>To find the inclusion probabilities, we assume that the paths are shortest paths w.r.t. some set of edge weights. Dall’Asta et al. [SAND 107] find the probabilities are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\pi_{i} &amp;\approx 1-\left(1-\rho_{s}-\rho_{t}\right) \exp \left(-\rho_{s} \rho_{t} b_{i}\right) \\
\tau_{\{i, j\}} &amp;\approx 1-\exp \left(-\rho_{s} \rho_{t} b_{i, j}\right)
\end{aligned}\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_i\)</span> is the vertex betweenness centrality of vertex <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_{i, j}\)</span> is the edge betweenness centrality of edge <span class="math notranslate nohighlight">\((i, j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\rho_{s} = \frac{n_s}{N_v} , \rho_t = \frac{n_t}{N_v}\)</span> are the source ant target sampling fractions respectively</p></li>
</ul>
<p>We see that the unequal probabilities varies with betweenness centrality <span class="math notranslate nohighlight">\(b_i\)</span> and <span class="math notranslate nohighlight">\(b_{i, j}\)</span>. Though they are not calculable, they lend interesting insight into the nature of this sampling design, to be introduced later.</p>
</div>
</div>
<div class="section" id="estimation">
<h2>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h2>
<p>(Review the <a class="reference internal" href="../12-probabilities/71-sampling.html#estimation-mean-total"><span class="std std-ref">estimation of total</span></a> section)</p>
<p>With appropriate choice of population <span class="math notranslate nohighlight">\(U\)</span> and unit values <span class="math notranslate nohighlight">\(y_i\)</span> for <span class="math notranslate nohighlight">\(i \in U\)</span>, many of the quantities <span class="math notranslate nohighlight">\(\eta(G)\)</span> of graph <span class="math notranslate nohighlight">\(G\)</span>, e.g. average degree, <span class="math notranslate nohighlight">\(N_e\)</span>, or even centrality, can be written in a form of population total <span class="math notranslate nohighlight">\(\sum_{i \in U} y_i\)</span>, as introduced below.</p>
<p>To estimate the total from a sampled graph <span class="math notranslate nohighlight">\(G^* = (V^*, E^*)\)</span> where <span class="math notranslate nohighlight">\(V^* \subseteq V, E^* \subseteq E\)</span>, we can use generalization of the Horvitz-Thompson estimator.</p>
<div class="section" id="types">
<h3>Types<a class="headerlink" href="#types" title="Permalink to this headline">¶</a></h3>
<p>Many estimation problems can be classified to totals over some order of vertex set <span class="math notranslate nohighlight">\(V^(k)\)</span>.</p>
<div class="section" id="totals-on-vertex">
<h4>Totals on Vertex<a class="headerlink" href="#totals-on-vertex" title="Permalink to this headline">¶</a></h4>
<p>Let <span class="math notranslate nohighlight">\(U=V\)</span>, we can define <span class="math notranslate nohighlight">\(y_i\)</span> according to the total we are interested.</p>
<ul class="simple">
<li><p>average degree: let <span class="math notranslate nohighlight">\(y_i = d_i\)</span>, then the average degree <span class="math notranslate nohighlight">\(\bar{d}\)</span> equals the population total <span class="math notranslate nohighlight">\(\sum_{i \in V} d_i\)</span> divided by <span class="math notranslate nohighlight">\(N_v\)</span></p></li>
<li><p>proportion of special vertices: let <span class="math notranslate nohighlight">\(y_i = \mathbb{I} \left\{ v_i \text{ has some property}  \right\}\)</span>, then the proportion of such special vertices equals the population total <span class="math notranslate nohighlight">\(\sum_{i \in V} 1\)</span> divided by <span class="math notranslate nohighlight">\(N_v\)</span>. For instance, proportion of gene responsible for the growth of an organism.</p></li>
</ul>
<p>Given a sample of vertices <span class="math notranslate nohighlight">\(V^* \subseteq V\)</span>, the Horvitz-Thompson estimator for vertex totals takes the form</p>
<div class="math notranslate nohighlight">
\[
\hat{\tau}_{\pi}=\sum_{i \in V^{*}} \frac{y_{i}}{\pi_{i}}
\]</div>
<p>Note that</p>
<ul class="simple">
<li><p>in some sampling design, the graph structure will be irrelevant for estimating a vertex total, e.g. when the graph structure is irrelevant to <span class="math notranslate nohighlight">\(y\)</span> and vertices are sampled through simple random sampling without replacement. <span class="math notranslate nohighlight">\(\pi_i\)</span> can be computed in the conventional way.</p></li>
<li><p>on the other hand, the graph structure matters, e.g. in snowball sampling the structure determines <span class="math notranslate nohighlight">\(V^*\)</span> and hence the calculation of <span class="math notranslate nohighlight">\(\pi_i\)</span>.</p></li>
</ul>
</div>
<div class="section" id="totals-on-vertex-pairs">
<span id="total-on-vertex-pairs"></span><h4>Totals on Vertex Pairs<a class="headerlink" href="#totals-on-vertex-pairs" title="Permalink to this headline">¶</a></h4>
<p>Now we are interested in <span class="math notranslate nohighlight">\(U = V^{(2)}\)</span>, the total is</p>
<div class="math notranslate nohighlight">
\[
\tau=\sum_{(i, j) \in V^{(2)}} y_{i j}
\]</div>
<ul class="simple">
<li><p>number of edges: let <span class="math notranslate nohighlight">\(y_{(i, j)} = \mathbb{I} \left\{ (i,j) \in E \right\}\)</span>, then the number of edges <span class="math notranslate nohighlight">\(N_e\)</span> is given by the total.</p></li>
<li><p>betweenness centrality: let <span class="math notranslate nohighlight">\(y_{(i,j)} = \mathbb{I} \left\{ v \in P(i,j) \right\}\)</span> where <span class="math notranslate nohighlight">\(P(i,j)\)</span> is the shortest path between <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, and <span class="math notranslate nohighlight">\(y_{(i, j)} = 1\)</span> if vertex <span class="math notranslate nohighlight">\(v\)</span> is in this shortest path. If all shortest paths are unique, then the betweenness centrality <span class="math notranslate nohighlight">\(c_{bet}(v)\)</span> of a vertex <span class="math notranslate nohighlight">\(v \in V\)</span> is given by the total, which counts how many shortest paths going through <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>number of homogeneous vertices: let <span class="math notranslate nohighlight">\(y_{(i,j)} = \mathbb{I} \left\{ \text{both } i \text{ and } j \text{ have some properties}  \right\}\)</span></p></li>
<li><p>average of some (dis)similarity value between vertex: let <span class="math notranslate nohighlight">\(y_{(i,j)} = s(i, j)\)</span> and then divide the total by <span class="math notranslate nohighlight">\(N_e\)</span>.</p></li>
</ul>
<p>The Horvitz-Thompson estimator takes the form</p>
<div class="math notranslate nohighlight">
\[
\hat{\tau}_{\pi}=\sum_{(i, j) \in V^{*(2)}} \frac{y_{i j}}{\pi_{i j}}
\]</div>
<p>If <span class="math notranslate nohighlight">\(y_{ij} \ne 0\)</span> iff <span class="math notranslate nohighlight">\((i, j) \in E\)</span>, then</p>
<ul class="simple">
<li><p>vertex pairs total <span class="math notranslate nohighlight">\(\tau\)</span> equals to an edge total</p></li>
<li><p>summation in the estimator <span class="math notranslate nohighlight">\(\hat{\tau}\)</span> is taken over <span class="math notranslate nohighlight">\(E^*\)</span>,</p></li>
<li><p>the inclusion probability <span class="math notranslate nohighlight">\(\pi_{ij}\)</span> is just the edge inclusion probability <span class="math notranslate nohighlight">\(\pi_{(i, j)}\)</span>, which equals</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\frac{n(n-1)}{N_v (N_v - 1)}\)</span> under induced graph sampling with simple random sampling without replacement in stage 1</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{p^2}\)</span> under induced graph sampling with Bernoulli sampling with probability <span class="math notranslate nohighlight">\(p\)</span> in stage 1</p></li>
</ul>
</li>
</ul>
<p>The variance of the above estimator, generalized from that for conventional Horvitz-Thompson estimator, is given by</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}\left(\hat{\tau}_{\pi}\right)=\sum_{(i, j) \in V^{(2)}} \sum_{(k, l) \in V^{(2)}} y_{i j} y_{k l}\left(\frac{\pi_{i j k l}}{\pi_{i j} \pi_{k l}}-1\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_{ijkl}\)</span> is the probability that units <span class="math notranslate nohighlight">\((i,j)\)</span> and <span class="math notranslate nohighlight">\((k,l)\)</span> are both included in the sample, and <span class="math notranslate nohighlight">\(\pi_{ijkl} = \pi_{ij}\)</span> for convenience when <span class="math notranslate nohighlight">\((i,j) = (k, l)\)</span>. Note that there can be <span class="math notranslate nohighlight">\(1 \le r \le 4\)</span> different vertices among <span class="math notranslate nohighlight">\(i,j,k,l\)</span>. The corresponding unbiased estimate of this variance is given by</p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathbb{V}}\left(\hat{\tau}_{\pi}\right)=\sum_{(i, j) \in V^{*(2)}} \sum_{(k, l) \in V^{*(2)}} y_{i j} y_{k l}\left(\frac{1}{\pi_{i j} \pi_{k l}}-\frac{1}{\pi_{i j k l}}\right)
\]</div>
<p>Note that these quantities can become increasingly complicated to compute under some sampling designs, since it is necessary to be able to evaluate probabilities <span class="math notranslate nohighlight">\(\pi_{ijkl}\)</span> for <span class="math notranslate nohighlight">\(1 \le r \le 4\)</span>. See Example 5.4 in SAND pg.139 for <span class="math notranslate nohighlight">\(p_r\)</span> in induced graph sampling and estimation of <span class="math notranslate nohighlight">\(N_e\)</span>. Results are shown below.</p>
<div class="figure align-default" id="graph-sampling-edge-total">
<a class="reference internal image-reference" href="../_images/graph-sampling-edge-total.png"><img alt="" src="../_images/graph-sampling-edge-total.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 187 </span><span class="caption-text">Histograms of estimates <span class="math notranslate nohighlight">\(\hat{N}_e\)</span> of <span class="math notranslate nohighlight">\(N_e\)</span> = 31201 and its estimated standard errors (right), under induced subgraph sampling, with Bernoulli sampling of vertices using <span class="math notranslate nohighlight">\(p=0.1, 0.2, 0.3\)</span> based on 10000 trials. [Kolaczyk 2009]</span><a class="headerlink" href="#graph-sampling-edge-total" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="totals-of-higher-order">
<h4>Totals of Higher Order<a class="headerlink" href="#totals-of-higher-order" title="Permalink to this headline">¶</a></h4>
<p>The expressions for higher order cases are more complicated. We introduce the case of triples, where <span class="math notranslate nohighlight">\(U = V^{(3)}\)</span> and <span class="math notranslate nohighlight">\(\tau=\sum_{(i, j, k) \in V^{(3)}} y_{i j k}\)</span>. The sample Horvitz-Thompson estimator is</p>
<div class="math notranslate nohighlight">
\[
\hat{\tau}_{\pi}=\sum_{(i, j, k) \in V^{*(3)}} \frac{y_{i j k}}{\pi_{i j k}}
\]</div>
<p>The expressions for variance and estimated variance follow in a like manner.</p>
<p>We see an example of estimating transitivity. Recall that</p>
<div class="math notranslate nohighlight">
\[
\operatorname{clus}_{T}(G)=\frac{3 \tau_{\Delta}(G)}{\tau_{\wedge}(G)}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\tau_{\Delta}(G)=\frac{1}{3} \sum_{v \in V} \tau_{\Delta}(v)\)</span> is the number of triangles in the graph</p></li>
<li><p><span class="math notranslate nohighlight">\(\tau_{\wedge}(G)=\sum_{v \in V} \tau_{\wedge}(v)\)</span> is the number of connected triples in the graph</p></li>
</ul>
<p>This quantity can be re-expressed in the form</p>
<div class="math notranslate nohighlight">
\[
\operatorname{clus}_{T}(G)=\frac{3 \tau_{\Delta}(G)}{\tau_{\wedge} ^ +(G) + 3 \tau_{\Delta}(G)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau_{\wedge} ^ +(G) = \tau_{\wedge}(G) -  3 \tau_{\Delta}(G)\)</span> is the number of vertex triples that are connected by <strong>exactly</strong> two edges. Then both <span class="math notranslate nohighlight">\(\tau_{\Delta}(G)\)</span> and <span class="math notranslate nohighlight">\(\tau_{\wedge}^+(G)\)</span> can becomputed as  a total <span class="math notranslate nohighlight">\(\sum_{(i,j,k) \in V^{(3)} }y_{ijk}\)</span> by setting, respectively,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_{ijk} = a_{ij}a_{jk}a_{ki}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_{i j k}=a_{i j} a_{j k}\left(1-a_{k i}\right)+a_{i j}\left(1-a_{j k}\right) a_{k i}+\left(1-a_{i j}\right) a_{j k} a_{k i}\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the <span class="math notranslate nohighlight">\(ij\)</span>-th entry in the adjacency matrix.</p>
<p>If we use induced subgraph sampling with Bernoulli sampling of vertices with probability <span class="math notranslate nohighlight">\(p\)</span> to obtain a sample <span class="math notranslate nohighlight">\(G^* = (V^*, E^*)\)</span>, then <span class="math notranslate nohighlight">\(\pi_{ijk} = p^{-3}\)</span>, and hence</p>
<div class="math notranslate nohighlight">
\[
\hat{\tau}_{\Delta}(G) =\sum_{(i, j, k) \in V^{*(3)}} \frac{y_{i j k}}{\pi_{i j k}} = p^{-3} \sum_{(i, j, k) \in V^{*(3)}}y_{i j k} = p^{-3} \tau_{\Delta}(G^*)
\]</div>
<p>and similarly <span class="math notranslate nohighlight">\(\hat{\tau}_{\wedge}^+(G) = p^{-3}\tau_{\wedge}^+(G^*)\)</span>.</p>
<p>We can then substitute these two values to obtain a plug-in estimator of transitivity <span class="math notranslate nohighlight">\(\operatorname{clus}_T (G)\)</span>. Note that the coefficient <span class="math notranslate nohighlight">\(p^{-3}\)</span> cancel out, hence <span class="math notranslate nohighlight">\(\widehat{\operatorname{clus}}_T (G) = \operatorname{clus} _T (G^*)\)</span>.</p>
</div>
</div>
<div class="section" id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>There are three conditions to make the estimation feasible</p>
<ol class="simple">
<li><p>the graph summary statistic <span class="math notranslate nohighlight">\(\eta(G)\)</span> must be expressed in terms of totals</p></li>
<li><p>the values <span class="math notranslate nohighlight">\(y\)</span> must be either observed or obtainable from the available measurements</p></li>
<li><p>the inclusion probabilities <span class="math notranslate nohighlight">\(\pi\)</span> must be computable for the sampling design</p></li>
</ol>
<p>But it is not always the case that all three elements are present at the same time. As the first example shown below.</p>
<p>In fact, many estimation problems can be viewed as species problems.</p>
<ul class="simple">
<li><p>average degree: how many (distinct) edges for each observed vertex?</p></li>
<li><p><span class="math notranslate nohighlight">\(N_e\)</span> via link tracing: how many distinct edges on multiple paths?</p></li>
</ul>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<div class="section" id="average-degree">
<h4>Average Degree<a class="headerlink" href="#average-degree" title="Permalink to this headline">¶</a></h4>
<p>We will see estimating average degree using two different sampling designs.</p>
<p>First consider unlabeled star sampling. Let the sampled graph be <span class="math notranslate nohighlight">\(G^*_{star} = (V^*_{star}, E^*_{star})\)</span>. The average degree is a rescaling of vertex total</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu}_{star} = \frac{\hat{\tau}_{star}}{N_v}  \quad \text{where} \quad \hat{\tau}_{star} = \sum_{i \in V_{star}^*} \frac{d_i}{n/N_v}
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(d_i\)</span> is observed.</p>
<p>On the other hand, under induced subgraph sampling, we do not observe <span class="math notranslate nohighlight">\(d_i\)</span>, but only a number <span class="math notranslate nohighlight">\(d_i^* \le d_i\)</span> for each <span class="math notranslate nohighlight">\(i \in V_{indu}^*\)</span>. As a result, <span class="math notranslate nohighlight">\(\tau\)</span> is not amenable to Horvitz-Thompson estimation methods as a vertex total.</p>
<p>However, we can use the relation <span class="math notranslate nohighlight">\(\mu = \frac{2N_e}{N_v}\)</span>, which shows an alternative way by estimating <span class="math notranslate nohighlight">\(N_e\)</span>. As introduced in <a class="reference internal" href="#total-on-vertex-pairs"><span class="std std-ref">total on vertex pairs</span></a> above, with inclusion probability <span class="math notranslate nohighlight">\(\pi_{ij}= \frac{n(n-1)}{N_v (N_v - 1)}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\hat{N}_{e, indu}=\sum_{(i, j) \in E_{indu}^{*}} \frac{1}{\pi_{ij}}=N_{e, indu}^{*} \cdot \frac{N_{v}\left(N_{v}-1\right)}{n(n-1)}
\]</div>
<p>which gives the unbiased estimator</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} _{indu} = \frac{2 \hat{N}_{e, indu}}{N_v}
\]</div>
<p>for <span class="math notranslate nohighlight">\(\mu\)</span></p>
<p>We can then compare these two estimator. Some re-writing gives</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu}_{star} = \frac{2N_{e, star}^*}{n} \quad \hat{\mu}_{I S}=\frac{2 N_{e, indu}^{*}}{n} \cdot \frac{N_{v}-1}{n-1}
\]</div>
<p>Hence under star sampling, it simply use the relation <span class="math notranslate nohighlight">\(\bar{d} = \frac{2N_e}{N_v}\)</span> to the sample. In contrast, under induced subgraph sampling, the analogous result (sample average degree) is scaled up by the factor <span class="math notranslate nohighlight">\(\frac{N_v - 1}{n-1}\)</span> to account for <span class="math notranslate nohighlight">\(d_{i, indu}^* \le d_i\)</span>.</p>
</div>
<div class="section" id="hidden-population-size">
<span id="sampling-hidden-pop-size"></span><h4>Hidden Population Size<a class="headerlink" href="#hidden-population-size" title="Permalink to this headline">¶</a></h4>
<p>The term ‘hidden population’ generally refers to one in which the individuals do not wish to expose themselves to view. For example, humans of socially sensitive status, such as illegal drug usage or prostitution. Two issues:</p>
<ul class="simple">
<li><p>they will not be inclined to disclose themselves</p></li>
<li><p>their population is small</p></li>
</ul>
<p>Frank and Snijders [SAND 154] describe how snowball sampling may
be used for this problem, using the idea that mimics capture-recapture methods.</p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V\)</span> be the set of all members of the hidden population</p></li>
<li><p><span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a directed graph associated with that population, in which an arc from vertex <span class="math notranslate nohighlight">\(i\)</span> to vertex <span class="math notranslate nohighlight">\(j\)</span> indicates that, if asked, individual <span class="math notranslate nohighlight">\(i\)</span> would mention individual <span class="math notranslate nohighlight">\(j\)</span> as a member of the hidden population (there are some concerns of trust, veracity etc). We want to estimate <span class="math notranslate nohighlight">\(N_v\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(G^*\)</span> be a subgraph of <span class="math notranslate nohighlight">\(G\)</span>, where the vertices <span class="math notranslate nohighlight">\(V^* = V_0^* \cup V_1 ^*\)</span> are obtained through a one-wave snowball sample, with the initial
sample <span class="math notranslate nohighlight">\(V_0^*\)</span> selected through Bernoulli sampling <span class="math notranslate nohighlight">\(Z_i \sim \operatorname{Ber}(p_0)\)</span> from <span class="math notranslate nohighlight">\(V\)</span>, where the sampling rate <span class="math notranslate nohighlight">\(p_0\)</span> is unknown. We have three random variables</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_v ^* = \left\vert V_0^* \right\vert\)</span> be the size of the initial sample</p></li>
<li><p><span class="math notranslate nohighlight">\(M_1\)</span> be the number of arcs <span class="math notranslate nohighlight">\((i, j)\)</span> in <span class="math notranslate nohighlight">\(V_0^*\)</span> (<span class="math notranslate nohighlight">\(i \in V_0^*\)</span> and <span class="math notranslate nohighlight">\(j \in V_0^*\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(M_2\)</span> be the number of arcs pointing from <span class="math notranslate nohighlight">\(i \in V_0^*\)</span> to <span class="math notranslate nohighlight">\(j \in V_1^*\)</span> (<span class="math notranslate nohighlight">\(i \in V_0^*\)</span> but <span class="math notranslate nohighlight">\(j \notin V_0^*\)</span>)</p></li>
</ul>
</li>
</ul>
<p>Out estimator of <span class="math notranslate nohighlight">\(N_v\)</span> will be derived using the method-of-moments. We first find the expectation of the three variables.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l}
\mathbb{E}(N_v ^*)=\mathbb{E}\left(\sum_{i} Z_{i}\right)=N_{v} p_{0} \\
\mathbb{E}\left(M_{1}\right)=\mathbb{E}\left(\sum_{i \neq j} Z_{i} Z_{j} A_{i j}\right)=\left(N_{e}-N_{v}\right) p_{0}^{2} \\
\mathbb{E}\left(M_{2}\right)=\mathbb{E}\left(\sum_{i \neq j} Z_{i}\left(1-Z_{j}\right) A_{i j}\right)=\left(N_{e}-N_{v}\right) p_{0}\left(1-p_{0}\right)
\end{array}
\end{split}\]</div>
<p>Setting the left-hand sides of these equations equal to their observed counterparts, say <span class="math notranslate nohighlight">\(n_v ^*, m_1\)</span> and <span class="math notranslate nohighlight">\(m_2\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{p}_0&amp;= \frac{m_1 + m_2}{m_1}  \\
\widehat{N_v} &amp;=  \frac{1}{\hat{p}_0}  n_v ^*\\
\end{aligned}\end{split}\]</div>
<p>In other words, the number of individuals observed initially is inflated by an estimate <span class="math notranslate nohighlight">\(\hat{p}_0\)</span> of the sampling rate, where that estimate reflects the relative number of arcs from individuals in the initial sample that point inwards among themselves.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Recall capture-recapture estimator <span class="math notranslate nohighlight">\(\frac{1}{m/n_2} n_1\)</span> where <span class="math notranslate nohighlight">\(n_1, n_2\)</span> are sample sizes, and <span class="math notranslate nohighlight">\(m\)</span> are marked individuals in stage 1. The denominator <span class="math notranslate nohighlight">\(m/n_2\)</span> can be seen as sampling rate <span class="math notranslate nohighlight">\(\hat{p}_0\)</span>.</p>
</div>
<p>To find the variance this estimator, we use the <a class="reference external" href="https://en.wikipedia.org/wiki/Jackknife_resampling">jackknife principle</a>. Let <span class="math notranslate nohighlight">\(\widehat{N}_{v}^{(-i)}\)</span> be the estimate of <span class="math notranslate nohighlight">\(N_v\)</span> obtained by removing <span class="math notranslate nohighlight">\(i \in V_0^*\)</span> and <span class="math notranslate nohighlight">\(j \in V_1^*\)</span> that has only one edge <span class="math notranslate nohighlight">\((i, j)\)</span> adjacent to it, and let <span class="math notranslate nohighlight">\(\bar{\widehat{N}}_v\)</span> be their average, then</p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathbb{V}}_{J}\left(\widehat{N}_{v}\right)=\frac{n-2}{2 n} \sum_{i \in V_{0}^{*}}\left(\widehat{N}_{v}^{(-i)}-\bar{\widehat{N}}_v\right)^{2}
\]</div>
</div>
<div class="section" id="graph-size-via-link-tracing">
<h4>Graph Size via Link Tracing<a class="headerlink" href="#graph-size-via-link-tracing" title="Permalink to this headline">¶</a></h4>
<p>We can estimate graph size via <a class="reference internal" href="#link-tracing"><span class="std std-ref">link tracing</span></a> (traceroute sampling).</p>
<p>Let <span class="math notranslate nohighlight">\(V_{(-j)}^{*}\)</span> denote the number of vertices discovered on sampled paths to targets other than <span class="math notranslate nohighlight">\(t_{j},\)</span> and define <span class="math notranslate nohighlight">\(\delta_{j}= \mathbb{I} \left\{t_{j} \notin V_{(-j)}^{*}\right\}\)</span> to be the indicator of the event that target <span class="math notranslate nohighlight">\(t_{j}\)</span> is not ‘discovered’ on sampled paths to any other target. Write the total number of such targets as <span class="math notranslate nohighlight">\(X=\sum_{j} \delta_{j}\)</span>. We want to find <span class="math notranslate nohighlight">\(\mathbb{E} [X]\)</span>, i.e. the average tendency of targets to be missed by other paths.</p>
<p>Given a set of pre-selected source nodes <span class="math notranslate nohighlight">\(S=\left\{s_{1}, \ldots, s_{n_{s}}\right\}\)</span> (chosen either randomly or not), if the target nodes in <span class="math notranslate nohighlight">\(T=\left\{t_{1}, \ldots, t_{n_{t}}\right\}\)</span> are chosen by simple random sampling without replacement from <span class="math notranslate nohighlight">\(V \backslash S\)</span>, the probability that target <span class="math notranslate nohighlight">\(t_{j}\)</span> is <strong>not</strong> discovered on the paths to other targets is given by</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbb{P}\left(\delta_{j}=1 \mid V_{(-j)}^{*}\right)=\frac{N_{v}-N_{(-j)}^{*}}{N_{v}-n_{s}-(n_{t}-1)}
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_{(-j)}^{*}=\left|V_{(-j)}^{*}\right|\)</span>. Note that, by symmetry under simple random sampling for <span class="math notranslate nohighlight">\(t_j \in T\)</span>, the expectation <span class="math notranslate nohighlight">\(\mathbb{E}[N_{(-j)}^{*}]\)</span> is the same for all <span class="math notranslate nohighlight">\(j .\)</span> We denote this quantity by <span class="math notranslate nohighlight">\(\mathbb{E}[N_{(-)}^{*}]\)</span> and, as a result, we obtain</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbb{E}[X]=\sum_{j=1}^{n_{t}} \mathbb{P}\left(\delta_{j}=1 \mid V_{(-j)}^{*}\right)=\frac{n_{t}\left[N_{v}-\mathbb{E}[N_{(-)}^{*}]\right]}{N_{v}-n_{s}-n_{t}+1}
\end{align*}
\]</div>
<p>The only unknown quantity in this equation is <span class="math notranslate nohighlight">\(N_v\)</span>, while others can be pre-set or estimated. Rewriting this equation we have</p>
<div class="math notranslate nohighlight">
\[
N_{v}=\frac{n_{t} \mathbb{E}[N_{(-)}^{*}]-\left(n_{s}+n_{t}-1\right) \mathbb{E}[X]}{n_{t}-\mathbb{E}[X]}
\]</div>
<p>Hence, we have a method-of-moments estimator for <span class="math notranslate nohighlight">\(N_v\)</span>. In a run of link tracing, an unbiased estimator of <span class="math notranslate nohighlight">\(\mathbb{E} [X]\)</span> is the <span class="math notranslate nohighlight">\(X\)</span> itself, and that of <span class="math notranslate nohighlight">\(\mathbb{E}[N_{(-)}^{*}]\)</span> is the average of <span class="math notranslate nohighlight">\(N_{(-i)}^*\)</span>. However, if <span class="math notranslate nohighlight">\(X = n_t\)</span>, then RHS denominator is not well defined.</p>
<p>To solve this, under a slight variation of this idea and ignoring trivial terms and factors, Viger et al. [SAND 388] arrive at an estimator of the form</p>
<div class="math notranslate nohighlight">
\[
\hat{N}_{v}=\left(n_{S}+n_{t}\right)+\frac{N_{v}^{*}-\left(n_{S}+n_{t}\right)}{1-w^{*}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(w^{*}=X /\left(n_{t}+1\right)\)</span>, hence the denominator can be viewed as the average tendency of targets to be <strong>discovered</strong> by other paths. A simple estimate of the variance of this by delta-method is</p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathbb{V}}\left(\hat{N}_{v}\right) \approx \frac{\left(N_{v}^{*}-n_{S}-n_{t}\right)^{2} w^{*}}{\left(1-w^{*}\right)^{3} n_{t}}
\]</div>
</div>
</div>
<div class="section" id="issues">
<h3>Issues<a class="headerlink" href="#issues" title="Permalink to this headline">¶</a></h3>
<p>Usually, we want to estimate some characteristic <span class="math notranslate nohighlight">\(\eta(G)\)</span> of a graph <span class="math notranslate nohighlight">\(G\)</span> through sampled subgraph <span class="math notranslate nohighlight">\(G^*\)</span>. Some sampling designs give biased characteristic <span class="math notranslate nohighlight">\(\eta(G^*) \ne \eta(G)\)</span> but can be adjusted, like the average degree example above. But if <span class="math notranslate nohighlight">\(\eta\)</span> is more involved, this is no trivial. For instance, many sampling designs induce degree distribution unrepresentative of the true underlying degree distribution [SAND pg.150].</p>
<p>Let <span class="math notranslate nohighlight">\(f_d\)</span> and <span class="math notranslate nohighlight">\(f_d^*\)</span> be the true and observed frequencies of degree <span class="math notranslate nohighlight">\(d\)</span> nodes in <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(G^*\)</span>, respectively. Frank [SAND 153] shows under induced subgraph sampling,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left(f_{d}^{*}\right)=\sum_{d^{\prime}=0}^{N_{v}-1} P\left(d, d^{\prime}\right) f_{d^{\prime}}, \quad \text{where}  P\left(d, d^{\prime}\right)=\frac{\binom{d^{\prime}}{d}\binom{N_{v}-1-d^{\prime}}{n-1-d}}{\binom{N_{v}-1}{n-1}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\binom{n}{k} = 0\)</span> if <span class="math notranslate nohighlight">\(k &gt;n\)</span>. In principle, to find <span class="math notranslate nohighlight">\(f_d\)</span>, we can substitute <span class="math notranslate nohighlight">\(f_d^*\)</span> to LHS, and obtain a system of equations for unknowns <span class="math notranslate nohighlight">\((f_0, \ldots, f_N)\)</span>. But since <span class="math notranslate nohighlight">\(n &lt; N\)</span>, this is system of equations is under-determined unless it is known that <span class="math notranslate nohighlight">\(d_\max &lt; n\)</span>. Even this is true, the solution is not guaranteed to be non-negative, and the variance of this estimator would not be easy to derive.</p>
<p>Other issues</p>
<ul class="simple">
<li><p>analysis of non-traditional sampling designs (e.g., particularly adaptive designs)</p></li>
<li><p>the estimation of quantities not easily expressed as totals (e.g., degree distribution exponents)</p></li>
<li><p>the incorporation of effects of sampling error and missingness</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./38-ml-for-graph-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="11-descriptive-analysis.html" title="previous page">Descriptive Analysis</a>
    <a class='right-next' id="next-link" href="21-modeling.html" title="next page">Modeling</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>