
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Graph Representation Learning &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Processes on Graphs" href="41-processes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Graph Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/38-ml-for-graph-data/51-graph-rep-learning.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F38-ml-for-graph-data/51-graph-rep-learning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#node-embeddings">
   Node Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepwalk">
     DeepWalk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#node2vec">
     Node2vec
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pagerank">
     PageRank
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model">
       Model
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computation">
       Computation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#personalized-pagerank">
       Personalized PageRank
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-t-matrix-factorization">
     R.t. Matrix Factorization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-embeddings">
   Graph Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anonymous-walk-embeddings">
     Anonymous Walk Embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anonymous">
     Anonymous+
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-embeddings">
     Hierarchical Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passing-and-node-classification">
   Message Passing and Node Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relational-classification">
     Relational Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iterative-classification">
     Iterative Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#belief-propagation">
     Belief Propagation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-acyclic-graphs">
       In Acyclic Graphs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-cyclic-graphs">
       In Cyclic Graphs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review">
       Review
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graphical-neural-networks">
   Graphical Neural Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure">
     Structure
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computation-graphs">
       Computation Graphs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#neurons">
       Neurons
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised">
       Supervised
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#node-level">
         Node-level
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#edge-level">
         Edge-level
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#graph-level">
         Graph-level
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised">
       Unsupervised
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#batch">
       Batch
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-test-splitting">
       Train-test Splitting
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id1">
         Node-level
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#link-level">
         Link-level
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros">
     Pros
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variants">
     Variants
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graphsage">
       GraphSAGE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#matrix-operations">
       Matrix Operations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-attention-networks">
       Graph Attention Networks
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gin">
       GIN
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#expressiveness">
         Expressiveness
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id2">
         GIN
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#r-t-wl-kernel">
         R.t. WL Kernel
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#improvement">
       Improvement
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-design">
     Layer Design
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modules">
       Modules
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#issue-with-deep-gnn">
       Issue with Deep GNN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#solution">
       Solution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-manipulation">
     Graph Manipulation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feature-augmentation">
       Feature Augmentation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#virtual-nodes-edges">
       Virtual Nodes/Edges
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#neighborhood-sampling">
       Neighborhood Sampling
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-generative-models">
     Graph Generative Models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graphrnn">
       GraphRNN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tractability">
       Tractability
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#similarity-of-graphs">
       Similarity of Graphs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Variants
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Limitations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#position-aware-gnns">
       Position-aware GNNs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#identity-aware-gnn">
       Identity-aware GNN
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reference">
     Reference
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="graph-representation-learning">
<h1>Graph Representation Learning<a class="headerlink" href="#graph-representation-learning" title="Permalink to this headline">¶</a></h1>
<p>Graph representation learning alleviates the need to do feature engineering every single time. We don’t need to design task-dependent features for node, link, graph, etc.</p>
<div class="figure align-default" id="graph-rep-learning">
<a class="reference internal image-reference" href="../_images/graph-rep-learning.png"><img alt="" src="../_images/graph-rep-learning.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 207 </span><span class="caption-text">Workflow of graph machine learning</span><a class="headerlink" href="#graph-rep-learning" title="Permalink to this image">¶</a></p>
</div>
<p>After embeddings are obtained, we can use it for downstream tasks. For instance, node embeddings <span class="math notranslate nohighlight">\(\boldsymbol{z} _i\)</span> can be used for clustering, node classification. For link prediction <span class="math notranslate nohighlight">\((i,j)\)</span>, we can use</p>
<ul class="simple">
<li><p>Concatenate: <span class="math notranslate nohighlight">\(f(\boldsymbol{z} _i, \boldsymbol{z} _j) = g([\boldsymbol{z} _i; \boldsymbol{z} _j])\)</span>. Good for directed graphs.</p></li>
<li><p>Hadamard: <span class="math notranslate nohighlight">\(f(\boldsymbol{z} _i, \boldsymbol{z} _j) = g(\boldsymbol{z} _i * \boldsymbol{z} _j)\)</span></p></li>
<li><p>Sum/Avg: <span class="math notranslate nohighlight">\(f(\boldsymbol{z} _i, \boldsymbol{z} _j) = g(\boldsymbol{z} _i + \boldsymbol{z} _j)\)</span></p></li>
<li><p>Distance: <span class="math notranslate nohighlight">\(f(\boldsymbol{z} _i, \boldsymbol{z} _j) = g(\left\| \boldsymbol{z} _i - \boldsymbol{z} _j \right\| _2)\)</span></p></li>
</ul>
<div class="section" id="node-embeddings">
<h2>Node Embeddings<a class="headerlink" href="#node-embeddings" title="Permalink to this headline">¶</a></h2>
<p>For node embeddings, we want to learn a function, aka encoder, <span class="math notranslate nohighlight">\(\operatorname{ENC} : V \rightarrow \mathbb{R} ^d\)</span>, such that</p>
<ul class="simple">
<li><p>For two nodes <span class="math notranslate nohighlight">\(u, v \in V\)</span>, similarity of embeddings <span class="math notranslate nohighlight">\(\boldsymbol{z}_u = \operatorname{ENC}(u), \boldsymbol{z} _v=\operatorname{ENC}(v)\)</span> in embedding space <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span> indicates similarity of nodes <span class="math notranslate nohighlight">\(u, v\)</span> in graph <span class="math notranslate nohighlight">\(G\)</span></p></li>
<li><p>Embeddings encode graph information</p></li>
<li><p>Embeddings are useful for downstream predictions</p></li>
</ul>
<p>Specifically, we need to define</p>
<ul class="simple">
<li><p>A measure of similarity of <span class="math notranslate nohighlight">\(u, v\)</span> in <span class="math notranslate nohighlight">\(G\)</span>, denoted <span class="math notranslate nohighlight">\(\operatorname{sim}(u, v)\)</span></p>
<ul>
<li><p>are adjacent</p></li>
<li><p>share common neighbors</p></li>
<li><p>have similar structural roles</p></li>
</ul>
</li>
<li><p>A measure of similarity of <span class="math notranslate nohighlight">\(\boldsymbol{z} _u, \boldsymbol{z} _v\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span>, aka <strong>decoder</strong>, denoted <span class="math notranslate nohighlight">\(\operatorname{DEC}: \mathbb{R} ^d \times \mathbb{R} ^d \rightarrow \mathbb{R}\)</span>. For instance, cosine similarity <span class="math notranslate nohighlight">\(\boldsymbol{z} _u ^{\top} \boldsymbol{z} _v\)</span>.</p></li>
</ul>
<p>Different models use different measures. The goal is to preserve similarity: if <span class="math notranslate nohighlight">\(\operatorname{sim}(u, v)\)</span> is small/large, we want <span class="math notranslate nohighlight">\(\operatorname{DEC}(\boldsymbol{z} _u, \boldsymbol{z} _v)\)</span> to be small/large.</p>
<p>The simplest encoding approach is embedding-lookup.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{ENC}(v) = \boldsymbol{z} _v = \boldsymbol{Z} \boldsymbol{v}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R} ^{d\times N_v}\)</span> is a matrix where each column is a node embedding, to be learned/optimized.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{v} \in \mathbb{I} ^{N_v}\)</span> is an indicator vector, with all zeros except a one indicating node <span class="math notranslate nohighlight">\(v\)</span></p></li>
</ul>
<p>Obviously, the number of parameters <span class="math notranslate nohighlight">\(d\times N_v\)</span> can be large if <span class="math notranslate nohighlight">\(N_v\)</span> is large. This method is not scalable. Examples include DeepWalk, node2vec.</p>
<div class="section" id="deepwalk">
<h3>DeepWalk<a class="headerlink" href="#deepwalk" title="Permalink to this headline">¶</a></h3>
<p>[Perozzl 2014]</p>
<p>Measures</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\operatorname{sim}(u, v)\)</span>: We introduce a similarity definition that uses random walks. Specifically, in graph <span class="math notranslate nohighlight">\(G\)</span>, let <span class="math notranslate nohighlight">\(N_R(u)\)</span> be the sequence of nodes visited on a random walk starting at <span class="math notranslate nohighlight">\(u\)</span> according to some random walk strategy <span class="math notranslate nohighlight">\(R\)</span> (e.g. uniform). We say <span class="math notranslate nohighlight">\(v \in N_R(u)\)</span> is similar to <span class="math notranslate nohighlight">\(u\)</span>. <span class="math notranslate nohighlight">\(N_R(u)\)</span> is aka random walk neighborhood of <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{DEC}(\boldsymbol{z} _u, \boldsymbol{z} _v)\)</span>: cosine similarity, but normalized over all nodes <span class="math notranslate nohighlight">\(\boldsymbol{z} _w, w\in V\)</span> by sigmoid function,</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{DEC}(\boldsymbol{z} _u, \boldsymbol{z} _v) = \frac{\exp( \boldsymbol{z} _u ^{\top} \boldsymbol{z} _v)}{ \sum_{w \in V} \exp (\boldsymbol{z} _u ^{\top} \boldsymbol{z} _w)}
  \]</div>
<p>This can be interpreted as the probability of visiting <span class="math notranslate nohighlight">\(v\)</span> on a random walk starting from <span class="math notranslate nohighlight">\(u\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[\mathbb{P} (v \in N_R(u)) = \operatorname{DEC}(\boldsymbol{z} _u, \boldsymbol{z} _v)\]</div>
<p>note that it is not symmetric.</p>
</li>
</ul>
<p>Pros of using random walks</p>
<ul class="simple">
<li><p>Expressivity: incorporates local and higher-order neighborhood information. If a random walk starting from node <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> visits <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> w.h.p, then <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> are similar, in terms of high-order multi-hop information</p></li>
<li><p>Efficiency: do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks</p></li>
</ul>
<p>Objective</p>
<ul>
<li><p>Given fixed <span class="math notranslate nohighlight">\(u\)</span>, we run a random walk from <span class="math notranslate nohighlight">\(u\)</span> and obtain <span class="math notranslate nohighlight">\(N_R(u)\)</span>, then the likelihood can be formulated as</p>
<div class="math notranslate nohighlight">
\[
  \prod_{v \in N_R(u)}\mathbb{P} (v \in N_R(u))
  \]</div>
</li>
<li><p>If we run random walk for each <span class="math notranslate nohighlight">\(u \in V\)</span>, the joint log-likelihood is</p>
<div class="math notranslate nohighlight">
\[
  \sum_{u \in V}\sum_{v \in N_R(u)} \log \mathbb{P} (v \in N_R(u))
  \]</div>
</li>
<li><p>substituting <span class="math notranslate nohighlight">\(\mathbb{P} (v \in N_R(u)) = \operatorname{DEC} (\boldsymbol{z} _u, \boldsymbol{z} _v)\)</span>, the loss function is then</p>
<div class="math notranslate nohighlight">
\[
  L(\boldsymbol{Z}) = -  \sum_{u\in V} \sum_{v \in N_R (u)} \log \left( \frac{\exp( \boldsymbol{z} _u ^{\top} \boldsymbol{z} _v)}{ \sum_{w \in V} \exp (\boldsymbol{z} _u ^{\top} \boldsymbol{z} _w)} \right)
  \]</div>
</li>
<li><p>Intuition: learn feature representations <span class="math notranslate nohighlight">\(\boldsymbol{z} _u\)</span> that are predictive of the nodes in its random walk neighborhood <span class="math notranslate nohighlight">\(N_R(u)\)</span></p></li>
</ul>
<p>Learning</p>
<ul class="simple">
<li><p>Run <strong>short fixed-length</strong> random walks starting from each node <span class="math notranslate nohighlight">\(u \in V\)</span> using some random walk strategy <span class="math notranslate nohighlight">\(R\)</span></p></li>
<li><p>Obtain <span class="math notranslate nohighlight">\(N_R(u)\)</span> for each <span class="math notranslate nohighlight">\(u\in V\)</span>. Note that <span class="math notranslate nohighlight">\(N_R(u)\)</span> can be a multiset (repeat elements)</p></li>
<li><p>Optimize embeddings that minimize the loss function <span class="math notranslate nohighlight">\(L(\boldsymbol{Z})\)</span></p></li>
</ul>
<p>Computation</p>
<ul>
<li><p>Two nested summations <span class="math notranslate nohighlight">\(u \in V\)</span> and <span class="math notranslate nohighlight">\(w \in V\)</span> indicates <span class="math notranslate nohighlight">\(\mathcal{O} (N_v ^2)\)</span> complexity. To alleviate this, we approximate the second summation by negative sampling.</p></li>
<li><p><strong>negative sampling</strong>: instead of normalizing w.r.t. all nodes <span class="math notranslate nohighlight">\(w \in V\)</span>, we just normalize against <span class="math notranslate nohighlight">\(k\)</span> random nodes <span class="math notranslate nohighlight">\(w_1, \ldots w_{k}\)</span> sampled from <span class="math notranslate nohighlight">\(V\)</span> according to some distribution <span class="math notranslate nohighlight">\(\mathbb{P} _V\)</span> over nodes.</p>
<div class="math notranslate nohighlight">
\[
  \log \left( \frac{\exp( \boldsymbol{z} _u ^{\top} \boldsymbol{z} _v)}{ \sum_{w \in V} \exp (\boldsymbol{z} _u ^{\top} \boldsymbol{z} _w)} \right) \approx \log \left( \sigma (\boldsymbol{z} _u ^{\top} \boldsymbol{z} _v) \right) - \sum_{i=1}^k \log \left( \sigma (\boldsymbol{z} _u ^{\top} \boldsymbol{z} _{w_i}) \right)
  \]</div>
<ul class="simple">
<li><p>In <span class="math notranslate nohighlight">\(\mathbb{P}_V\)</span>, the probability is proportional to its degree</p></li>
<li><p>Higher <span class="math notranslate nohighlight">\(k\)</span> gives more robust estimates, but corresponds to higher bias on negative events. In practice <span class="math notranslate nohighlight">\(k=5 \sim 20\)</span>.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title"> Why is the approximation valid?</p>
<p>This is a different objective, but negative sampling is a form of noise contrastive estimation which approximately maximizes the log probability of softmax function. The new formulation stands for a logistic regression (sigmoid function) to distinguish the target node <span class="math notranslate nohighlight">\(v\)</span> from nodes <span class="math notranslate nohighlight">\(w_i \sim \mathbb{P}_V\)</span>. See arxiv.1402.3722.</p>
</div>
</li>
<li><p>Then solve by SGD.</p></li>
</ul>
<p>DeepWalk is an unsupervised way since it does not utilizing node labels or node features.</p>
<p>How to choose strategy <span class="math notranslate nohighlight">\(R\)</span>? If uniform from neighbors, then it might be too constrained. Node2vec generalizes this.</p>
</div>
<div class="section" id="node2vec">
<h3>Node2vec<a class="headerlink" href="#node2vec" title="Permalink to this headline">¶</a></h3>
<p>[Grover &amp; Leskovec, 2016]</p>
<p>In node2vec, we use biased random walks that can trade off between local and global views of the network. Local ~ BFS, global ~ DFS.</p>
<div class="figure align-default" id="node-emb-node2vec-ep">
<a class="reference internal image-reference" href="../_images/node-emb-node2vec-ep.png"><img alt="" src="../_images/node-emb-node2vec-ep.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 208 </span><span class="caption-text">Micro- vs Macro-view of neighbourhood [Leskovec 2021]</span><a class="headerlink" href="#node-emb-node2vec-ep" title="Permalink to this image">¶</a></p>
</div>
<p>Two parameters</p>
<ul class="simple">
<li><p>Return parameter <span class="math notranslate nohighlight">\(p\)</span>: return back to the previous node</p></li>
<li><p>In-out parameter <span class="math notranslate nohighlight">\(q\)</span> moving outwards (DFS) vs inwards (BFS). Intuitively, <span class="math notranslate nohighlight">\(q\)</span> is the ratio of BFS vs DFS</p></li>
</ul>
<div class="figure align-default" id="node-emb-node2vec-walk">
<a class="reference internal image-reference" href="../_images/node-emb-node2vec-walk.png"><img alt="" src="../_images/node-emb-node2vec-walk.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 209 </span><span class="caption-text">Walker moved from <span class="math notranslate nohighlight">\(S_1\)</span> to <span class="math notranslate nohighlight">\(W\)</span>, what’s next? [Leskovec 2021]</span><a class="headerlink" href="#node-emb-node2vec-walk" title="Permalink to this image">¶</a></p>
</div>
<p>Learning</p>
<ul class="simple">
<li><p>compute random walk probabilities</p></li>
<li><p>simulate <span class="math notranslate nohighlight">\(r\)</span> biased random walks of length <span class="math notranslate nohighlight">\(\ell\)</span> starting from each node <span class="math notranslate nohighlight">\(u \in V\)</span>, obtain <span class="math notranslate nohighlight">\(N_R(u)\)</span></p></li>
<li><p>use the same objective function, negative sampling, SGD, as DeepWalk</p></li>
</ul>
<p>Computation</p>
<ul class="simple">
<li><p>Linear-time complexity</p></li>
<li><p>All 3 steps are individually parallelizable</p></li>
</ul>
<p>Extensions</p>
<ul class="simple">
<li><p>different kinds of biased random walks</p>
<ul>
<li><p>based on node attributes</p></li>
<li><p>based on learned weights</p></li>
</ul>
</li>
<li><p>run random walks on modified versions of the original network</p></li>
</ul>
<p>Remarks</p>
<ul class="simple">
<li><p>node2vec performs better on node classification, while alternative methods perform better on link prediction [Goyal &amp; Ferrara 2017]</p></li>
<li><p>in practice, choose definition of node similarity that matches application</p></li>
</ul>
</div>
<div class="section" id="pagerank">
<h3>PageRank<a class="headerlink" href="#pagerank" title="Permalink to this headline">¶</a></h3>
<p>In WWWW, Consider page as nodes and hyperlinks as directed edges. We want to rank the importance of the pages. The importance can be seen as 1-dimensional node embeddings.</p>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>Assumption</p>
<ul class="simple">
<li><p>a page is more important if it has more in-coming links</p></li>
<li><p>links from important pages worth more</p></li>
<li><p>all pages have at least one out-going links, <span class="math notranslate nohighlight">\(d_i \ge 1\)</span></p></li>
<li><p>if a page <span class="math notranslate nohighlight">\(i\)</span> with importance <span class="math notranslate nohighlight">\(r_i\)</span> has <span class="math notranslate nohighlight">\(d_i\)</span> out-links, each link gets <span class="math notranslate nohighlight">\(r_i/d_i\)</span> importance.</p></li>
<li><p>page <span class="math notranslate nohighlight">\(j\)</span>’s own importance <span class="math notranslate nohighlight">\(r_j\)</span> is the sum of the votes on its in-links. <span class="math notranslate nohighlight">\(r_j = \sum_{i: i \rightarrow  j} r_i/d_i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i=1}^{N_v} r_i =1\)</span>.</p></li>
</ul>
<p>Define a matrix <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> such that <span class="math notranslate nohighlight">\(M_{ij} = \frac{u, v}{d_j}\)</span> if <span class="math notranslate nohighlight">\(j \rightarrow i\)</span>. We can see it is a column stochastic matrix. The above assumptions leads to the flow equation</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{r} = \boldsymbol{M} \boldsymbol{r}  
\]</div>
</div>
<div class="section" id="computation">
<h4>Computation<a class="headerlink" href="#computation" title="Permalink to this headline">¶</a></h4>
<p>To solve <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span>, we can use a linear system, but not scalable.</p>
<p>Note that the column stochastic matrix <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> can define a <a class="reference internal" href="../35-graphical-models/03-random-walks.html#rw-graph"><span class="std std-ref">random walk over graphs</span></a>: a walker at <span class="math notranslate nohighlight">\(i\)</span> follows an out-link from <span class="math notranslate nohighlight">\(i\)</span> uniformly at random. Since <span class="math notranslate nohighlight">\(\boldsymbol{r} = \boldsymbol{M} \boldsymbol{r}\)</span>, we know that <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span> is a stationary distribution for the random walk.</p>
<div class="seealso admonition">
<p class="admonition-title"> R.t. Eigenvector centrality</p>
<p>Recall <a class="reference internal" href="11-descriptive-analysis.html#eig-centrality"><span class="std std-ref">eigenvector centrality</span></a> <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span> can be solved by the first eigenvector of adjacency matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A} \boldsymbol{c} = \lambda \boldsymbol{c}
\]</div>
<p>In this problem, <span class="math notranslate nohighlight">\(\boldsymbol{M} = \boldsymbol{A} \boldsymbol{D} ^{-1}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span> is the principal eigenvector (i.e. with eigenvalue 1) of <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span>.</p>
</div>
<p>We can use power iteration to find <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span>. But</p>
<ul class="simple">
<li><p>if there is a page <span class="math notranslate nohighlight">\(i\)</span> without out-going links (aka ‘dead end’), then <span class="math notranslate nohighlight">\(M_{\cdot i} = 0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> is not column stochastic which violates the assumption;</p></li>
<li><p>if <span class="math notranslate nohighlight">\(i\)</span> only has a self-loop (aka ‘spider traps’), i.e. <span class="math notranslate nohighlight">\(M_{ii}=1\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span> is degenerate: <span class="math notranslate nohighlight">\(r_i = 1\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{r}\)</span>, this page has dominates importance.</p></li>
</ul>
<p>To overcome these issues, we use teleport trick:</p>
<ul class="simple">
<li><p>w.p. <span class="math notranslate nohighlight">\(\beta\)</span> follow a link uniformly at random, usually <span class="math notranslate nohighlight">\(\beta = 0.8, 0.9\)</span></p></li>
<li><p>w.p. <span class="math notranslate nohighlight">\(1-\beta\)</span> jumpy to a random page in <span class="math notranslate nohighlight">\(V\)</span></p></li>
</ul>
<p>Hence, the PageRank equation [Brin-Page 98] is</p>
<div class="math notranslate nohighlight">
\[
r_j = \sum_{i: i \rightarrow j} \beta \frac{r_i}{d_i}  + (1-\beta) \frac{u, v}{N_v}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{r} = \beta \boldsymbol{M} \boldsymbol{r} + (1-\beta)/N_v  \boldsymbol{u, v}  
\]</div>
<p>Note that this formulation assumes that <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> has no dead ends. We can either preprocess matrix <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> to remove all dead ends or explicitly follow random teleport links with probability 1 from dead-ends.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{P} = \beta \boldsymbol{M} + (1- \boldsymbol{\beta} )/N_v \boldsymbol{u, v} \boldsymbol{u, v} ^{\top}\)</span>, then we have <span class="math notranslate nohighlight">\(\boldsymbol{r}  = \boldsymbol{P} \boldsymbol{r}\)</span>. The random walk characterized by column-stochastic matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> has no dead ends or spider traps, hence we can use the power method over <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span>.</p>
</div>
<div class="section" id="personalized-pagerank">
<h4>Personalized PageRank<a class="headerlink" href="#personalized-pagerank" title="Permalink to this headline">¶</a></h4>
<p>Aka Topic-specific PageRank</p>
<p>In personalized PageRank, a walker does not teleport to all nodes <span class="math notranslate nohighlight">\(V\)</span>, but to some subset <span class="math notranslate nohighlight">\(S\)</span> of nodes.</p>
<p>If <span class="math notranslate nohighlight">\(S\)</span> is the start node, then we call this <strong>random walks with restarts</strong>. We can then use this kind of random walk to form a proximity measure of two nodes: simulate multiple random walk starting from node <span class="math notranslate nohighlight">\(s\)</span> with restarts, and then count the number of visits to other nodes. Nodes with higher visit count have higher proximity.</p>
<p>The relative frequencies can also be found using power iteration. The uniform teleport probability <span class="math notranslate nohighlight">\((1-\beta)/N_v  \boldsymbol{u, v}\)</span> in PageRank now becomes <span class="math notranslate nohighlight">\(\boldsymbol{e} _s\)</span>.</p>
<p>This method also applies to a subset <span class="math notranslate nohighlight">\(S\)</span> of multiple nodes. The teleport probability is non-zero for <span class="math notranslate nohighlight">\(v \in S\)</span> but 0 otherwise.</p>
<p>Random walks with restarts can be used in recommender systems. The user-item networks can be viewed as a bipartite graph. We can let <span class="math notranslate nohighlight">\(S\)</span> be a subset of items, and run the above algorithm, but only count the number of visits to items. Than we obtain a proximity measure of items.</p>
<p>pseudo-code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">item</span> <span class="o">=</span> <span class="n">QUERY_NODES</span><span class="o">.</span><span class="n">sample_by_weight</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span> <span class="n">N_STEPS</span> <span class="p">):</span>
  <span class="n">user</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">get_random_neighbor</span><span class="p">()</span>
  <span class="n">item</span> <span class="o">=</span> <span class="n">user</span><span class="o">.</span><span class="n">get_random_neighbor</span><span class="p">()</span>
  <span class="n">item</span><span class="o">.</span><span class="n">visit_count</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">random</span><span class="p">(</span> <span class="p">)</span> <span class="o">&gt;</span> <span class="n">beta</span><span class="p">:</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">QUERY_NODES</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">by_weight</span> <span class="p">()</span>
</pre></div>
</div>
<p>The similarity consider</p>
<ul class="simple">
<li><p>multiple connections</p></li>
<li><p>multiple paths</p></li>
<li><p>direct and indirect connections</p></li>
<li><p>degree of the node</p></li>
</ul>
</div>
</div>
<div class="section" id="r-t-matrix-factorization">
<h3>R.t. Matrix Factorization<a class="headerlink" href="#r-t-matrix-factorization" title="Permalink to this headline">¶</a></h3>
<p>Consider two simple measures</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{similarity}(u, v)\)</span>: two nodes are similar if they are adjacent.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{DEC(\boldsymbol{z} _u, \boldsymbol{z} _v)} = \boldsymbol{z} _u ^{\top} \boldsymbol{z} _v\)</span></p></li>
</ul>
<p>Let our <span class="math notranslate nohighlight">\(d\)</span>-dimensional embeddings be <span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R} ^{d \times n}\)</span>. Then we want to find <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span> such that <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{Z} ^{\top} \boldsymbol{Z}\)</span>. However, exact factorization of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is generally impossible (unless it is p.s.d.). We can then approximate the adjacency matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{Z} ^{\top} \boldsymbol{Z}\)</span>. The problem can be formulated as</p>
<div class="math notranslate nohighlight">
\[
\min\ \left\| \boldsymbol{A} - \boldsymbol{Z} ^{\top} \boldsymbol{Z}  \right\| _F
\]</div>
<p>Conclusion: inner product decoder with node similarity defined by edge connectivity is equivalent to matrix factorization of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>.</p>
<p>DeepWalk and node2vec can also be formulated as matrix factorization problem.</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{M} = \log \left(\operatorname{vol}(G)\left(\frac{u, v}{T} \sum_{r=1}^{T}\left(D^{-1} A\right)^{r}\right) D^{-1}\right)-\log b
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{vol}(G) = \sum_{i,j}^n a_{ij}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(T = \left\vert N_R(u) \right\vert\)</span> is the length of random walks</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span> is the number of negative samples.</p></li>
</ul>
<p>The matrix for node2vec is more complex.</p>
<p>Hence rather than simulating random walks and then use SGD to find <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span>, we can solve the minimization problem <span class="math notranslate nohighlight">\(\min \left\| \boldsymbol{M} - \boldsymbol{Z} ^{\top} \boldsymbol{Z}  \right\| _F\)</span>.</p>
<p>See Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec [WSDM 18]</p>
</div>
<div class="section" id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h3>
<p>There are limitations of node embeddings via matrix factorization and random walks.</p>
<ul class="simple">
<li><p>They learn the embeddings, not a function for embedding. Hence, they cannot obtain embeddings for nodes not in the training set. (similar problem happens in PCA, MDS).</p></li>
<li><p>Cannot capture structural similarity of two nodes. Hard to define <span class="math notranslate nohighlight">\(\operatorname{similarity} (u, v)\)</span> to measure structural similarity of two nodes, e.g. both are a vertex in a triangle in two different subgraphs in the graph. Anonymous random walk introduced below solves this problem.</p></li>
<li><p>Cannot utilize node, edge and graph features. Sol: graph neural networks.</p></li>
</ul>
</div>
</div>
<div class="section" id="graph-embeddings">
<h2>Graph Embeddings<a class="headerlink" href="#graph-embeddings" title="Permalink to this headline">¶</a></h2>
<p>Can we entire an entire graph <span class="math notranslate nohighlight">\(G\)</span> or some subgraph? For instance, classification of molecules, or identifying anomalous graphs.</p>
<p>Simple ideas:</p>
<ul class="simple">
<li><p>to embed an entire graph, first obtain node embeddings, then take sum or average. [Duvenaud+ 2016]</p></li>
<li><p>to embed a subgraph <span class="math notranslate nohighlight">\(S\)</span> of <span class="math notranslate nohighlight">\(G\)</span>, add a virtual node <span class="math notranslate nohighlight">\(v\)</span> to <span class="math notranslate nohighlight">\(G\)</span> with edges <span class="math notranslate nohighlight">\((v, u)\)</span> for all <span class="math notranslate nohighlight">\(u \in H\)</span>. Obtain node embedding of <span class="math notranslate nohighlight">\(v\)</span> in <span class="math notranslate nohighlight">\(\left\{ G \cup \left\{ v \right\} \right\}\)</span>, use it as the embedding of <span class="math notranslate nohighlight">\(S\)</span>. [Li+ 2016]</p></li>
</ul>
<div class="figure align-default" id="graph-emb-2">
<a class="reference internal image-reference" href="../_images/graph-emb-2.png"><img alt="" src="../_images/graph-emb-2.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 210 </span><span class="caption-text">Graph embedding by virtual node [Leskovec 2021]</span><a class="headerlink" href="#graph-emb-2" title="Permalink to this image">¶</a></p>
</div>
<p>Another example use anonymous walk over graphs.</p>
<div class="section" id="anonymous-walk-embeddings">
<h3>Anonymous Walk Embeddings<a class="headerlink" href="#anonymous-walk-embeddings" title="Permalink to this headline">¶</a></h3>
<p>arxiv.1805.11921 2018</p>
<dl class="simple myst">
<dt>Definition (Anonymous walks)</dt><dd><p>An anonymous walk is a special type of random walk where the states correspond to the index of the first time we visited the node, rather than the node label itself.</p>
</dd>
</dl>
<p>The states are agnostic to the identity of the nodes visited (hence anonymous)</p>
<div class="figure align-default" id="graph-emb-anon-walk">
<a class="reference internal image-reference" href="../_images/graph-emb-anon-walk.png"><img alt="" src="../_images/graph-emb-anon-walk.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 211 </span><span class="caption-text">Anonymous walks</span><a class="headerlink" href="#graph-emb-anon-walk" title="Permalink to this image">¶</a></p>
</div>
<p>Let <span class="math notranslate nohighlight">\(\eta_\ell\)</span> be the number of distinct anonymous walks of length <span class="math notranslate nohighlight">\(\ell\)</span>. It is easy to see then when length <span class="math notranslate nohighlight">\(\ell\)</span> of a anonymous walk is <span class="math notranslate nohighlight">\(3\)</span>, there are 5 anonymous walks</p>
<div class="math notranslate nohighlight">
\[
w_{u, v}=111, w_{2}=112, w_{3}=121, w_{4}=122, w_{5}=123
\]</div>
<p>The number <span class="math notranslate nohighlight">\(\eta_\ell\)</span> grows exponentially wicvvc th <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<div class="figure align-default" id="graph-emb-anon-walk-number">
<a class="reference internal image-reference" href="../_images/graph-emb-anon-walk-number.png"><img alt="" src="../_images/graph-emb-anon-walk-number.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 212 </span><span class="caption-text">Number of anonymous walks</span><a class="headerlink" href="#graph-emb-anon-walk-number" title="Permalink to this image">¶</a></p>
</div>
<p>Learning</p>
<ul class="simple">
<li><p>Simulate independently a set of <span class="math notranslate nohighlight">\(m\)</span> anonymous walks <span class="math notranslate nohighlight">\(w\)</span> of <span class="math notranslate nohighlight">\(\ell\)</span> steps and record their counts</p></li>
<li><p>Use the sample distribution of the walks as <span class="math notranslate nohighlight">\(\boldsymbol{z} _G\)</span>. For instance, if <span class="math notranslate nohighlight">\(\ell =3\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{z} _G \in \mathbb{R}^5\)</span>.</p></li>
</ul>
<p>Computation</p>
<ul>
<li><p>How many anonymous walks <span class="math notranslate nohighlight">\(m\)</span> do we need? If we want the distribution has error <span class="math notranslate nohighlight">\(\epsilon\)</span> w.p. less than <span class="math notranslate nohighlight">\(\delta\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
  m=\left[\frac{2}{\varepsilon^{2}}\left(\log \left(2^{\eta_\ell}-2\right)-\log (\delta)\right)\right]
  \]</div>
</li>
</ul>
</div>
<div class="section" id="anonymous">
<h3>Anonymous+<a class="headerlink" href="#anonymous" title="Permalink to this headline">¶</a></h3>
<p>We learn <span class="math notranslate nohighlight">\(\boldsymbol{z} _G\)</span> together with anonymous walk embeddings <span class="math notranslate nohighlight">\(\boldsymbol{z} _i\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, \eta\)</span> where <span class="math notranslate nohighlight">\(\eta\)</span> is the number of simulated distinct anonymous walks.</p>
<p>The intuition is, for <span class="math notranslate nohighlight">\(T\)</span> independently simulated anonymous walks of length <span class="math notranslate nohighlight">\(\ell\)</span> starting from the same node <span class="math notranslate nohighlight">\(u\)</span>, denoted <span class="math notranslate nohighlight">\(w_1^u, w_2^u, \ldots, w_T^u\)</span>, they should be ‘similar’. The embeddings can be optimized such that the walk <span class="math notranslate nohighlight">\(w_t^u\)</span> can be predicted by its left and right walk ‘neighbors’ in a <span class="math notranslate nohighlight">\(\Delta\)</span>-size window: <span class="math notranslate nohighlight">\(w_s^u\)</span> for <span class="math notranslate nohighlight">\(s=t-\Delta, t-\Delta+1, \ldots, t-1, t+1, \ldots, t+\Delta\)</span>.</p>
<p>Objective:</p>
<div class="math notranslate nohighlight">
\[
\max _{\mathrm{\boldsymbol{Z}}, \mathrm{d}} \sum_{u \in V} \frac{u, v}{T} \sum_{t=\Delta}^{T-\Delta} \log \mathbb{P} \left(w^u_{t} \mid\left\{w^u_{t-\Delta}, \ldots, w^u_{t+\Delta}, \boldsymbol{z} _{G}\right\}\right)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{P} \left(w_{t} \mid\left\{w_{t-\Delta}, \ldots, w_{t+\Delta}, \boldsymbol{z}_{\boldsymbol{G}}\right\}\right)=\frac{\exp \left(y\left(w_{t}\right)\right)}{\sum_{i=1}^{\eta} \exp \left(y\left(w_{i}\right)\right)}\)</span>. Note the denominator is over <span class="math notranslate nohighlight">\(\eta\)</span> distinct sampled walks (require negative sampling)</p></li>
<li><p><span class="math notranslate nohighlight">\(y\left(w_{t}\right)=\beta_0 + \boldsymbol{\beta} ^{\top} [\frac{u, v}{2 \Delta} \sum_{i=-\Delta}^{\Delta} \boldsymbol{z}_{i}; \boldsymbol{z}_{\boldsymbol{G}}]\)</span>, where <span class="math notranslate nohighlight">\(\beta_0, \boldsymbol{\beta}\)</span> are learnable parameters. ‘;’ stands for vertical concatenation This step represents a linear layer.</p></li>
</ul>
</div>
<div class="section" id="hierarchical-embeddings">
<h3>Hierarchical Embeddings<a class="headerlink" href="#hierarchical-embeddings" title="Permalink to this headline">¶</a></h3>
<p>We can hierarchically cluster nodes in graphs, and sum/avg the node embeddings according to these clusters.</p>
<div class="figure align-default" id="graph-hier-emb">
<a class="reference internal image-reference" href="../_images/graph-hier-emb.png"><img alt="" src="../_images/graph-hier-emb.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 213 </span><span class="caption-text">Hierarchical Embeddings</span><a class="headerlink" href="#graph-hier-emb" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="message-passing-and-node-classification">
<h2>Message Passing and Node Classification<a class="headerlink" href="#message-passing-and-node-classification" title="Permalink to this headline">¶</a></h2>
<p>Consider a classification problem: Given a network with node features. Some node are labeled. How do we assign labels to all other non-labeled nodes in the network?</p>
<p>One may use node embeddings to build a classifier. We also introduce a method called message passing.</p>
<div class="figure align-default" id="graph-node-classification">
<a class="reference internal image-reference" href="../_images/graph-node-classification.png"><img alt="" src="../_images/graph-node-classification.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 214 </span><span class="caption-text">Node classification [Leskovec 2021]</span><a class="headerlink" href="#graph-node-classification" title="Permalink to this image">¶</a></p>
</div>
<p>Notation</p>
<ul class="simple">
<li><p>Labeled data of size <span class="math notranslate nohighlight">\(\ell\)</span>: <span class="math notranslate nohighlight">\((\mathcal{X}_\ell, \mathcal{Y}_\ell) = \left\{ (x_{1:\ell}, y_{1:\ell}) \right\}\)</span></p></li>
<li><p>Unlabeled data <span class="math notranslate nohighlight">\(\mathcal{X}_u = \left\{ x_{\ell + 1:n} \right\}\)</span></p></li>
<li><p>adjacency matrix of <span class="math notranslate nohighlight">\(n\)</span> nodes <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, from which we can have <span class="math notranslate nohighlight">\(\boldsymbol{A} _\ell\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{A} _u\)</span></p></li>
</ul>
<p>This can be viewed as a <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html#semi-supervised"><span class="std std-ref">semi-supervised</span></a> method, where we use <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _\ell, \mathcal{Y} _\ell, \mathcal{X} _u, \boldsymbol{A} \right\}\)</span> to predict <span class="math notranslate nohighlight">\(\mathcal{Y} _u\)</span>. It is also a collective classification method, which assigns labels to all nodes simultaneously.</p>
<p>Key observation: correlation exists in networks, nearby nodes have the same characteristics. In social science, there are two concepts</p>
<ul class="simple">
<li><p>Homophily: individual characteristics affect social connection. Individuals with similar characteristics tend to be close.</p></li>
<li><p>Influence: social connection affects individual characteristics. One node’s characteristics can affect that of nearby nodes.</p></li>
</ul>
<p>Hence, the label of <span class="math notranslate nohighlight">\(v\)</span> may depend on</p>
<ul class="simple">
<li><p>its feature <span class="math notranslate nohighlight">\(x_v\)</span></p></li>
<li><p>its nearby nodes’ features <span class="math notranslate nohighlight">\(x_u\)</span></p></li>
<li><p>its nearby nodes’ labels <span class="math notranslate nohighlight">\(y_u\)</span></p></li>
</ul>
<p>In general, collective classification has three steps</p>
<ol class="simple">
<li><p>Learn a <strong>local</strong> classifier to assign initial labels, using <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _\ell, \mathcal{Y} _\ell, \mathcal{X} _u \right\}\)</span> without using network information <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span></p></li>
<li><p>Learn a <strong>relational</strong> classifier to label one node based on the labels and/or features of its neighbors. This step uses <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, and captures correlation between nodes.</p></li>
<li><p>Collective inference: apply relational classifier to each node iteratively, until convergence of <span class="math notranslate nohighlight">\(\mathcal{Y}_\ell\)</span>.</p></li>
</ol>
<p>In the following we introduce some traditional methods, which are motivation for graphical neural networks.</p>
<div class="section" id="relational-classification">
<h3>Relational Classification<a class="headerlink" href="#relational-classification" title="Permalink to this headline">¶</a></h3>
<p>Model: Class probability of a node equals the weighted average of class probability of its neighbors.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P} \left(Y_{v}=c\right)
&amp;= \frac{u, v}{d_v} \sum_{u \in \mathscr{N} (v)} \mathbb{P} (Y_u = c) \\  
&amp;=\frac{u, v}{\sum_{(v, u) \in E} A_{v, u}} \sum_{(v, u) \in E} A_{v, u} \mathbb{P} \left(Y_{u}=c\right)
\end{aligned}\end{split}\]</div>
<p>Algorithm</p>
<ul class="simple">
<li><p>Initialize</p>
<ul>
<li><p>for labeled nodes, use ground-truth label <span class="math notranslate nohighlight">\(y_v\)</span></p></li>
<li><p>for unlabeled nodes, use <span class="math notranslate nohighlight">\(Y_v = 0.5\)</span></p></li>
</ul>
</li>
<li><p>Run iterations until convergence of labels or maximum number of iterations achieved</p>
<ul>
<li><p>Update labels of all non-labeled nodes in a random order</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="graph-relational-classification">
<a class="reference internal image-reference" href="../_images/graph-relational-classification.png"><img alt="" src="../_images/graph-relational-classification.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 215 </span><span class="caption-text">Relational classification [Leskovec 2021]</span><a class="headerlink" href="#graph-relational-classification" title="Permalink to this image">¶</a></p>
</div>
<p>If edge weights is provided, we can replace <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span></p>
<p>Cons</p>
<ul class="simple">
<li><p>Convergence is not guaranteed</p></li>
<li><p>This method do not use features <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p></li>
</ul>
</div>
<div class="section" id="iterative-classification">
<h3>Iterative Classification<a class="headerlink" href="#iterative-classification" title="Permalink to this headline">¶</a></h3>
<p>Iterative classification uses both features and labels.</p>
<p>Train two classifiers</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi_1 (x _v)\)</span> to predict node label <span class="math notranslate nohighlight">\(y_v\)</span> based on node feature vector <span class="math notranslate nohighlight">\(x_v\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_w (f_v, z_v)\)</span> to predict node label <span class="math notranslate nohighlight">\(y_v\)</span> based on node feature vector <span class="math notranslate nohighlight">\(f_v\)</span> and summary <span class="math notranslate nohighlight">\(z_v\)</span> of labels of its neighbors <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span>. <span class="math notranslate nohighlight">\(z_v\)</span> can be</p>
<ul>
<li><p>relative frequencies of the number of each label in <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span></p></li>
<li><p>most common label in <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span></p></li>
<li><p>number of different labels in <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span></p></li>
</ul>
</li>
</ul>
<p>Learning</p>
<ul class="simple">
<li><p>Phase 1: train classifiers on a training set <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _\ell, \mathcal{Y} _\ell \right\}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\phi_1 (x)\)</span> using <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _\ell, \mathcal{Y} _\ell \right\}\)</span></p></li>
<li><p>compute <span class="math notranslate nohighlight">\(\mathcal{Z}_\ell\)</span> using <span class="math notranslate nohighlight">\(\mathcal{Y} _\ell\)</span> and network information <span class="math notranslate nohighlight">\(\boldsymbol{A} _\ell\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_2 (x, z)\)</span> using <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _\ell, \mathcal{Z}_\ell, \mathcal{Y} _\ell \right\}\)</span></p></li>
</ul>
</li>
<li><p>Phase 2: iteration</p>
<ul>
<li><p>on test set <span class="math notranslate nohighlight">\(\left\{ \mathcal{X} _u \right\}\)</span></p>
<ul>
<li><p>initialize label <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 1}\)</span> by <span class="math notranslate nohighlight">\(\phi_1 (x_u)\)</span></p></li>
<li><p>compute <span class="math notranslate nohighlight">\(z_u\)</span> by <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 1}\)</span> and network information <span class="math notranslate nohighlight">\(\boldsymbol{A} _u\)</span></p></li>
<li><p>update label <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 2}\)</span> by <span class="math notranslate nohighlight">\(\phi_2 (x_u, z_u)\)</span></p></li>
</ul>
</li>
<li><p>repeat for <strong>each</strong> node until labels <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 2}\)</span> stabilize or max number of iterations is reached</p>
<ul>
<li><p>compute <span class="math notranslate nohighlight">\(z_u\)</span> by <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 2}\)</span> and network information <span class="math notranslate nohighlight">\(\boldsymbol{A} _u\)</span></p></li>
<li><p>update label <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 2}\)</span> by <span class="math notranslate nohighlight">\(\phi_2 (x_u, z_u)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Remarks</p>
<ul class="simple">
<li><p>training set is only used for training <span class="math notranslate nohighlight">\(\phi_1, \phi_2\)</span>, not involved in iteration</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_1\)</span> is used to initialize labels <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 1}\)</span>, which is then used in iteration</p></li>
<li><p>the output <span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}_{u, 2}\)</span>, obtained from <span class="math notranslate nohighlight">\(\phi_2\)</span>, use information from both node features and labels.</p></li>
<li><p>convergence is not guaranteed.</p></li>
</ul>
</div>
<div class="section" id="belief-propagation">
<h3>Belief Propagation<a class="headerlink" href="#belief-propagation" title="Permalink to this headline">¶</a></h3>
<p>Belief propagation is a dynamic programming approach to answering probability queries in a graph. It is an iterative process of passing messages to neighbors. The message sent from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span></p>
<ul class="simple">
<li><p>depends on messages <span class="math notranslate nohighlight">\(i\)</span> received from its neighbors</p></li>
<li><p>contains <span class="math notranslate nohighlight">\(i\)</span>’s belief of the state of <span class="math notranslate nohighlight">\(j\)</span>, e.g. when the state is label, the belief can be ‘node <span class="math notranslate nohighlight">\(i\)</span> believes node <span class="math notranslate nohighlight">\(j\)</span> belong to class 1 with likelihood …’.</p></li>
</ul>
<p>When consensus is reached, we can calculate final belief.</p>
<div class="section" id="in-acyclic-graphs">
<h4>In Acyclic Graphs<a class="headerlink" href="#in-acyclic-graphs" title="Permalink to this headline">¶</a></h4>
<p>We introduce belief on labels in acyclic graphs as an example. Define</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is the set of all classes/labels</p></li>
<li><p><strong>Label-label potential matrix</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\psi}\)</span> over <span class="math notranslate nohighlight">\(\mathcal{L} \times \mathcal{L}\)</span>. The entry is</p>
<div class="math notranslate nohighlight">
\[\psi(Y_i, Y_j) \propto \mathbb{P} (Y_j \mid Y_i)\]</div>
<p>is proportional to the probability of a node <span class="math notranslate nohighlight">\(j\)</span> being in class <span class="math notranslate nohighlight">\(Y_j\)</span> given that it has neighbor <span class="math notranslate nohighlight">\(i\)</span> in class <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
</li>
<li><p><strong>Prior belief</strong> <span class="math notranslate nohighlight">\(\phi\)</span> over <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\phi(Y_i)\propto \mathbb{P} (Y_i)\]</div>
<p>is proportional to the probability that node <span class="math notranslate nohighlight">\(i\)</span> being in class <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(m_{i \rightarrow j}(Y_j)\)</span> is <span class="math notranslate nohighlight">\(i\)</span>’s belief/<strong>message</strong>/estimate of <span class="math notranslate nohighlight">\(j\)</span> being in class <span class="math notranslate nohighlight">\(Y_j\)</span>, which can be compute by</p>
<div class="math notranslate nohighlight">
\[
  m_{i \rightarrow j}(Y_j) = \sum_{Y_i \in \mathcal{L}} \left[ \psi(Y_i, Y_j) \phi (Y_i) \prod_{k \in N_i \setminus j} m_{k \rightarrow i} (Y_i) \right] \quad \forall Y_j \in \mathcal{L}
  \]</div>
<p>This message is a ‘combination’ of conditional probabilities, prior probabilities, and ‘prior’ message from <span class="math notranslate nohighlight">\(i\)</span>’s neighbors.</p>
</li>
</ul>
<div class="figure align-default" id="graph-belief-prop">
<a class="reference internal image-reference" href="../_images/graph-belief-prop.png"><img alt="" src="../_images/graph-belief-prop.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 216 </span><span class="caption-text">Belief propagation [Leskovec 2021]</span><a class="headerlink" href="#graph-belief-prop" title="Permalink to this image">¶</a></p>
</div>
<p>Learning</p>
<ul class="simple">
<li><p>Learn <span class="math notranslate nohighlight">\(\boldsymbol{\Psi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> by some methods.</p></li>
<li><p>Initialize all messages <span class="math notranslate nohighlight">\(m\)</span> to <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p>Since the graph is acyclic, we can define an ordering of nodes. Start from some node, we follow this ordering to compute <span class="math notranslate nohighlight">\(m\)</span> for each node. Repeat until convergence</p></li>
<li><p>Compute self belief as output: node <span class="math notranslate nohighlight">\(i\)</span>’s belief of being in class <span class="math notranslate nohighlight">\(Y_i\)</span>
$<span class="math notranslate nohighlight">\(b_i (Y_i) = \phi(Y_i) \prod_{k \in N_i} m_{k \rightarrow  i} (Y_i)\quad \forall Y_j \in \mathcal{L}\)</span>$</p></li>
</ul>
<p>The messages in the starting node can be viewed as <strong>separate evidence</strong>, since they do not depend on each other.</p>
</div>
<div class="section" id="in-cyclic-graphs">
<h4>In Cyclic Graphs<a class="headerlink" href="#in-cyclic-graphs" title="Permalink to this headline">¶</a></h4>
<p>It is also called loopy belief propagation since people also used it over graphs with cycles.</p>
<p>Problems in cyclic graphs</p>
<ul class="simple">
<li><p>Messages from different subgraphs are no longer independent. There is no ‘separate’ evidence.</p></li>
<li><p>The initial belief of <span class="math notranslate nohighlight">\(i\)</span> (which could be incorrect) is reinforced/amplified by a cycle, e.g. <span class="math notranslate nohighlight">\(i \rightarrow j \rightarrow k \rightarrow  u \rightarrow i\)</span></p></li>
<li><p>convergence guarantee and the previous interpretation may be lost</p></li>
</ul>
<div class="figure align-default" id="graph-belief-prop-cycle">
<a class="reference internal image-reference" href="../_images/graph-belief-prop-cycle.png"><img alt="" src="../_images/graph-belief-prop-cycle.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 217 </span><span class="caption-text">Loopy belief propagation on a cyclic graph</span><a class="headerlink" href="#graph-belief-prop-cycle" title="Permalink to this image">¶</a></p>
</div>
<p>In practice, Loopy BP is still a good heuristic for complex graphs which contain many branches, few cycles, or long cycles (weak ).</p>
<p>Since there is no ordering, some modification of the algorithm is necessary.</p>
<ul class="simple">
<li><p>start from arbitrary nodes.</p></li>
<li><p>follow the edges to update the neighboring nodes, like a random walker.</p></li>
</ul>
</div>
<div class="section" id="review">
<h4>Review<a class="headerlink" href="#review" title="Permalink to this headline">¶</a></h4>
<p>Advantages:</p>
<ul class="simple">
<li><p>Easy to program &amp; parallelize</p></li>
<li><p>Generalize: can apply to any graph model with any form of potentials</p>
<ul>
<li><p>e.g. higher order: e.g. <span class="math notranslate nohighlight">\(\phi (Y_i, Y_j, Y_k)\)</span></p></li>
</ul>
</li>
<li><p>Challenges:</p>
<ul>
<li><p>Convergence is not guaranteed (when to stop?), especially if many closed loops</p></li>
<li><p>Potential functions (parameters) need to be estimated</p></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="graphical-neural-networks">
<h2>Graphical Neural Networks<a class="headerlink" href="#graphical-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Recall the limitations of shallow embedding methods:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{O} (N_v)\)</span> parameters are needed</p>
<ul>
<li><p>no sharing of parameters between nodes</p></li>
</ul>
</li>
<li><p>transductive, no out-of-sample prediction.</p></li>
<li><p>do not incorporate node features</p></li>
</ul>
<p>Now we introduce deep graph encoders for node embeddings, where <span class="math notranslate nohighlight">\(\operatorname{ENC}(v)\)</span> are deep neural networks. Note that the <span class="math notranslate nohighlight">\(\operatorname{similarity}(u,v)\)</span> measures can also be incorporated into GNN. The output can be node embeddings, sub graph embeddings, or entire graph embeddings, to be used for downstream tasks.</p>
<div class="figure align-default" id="gnn-pipeline">
<a class="reference internal image-reference" href="../_images/gnn-pipeline.png"><img alt="" src="../_images/gnn-pipeline.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 218 </span><span class="caption-text">Pipeline of graphical neural networks</span><a class="headerlink" href="#gnn-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(G\)</span> be an undirected graph of order <span class="math notranslate nohighlight">\(N_v\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> be the <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> adjacency matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> be the <span class="math notranslate nohighlight">\(N_v \times d\)</span> node features</p></li>
</ul>
<p>A naive idea to represent the graph with features is to use the <span class="math notranslate nohighlight">\(N_v \times (N_v + d)\)</span> matrix <span class="math notranslate nohighlight">\([\boldsymbol{A} , \boldsymbol{X}]\)</span>, and then feed into NN. However, there are some issues</p>
<ul class="simple">
<li><p>for node-level task</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_v + d\)</span> parameters &gt; <span class="math notranslate nohighlight">\(N_v\)</span> examples (nodes), easy overfit</p></li>
</ul>
</li>
<li><p>for graph-level task:</p>
<ul>
<li><p>not applicable to graphs of different sizes</p></li>
<li><p>sensitive to node ordering</p></li>
</ul>
</li>
</ul>
<div class="section" id="structure">
<h3>Structure<a class="headerlink" href="#structure" title="Permalink to this headline">¶</a></h3>
<p>To solve the above issues, GNN borrows idea of CNN filters (hence GNN is also called graphical convolutional neural networks GCN).</p>
<div class="section" id="computation-graphs">
<h4>Computation Graphs<a class="headerlink" href="#computation-graphs" title="Permalink to this headline">¶</a></h4>
<p>In CNN, a convolutional operator can be viewed as an operator over lattices. Can we generalize it to subgraphs? How to define sliding windows?</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Bipartite graph can be ‘projected’ to obtain ‘neighbors’. Useful in recommender systems.</p>
</div>
<p>Consider a 3 by 3 filter in CNN. We aggregate information in 9 cells and and then output one cell. In a graph, we can aggregate information in a neighborhood <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span> and output one ‘node’. For instance, given messages <span class="math notranslate nohighlight">\(h_j\)</span> from neighbor <span class="math notranslate nohighlight">\(j\)</span> with weight <span class="math notranslate nohighlight">\(w_j\)</span>, we can output new message <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{N} (v)} w_j h_j\)</span>. In GNN, every node defines a <strong>computation graph</strong> based on its neighborhood</p>
<div class="figure align-default" id="gnn-cnn-filter">
<a class="reference internal image-reference" href="../_images/gnn-cnn-filter.png"><img alt="" src="../_images/gnn-cnn-filter.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 219 </span><span class="caption-text">CNN filter and GNN</span><a class="headerlink" href="#gnn-cnn-filter" title="Permalink to this image">¶</a></p>
</div>
<p>It also borrows idea from message passing. The information in GNN propagate through neighbors like those in belief networks. The number of hops determines the number of layers of GNN. For a GNN designed to find node embeddings, the information at each layer is node embeddings.</p>
<div class="figure align-default" id="gnn-aggregate">
<a class="reference internal image-reference" href="../_images/gnn-aggregate.png"><img alt="" src="../_images/gnn-aggregate.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 220 </span><span class="caption-text">Aggregation of neighbors information</span><a class="headerlink" href="#gnn-aggregate" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="neurons">
<h4>Neurons<a class="headerlink" href="#neurons" title="Permalink to this headline">¶</a></h4>
<p>The block in the previous computation graph represents an aggregation-and-transform step, where we use neighbors’ embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _u ^{(\ell)}\)</span>’s and self embedding <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> to obtain <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}\)</span>. This step work like a neuron. Different GNN models differ in this step.</p>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<p>One important property of aggregation is that the aggregation operator should be permutation invariant, since neighbors have no orders.</p>
</div>
<p>A basic approach of aggregation-and-transform is to average last layer information, take linear transformation, and then non-linear activation. Consider an <span class="math notranslate nohighlight">\(L\)</span>-layer GNN to obtain <span class="math notranslate nohighlight">\(k\)</span>-dimensional embeddings</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(0)} = \boldsymbol{x} _v\)</span>: initial <span class="math notranslate nohighlight">\(0\)</span>-th layer embeddings, equal to node features</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell +1)} = \sigma \left( \boldsymbol{W} _\ell \frac{u, v}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)} + \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)} \right)\)</span> for <span class="math notranslate nohighlight">\(\ell = \left\{ 0, \ldots, L-1 \right\}\)</span></p>
<ul>
<li><p>average last layer (its neighbors’) hidden embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _u ^{(\ell)}\)</span>, linearly transformed by <span class="math notranslate nohighlight">\(k \times k\)</span> weight matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}_ \ell\)</span></p></li>
<li><p>also take as input its hidden embedding <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> at last layer (last updated embedding?? stored in <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span>??), linearly transformed by <span class="math notranslate nohighlight">\(k\times k\)</span> weight matrix <span class="math notranslate nohighlight">\(\boldsymbol{B}_ \ell\)</span></p></li>
<li><p>finally activated by <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{z} _v = \boldsymbol{h} _v ^{(L)}\)</span> final output embeddings.</p></li>
</ul>
<p>The weight parameters in layer <span class="math notranslate nohighlight">\(\ell\)</span> are <span class="math notranslate nohighlight">\(\boldsymbol{W} _\ell\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{B} _\ell\)</span>, which are shared across neurons in layer <span class="math notranslate nohighlight">\(\ell\)</span>. Hence, the number of model parameters is sub-linear in <span class="math notranslate nohighlight">\(N_v\)</span>.</p>
<p>If we write the hidden embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> as rows in a matrix <span class="math notranslate nohighlight">\(\boldsymbol{H}^{(\ell)}\)</span>, then the aggregation step can be written as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}=\sigma\left(\boldsymbol{D} ^{-1} \boldsymbol{A} \boldsymbol{H} ^{(\ell)} \boldsymbol{W} _{\ell}^{\top}+\boldsymbol{H} ^{(\ell)} \boldsymbol{B} _{\ell}^{\top}\right)
\]</div>
<p>where we update the <span class="math notranslate nohighlight">\(v\)</span>-th row at a neuron for node <span class="math notranslate nohighlight">\(v\)</span>, or several rows corresponding to neurons in a layer.</p>
<p>In practice, <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is sparse, hence some sparse matrix multiplication can be used. But not all GNNs can be expressed in matrix form, when
aggregation function is complex.</p>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<div class="section" id="supervised">
<h4>Supervised<a class="headerlink" href="#supervised" title="Permalink to this headline">¶</a></h4>
<p>After build GNN layers, to train it, we compute loss and do SGD. The pipeine is</p>
<div class="figure align-default" id="gnn-training-pipeline">
<a class="reference internal image-reference" href="../_images/gnn-training-pipeline.png"><img alt="" src="../_images/gnn-training-pipeline.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 221 </span><span class="caption-text">GNN Training Piepline</span><a class="headerlink" href="#gnn-training-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>The prediction heads depend on whether the task is at node-level, edge-level, or graph-level.</p>
<div class="section" id="node-level">
<h5>Node-level<a class="headerlink" href="#node-level" title="Permalink to this headline">¶</a></h5>
<p>If we have some node label, we can minimize the loss</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_v, f(\boldsymbol{z} _v))
\]</div>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y\)</span> is a real number, then <span class="math notranslate nohighlight">\(f\)</span> maps embedding from <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is L2 loss</p></li>
<li><p>if <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(C\)</span>-way categorical, <span class="math notranslate nohighlight">\(f\)</span> maps embedding from <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R} ^C\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is cross-entropy.</p></li>
</ul>
</div>
<div class="section" id="edge-level">
<h5>Edge-level<a class="headerlink" href="#edge-level" title="Permalink to this headline">¶</a></h5>
<p>If we have some edge label <span class="math notranslate nohighlight">\(y_{uv}\)</span>, then the loss is</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_{uv}, f(\boldsymbol{z} _u, \boldsymbol{z} _v))
\]</div>
<p>To aggregate the two embedding vectors, <span class="math notranslate nohighlight">\(f\)</span> can be</p>
<ul class="simple">
<li><p>concatenation and then linear transformation</p></li>
<li><p>inner product <span class="math notranslate nohighlight">\(\boldsymbol{z} _u ^{\top} \boldsymbol{z} _v\)</span> (for 1-way prediction)</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{uv} ^{(r)} = \boldsymbol{z} _u ^{\top} \boldsymbol{W} ^{(r)} \boldsymbol{z} _v\)</span> then <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}} _{uv} = [y_{uv} ^{(1)}, \ldots , y_{uv} ^{(R)}]\)</span>. The weights <span class="math notranslate nohighlight">\(\boldsymbol{W} ^{(r)}\)</span> are trainable.</p></li>
</ul>
</div>
<div class="section" id="graph-level">
<h5>Graph-level<a class="headerlink" href="#graph-level" title="Permalink to this headline">¶</a></h5>
<p>For graph-level task, we make prediction using all the node embeddings in our graph. The loss is</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_{G}, f \left( \left\{ \boldsymbol{z} _v, \forall v \in V \right\} \right))
\]</div>
<p>where <span class="math notranslate nohighlight">\(f \left( \left\{ \boldsymbol{z} _v, \forall v \in V \right\} \right)\)</span> is similar to <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> in a GNN layer</p>
<ul>
<li><p>global pooling: <span class="math notranslate nohighlight">\(f = \operatorname{max}, \operatorname{mean}, \operatorname{sum}\)</span></p></li>
<li><p>hierarchical pooling: global pooling may lose information. We can apply pooling to some subgraphs to obtain subgraph embeddings, and then pool these subgraph embeddings.</p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/graph-hier-emb.png"><img alt="" src="../_images/graph-hier-emb.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 222 </span><span class="caption-text">Hierarchical Pooling</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>To decide subgraph assignment ??</p>
</li>
</ul>
</div>
</div>
<div class="section" id="unsupervised">
<h4>Unsupervised<a class="headerlink" href="#unsupervised" title="Permalink to this headline">¶</a></h4>
<p>In unsupervised setting, we use information from the graph itself as labels.</p>
<ul class="simple">
<li><p>Node-level: some node statistics, e.g. clustering coefficient, PageRank</p></li>
<li><p>Edge-level: <span class="math notranslate nohighlight">\(y_{u, v} = 1\)</span> when node <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are similar. Similarity can be defined using random walks, node proximity, etc.</p></li>
<li><p>Graph-level: some graph statistic, e.g. predict if two graphs are isomorphic</p></li>
</ul>
</div>
<div class="section" id="batch">
<h4>Batch<a class="headerlink" href="#batch" title="Permalink to this headline">¶</a></h4>
<p>We also use batch gradient descent. In each iteration, we train on a set of nodes, i.e., a batch of compute graphs.</p>
</div>
<div class="section" id="train-test-splitting">
<h4>Train-test Splitting<a class="headerlink" href="#train-test-splitting" title="Permalink to this headline">¶</a></h4>
<p>Given a graph input with features and labels <span class="math notranslate nohighlight">\(\boldsymbol{G} = (V, E, \boldsymbol{X} , \boldsymbol{y})\)</span>, how do we split it into train / validation / test set? The speciality of graph is that nodes as observations are connected by edges, they are not independent due to message passing.</p>
<div class="section" id="id1">
<h5>Node-level<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>“transductive” means the entire graph can be observed in all dataset splits</p>
</div>
<p>Transductive setting</p>
<ul class="simple">
<li><p>training: hide some nodes’ label <span class="math notranslate nohighlight">\(\boldsymbol{y} _h\)</span>, use all the remaining information <span class="math notranslate nohighlight">\((V, E, \boldsymbol{X} , \boldsymbol{y} \setminus \boldsymbol{y} _h)\)</span> to train a GNN. Of course, the computation graphs are for those labeled nodes.</p></li>
<li><p>test: evaluate on <span class="math notranslate nohighlight">\(\boldsymbol{y} _h\)</span>, i.e. the computation graphs are for those unlabeled nodes.</p></li>
</ul>
<p>Inductive setting</p>
<ul class="simple">
<li><p>partition the graph into training subgraph <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span> and test subgraph <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span>, remove across-subgraph edges. Then the two subgraphs are independent.</p></li>
<li><p>training: use <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span></p></li>
<li><p>test: use the trained model, evaluate on <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span></p></li>
<li><p>applicable to node / edge / graph tasks</p></li>
</ul>
<div class="figure align-default" id="gnn-train-test-split">
<a class="reference internal image-reference" href="../_images/gnn-train-test-split.png"><img alt="" src="../_images/gnn-train-test-split.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 223 </span><span class="caption-text">Splitting graph, transductive (left) and inductive (right)</span><a class="headerlink" href="#gnn-train-test-split" title="Permalink to this image">¶</a></p>
</div>
<p>In the first layer only features not labels are fed into GNN???</p>
</div>
<div class="section" id="link-level">
<h5>Link-level<a class="headerlink" href="#link-level" title="Permalink to this headline">¶</a></h5>
<p>For link-prediction task, we first</p>
<p>Inductive setting</p>
<ul class="simple">
<li><p>Partition edges <span class="math notranslate nohighlight">\(E\)</span> into</p>
<ul>
<li><p>message edges <span class="math notranslate nohighlight">\(E_m\)</span>, used for GNN message passing, and</p></li>
<li><p>supervision edges <span class="math notranslate nohighlight">\(E_s\)</span>, use for computing objective, not fed into GNN</p></li>
</ul>
</li>
<li><p>Partition graph into training subgraph <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span> and test subgraph <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span>, remove across-subgraph edges. Each subgraph will have some <span class="math notranslate nohighlight">\(E_m\)</span> and some <span class="math notranslate nohighlight">\(E_s\)</span>.</p></li>
<li><p>Training on <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span></p></li>
<li><p>Test on <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span></p></li>
</ul>
<div class="figure align-default" id="gnn-split-edge-ind">
<a class="reference internal image-reference" href="../_images/gnn-split-edge-ind.png"><img alt="" src="../_images/gnn-split-edge-ind.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 224 </span><span class="caption-text">Inductive splitting for link prediction</span><a class="headerlink" href="#gnn-split-edge-ind" title="Permalink to this image">¶</a></p>
</div>
<p>Transductive setting (common setting)</p>
<ul class="simple">
<li><p>Partition the edges into</p>
<ul>
<li><p>training message edges <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span></p></li>
<li><p>training supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span></p></li>
<li><p>test edges <span class="math notranslate nohighlight">\(E_{\text{test}}\)</span></p></li>
</ul>
</li>
<li><p>Training: use training message edges <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span> to predict training supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span></p></li>
<li><p>Test: Use <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span>, <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span> to predict <span class="math notranslate nohighlight">\(E_{\text{test}}\)</span></p></li>
</ul>
<p>After training, supervision edges are <strong>known</strong> to GNN. Therefore, an ideal model should use supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span> in message passing at test time. If there is a validation step, then the validation edges are also used to predict <span class="math notranslate nohighlight">\(E_{\text{test}}\)</span>.</p>
<div class="figure align-default" id="gnn-split-edge-tran">
<a class="reference internal image-reference" href="../_images/gnn-split-edge-tran.png"><img alt="" src="../_images/gnn-split-edge-tran.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 225 </span><span class="caption-text">Transductive splitting for link prediction</span><a class="headerlink" href="#gnn-split-edge-tran" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
<div class="section" id="pros">
<h3>Pros<a class="headerlink" href="#pros" title="Permalink to this headline">¶</a></h3>
<p>Inductive capability</p>
<ul class="simple">
<li><p>new graph: after training GNN on one graph, we can generalize it to an unseen graph. For instance, train on protein interaction graph from model organism A and generate embeddings on newly collected data about organism B.</p></li>
<li><p>new nodes: if an unseen node is added to the graph, we can directly run forward propagation to obtain its embedding.</p></li>
</ul>
</div>
<div class="section" id="variants">
<h3>Variants<a class="headerlink" href="#variants" title="Permalink to this headline">¶</a></h3>
<p>As said, different GNN models mainly differ in the aggregation-and-transform step. Let’s write the aggregation step as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{AGG} \left( \left\{ \boldsymbol{h} _u ^{(\ell)}, \forall u \in \mathscr{N} (v)  \right\} \right)
\]</div>
<p>In basic GNN, the aggregation function is just average. And the update function is</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{h} _v ^{(\ell +1)} = \sigma \left( \boldsymbol{W} _\ell \frac{u, v}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)} + \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)} \right)\]</div>
<p>This is called graph convolutional networks [Kipf and Welling ICLR 2017].</p>
<p>There are many variants and extensions to this update function. Before aggregation, there can be some transformation of the neighbor embeddings. The aggregation-and-transform step then becomes transform-aggregation-transform.</p>
<div class="section" id="graphsage">
<h4>GraphSAGE<a class="headerlink" href="#graphsage" title="Permalink to this headline">¶</a></h4>
<p>[Hamilton et al., NIPS 2017]</p>
<p>In GraphSAGE, the update function is more general,</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{h} _v ^{(\ell + 1)} = \sigma \left( \left[ \boldsymbol{W} _\ell \operatorname{AGG} \left( \left\{ \boldsymbol{h} _u ^{(\ell)}, \forall u \in \mathscr{N} (v)  \right\} \right), \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)}\right] \right)\]</div>
<p>where we concatenate two vectors instead of summation, and <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> is a flexible aggregation function. L2 normalization of <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}\)</span> to a unit length embedding vector can also be applied. In some cases (not always), normalization of embedding results in performance improvement</p>
<p>AGG can be</p>
<ul>
<li><p>Mean</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{AGG} = \frac{u, v}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)}
  \]</div>
</li>
<li><p>Pool: Transform neighbor vectors and apply symmetric vector function <span class="math notranslate nohighlight">\(\gamma\)</span>, e.g. mean, max</p>
<div class="math notranslate nohighlight">
\[\operatorname{AGG} = \gamma \left( \left\{ \operatorname{MLP} (\boldsymbol{h} _u ^{(\ell)} ), \forall u \in \mathscr{N} (v)   \right\} \right)\]</div>
</li>
<li><p>LSTM: apply LSTM, but to make it permutation invariant, we reshuffle the neighbors (some random order)</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{AGG}  = \operatorname{LSTM} \left( \left[ \boldsymbol{h} _u ^{(\ell)} , \forall u \in \pi (\mathscr{N} (v)) \right] \right)
  \]</div>
</li>
</ul>
</div>
<div class="section" id="matrix-operations">
<h4>Matrix Operations<a class="headerlink" href="#matrix-operations" title="Permalink to this headline">¶</a></h4>
<p>If we use mean, then the aggregation step (ignore <span class="math notranslate nohighlight">\(\boldsymbol{B}_\ell\)</span>) can be written as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}  = \boldsymbol{D} ^{-1} \boldsymbol{A} \boldsymbol{H}  ^{(\ell)}
\]</div>
<p>A variant [Kipf+ 2017]</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}  = \boldsymbol{D} ^{-1/2} \boldsymbol{A} \boldsymbol{D} ^{-1/2} \boldsymbol{H}  ^{(\ell)}
\]</div>
<p>!!Laplacian</p>
</div>
<div class="section" id="graph-attention-networks">
<h4>Graph Attention Networks<a class="headerlink" href="#graph-attention-networks" title="Permalink to this headline">¶</a></h4>
<p>If AGG is mean, then the message from neighbors are of equal importance with the same weight <span class="math notranslate nohighlight">\(\frac{u, v}{d_v}\)</span>. Can we specify some unequal weight/attention <span class="math notranslate nohighlight">\(\alpha_{vu}\)</span>?</p>
<div class="math notranslate nohighlight">
\[
\operatorname{AGG} = \sum_{u \in \mathscr{N} (v)} \alpha_{vu} \boldsymbol{h} _u ^ {(\ell)}
\]</div>
<ul>
<li><p>Compute <strong>attention coefficients</strong> <span class="math notranslate nohighlight">\(e_{vu}\)</span> across pairs of nodes <span class="math notranslate nohighlight">\(u, v\)</span> based on their messages, by some <strong>attention mechanism</strong> <span class="math notranslate nohighlight">\(a\)</span></p>
<div class="math notranslate nohighlight">
\[
  e_{v u}=a\left(\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{W}^{(l)} \boldsymbol{h}_{v}^{(l-1)}\right)
  \]</div>
<p>The attention coefficients <span class="math notranslate nohighlight">\(e_{vu}\)</span> indicates the importance of <span class="math notranslate nohighlight">\(u\)</span>’s message to node <span class="math notranslate nohighlight">\(v\)</span>.</p>
</li>
<li><p>Normalize <span class="math notranslate nohighlight">\(e_vu\)</span> into the final <strong>attention weight</strong> <span class="math notranslate nohighlight">\(\alpha_{vu}\)</span> by softmax function</p>
<div class="math notranslate nohighlight">
\[
  \alpha_{v u}=\frac{\exp \left(e_{v u}\right)}{\sum_{k \in N(v)} \exp \left(e_{v k}\right)}
  \]</div>
</li>
</ul>
<p>How to design attention mechanism <span class="math notranslate nohighlight">\(a\)</span>? For instance, <span class="math notranslate nohighlight">\(a\)</span> can be some MLP. The parameters in <span class="math notranslate nohighlight">\(a\)</span> are trained jointly.</p>
<p>We can generalize to use multi-head attention, which are shown to stabilizes the learning process of attention mechanism [Velickovic+ 2018]. There are <span class="math notranslate nohighlight">\(R\)</span> independent attention mechanisms are used. Each one of them, namely a ‘heard’, computes a set of attention weight <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _{v} ^{(r)}\)</span>. Finally, we aggregate the <span class="math notranslate nohighlight">\(\boldsymbol{h} _v\)</span> again, by concatenation or summation.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell+1, \color{red}{r})} = \sigma \left( \boldsymbol{W} ^{(\ell) }\sum_{u \in \mathscr{N} (v)} \alpha_{vu}^ {(\color{red}{r})} \boldsymbol{h} _u ^ {(\ell)}  \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}  = \operatorname{AGG} \left( \left\{ \boldsymbol{h} _v ^{(\ell+1, \color{red}{r})}, r = 1, \ldots, R \right\}  \right)\)</span></p></li>
</ul>
<p>Pros</p>
<ul class="simple">
<li><p>allow for implicitly specifying weights</p></li>
<li><p>computationally efficient, parallelizable</p></li>
<li><p>storage efficient, total number of parameters <span class="math notranslate nohighlight">\(\mathcal{O} (N_v + N_e)\)</span></p></li>
</ul>
<p>If edge weights <span class="math notranslate nohighlight">\(w_{vu}\)</span> are given, we can</p>
<ul class="simple">
<li><p>use it as weights <span class="math notranslate nohighlight">\(\alpha_vu\)</span>, e.g. by softmax function</p></li>
<li><p>incorporate it into the design of attention mechanism <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
<p>In many cases, attention leads to performance gains.</p>
</div>
<div class="section" id="gin">
<h4>GIN<a class="headerlink" href="#gin" title="Permalink to this headline">¶</a></h4>
<div class="section" id="expressiveness">
<h5>Expressiveness<a class="headerlink" href="#expressiveness" title="Permalink to this headline">¶</a></h5>
<p>Consider a special case that node features are the same <span class="math notranslate nohighlight">\(\boldsymbol{x} _{v_1} = \boldsymbol{x} _{v_2} = \ldots\)</span>, represented by colors in the discussion below. Then if the computational graph are exactly the same for two nodes, then they have the same embeddings.</p>
<div class="figure align-default" id="gnn-expr-same">
<a class="reference internal image-reference" href="../_images/gnn-expr-same.png"><img alt="" src="../_images/gnn-expr-same.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 226 </span><span class="caption-text">Same computational graphs of two nodes</span><a class="headerlink" href="#gnn-expr-same" title="Permalink to this image">¶</a></p>
</div>
<p>Computational graphs are identical to <strong>rooted subtree</strong> structures around each node. GNN’s node embeddings capture rooted subtree structures. Most expressive GNN maps different rooted subtrees into different node embeddings, i.e. should be like an injective function.</p>
<div class="figure align-default" id="gnn-injective">
<a class="reference internal image-reference" href="../_images/gnn-injective.png"><img alt="" src="../_images/gnn-injective.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 227 </span><span class="caption-text">Injective mapping from computational graph to embeddings</span><a class="headerlink" href="#gnn-injective" title="Permalink to this image">¶</a></p>
</div>
<p>Some of the previously seen models do not use injective function at the neighbor aggregation step. Note that neighbor aggregation is a function over multi-sets (sets with repeating elements). They are not maximally powerful GNNs in terms of expressiveness.</p>
<ul class="simple">
<li><p>GCN (mean-pool)</p></li>
<li><p>GraphSAGE aggregation function (MLP + max-pool) cannot distinguish different multi-sets with the same set of distinct colors.</p></li>
</ul>
<div class="figure align-default" id="gnn-expr-fail">
<a class="reference internal image-reference" href="../_images/gnn-expr-fail.png"><img alt="" src="../_images/gnn-expr-fail.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 228 </span><span class="caption-text">Mean and max pooling failure cases</span><a class="headerlink" href="#gnn-expr-fail" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id2">
<h5>GIN<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>How to design maximally powerful GNNs? Can we design a neural network in the aggregation step that can model injective multi-set function.</p>
<p>Theorem [Xu et al. ICLR 2019]: Any injective multi-set function can be expressed
as:</p>
<div class="math notranslate nohighlight">
\[
\Phi \left( \sum_{x \in S} f(x)  \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f, \Phi\)</span> are non-linear functions. To model them, we can use MLP which have universal approximation power.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MLP}_{\Phi}  \left( \sum_{x \in S} \operatorname{MLP}_{f} (x)  \right)
\]</div>
<p>In practice, MLP hidden dimensionality of 100 to 500 is sufficient. The model is called Graph Isomorphism Network (GIN) [Xu+ 2019].</p>
<p>GIN‘s neighbor aggregation function is injective. It is the most expressive GNN in the class of message-passing GNNs. The key is to use element-wise sum pooling, instead of mean-/max-pooling.</p>
</div>
<div class="section" id="r-t-wl-kernel">
<h5>R.t. WL Kernel<a class="headerlink" href="#r-t-wl-kernel" title="Permalink to this headline">¶</a></h5>
<p>It is a “neural network” version of the <a class="reference internal" href="11-descriptive-analysis.html#wl-kernel"><span class="std std-ref">WL graph kernel</span></a>, where the color update function is</p>
<div class="math notranslate nohighlight">
\[
c^{(k+1)}(v)=\operatorname{HASH}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)
\]</div>
<p>Xu proved that any injective function over the tuple <span class="math notranslate nohighlight">\(\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)\)</span> can be modeled as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GINConv}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right) =  \left.\mathrm{MLP}_{\Phi}\left((1+\epsilon) \cdot \mathrm{MLP}_{f}\left(c^{(k)}(v)\right)\right)+\sum_{u \in N(v)} \mathrm{MLP}_{f}\left(c^{(k)}(u)\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon\)</span> is a learnable scalar.</p>
<p>Advantages of GIN over the WL graph kernel are:</p>
<ul class="simple">
<li><p>Node embeddings are low-dimensional; hence, they can capture the fine-grained similarity of different nodes.</p></li>
<li><p>Parameters of the update function can be learned for the downstream tasks.</p></li>
</ul>
<p>Because of the relation between GIN and the WL graph kernel, their expressive is exactly the same. If two graphs can be distinguished by GIN, they can be also distinguished by the WL kernel, and vice versa. They are powerful to distinguish most of the real graphs!</p>
</div>
</div>
<div class="section" id="improvement">
<h4>Improvement<a class="headerlink" href="#improvement" title="Permalink to this headline">¶</a></h4>
<p>There are basic graph structures that existing GNN framework cannot distinguish, such as difference in cycles. One node in 3-node-cycle and the other in 4-node cycle have the same computational graph when layer. GNNs’ expressive power can be improved to resolve the above problem. [You et al. AAAI 2021, Li et al. NeurIPS 2020]</p>
</div>
</div>
<div class="section" id="layer-design">
<h3>Layer Design<a class="headerlink" href="#layer-design" title="Permalink to this headline">¶</a></h3>
<div class="section" id="modules">
<h4>Modules<a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h4>
<p>We can include modern deep learning modules that proved to be useful in many domains, including</p>
<ul class="simple">
<li><p>BatchNorm to stabilize neural network training, used to normalize embeddings.</p></li>
<li><p>Dropout to prevent overfitting, used in linear layers <span class="math notranslate nohighlight">\(\boldsymbol{W} _\ell \boldsymbol{h} _u ^{(\ell)}\)</span>.</p></li>
</ul>
<div class="figure align-default" id="gnn-layer">
<a class="reference internal image-reference" href="../_images/gnn-layer.png"><img alt="" src="../_images/gnn-layer.png" style="width: 20%;" /></a>
<p class="caption"><span class="caption-number">Fig. 229 </span><span class="caption-text">A GNN Layer</span><a class="headerlink" href="#gnn-layer" title="Permalink to this image">¶</a></p>
</div>
<p>Try design ideas in <a class="reference external" href="https://github.com/snap-stanford/GraphGym">GraphGym</a>.</p>
<p>We then introduce how to add skip connection across layers.</p>
</div>
<div class="section" id="issue-with-deep-gnn">
<h4>Issue with Deep GNN<a class="headerlink" href="#issue-with-deep-gnn" title="Permalink to this headline">¶</a></h4>
<p>Unlike some other NN models, adding more GNN layers do not always help. The issue of stacking many GNN layers is that GNN suffers from the <strong>over-smoothing</strong> problem, where all the node embeddings converge to the same value.</p>
<p>Let receptive field be the set of nodes that determine the embedding of a node of interest. In a <span class="math notranslate nohighlight">\(L\)</span>-layer GNN, this set is the union of <span class="math notranslate nohighlight">\(\ell\)</span>-hop neighborhood for <span class="math notranslate nohighlight">\(\ell=1, 2, \ldots, L\)</span>. As the number of layers <span class="math notranslate nohighlight">\(L\)</span> goes large, the <span class="math notranslate nohighlight">\(L\)</span>-hop neighborhood increases exponentially. The receptive filed quickly covers all nodes in the graph.</p>
<div class="figure align-default" id="gnn-receptive-filed">
<a class="reference internal image-reference" href="../_images/gnn-receptive-filed.png"><img alt="" src="../_images/gnn-receptive-filed.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 230 </span><span class="caption-text">Receptive field for different layers of GNN</span><a class="headerlink" href="#gnn-receptive-filed" title="Permalink to this image">¶</a></p>
</div>
<p>For different nodes, the <strong>shared</strong> neighbors quickly grow when we
increase the number of hops (num of GNN layers). Their computation graphs are similar, hence similar embeddings.</p>
</div>
<div class="section" id="solution">
<h4>Solution<a class="headerlink" href="#solution" title="Permalink to this headline">¶</a></h4>
<p>How to enhance the expressive power of a shallow GNN?</p>
<ul class="simple">
<li><p>Increase the expressive power within each GNN layer</p>
<ul>
<li><p>make <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> a neural network</p></li>
</ul>
</li>
<li><p>Add layers that do not pass messages</p>
<ul>
<li><p>MLP before and after GNN layers, as pre-process layers (image, text) and post-process layers for downstream tasks.</p></li>
</ul>
</li>
</ul>
<p>We can also add shortcut connections (aka skip connections) in GNN. Then we automatically get a mixture of shallow GNNs and deep GNNs. We can even add shortcuts from each layer to the final layer, then final layer directly aggregates from the all the node embeddings in the previous layers</p>
<div class="figure align-default" id="gnn-skip-connection">
<a class="reference internal image-reference" href="../_images/gnn-skip-connection.png"><img alt="" src="../_images/gnn-skip-connection.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 231 </span><span class="caption-text">Skip connection in GNN</span><a class="headerlink" href="#gnn-skip-connection" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="graph-manipulation">
<h3>Graph Manipulation<a class="headerlink" href="#graph-manipulation" title="Permalink to this headline">¶</a></h3>
<p>It is unlikely that the input graph happens to be the optimal computation graph for embeddings.</p>
<p>Issues and solutions:</p>
<ul class="simple">
<li><p>Graph Feature manipulation</p>
<ul>
<li><p>prob: the input graph lacks features.</p></li>
<li><p>sol: feature augmentation</p></li>
</ul>
</li>
<li><p>Graph Structure manipulation if the graph is</p>
<ul>
<li><p>too sparse</p>
<ul>
<li><p>prob: inefficient message passing</p></li>
<li><p>sol: add virtual nodes / edges</p></li>
</ul>
</li>
<li><p>too dense</p>
<ul>
<li><p>prob: message passing is too costly</p></li>
<li><p>sol: sample neighbors when doing message passing</p></li>
</ul>
</li>
<li><p>too large</p>
<ul>
<li><p>prob: cannot fit the computational graph into a GPU</p></li>
<li><p>sol: sample subgraphs to compute embeddings</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="section" id="feature-augmentation">
<h4>Feature Augmentation<a class="headerlink" href="#feature-augmentation" title="Permalink to this headline">¶</a></h4>
<p>Sometimes input graph does not have node features, e.g. we only have the adjacency matrix.</p>
<p>Standard approaches to augment node features</p>
<ul class="simple">
<li><p>assign constant values to nodes</p></li>
<li><p>assign unique IDs to nodes, in the form of <span class="math notranslate nohighlight">\(N_v\)</span>-dimensional one-hot vectors</p></li>
<li><p>assign other node-level descriptive features, that are hard to learn by GNN</p>
<ul>
<li><p>centrality</p></li>
<li><p>clustering coefficients</p></li>
<li><p>PageRank</p></li>
<li><p>cycle count (as one-hot encoding vector)</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="gnn-feature-aug">
<a class="reference internal image-reference" href="../_images/gnn-feature-aug.png"><img alt="" src="../_images/gnn-feature-aug.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 232 </span><span class="caption-text">Comparison of feature augmentation methods</span><a class="headerlink" href="#gnn-feature-aug" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="virtual-nodes-edges">
<h4>Virtual Nodes/Edges<a class="headerlink" href="#virtual-nodes-edges" title="Permalink to this headline">¶</a></h4>
<p>If the graph is too sparse, then the receptive field of a node covers small number of nodes. The message passing is then inefficient.</p>
<p>We can add a virtual node, and connected it to all <span class="math notranslate nohighlight">\(N_v\)</span> nodes in the graph. Hence all nodes will have a distance at most 2. (too dense?? if <span class="math notranslate nohighlight">\(L=2\)</span> then the input layer covers all nodes??)</p>
</div>
<div class="section" id="neighborhood-sampling">
<h4>Neighborhood Sampling<a class="headerlink" href="#neighborhood-sampling" title="Permalink to this headline">¶</a></h4>
<p>In the standard setting, for a node <span class="math notranslate nohighlight">\(v\)</span>, all the nodes in <span class="math notranslate nohighlight">\(\mathscr{N} _(v)\)</span> are used for message passing. We can actually randomly sample a subset of a node’s neighborhood for message passing.</p>
<p>Next time when we compute the embeddings, we can sample <strong>different</strong> neighbors. In expectation, we will still use all neighbors vectors.</p>
<p>Benefits: greatly reduce computational cost. Allows for scaling to large graphs.</p>
</div>
</div>
<div class="section" id="graph-generative-models">
<h3>Graph Generative Models<a class="headerlink" href="#graph-generative-models" title="Permalink to this headline">¶</a></h3>
<p>Types</p>
<ul class="simple">
<li><p>Realistic graph generation: generate graphs that are similar to a given set of graphs. (our focus)</p></li>
<li><p>goal-directed graph generation: generate graphs that optimize given objectives/constraints, e.g. molecules chemical properties</p></li>
</ul>
<p>Given a set of graph, we want to</p>
<ul class="simple">
<li><p><strong>Density estimation</strong>: model the distribution of these graphs by <span class="math notranslate nohighlight">\(p_{\text{model} }\)</span>, hope it is close to <span class="math notranslate nohighlight">\(p_{\text{data} }\)</span>, e.g. maximum likelihood</p></li>
<li><p><strong>Sampling</strong>: sample from <span class="math notranslate nohighlight">\(p_{\text{model} }\)</span>. A common approach is</p>
<ul>
<li><p>sample  from noise <span class="math notranslate nohighlight">\(z_i \sim \mathcal{N} (0, 1)\)</span></p></li>
<li><p>transform the noise <span class="math notranslate nohighlight">\(z_i\)</span> via <span class="math notranslate nohighlight">\(f\)</span> to obtain <span class="math notranslate nohighlight">\(x_i\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> can be trained NN.</p></li>
</ul>
</li>
</ul>
<p>Challenge</p>
<ul class="simple">
<li><p>large output space: <span class="math notranslate nohighlight">\(N^2\)</span> bits of adjacency matrix?</p></li>
<li><p>variable output space: how to generate graph of different sizes?</p></li>
<li><p>non-unique representation: for a fixed graph, its adjacency matrix has <span class="math notranslate nohighlight">\(N_v !\)</span> ways of representation.</p></li>
<li><p>dependency: existence of an edge may depend on the entire graph</p></li>
</ul>
<div class="section" id="graphrnn">
<h4>GraphRNN<a class="headerlink" href="#graphrnn" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf">You+ 2018</a></p>
<p>Recall that by chain rule, a joint distribution can be factorized as</p>
<div class="math notranslate nohighlight">
\[
p_{\text {model }}(\boldsymbol{x} ; \theta)=\prod_{t=1}^{n} p_{\text {model }}\left(x_{t} \mid x_{u, v}, \ldots, x_{t-1} ; \theta\right)
\]</div>
<p>In our case, <span class="math notranslate nohighlight">\(x_t\)</span> will be the <span class="math notranslate nohighlight">\(t\)</span>-th action (add node, add edge). The ordering of nodes is a random ordering <span class="math notranslate nohighlight">\(\pi\)</span>. For a fixed ordering <span class="math notranslate nohighlight">\(\pi\)</span> of nodes, for each node, we add a sequence of edges.</p>
<div class="figure align-default" id="gnn-gen">
<a class="reference internal image-reference" href="../_images/gnn-gen.png"><img alt="" src="../_images/gnn-gen.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 233 </span><span class="caption-text">Generation process of a graph</span><a class="headerlink" href="#gnn-gen" title="Permalink to this image">¶</a></p>
</div>
<p>It is like propagating along columns of upper-triangularized adjacency matrix corresponding to the ordering <span class="math notranslate nohighlight">\(\pi\)</span>. Essentially, we have transformed graph generation problem into a sequence generation problem. We need to model two processes</p>
<ol class="simple">
<li><p>Generate a state for a new node (Node-level sequence)</p></li>
<li><p>Generate edges for the new node based on its state (Edge-level sequence)</p></li>
</ol>
<p>To model these two sequences, we can use <a class="reference internal" href="../37-neural-networks/31-sequential-models.html#rnn"><span class="std std-ref">RNNs</span></a>.</p>
<p>GraphRNN has a node-level RNN and an edge-level RNN.</p>
<ul class="simple">
<li><p>Node-level RNN generates the initial state for edge-level RNN</p></li>
<li><p>Edge-level RNN sequentially predict the probability that the new node will connect to each of the previous node. Then the last hidden state is used to run node RNN for another step.</p></li>
</ul>
<div class="figure align-default" id="graph-rnn-pipeline">
<a class="reference internal image-reference" href="../_images/graph-rnn-pipeline.png"><img alt="" src="../_images/graph-rnn-pipeline.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 234 </span><span class="caption-text">GraphRNN Pipeline (node RNN + edge RNN)</span><a class="headerlink" href="#graph-rnn-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>The training and test pipeline is:</p>
<div class="figure align-default" id="graph-rnn-train-test">
<a class="reference internal image-reference" href="../_images/graph-rnn-train-test.png"><img alt="" src="../_images/graph-rnn-train-test.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 235 </span><span class="caption-text">GraphRNN training and test example</span><a class="headerlink" href="#graph-rnn-train-test" title="Permalink to this image">¶</a></p>
</div>
<ul>
<li><p>SOS means start of sequence, EOS means end of sequence.</p></li>
<li><p>If Edge RNN outputs EOS at step 1, we know no edges are connected to the new node. We stop the graph generation. At test time, this means node sequence length is not fixed.</p></li>
<li><p>In training of edge RNN, teacher enforcing of edge existence is applied. The loss is binary cross entropy</p>
<div class="math notranslate nohighlight">
\[L=- \sum _{u, v} \left[y_{u, v} \log \left(\hat{y}_{u, v}\right)+\left(1-y_{u, v}\right) \log \left(1-\hat{y}_{u, v}\right)\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_{u, v}\)</span> is predicted probability of existence of edge <span class="math notranslate nohighlight">\((u, v)\)</span> and <span class="math notranslate nohighlight">\(y_{u,v}\)</span> is ground truth 1 or 0.</p>
</li>
</ul>
</div>
<div class="section" id="tractability">
<h4>Tractability<a class="headerlink" href="#tractability" title="Permalink to this headline">¶</a></h4>
<p>In the structure introduced above, each edge RNN can have at most <span class="math notranslate nohighlight">\(n-1\)</span> step. How to limit this?</p>
<p>The answer is to use Breadth-First search node ordering rather than random ordering of nodes.</p>
<div class="figure align-default" id="graph-rnn-ordering">
<a class="reference internal image-reference" href="../_images/graph-rnn-ordering.png"><img alt="" src="../_images/graph-rnn-ordering.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 236 </span><span class="caption-text">Random ordering and BFS ordering</span><a class="headerlink" href="#graph-rnn-ordering" title="Permalink to this image">¶</a></p>
</div>
<p>Illustrated in adjacency matrix, we only look at connectivity with nodes in the BFS frontier, rather than all previous nodes.</p>
<div class="figure align-default" id="graph-rnn-ordering-2">
<a class="reference internal image-reference" href="../_images/graph-rnn-ordering-2.png"><img alt="" src="../_images/graph-rnn-ordering-2.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 237 </span><span class="caption-text">Random ordering and BFS ordering in adjacency matrices</span><a class="headerlink" href="#graph-rnn-ordering-2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="similarity-of-graphs">
<h4>Similarity of Graphs<a class="headerlink" href="#similarity-of-graphs" title="Permalink to this headline">¶</a></h4>
<p>How to compare two graphs? Qualitatively, we can compare visual similarity. Quantitatively, we can compare graph statistics distribution such as</p>
<ul class="simple">
<li><p>degree distribution</p></li>
<li><p>clustering coefficient distribution</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Graphlets#Graphlet-based_network_properties">Graphlet-based</a> distribution
The distance between two distributions can be measured by <a class="reference external" href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">earth mover distance</a> (aka Wasserstein metric in math), which measure the minimum effort that move earth from one pile to the other.</p></li>
</ul>
<div class="figure align-default" id="emd">
<a class="reference internal image-reference" href="../_images/emd.png"><img alt="" src="../_images/emd.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 238 </span><span class="caption-text">Earth mover distance illustration</span><a class="headerlink" href="#emd" title="Permalink to this image">¶</a></p>
</div>
<p>How two compare two sets of graphs by some set distance? Each graph element may have different <span class="math notranslate nohighlight">\(N_v, N_e\)</span>. First we compute a statistic for each graph element, then compute Maximum Mean Discrepancy (MDD). If <span class="math notranslate nohighlight">\(\mathcal{X} = \mathcal{H} = \mathbb{R} ^d\)</span> and we choose <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> to be <span class="math notranslate nohighlight">\(\phi(x)=x\)</span>, then it becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{MMD}(P, Q) &amp;=\left\|\mathbb{E}_{X \sim P}[\varphi(X)]-\mathbb{E}_{Y \sim Q}[\varphi(Y)]\right\|_{\mathcal{H}} \\
&amp;=\left\|\mathbb{E}_{X \sim P}[X]-\mathbb{E}_{Y \sim Q}[Y]\right\|_{\mathbb{R}^{d}} \\
&amp;=\left\|\mu_{P}-\mu_{Q}\right\|_{\mathbb{R}^{d}}
\end{aligned}
\end{split}\]</div>
<p>Given a set of input graphs, we can generate a set of graphs using some algorithms. Then compare the set distance. Many algorithms are particularly designed to generate certain graphs, but GraphRNN can learn from the input and generate any types of graphs (e.g. grid).</p>
</div>
<div class="section" id="id3">
<h4>Variants<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Graph convolutional policy network</p>
<ul class="simple">
<li><p>use GNN to predict the generation action</p></li>
<li><p>further uses RL to direct graph generation to certain goals</p></li>
</ul>
<p>Hierarchical generation: generate subgraphs at each step</p>
</div>
</div>
<div class="section" id="id4">
<h3>Limitations<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>For a perfect GNN:</p>
<ol class="simple">
<li><p>If two nodes have the same neighborhood structure, they must have the same embedding</p></li>
<li><p>If two nodes have different neighborhood structure, they must have different embeddings</p></li>
</ol>
<p>However,</p>
<ul class="simple">
<li><p>point 1 may not be practical, sometimes we want to assign different embeddings to two nodes with the same neighborhood structure. Solution: position-aware GNNs.</p></li>
<li><p>point 2 often cannot be satisfied. Nodes on two rings have the same computational graphs. Sol: idendity-aware GNNs</p></li>
</ul>
<div class="section" id="position-aware-gnns">
<h4>Position-aware GNNs<a class="headerlink" href="#position-aware-gnns" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://arxiv.org/abs/1906.04817">J. You, R. Ying, J. Leskovec. Position-aware Graph Neural Networks, ICML 2019</a></p>
<ul class="simple">
<li><p>structure-aware task: nodes are labeled by their structural roles in the graph</p></li>
<li><p>position-aware task: nodes are labeled by their positions in the graph</p></li>
</ul>
<div class="figure align-default" id="gnn-struc-posi-aware">
<a class="reference internal image-reference" href="../_images/gnn-struc-posi-aware.png"><img alt="" src="../_images/gnn-struc-posi-aware.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 239 </span><span class="caption-text">Two types of labels.</span><a class="headerlink" href="#gnn-struc-posi-aware" title="Permalink to this image">¶</a></p>
</div>
<p>GNNs differentiate nodes by their computational graphs. Thus, they often work well for structure-aware task, but fail for position-aware tasks (but node features are different??)</p>
<p>To solve this, we introduce anchors. Randomly pick some nodes or some set of nodes in the graph as <strong>anchor-sets</strong>. Then we compute the relative distances from every nodes to these anchor-sets.</p>
<div class="figure align-default" id="gnn-anchor-set">
<a class="reference internal image-reference" href="../_images/gnn-anchor-set.png"><img alt="" src="../_images/gnn-anchor-set.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 240 </span><span class="caption-text">Anchors</span><a class="headerlink" href="#gnn-anchor-set" title="Permalink to this image">¶</a></p>
</div>
<p>The distance can then be used as <strong>position encoding</strong>, which represent a node’s position by its distance to randomly selected anchor-set.</p>
<p>Note that each dimension of the position encoding is tied to an anchor-set. Permutation of the order does not change the meaning of the encoding. Thus, we cannot directly use this encoding as augmented feature.</p>
<p>We require a special NN that can maintain the permutation invariant property of position encoding. Permuting the input feature dimension will only result in the permutation of the output dimension, the <strong>value</strong> in each dimension won’t change.</p>
</div>
<div class="section" id="identity-aware-gnn">
<h4>Identity-aware GNN<a class="headerlink" href="#identity-aware-gnn" title="Permalink to this headline">¶</a></h4>
<p>[J. You, J. Gomes-Selman, R. Ying, J. Leskovec. Identity-aware Graph Neural Networks, AAAI 2021]</p>
<p>Heterogenous: different types of message passing is applied to different nodes. Suppose two nodes <span class="math notranslate nohighlight">\(v_1, v_2\)</span> have the same computational graph structure, but have different node colorings. Since we will apply different neural network for embedding computation, their embeddings will be different.</p>
<div class="figure align-default" id="gnn-idgnn">
<a class="reference internal image-reference" href="../_images/gnn-idgnn.png"><img alt="" src="../_images/gnn-idgnn.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 241 </span><span class="caption-text">Identity-aware GNN</span><a class="headerlink" href="#gnn-idgnn" title="Permalink to this image">¶</a></p>
</div>
<p>ID-GNN can count cycles originating from a given node, but GNN cannot.</p>
<p>Rather than to heterogenous message passing, we can include identity information as an augmented node feature (no need to do heterogenous message passing). Use cycle counts in each layer as an augmented node feature.</p>
<div class="figure align-default" id="gnn-idgnn-cycle">
<a class="reference internal image-reference" href="../_images/gnn-idgnn-cycle.png"><img alt="" src="../_images/gnn-idgnn-cycle.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 242 </span><span class="caption-text">Cycle count at each level forms a vector</span><a class="headerlink" href="#gnn-idgnn-cycle" title="Permalink to this image">¶</a></p>
</div>
<p>ID-GNN is more expressive than their GNN counterparts. ID-GNN is the first message passing GNN that is more expressive than 1-WL test.</p>
</div>
</div>
<div class="section" id="reference">
<h3>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Tutorials and overviews:</p>
<ul>
<li><p>Relational inductive biases and graph networks (Battaglia et al., 2018)</p></li>
<li><p>Representation learning on graphs: Methods and applications (Hamilton et al., 2017)</p></li>
</ul>
</li>
<li><p>Attention-based neighborhood aggregation:</p>
<ul>
<li><p>Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)</p></li>
</ul>
</li>
<li><p>Embedding entire graphs:</p>
<ul>
<li><p>Graph neural nets with edge embeddings (Battaglia et al., 2016; Gilmer et. al., 2017)</p></li>
<li><p>Embedding entire graphs (Duvenaud et al., 2015; Dai et al., 2016; Li et al., 2018) and graph pooling (Ying et al., 2018, Zhang et al., 2018)</p></li>
<li><p>Graph generation and relational inference (You et al., 2018; Kipf et al., 2018)</p></li>
<li><p>How powerful are graph neural networks(Xu et al., 2017)</p></li>
</ul>
</li>
<li><p>Embedding nodes:</p>
<ul>
<li><p>Varying neighborhood: Jumping knowledge networks (Xu et al., 2018), GeniePath (Liu et al., 2018)</p></li>
<li><p>Position-aware GNN (You et al. 2019)</p></li>
</ul>
</li>
<li><p>Spectral approaches to graph neural networks:</p>
<ul>
<li><p>Spectral graph CNN &amp; ChebNet (Bruna et al., 2015; Defferrard et al., 2016)</p></li>
<li><p>Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)</p></li>
</ul>
</li>
<li><p>Other GNN techniques:</p>
<ul>
<li><p>Pre-training Graph Neural Networks (Hu et al., 2019)</p></li>
<li><p>GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)</p></li>
</ul>
</li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./38-ml-for-graph-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="41-processes.html" title="previous page">Processes on Graphs</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>