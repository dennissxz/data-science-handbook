
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Topology Inference &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Processes on Graphs" href="41-processes.html" />
    <link rel="prev" title="Modeling" href="21-modeling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesianâ€™s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-graph-rep-learning.html">
     Graph Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/38-ml-for-graph-data/31-topology-inference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F38-ml-for-graph-data/31-topology-inference.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#link-prediction">
   Link Prediction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scoring">
     Scoring
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logistic-regression">
       Logistic Regression
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#latent-variables">
       Latent Variables
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#association-networks">
   Association Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#correlation-networks">
     Correlation Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#buildup">
       Buildup
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#testing">
       Testing
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multiple-testing">
       Multiple Testing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-correlation-networks">
     Partial Correlation Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partial-correlation">
       Partial Correlation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Buildup
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Testing
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Multiple Testing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-graphical-model-networks">
     Gaussian Graphical Model Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Buildup
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Testing
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#standard-method">
         Standard method
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#methods-based-on-z-i-j-mid-v-backslash-i-j">
         Methods based on
         <span class="math notranslate nohighlight">
          \(z_{i j\mid V \backslash\{i, j\}}\)
         </span>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#methods-based-on-penalized-linear-regression">
         Methods based on Penalized Linear Regression
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-study-of-gene">
     Case Study of Gene
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tomographic-inference">
   Tomographic Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#for-tree-topologies">
     For Tree Topologies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-clustering-based">
     Hierarchical Clustering-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-based">
     Likelihood-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summarizing-collections-of-trees">
     Summarizing Collections of Trees
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more">
   More
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="topology-inference">
<h1>Topology Inference<a class="headerlink" href="#topology-inference" title="Permalink to this headline">Â¶</a></h1>
<p>Can we do inference on graphs (e.g. connectivity) like we can do for usual data matrix? Do we have the concepts and tools such as statistical consistency, efficiency, and robustness? Unfortunately, there is at present no single coherent body of formal results on
inference problems over graphs.</p>
<p>The frameworks developed in these settings naturally take various forms, as dictated by context, with differences driven primarily by the nature of the topology to be inferred and the type of data available.</p>
<div class="section" id="link-prediction">
<h2>Link Prediction<a class="headerlink" href="#link-prediction" title="Permalink to this headline">Â¶</a></h2>
<p>Given</p>
<ul class="simple">
<li><p>full knowledge of all the vertices attributes <span class="math notranslate nohighlight">\(\boldsymbol{x}=\left(x_{1}, \ldots, x_{N_{v}}\right)^{\top}\)</span></p></li>
<li><p>status of some of the edges/non-edges <span class="math notranslate nohighlight">\(\boldsymbol{Y}^{obs}\)</span></p></li>
</ul>
<p>Infer</p>
<ul class="simple">
<li><p>the rest of the edges/non-edges <span class="math notranslate nohighlight">\(\boldsymbol{Y}^{miss}\)</span>, using <span class="math notranslate nohighlight">\(\boldsymbol{Y}^{obs}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p></li>
</ul>
<p>Sometimes we need additional modeling for the mechanisms of missingness.</p>
<ul class="simple">
<li><p>missing at random: probability of missing <span class="math notranslate nohighlight">\(Y_{ij}\)</span> only depends on the values of those other edge variables</p></li>
<li><p>informative missingness: probability of missing <span class="math notranslate nohighlight">\(Y_{ij}\)</span> depends on themselves</p></li>
</ul>
<p>A basic framework:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left(\boldsymbol{Y}^{m i s s} \mid \boldsymbol{Y}^{o b s}=\boldsymbol{y}^{o b s}, \boldsymbol{X}=\boldsymbol{x}\right)
\]</div>
<p>But there are many challenges to predict <span class="math notranslate nohighlight">\(Y_{ij}^{miss}\)</span> jointly. Many methods predict individual <span class="math notranslate nohighlight">\(Y_{ij}^{miss}\)</span>, as introduced below.</p>
<div class="section" id="scoring">
<h3>Scoring<a class="headerlink" href="#scoring" title="Permalink to this headline">Â¶</a></h3>
<p>Scoring methods are based on the use of score functions. These methods are less formal than the model-based methods, but can be quite effective, and often serve as a useful starting point.</p>
<p>For each potential edge <span class="math notranslate nohighlight">\((i, j) \in V^{(2)}_{miss}\)</span> , a score <span class="math notranslate nohighlight">\(s(i, j)\)</span> is computed. A set of predicted edges may then be returned by</p>
<ul class="simple">
<li><p>applying a threshold <span class="math notranslate nohighlight">\(s^*\)</span> to these scores, or</p></li>
<li><p>ordering them and keeping those pairs with the top <span class="math notranslate nohighlight">\(n^*\)</span> values</p></li>
</ul>
<p>There are many scores, designed to assess certain structural characteristics of a graph <span class="math notranslate nohighlight">\(G^{obs}\)</span>.</p>
<ul class="simple">
<li><p>negative shortest distance <span class="math notranslate nohighlight">\(- \operatorname{dist}_{G^{obs}}(i, j)\)</span>: inspired by the small-world principal, the closer the two nodes are, more likely to form an edge</p></li>
<li><p>#(common neighbors) <span class="math notranslate nohighlight">\(\left\vert N_i^{obs} \cap N_j^{obs} \right\vert\)</span>: more common neighbors, more likely to form an edge</p></li>
<li><p><strong>Jaccard coefficient</strong> <span class="math notranslate nohighlight">\(\frac{\left\vert N_i^{obs} \cap N_j^{obs} \right\vert}{\left\vert N_i^{obs} \cup N_j^{obs} \right\vert}\)</span>: a standardized version of the above value.</p></li>
<li><p><strong>Adamic-Adar index</strong> <span class="math notranslate nohighlight">\(\sum_{k \in N_{i}^{obs} \cap N_{j}^{o b s}} \log \frac{1}{\left|N_{k}^{o b s}\right|}\)</span>: variation of the above, weighting more heavily those common neighbors of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> that are themselves <strong>not</strong> highly connected with other vertices.</p></li>
</ul>
<p>There score functions only assess local structure in <span class="math notranslate nohighlight">\(G^{obs}\)</span>. If two nodes do not have any common neighbors, then the score is 0, but they may potentially be connected in <span class="math notranslate nohighlight">\(G^{miss}\)</span>.</p>
<p>To fix this, we an use global neighborhood overlap scores</p>
<ul>
<li><p><strong>Katz index</strong> <span class="math notranslate nohighlight">\(S(i,j) = \sum_{\ell=1}^\infty \beta^\ell [\boldsymbol{A} ^\ell]_{ij}\)</span>: count the number of paths of all lengths between a given pair of nodes, and then discounted by factor <span class="math notranslate nohighlight">\(\beta \in (0, 1)\)</span> and sum up. To compute the number of paths, use powers of the graph adjacency matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. In fact, it Katz index matrix can be computed in closed-form by geometric series of matrices</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{S} = \sum_{\ell=1}^\infty \beta^\ell \boldsymbol{A} ^\ell = \sum_{\ell=0}^\infty \beta^\ell \boldsymbol{A} ^\ell  - \boldsymbol{I}  =  (\boldsymbol{I} - \beta \boldsymbol{A} ) ^{-1}  - \boldsymbol{I}
  \]</div>
</li>
</ul>
<p>For others defined through spectral characteristics of <span class="math notranslate nohighlight">\(G^{obs}\)</span>, see [SAND 261]. Examples:</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">Â¶</a></h3>
<p>Can we approach link prediction as a classification problem?</p>
<ul class="simple">
<li><p>(binary) labels: <span class="math notranslate nohighlight">\(\boldsymbol{y} ^{obs}\)</span></p></li>
<li><p>features: <span class="math notranslate nohighlight">\(\boldsymbol{x}^{obs}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y} ^{obs}\)</span></p></li>
<li><p>predict: <span class="math notranslate nohighlight">\(\boldsymbol{Y} ^{miss}\)</span>.</p></li>
</ul>
<div class="section" id="logistic-regression">
<h4>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">Â¶</a></h4>
<p>A common choice is logistic regression.</p>
<div class="math notranslate nohighlight">
\[
\log \left[
\frac{\mathbb{P}_{\beta}\left(Y_{i j}=1 \mid \boldsymbol{Z}_{i j}=\boldsymbol{z}\right)}{\mathbb{P}_{\beta}\left(Y_{i j}=0 \mid \boldsymbol{Z}_{i j}=\boldsymbol{z}\right)}
\right]=\boldsymbol{\beta}^{\top} \boldsymbol{z}
\]</div>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{Z} _{ij}\)</span> is a vector of explanatory variables indexed in the unordered pairs <span class="math notranslate nohighlight">\((i, j)\)</span>. In general it is some transformation of <span class="math notranslate nohighlight">\(\boldsymbol{Y} ^{obs}_{(-ij)}\)</span> and/or <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{Z}_{i j}=\left(g_{1}\left(\boldsymbol{Y}_{(-i j)}^{o b s}, \boldsymbol{X}\right), \ldots, g_{K}\left(\boldsymbol{Y}_{(-i j)}^{o b s}, \boldsymbol{X}\right)\right)^{\top}\]</div>
<p>For instance, <span class="math notranslate nohighlight">\(g\)</span> can be</p>
<ul class="simple">
<li><p>network structure measures using <span class="math notranslate nohighlight">\(\boldsymbol{Y} ^{obs}_{-ij}\)</span>, e.g. score functions introduced above</p></li>
<li><p>similarity measures between the <span class="math notranslate nohighlight">\(k\)</span>-th vertex attributes of vertex <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>,</p>
<ul>
<li><p>additive <span class="math notranslate nohighlight">\(X_{ik} + X_{jk}\)</span> for continuous values</p></li>
<li><p>indicator <span class="math notranslate nohighlight">\(\mathbb{I} \left\{ X_{ik} = X_{jk} \right\}\)</span> for discrete values</p></li>
</ul>
</li>
</ul>
</li>
<li><p>the coefficient <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is assumed common to all pairs.</p></li>
</ul>
<dl class="simple myst">
<dt>Prediction</dt><dd><p>We compare the predicted value vs some threshold, e.g. 0.5, and then decide classification.</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P}_{\hat{\beta}}\left(Y_{i j}^{miss}=1 \mid \boldsymbol{Z}_{i j}=\boldsymbol{z}\right)= \frac{\exp (\hat{\boldsymbol{\beta} } ^{\top} \boldsymbol{z} )}{1 + \exp (\hat{\boldsymbol{\beta} } ^{\top} \boldsymbol{z} )}
  \]</div>
</dd>
</dl>
<p>Issues</p>
<ul class="simple">
<li><p>Need to consider the missing mechanism. If <span class="math notranslate nohighlight">\(\boldsymbol{Y} ^{miss}\)</span> is <strong>not</strong> at random, the accuracy of the classification approach will suffer.</p></li>
<li><p>In a graph, <span class="math notranslate nohighlight">\(Y_{ij}\)</span> are usually not independent given explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span>, which is assumed in logistic models (no formal work to date exploring the implications on prediction accuracy of ignoring possible dependencies in this manner). Introducing latent variable solve this issue, as discussed below</p></li>
</ul>
</div>
<div class="section" id="latent-variables">
<h4>Latent Variables<a class="headerlink" href="#latent-variables" title="Permalink to this headline">Â¶</a></h4>
<p>The use of latent variables is an intuitively appealing way to indirectly model unobserved factors driving the formation of network structure. Let <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> be an unknown, random, symmetric <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> matrix of latent variables, defined as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{M} = \boldsymbol{U} ^{\top} \boldsymbol{\Lambda} \boldsymbol{U} + \boldsymbol{E}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is an <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> random orthonormal matrix,</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> is an <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> random diagonal matrix,</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{E}\)</span> is a symmetric matrix of i.i.d. noise variables</p></li>
</ul>
<p>Then each entry of <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
M_{ij} = \boldsymbol{u} _i ^{\top} \boldsymbol{\Lambda} \boldsymbol{u} _j + \epsilon_{ij}
\]</div>
<dl class="simple myst">
<dt>Intuition</dt><dd><p>The latent variable matrix <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> is intended to capture effects of network structural characteristics or processes not already described by the observed explanatory variables <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{ij}\)</span>. We add <span class="math notranslate nohighlight">\(M_{ij}\)</span> as an explanatory variable (like that in random effect models). The model becomes</p>
<div class="math notranslate nohighlight">
\[
  \log \left[
  \frac{\mathbb{P}_{\beta}\left(Y_{i j}=1 \mid \boldsymbol{Z}_{i j}=\boldsymbol{z}, M_{ij}=m\right)}{\mathbb{P}_{\beta}\left(Y_{i j}=0 \mid \boldsymbol{Z}_{i j}=\boldsymbol{z}, M_{ij}=m\right)}
  \right]=\beta^{\top} \boldsymbol{z} + m
  \]</div>
<p>Now <span class="math notranslate nohighlight">\(Y_{ij}\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{ij}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{M} _{ij}\)</span>, but conditionally <em>dependent</em> given only the <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{ij}\)</span>.</p>
</dd>
</dl>
<p>Commonly used distributions for <span class="math notranslate nohighlight">\(\boldsymbol{U} , \boldsymbol{\Lambda} , \boldsymbol{E}\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span>: uniform distribution on the space of all <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> orthonormal matrices</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} , \boldsymbol{E}\)</span>: multivariate Gaussian (facilitate MCMC sampling)</p></li>
</ul>
<dl class="simple myst">
<dt>Prediction</dt><dd><p>Compare the expected probability of <span class="math notranslate nohighlight">\(Y_{ij}=1\)</span> with some threshold, which may be approximated numerically to any desired accuracy by the corresponding sample average of draws from the posterior indicated</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{E}\left(\frac{\exp \left\{\beta^{T} \boldsymbol{Z}_{i j}+M_{i j}\right\}
  }{1+\exp \left\{\beta^{T} \boldsymbol{Z}_{i j}+M_{i j}\right\}}
   \mid \boldsymbol{Y}^{o b s}=\boldsymbol{y}^{o b s}, \boldsymbol{Z}_{i j}=\boldsymbol{z}\right)
  \]</div>
</dd>
<dt>Cons</dt><dd><p>MCMC computation cost, mainly driven by the need to draw <span class="math notranslate nohighlight">\(N_v ^2\)</span> unobserved variables <span class="math notranslate nohighlight">\(U_{ij}\)</span>.
Sol: let <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> have only <span class="math notranslate nohighlight">\(K\)</span> non-zero column vectors for <span class="math notranslate nohighlight">\(K \ll N_v\)</span>, hence low-rank of <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span>. In fact <span class="math notranslate nohighlight">\(K=2, 3\)</span> work well in practice. [SAND 200, 201]</p>
</dd>
</dl>
<p>For a case study see [SAND pg.205].</p>
</div>
</div>
</div>
<div class="section" id="association-networks">
<h2>Association Networks<a class="headerlink" href="#association-networks" title="Permalink to this headline">Â¶</a></h2>
<p>We use non-trivial level of association (e.g. correlation) between certain characteristics of the vertices to decide edge assignment. But the association is itself unobserved and must be inferred from measurements reflecting these characteristics.</p>
<p>Given</p>
<ul class="simple">
<li><p>no knowledge of edge status anywhere</p></li>
<li><p>relevant measurements at all of the vertices <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x}_1, \ldots, \boldsymbol{x}_{N_v} \right\}\)</span></p></li>
</ul>
<p>Infer</p>
<ul class="simple">
<li><p>edge status <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> using these measurements <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span></p></li>
</ul>
<div class="section" id="correlation-networks">
<h3>Correlation Networks<a class="headerlink" href="#correlation-networks" title="Permalink to this headline">Â¶</a></h3>
<p>An intuitive measure of similarity between a vertex pair <span class="math notranslate nohighlight">\((i, j)\)</span> is correlation.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{sim}(i, j)  = \rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}
\]</div>
<div class="section" id="buildup">
<h4>Buildup<a class="headerlink" href="#buildup" title="Permalink to this headline">Â¶</a></h4>
<p>Suppose for each vertex <span class="math notranslate nohighlight">\(i\)</span>, we have <span class="math notranslate nohighlight">\(n\)</span> independent observations <span class="math notranslate nohighlight">\(\left\{ x_{i1}, \ldots, x_{in} \right\}\)</span>, e.g. gene expression levels from <span class="math notranslate nohighlight">\(n\)</span> experiments. We can then form an <span class="math notranslate nohighlight">\(n \times N_v\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, and compute the <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, hence obtain the entries <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> and use that to compute <span class="math notranslate nohighlight">\(\hat{\rho}\)</span>.</p>
<p>The corresponding association graph <span class="math notranslate nohighlight">\(G\)</span> is the graph with edge set</p>
<div class="math notranslate nohighlight">
\[
E=\left\{\{i, j\} \in V^{(2)}: \rho_{i j} \neq 0\right\}
\]</div>
<p>Hence, the the task is to infer the set of non-zero correlations, which can be approached through hypotheses testing</p>
<div class="math notranslate nohighlight">
\[
H_{0}: \rho_{i j}=0 \quad \text { versus } \quad H_{1}: \rho_{i j} \neq 0
\]</div>
<p>Problems</p>
<ul class="simple">
<li><p>what test statistics?</p></li>
<li><p>what is the null distribution of that test statistic?</p></li>
<li><p>there are <span class="math notranslate nohighlight">\(N_v (N_v - 1)/2\)</span> potential edges, which implies multiple testing problem.</p></li>
</ul>
</div>
<div class="section" id="testing">
<h4>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">Â¶</a></h4>
<p>If <span class="math notranslate nohighlight">\((X_i, X_j)\)</span> follow bivariate Gaussian, then <span class="math notranslate nohighlight">\(\hat{\rho}_{ij}\)</span> under <span class="math notranslate nohighlight">\(H_0: \rho_{ij}=0\)</span> has a closed-form, but the computation of <span class="math notranslate nohighlight">\(p\)</span>-values is hard. Therefore, some transformed versions of <span class="math notranslate nohighlight">\(\hat{\rho}_{ij}\)</span> may be preferable</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z_{i j}=\frac{\hat{\rho}_{i j} \sqrt{n-2}}{\sqrt{1-\hat{\rho}_{i j}^{2}}} \sim t_{n-1}\)</span>, and under <span class="math notranslate nohighlight">\(H_0\)</span> it is robust to departures of <span class="math notranslate nohighlight">\(X_i\)</span> from Gaussianity.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_{i j}=\tanh ^{-1}\left(\hat{\rho}_{i j}\right)=\frac{1}{2} \log \left[\frac{\left(1+\hat{\rho}_{i j}\right)}{\left(1-\hat{\rho}_{i j}\right)} \right]\)</span>, aka Fisher transformation.</p>
<ul>
<li><p>for bivariate Gaussian pairs, the distribution of <span class="math notranslate nohighlight">\(z_{ij}\)</span> does not have a simple exact form. But under <span class="math notranslate nohighlight">\(H_0\)</span> this distribution is well approximated by <span class="math notranslate nohighlight">\(\mathcal{N} (0, \frac{1}{n-3} )\)</span> even for moderately large <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Permutation methods can also be used, but is computationally intensive for large <span class="math notranslate nohighlight">\(N_v\)</span>.</p>
</div>
<div class="section" id="multiple-testing">
<h4>Multiple Testing<a class="headerlink" href="#multiple-testing" title="Permalink to this headline">Â¶</a></h4>
<p>Recall the false discovery rate is defined to be</p>
<div class="math notranslate nohighlight">
\[
\mathrm{FDR}=\mathbb{E}\left(\frac{R_{\text {false }}}{R} \mid R&gt;0\right) \mathbb{P}(R&gt;0)
\]</div>
<p>where <span class="math notranslate nohighlight">\(R\)</span> is the number of rejections among our tests and <span class="math notranslate nohighlight">\(R_{\text {false }}\)</span> is the number of false rejections.</p>
<p>To guarantee <span class="math notranslate nohighlight">\(\mathrm{FDR} \le \gamma\)</span>, we use the original method proposed by Benjamini and Hochberg [SAND 33],</p>
<ul class="simple">
<li><p>sort the <span class="math notranslate nohighlight">\(p\)</span>-values from our <span class="math notranslate nohighlight">\(N =N_v (N_vâˆ’1)/2\)</span> tests, yielding a sequence <span class="math notranslate nohighlight">\(p_{(1)}\le p_{(2)} \le \ldots \le p_{(N)}\)</span>,</p></li>
<li><p>reject the null hypothesis for all potential edges for which <span class="math notranslate nohighlight">\(p_{(k)} \leq(k / N) \gamma\)</span>.</p></li>
</ul>
<p>Alternatively, we can use Storey [SAND 370] method and declare edges to be present using a particular <span class="math notranslate nohighlight">\(q\)</span>-value. Then only <span class="math notranslate nohighlight">\(qN\)</span> of the edges will be included erroneously.</p>
<p>When dependency of tests [??] exists, the first method still holds, and there are other methods.</p>
</div>
</div>
<div class="section" id="partial-correlation-networks">
<h3>Partial Correlation Networks<a class="headerlink" href="#partial-correlation-networks" title="Permalink to this headline">Â¶</a></h3>
<p>If it is felt desirable to construct a graph <span class="math notranslate nohighlight">\(G\)</span> where the inferred edges are more reflective of direct influence among vertices, rather than indirect influence through some common neighbor (confounders), the notion of partial correlation becomes relevant.</p>
<div class="section" id="partial-correlation">
<h4>Partial Correlation<a class="headerlink" href="#partial-correlation" title="Permalink to this headline">Â¶</a></h4>
<p>Continuing from the setting of <span class="math notranslate nohighlight">\(n \times N_v\)</span> measurement matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<dl class="simple myst">
<dt>Definition (Partial correlation)</dt><dd><p>The partial correlation of attributes <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> of vertices <span class="math notranslate nohighlight">\(i, j \in V\)</span> w.r.t. the attributes <span class="math notranslate nohighlight">\(X_{k_1}, \ldots, X_{k_m}\)</span> of vertices <span class="math notranslate nohighlight">\(k_1, \ldots, k_m \in V \setminus \left\{ i, j \right\}\)</span>, is the correlation between <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> left over, after adjusting for those effects common to both. Let <span class="math notranslate nohighlight">\(S_m = \left\{ k_1, \ldots, k_m \right\}\)</span>, the partial correlation of <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> adjusting for <span class="math notranslate nohighlight">\(\boldsymbol{X} _{S_m} = [X_{k_1}, \ldots, X_{k_m}] ^{\top}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
  \rho_{i j \mid S_{m}}=\frac{\sigma_{i j \mid S_{m}}}{\sqrt{\sigma_{i i\mid S_{m}} \sigma_{j j \mid S_{m}}} }
  \]</div>
</dd>
</dl>
<p>To compute it, let <span class="math notranslate nohighlight">\(\boldsymbol{W} _1 = [X_i, X_j] ^{\top}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{W} _2 = \boldsymbol{X} _{S_m}\)</span>. We can partition the covariance matrix to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{Cov}\left(\begin{array}{l}
\boldsymbol{W}_{1} \\
\boldsymbol{W}_{2}
\end{array}\right)=\left[\begin{array}{ll}
\boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} &amp; \boldsymbol{\Sigma}_{22}
\end{array}\right]
\end{split}\]</div>
<p>Then the <span class="math notranslate nohighlight">\(2 \times 2\)</span> partial covariance matrix is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma}_{11 \mid 2}=\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1} \boldsymbol{\Sigma}_{21}
\]</div>
<p>The values <span class="math notranslate nohighlight">\(\sigma_{ii\vert S_m}, \sigma_{jj\vert S_m}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{ij\vert S_m} = \sigma_{ji\vert S_m}\)</span> are diagonal and off-diagonal elements of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{11 \mid 2}\)</span>.</p>
<p>In particular,</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(m=0\)</span>, the partial correlation reduces to the Pearson correlation, i.e. unconditional case.</p></li>
<li><p>if <span class="math notranslate nohighlight">\([X_{i}, X_{j}, X_{k_{1}}, \ldots, X_{k_{m}}]^{\top}\)</span> has a multivariate Gaussian, then <span class="math notranslate nohighlight">\(\rho_{ij \vert S_m}=0\)</span> if and only if <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> are independent conditional on <span class="math notranslate nohighlight">\(\boldsymbol{X} _{S_m}\)</span>. For more general distributions, however, zero partial correlation will not necessarily imply independence (the converse, of course, is still true).</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title"> Computation Issue of <span class="math notranslate nohighlight">\(\hat{\rho}_{i j \mid S_{m}}\)</span></p>
<ul class="simple">
<li><p>To compute <span class="math notranslate nohighlight">\(\rho_{ij \mid S_m}\)</span> for all <span class="math notranslate nohighlight">\(S_m\)</span> is hard. It is more computationally efficient to use recursive expressions between <span class="math notranslate nohighlight">\(\rho_{ij \mid S_m}\)</span> and <span class="math notranslate nohighlight">\(\rho_{ij \mid S_{m-1}}\)</span>, see Anderson [SAND 11].</p></li>
<li><p>If <span class="math notranslate nohighlight">\(m &gt; n\)</span>, then <span class="math notranslate nohighlight">\(\hat{\rho}_{i j \mid S_{m}}\)</span> is not well defined since <span class="math notranslate nohighlight">\(\hat{\boldsymbol{S}} _{22}\)</span> is note invertible.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(m &lt;n\)</span> and <span class="math notranslate nohighlight">\(m\)</span> is large w.r.t. the number <span class="math notranslate nohighlight">\(n\)</span> of measurements per vertex, then <span class="math notranslate nohighlight">\(\hat{\rho}_{i j \mid S_{m}}\)</span> is a poor estimate of <span class="math notranslate nohighlight">\(\rho_{i j \mid S_{m}}\)</span>.</p></li>
<li><p>Computational costs grow exponentially in <span class="math notranslate nohighlight">\(m\)</span>. <span class="math notranslate nohighlight">\(m=2\)</span> is advocated in the context of inference of biochemical networks.</p></li>
<li><p>An algorithmic definition of this value is that it is the result of</p>
<ol class="simple">
<li><p>performing separate multiple linear regressions of the observations of <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span>, respectively, on the observed values of <span class="math notranslate nohighlight">\(\boldsymbol{X} _{S_m}\)</span>, and then</p></li>
<li><p>computing the empirical Pearson correlation between the two resulting sets of residuals.</p></li>
</ol>
</li>
</ul>
</div>
</div>
<div class="section" id="id1">
<h4>Buildup<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h4>
<p>Given <span class="math notranslate nohighlight">\(m\)</span>, there are many ways to define edge set using partial correlations. For instance, there is an edge <span class="math notranslate nohighlight">\(e(i,j)\)</span> iff the partial correlation <span class="math notranslate nohighlight">\(\rho_{i j \mid S_{m}} \neq 0\)</span> regardless of which <span class="math notranslate nohighlight">\(m\)</span> other vertices are conditioned upon.</p>
<div class="math notranslate nohighlight">
\[E=\left\{\{i, j\} \in V^{(2)}: \rho_{i j \mid S_{m}} \neq 0 \ \forall \ S_{m} \in V_{\backslash\{i, j\}}^{(m)}\right\}\]</div>
<p>The testing problem is then</p>
<div class="math notranslate nohighlight">
\[
H_{0}: \rho_{i j \mid S_{m}}=0 \quad \text { for some } \quad S_{m} \in V_{\backslash\{i, j\}}^{(m)}
\]</div>
<p>versus</p>
<div class="math notranslate nohighlight">
\[
H_{1}: \rho_{i j \mid S_{m}} \neq 0 \quad \text { for all } \quad S_{m} \in V_{\backslash\{i, j\}}^{(m)}
\]</div>
<p>Then we select a test statistic, construct an appropriate null distribution, and adjust for multiple testing, as the correlation networks above.</p>
</div>
<div class="section" id="id2">
<h4>Testing<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h4>
<p>The above test can be considered as a collection of smaller testing sub-problems of the form</p>
<div class="math notranslate nohighlight">
\[
H_{0}^{\prime}: \rho_{i j \mid S_{m}}=0 \quad \text { versus } \quad H_{1}^{\prime}: \rho_{i j \mid S_{m}} \neq 0
\]</div>
<p>Under the joint Gaussian assumption, the null empirical distribution <span class="math notranslate nohighlight">\(\hat{\rho}_{ij \mid S_m}\)</span> is known but hard to compute the <span class="math notranslate nohighlight">\(p\)</span>-value. Fisher transformation can also be used here</p>
<div class="math notranslate nohighlight">
\[z_{i j \vert S_m}=\tanh ^{-1}\left(\hat{\rho}_{i j\vert S_m}\right)=\frac{1}{2} \log \left[\frac{\left(1+\hat{\rho}_{i j\vert S_m}\right)}{\left(1-\hat{\rho}_{i j\vert S_m}\right)} \right] \rightarrow \mathcal{N} \left( 0, \frac{1}{n-m-3} \right)\]</div>
<p>Then, we can aggregate the <span class="math notranslate nohighlight">\(p\)</span>-values from sub-problems and define</p>
<div class="math notranslate nohighlight">
\[
p_{i j, \max }=\max \left\{p_{i j \mid S_{m}}: S_{m} \in V_{\backslash\{i, j\}}^{(m)}\right\}
\]</div>
<p>to be the <span class="math notranslate nohighlight">\(p\)</span>-value for the original testing problem.</p>
<div class="warning admonition">
<p class="admonition-title"> Change of conclusion after conditioning</p>
<p>In practice, we may see examples where</p>
<ul class="simple">
<li><p>significant <span class="math notranslate nohighlight">\(\rho_{ij} &gt; 0\)</span> but insignificant <span class="math notranslate nohighlight">\(\rho_{ij \mid S_m}\)</span> after conditioning, or</p></li>
<li><p>both significant <span class="math notranslate nohighlight">\(\rho_{ij} &gt; 0\)</span> and <span class="math notranslate nohighlight">\(\rho_{ij \mid S_m} &lt; 0\)</span>, i.e. reverse sign after conditioning.</p></li>
</ul>
</div>
</div>
<div class="section" id="id3">
<h4>Multiple Testing<a class="headerlink" href="#id3" title="Permalink to this headline">Â¶</a></h4>
<p>Given the full collection of <span class="math notranslate nohighlight">\(\left\{ p_{ij, \max} \right\}\)</span>, over all potential edges <span class="math notranslate nohighlight">\((i, j)\)</span>, an FDR procedure may be applied to this collection to choose an appropriate testing threshold, analogous to the manner described above.</p>
</div>
</div>
<div class="section" id="gaussian-graphical-model-networks">
<h3>Gaussian Graphical Model Networks<a class="headerlink" href="#gaussian-graphical-model-networks" title="Permalink to this headline">Â¶</a></h3>
<div class="section" id="id4">
<h4>Buildup<a class="headerlink" href="#id4" title="Permalink to this headline">Â¶</a></h4>
<p>A special case of the use of partial correlation coefficients: assume the vertex attributes have multivariate joint Gaussian, and set <span class="math notranslate nohighlight">\(m=N_v -2\)</span>, i.e. conditioning on all other vertices, then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho_{i j \mid V \backslash\{i, j\}} \neq 0\)</span> iff <span class="math notranslate nohighlight">\(X_i \perp X_j \mid V \backslash\{i, j\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(E=\left\{\{i, j\} \in V^{(2)}: \rho_{i j \mid V \backslash\{i, j\}} \neq 0\right\}\)</span></p></li>
</ul>
<p>The resulting graph is called a <strong>conditional independence graph</strong>, where the edges encode conditional dependence, and the non-edges encode conditional independence. The overall model is called a <strong>Gaussian graphical model</strong>.</p>
<p>Properties</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho_{i j \mid V \backslash\{i, j\}}=\frac{-\omega_{i j}}{\sqrt{\omega_{i i} \omega_{j j}}}\)</span> where <span class="math notranslate nohighlight">\(w_{ij}\)</span> is the entry of <span class="math notranslate nohighlight">\(\boldsymbol{\Omega} = \boldsymbol{\Sigma} ^{-1}\)</span>.</p></li>
<li><p>The matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Omega}\)</span> is known as the <strong>concentration</strong> or <strong>precision</strong> matrix, and its non-zero off-diagonal entries are linked in one-to-one correspondence with the edges in <span class="math notranslate nohighlight">\(G\)</span> as defined above, hence <span class="math notranslate nohighlight">\(G\)</span> is also called a <strong>concentration graph</strong>.</p></li>
</ul>
<p>The testing problem is then</p>
<div class="math notranslate nohighlight">
\[
H_{0}: \rho_{i j \mid V \backslash\{i, j\}}=0 \quad vs \quad
H_{1}: \rho_{i j \mid V \backslash\{i, j\}} \neq 0
\]</div>
</div>
<div class="section" id="id5">
<h4>Testing<a class="headerlink" href="#id5" title="Permalink to this headline">Â¶</a></h4>
<p>The problem of inferring <span class="math notranslate nohighlight">\(G\)</span> from data <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is known as the <strong>covariance selection problem</strong> [SAND 115]. There are many approaches with pros and cons.</p>
<div class="section" id="standard-method">
<h5>Standard method<a class="headerlink" href="#standard-method" title="Permalink to this headline">Â¶</a></h5>
<p>[SAND 403, Ch. 6], [SAND 248, Ch. 5]</p>
<p>A standard method is to employ a recursive, likelihood-based procedure. Given significance, level <span class="math notranslate nohighlight">\(\alpha\)</span>,</p>
<ol class="simple">
<li><p>Start with a complete graph on <span class="math notranslate nohighlight">\(N_v\)</span> vertices as an initial estimate <span class="math notranslate nohighlight">\(G ^{(0)}\)</span>, and sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span></p></li>
<li><p>Given <span class="math notranslate nohighlight">\(G ^{(t)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{S} ^{(t)}\)</span>, compute <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Omega}} = \boldsymbol{S} ^{-1}\)</span> and <span class="math notranslate nohighlight">\(\hat{\rho}_{i j \mid V \backslash\{i, j\}}\)</span>. If <span class="math notranslate nohighlight">\(H_0: \rho_{i j \mid V \backslash\{i, j\}}=0\)</span> is not rejected at level <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<ul class="simple">
<li><p>Remove edge <span class="math notranslate nohighlight">\(e(i, j)\)</span>, obtain <span class="math notranslate nohighlight">\(G ^{(t+1)}\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(s_{ij}=0\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{S} ^{(t+1)}\)</span> (??)</p></li>
</ul>
</li>
</ol>
<p>Cons for large graphs</p>
<ul class="simple">
<li><p>computationally intensive</p></li>
<li><p>no attention to multiple testing</p></li>
<li><p>if <span class="math notranslate nohighlight">\(n \ll N_v\)</span>, then</p>
<ul>
<li><p>the sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> is not invertible. Sol: use pseudo inverse <span class="math notranslate nohighlight">\(\boldsymbol{S} ^\dagger\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> has large variance. Sol : to reduce variance, use bagging to sample rows from <span class="math notranslate nohighlight">\(n \times N_v\)</span> data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> for <span class="math notranslate nohighlight">\(b=1, \ldots, B\)</span> times. Each time an estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{S}}_b\)</span> is computed. Then compute a smoothed covariance estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{S}}_{\text{bag} }\)</span>. For constructing the null distribution of <span class="math notranslate nohighlight">\(\hat{\rho}_{i j \mid V \setminus i, j\}}^{\text{bag} }\)</span>, see [SAND 340, 341].</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="methods-based-on-z-i-j-mid-v-backslash-i-j">
<h5>Methods based on <span class="math notranslate nohighlight">\(z_{i j\mid V \backslash\{i, j\}}\)</span><a class="headerlink" href="#methods-based-on-z-i-j-mid-v-backslash-i-j" title="Permalink to this headline">Â¶</a></h5>
<p>[SAND 127]</p>
<p>Alternatively, we can turn to test the Fisher transformation <span class="math notranslate nohighlight">\(z_{i j\mid V \backslash\{i, j\}}\)</span>. We assign edge <span class="math notranslate nohighlight">\(e(i, j)\)</span> iff Fisher transformation</p>
<div class="math notranslate nohighlight">
\[\left\vert z_{i j \mid V \backslash\{i, j\}} \right\vert&gt;(n-N_v)^{-1 / 2} c_{N_{v}}(\alpha)\]</div>
<p>where <span class="math notranslate nohighlight">\(c_{N_{v}}(\alpha)=\Phi^{-1}\left[0.5(1-\alpha)^{\left[2 / N_{v}\left(N_{v}-1\right)\right]}+0.5\right]\)</span>.</p>
<p>These methods address the problem of multiple testing when <span class="math notranslate nohighlight">\(n &gt; N_v\)</span>. If <span class="math notranslate nohighlight">\(n \le N_v\)</span>, use the bagging remedy introduced above. The true <span class="math notranslate nohighlight">\(G\)</span> will be correctly inferred by this procedure with probability at least <span class="math notranslate nohighlight">\(1-\alpha\)</span>, for large <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
<div class="section" id="methods-based-on-penalized-linear-regression">
<h5>Methods based on Penalized Linear Regression<a class="headerlink" href="#methods-based-on-penalized-linear-regression" title="Permalink to this headline">Â¶</a></h5>
<p>There is a link between this inference problem and linear regression. <a class="reference internal" href="../12-probabilities/91-exponential-families.html#multi-gaussian"><span class="std std-ref">Recall</span></a> that for multivariate Gaussians with zero means, the conditional expectation of the <span class="math notranslate nohighlight">\(i\)</span>-th variable can be written as a linear combination of other variables</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[X_{i} \mid \boldsymbol{X}^{(-i)}=\boldsymbol{x}^{(-i)}\right]=\left(\boldsymbol{\beta} ^{(-i)}\right)^{\top} \boldsymbol{x}^{(-i)}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}^{(-i)}=\left(X_{1}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{N_{v}}\right)^{\top}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{i, -i} \boldsymbol{\Sigma} _{-i,-i} ^{-1} = \boldsymbol{\beta} ^{(-i)} \in \mathbb{R}^{N_{v}-1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_j^{(-i)} = - \omega_{ij}/\omega_{ii}\)</span></p></li>
</ul>
<p>Therefore, <span class="math notranslate nohighlight">\(\rho_{i j \mid V \backslash\{i, j\}}=\frac{-\omega_{i j}}{\sqrt{\omega_{i i} \omega_{j j}}} = 0\)</span> if and only if <span class="math notranslate nohighlight">\(\beta_j^{(-i)} = 0\)</span>. The problem of testing <span class="math notranslate nohighlight">\(\rho_{i j \mid V \backslash\{i, j\}}=\frac{-\omega_{i j}}{\sqrt{\omega_{i i} \omega_{j j}}} = 0\)</span> is now transformed to testing <span class="math notranslate nohighlight">\(\beta_j^{(-i)} = 0\)</span>, which can be done using regression-based methods of estimation and variable selection. More precisely, we regress <span class="math notranslate nohighlight">\(n\)</span> observations of <span class="math notranslate nohighlight">\(X_i\)</span> over all other <span class="math notranslate nohighlight">\(N_v -1\)</span> variables <span class="math notranslate nohighlight">\(\boldsymbol{X} ^{(-i)}\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(n \ll N_v\)</span>, a penalized regression strategy is prudent. For instance, we can use LASSO with penalty coefficient <span class="math notranslate nohighlight">\(\lambda\)</span>, that performs simultaneous estimation and variable selection.</p>
<p>We repeat this process for all vertices <span class="math notranslate nohighlight">\(i \in V\)</span>. Note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{j}^{(-i)} \neq 0\)</span> does not necessarily imply <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{i}^{(-j)} \neq 0\)</span>. One can then assign edge <span class="math notranslate nohighlight">\((i, j)\)</span> if</p>
<ul class="simple">
<li><p>either one is non-zero</p></li>
<li><p>both are non-zero</p></li>
</ul>
<p>Meinshausen and Buhlmann [SAND 275], show that under conditions on <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} , \lambda, N_v\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, the true graph <span class="math notranslate nohighlight">\(G\)</span> will be inferred with high probability using either of these conventions, even in cases where <span class="math notranslate nohighlight">\(n \ll N_v\)</span>. Meanwhile, the choice of <span class="math notranslate nohighlight">\(\lambda\)</span> is important. They show that selecting <span class="math notranslate nohighlight">\(\lambda\)</span> by cross-validation will yield provably <strong>bad</strong> results, because the goal is one of variable selection and not prediction.</p>
<p>Other related study using penalized regression methods for inferring <span class="math notranslate nohighlight">\(G\)</span></p>
<ul class="simple">
<li><p>Bayesian approach [SAND 119]</p></li>
<li><p>Lasso-like penalized logistic regression [SAND 390]</p></li>
<li><p>penalized maximum likelihood [SAND 410], which can be solved by graphical lasso [SAND 161]</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="case-study-of-gene">
<h3>Case Study of Gene<a class="headerlink" href="#case-study-of-gene" title="Permalink to this headline">Â¶</a></h3>
<p>Example: Though experimentally infeasible, can we construct the gene regulatory (activation or repression) networks as a problem of network inference, given measurements sufficiently reflective of gene regulatory activity?</p>
<dl class="simple myst">
<dt>Definition</dt><dd><ul class="simple">
<li><p><strong>Genes</strong> are sets of segments of DNA that encode information necessary to the proper functioning of a cell.</p></li>
<li><p>such information is utilized in the <strong>expression</strong> of genes, whereby biochemical products, in the form of RNA or proteins, are created</p></li>
<li><p>The <strong>regulation</strong> of a gene refers to the control of its expression.</p></li>
<li><p>A gene that plays a role in controlling gene expression at transcription stage (DNA is copied to RNA) is called a <strong>transcription factor</strong> (TF), and the genes that are controlled by it, gene <strong>targets</strong>.</p></li>
<li><p>The problem of inferring regulatory interactions among genes in this context refers to the identification of <strong>TF/target gene pairs</strong>.</p></li>
</ul>
</dd>
</dl>
<p>Measurements</p>
<ul class="simple">
<li><p>The relative levels of RNA expression of genes in a cell, under a given set of conditions, can be measured efficiently on a genome-wide scale using <strong>microarray</strong> technologies.</p></li>
<li><p>In particular, for each gene <span class="math notranslate nohighlight">\(i\)</span>, the vertex attribute vector <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \in \mathbb{R} ^m\)</span> typically consists of RNA relative expression levels measured for that gene over a compendium of <span class="math notranslate nohighlight">\(m\)</span> experiments, e.g. changes in pH, growing media, heat, oxygen concentrations, and genetic composition.</p></li>
</ul>
<p>Observation</p>
<ul class="simple">
<li><p>Due to the nature of the regulation process, the expression levels of TFs and their targets often can be expected to be highly correlated with each other</p></li>
<li><p>Given that many TFs have multiple targets, including other TFs, the expression levels among many of the TFs can be expected to be highly correlated as well</p></li>
<li><p>Such correlations are evident in the image plot, where the order of the
genes (rows) and experiments (columns) has been chosen using a cluster analysis, to enhance the visual assessment of correlation.</p></li>
</ul>
<div class="figure align-default" id="graph-gene-exp">
<a class="reference internal image-reference" href="../_images/graph-gene-exp.png"><img alt="" src="../_images/graph-gene-exp.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 207 </span><span class="caption-text">Image representation of 445 microarray expression profiles collected for E. coli, under various conditions, for the 153 genes that are listed as known transcription factors (TF) in the RegulonDB, using blue-organge scale. [Kolaczyk 2009]
database.</span><a class="headerlink" href="#graph-gene-exp" title="Permalink to this image">Â¶</a></p>
</div>
<p>Challenge</p>
<ul class="simple">
<li><p>A TF can actually be a target of another TF. And so direct correlation between measurements of a TF and a gene target may actually just be a reflection of the regulation of that TF by another TF. Sol: use partial correlation.</p></li>
</ul>
<p>Methods and Results</p>
<ul class="simple">
<li><p>Use three methods above, all use Fisher transformation for testing. See SAND pg.221-223</p></li>
<li><p>All methods have high precision and low recall. Faith et al. [SAND 140] have argued that the low recall is due primarily to limitations in both the number and the diversity of the expression profiles produced by the available experiments.</p></li>
<li><p>Takeaway: high precision model is still good for prediction purpose, though not good at recover old true labels (low recall).</p></li>
</ul>
</div>
</div>
<div class="section" id="tomographic-inference">
<h2>Tomographic Inference<a class="headerlink" href="#tomographic-inference" title="Permalink to this headline">Â¶</a></h2>
<p>In tomographic network topology inference problems, measurements are available only at vertices that are somehow at the â€˜exteriorâ€™ of the network, and it is necessary to infer the presence or absence of both edges and vertices in the â€˜interior.â€™ Here â€˜exteriorâ€™ and â€˜interiorâ€™ are somewhat relative terms and generally are used simply to distinguish between vertices where it is and is not possible (or, at least, not convenient) to obtain measurements. For example, in computer networks, desktop and laptop computers are typical instances of â€˜exteriorâ€™ vertices, while Internet routers to which we do not have access are effectively â€˜interiorâ€™ vertices.</p>
<p>The problem of tomographic network topology inference is just one instance of
what are more broadly referred to as problems of <strong>network tomography</strong>.</p>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">Â¶</a></h3>
<p>Given</p>
<ul class="simple">
<li><p>measurements at only a particular subset of vertices</p></li>
</ul>
<p>Infer</p>
<ul class="simple">
<li><p>topology of the rest</p></li>
</ul>
<p>Challenge</p>
<ul class="simple">
<li><p>For a given set of measurements, there are likely many network topologies that conceivably could have generated them</p></li>
<li><p>Without any further constraints on aspects like the number of internal vertices and edges, and the manner in which they connect to one another, we have no sensible way of choosing among these possible solutions.</p></li>
<li><p>The problem is thus an example of an â€˜ill-posed inverse problemâ€™ in mathematics â€“ an inverse problem, in the sense of inverting the mapping
from internal to external, and ill-posed, in the sense of that mapping being many-to-one.</p></li>
<li><p>In order to obtain useful solutions to such ill-posed problems, we impose assumptions on the internal structure.</p></li>
</ul>
<p>Assumption</p>
<ul class="simple">
<li><p>a key structural simplification has been the restriction to inference of networks in the form of trees.</p></li>
</ul>
<p>Related fields</p>
<ul class="simple">
<li><p>inference of phylogenies: to construct trees (i.e., phylogenies) from data, for the purpose of describing evolutionary relationships among biological species.</p></li>
<li><p>computer network analysis: to infer the tree formed by a set of paths along which traffic flows from a given origin Internet address to a set of destination addresses, including physical or logical topologies.</p></li>
</ul>
</div>
<div class="section" id="for-tree-topologies">
<h3>For Tree Topologies<a class="headerlink" href="#for-tree-topologies" title="Permalink to this headline">Â¶</a></h3>
<p>Given a tree <span class="math notranslate nohighlight">\(T=(V_T, E_T)\)</span>, let <span class="math notranslate nohighlight">\(r \in V_T\)</span> be its root, <span class="math notranslate nohighlight">\(R \in V_T\)</span> be its leaves, and <span class="math notranslate nohighlight">\(V \setminus \left\{ \left\{ r \right\} \cup \left\{ R \right\}\right\}\)</span> be the <strong>internal vertices</strong>. The edges <span class="math notranslate nohighlight">\(E_T\)</span> are often referred to as branches.</p>
<p>A binary three is also called a <strong>bifurcating tree</strong>, where each internal vertex has at most two children. We will restrict our attention here almost entirely to binary trees. Trees with more general branching structure can always be represented as binary trees and, indeed, methods for their inference can be built upon inferential methods for binary trees.</p>
<div class="figure align-default" id="graph-topo-tree">
<a class="reference internal image-reference" href="../_images/graph-topo-tree.png"><img alt="" src="../_images/graph-topo-tree.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 208 </span><span class="caption-text">Schematic representation of a binary tree. Measurements are available at the leaves 1, 2, 3, 4, and 5 (<span style="color:yellow">yellow</span>). Other elements (possibly including the root <span class="math notranslate nohighlight">\(r\)</span>) are unknown.</span><a class="headerlink" href="#graph-topo-tree" title="Permalink to this image">Â¶</a></p>
</div>
<p>The leaves <span class="math notranslate nohighlight">\(R\)</span> are known, but the internal vertices and the branches are not. The root <span class="math notranslate nohighlight">\(r\)</span> may or may not be known. We associate with each branch a weight <span class="math notranslate nohighlight">\(w\)</span>, which we may or may not wish to infer.</p>
<p>For a set of <span class="math notranslate nohighlight">\(N_\ell\)</span> vertices, we have <span class="math notranslate nohighlight">\(n\)</span> independent and identically distributed observations of some random variables <span class="math notranslate nohighlight">\(\left\{ X_1, X_2, \ldots, X_{N_\ell} \right\}\)</span>. Assume that these <span class="math notranslate nohighlight">\(N_\ell\)</span> vertices can be identified with the leaves <span class="math notranslate nohighlight">\(R\)</span> of a tree <span class="math notranslate nohighlight">\(T\)</span>, we aim to find that tree <span class="math notranslate nohighlight">\(T\)</span> in the set <span class="math notranslate nohighlight">\(\mathcal{T}_{N_\ell}\)</span> of all binary trees with <span class="math notranslate nohighlight">\(N_\ell\)</span> labeled leaves that best explains the data, in some well-defined sense. If we have knowledge of a root <span class="math notranslate nohighlight">\(r\)</span>, then the roots of the trees in <span class="math notranslate nohighlight">\(\mathcal{T}_{N_\ell}\)</span> will all be identified with <span class="math notranslate nohighlight">\(r\)</span>. In some contexts we may also be interested in inferring a set of weights <span class="math notranslate nohighlight">\(w\)</span> for the branches in <span class="math notranslate nohighlight">\(T\)</span>.</p>
<dl class="simple myst">
<dt>Example (Binary multicast tree)</dt><dd><p>For instance, consider a binary multicast tree <span class="math notranslate nohighlight">\(T\)</span>. There is a packet sending from root <span class="math notranslate nohighlight">\(r\)</span> to all <span class="math notranslate nohighlight">\(N_\ell\)</span> leaves. If the packet is â€˜lostâ€™ at an internal vertex, then all its descendants cannot receive the packet. Let <span class="math notranslate nohighlight">\(X_i\)</span> be binary variables, which is <span class="math notranslate nohighlight">\(1\)</span> if leave <span class="math notranslate nohighlight">\(i\)</span> receives the packet and <span class="math notranslate nohighlight">\(0\)</span> otherwise. We may send the packet for <span class="math notranslate nohighlight">\(n\)</span> times, obtain <span class="math notranslate nohighlight">\(n\)</span> binary vectors, and use them to infer the tree structure. Here are some properties of the tree which can be exploited in designing methods of inference:</p>
</dd>
</dl>
<ul class="simple">
<li><p>For a subset <span class="math notranslate nohighlight">\(U \in R\)</span> of leaves, let <span class="math notranslate nohighlight">\(a(U)\)</span> be their closest common ancestor. If <span class="math notranslate nohighlight">\(X_{a(U)} = 0\)</span>, then <span class="math notranslate nohighlight">\(X_{u} = 0\)</span> for all leaves <span class="math notranslate nohighlight">\(u \in U\)</span>.</p></li>
<li><p>For an internal vertex <span class="math notranslate nohighlight">\(v\)</span>, let <span class="math notranslate nohighlight">\(C(v)\)</span> be its children. If <span class="math notranslate nohighlight">\(X_c = 1\)</span> for at least one <span class="math notranslate nohighlight">\(c \in C(v)\)</span>, then <span class="math notranslate nohighlight">\(X_v = 1\)</span>.</p></li>
</ul>
<dl class="simple myst">
<dt>Example (Binary Phylogenetic tree)</dt><dd><p>Another example is DNA sequence. If we group DNA bases <span class="math notranslate nohighlight">\(\left\{ A, G, C, T \right\}\)</span> in pairs <span class="math notranslate nohighlight">\(\left\{ A, G \right\}\)</span> and <span class="math notranslate nohighlight">\(\left\{ C, T \right\}\)</span>, i.e. by purines and pyrimidines respectively, and coded <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. We can then use an <span class="math notranslate nohighlight">\(N_\ell\)</span>-tuple of measurements to indicate whether each of <span class="math notranslate nohighlight">\(N_\ell\)</span> species being studied had a purine or pyrimidine at a given location in the genome. Repeat this for <span class="math notranslate nohighlight">\(n\)</span> different locations. Then we can form the notion of a tree-based evolutionary process generating sequence data at the leaves.</p>
</dd>
</dl>
<p>There are many methods for tomographic inference of tree topologies, which differ in</p>
<ul class="simple">
<li><p>how the data are utilized (all or part)</p></li>
<li><p>criteria for assessing the merit of a candidate tree <span class="math notranslate nohighlight">\(T \in \mathcal{T} _{N\ell}\)</span> in describing the data</p></li>
<li><p>the number in which the space <span class="math notranslate nohighlight">\(\mathcal{T} _{N_\ell}\)</span> is searched in attempting to find a tree(s) best meeting this criteria.</p>
<ul>
<li><p>Note that the space <span class="math notranslate nohighlight">\(\mathcal{T} _{N_\ell}\)</span> of (semi)labeled, rooted, binary trees is found to have <span class="math notranslate nohighlight">\(\left(2 N_{l}-3\right) !! \approx \left(N_{l}-1\right)^{N_{l}-1}\)</span> elements. Hence, exhaustive search is unrealistic and alternative approaches, based on greedy or randomized search algorithms, are utilized</p></li>
<li><p>maximum parsimony in phylogenetic inference, which seeks to construct a tree for data involving the minimum number of necessary evolutionary changes</p></li>
<li><p>branch-and-bound</p></li>
</ul>
</li>
</ul>
<p>Two popular classes of methods: hierarchical clustering-based, and likelihood-based. They both seek to exploit the supposition that, the closer two leaves are in the underlying tree, the more similar their observed characteristics (i.e., the measurements) will be.</p>
<ul class="simple">
<li><p>the observed rate of shared losses of packets should be fairly indicative of how close two leaf vertices</p></li>
<li><p>two biological species are presumed to share more of their genome if they split from a common ancestor later in evolutionary time.</p></li>
</ul>
</div>
<div class="section" id="hierarchical-clustering-based">
<h3>Hierarchical Clustering-based<a class="headerlink" href="#hierarchical-clustering-based" title="Permalink to this headline">Â¶</a></h3>
<p>Hierarchical <a class="reference internal" href="../34-clustering/00-clustering.html#clustering"><span class="std std-ref">clustering</span></a> method can be used to the <span class="math notranslate nohighlight">\(n \times N_\ell\)</span> data matrix. Note that</p>
<ul class="simple">
<li><p>traditional clustering methods aim to find cluster assignment, while here we want the hierarchical tree <span class="math notranslate nohighlight">\(\hat{T}\)</span>.</p></li>
<li><p>the (dis)similarity measure can be customized, e.g. (true) shared loss, or genetic distance.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title"> True, false and net shared loss in a multicast tree</p>
<p>There are two different types of shared loss between a pair of leaf vertices <span class="math notranslate nohighlight">\(\left\{ j, k \right\}\)</span> â€“ termed â€˜trueâ€™ and â€˜falseâ€™ shared loss by Ratnasamy and McCanne [323].</p>
<ul class="simple">
<li><p>The true shared losses are due to loss of packets on the path common to the vertices <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>while the false shared losses are due to loss on the parts of paths following after the closest common ancestor, <span class="math notranslate nohighlight">\(a(\left\{ j, k \right\})\)</span>.</p></li>
</ul>
<p>For example, in the above tree,</p>
<ul class="simple">
<li><p>true shared losses for the leaves 1 and 3 would be losses incurred on the path from <span class="math notranslate nohighlight">\(r\)</span> to the internal vertex <span class="math notranslate nohighlight">\(i_1\)</span>;</p></li>
<li><p>false shared losses would refer to cases where packets were lost separately on the two paths from <span class="math notranslate nohighlight">\(i_1\)</span> to the vertices 1 and 3, respectively.</p></li>
</ul>
<p>Since the net shared loss rate (i.e., the fraction of packets commonly lost to <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(k\)</span>) includes the contribution of <strong>both</strong> types of losses, it can be misleading to use this number as a similarity. Fortunately, it is possible to obtain information on the <strong>true</strong> loss rates from these net loss rates, through the use of a simple packet-loss model.</p>
</div>
<p>Here we introduce a Markov cascade process for multicast data. Consider the cascade process <span class="math notranslate nohighlight">\(\left\{ X_j \right\}_{j \in V_T}\)</span>, assume <span class="math notranslate nohighlight">\(X_r=1\)</span>. For each internal vertex <span class="math notranslate nohighlight">\(k\)</span>, if <span class="math notranslate nohighlight">\(X_k =0\)</span> then <span class="math notranslate nohighlight">\(X_j = 0\)</span> for all child <span class="math notranslate nohighlight">\(j \in C(k)\)</span>. If <span class="math notranslate nohighlight">\(X_k = 1\)</span>, then we define</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left(X_{j}=1 \mid X_{k}=1\right)=1-\mathbb{P}\left(X_{j}=0 \mid X_{k}=1\right)=\alpha_{j}
\]</div>
<p>and hence define</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A(k) = \prod_{j \succ k} \alpha_j\)</span> to be the probability that a packet is transmitted from <span class="math notranslate nohighlight">\(r\)</span> to <span class="math notranslate nohighlight">\(k\)</span>, where <span class="math notranslate nohighlight">\(j \succ k\)</span> indicates ancestral vertices <span class="math notranslate nohighlight">\(j\)</span> of <span class="math notranslate nohighlight">\(k\)</span> on the path from <span class="math notranslate nohighlight">\(r\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(1 - A(a(\left\{ j, k \right\}))\)</span> to be the true shared losses rate, i.e. shared loss due to loss of packets on the path common to the two leaf vertices <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(R(k)\)</span> to be the set of leaf vertices in <span class="math notranslate nohighlight">\(R\)</span> that are descendants of internal vertex <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma(k)=\mathbb{P}\left(\cup_{j \in R(k)}\left\{X_{j}=1\right\}\right)\)</span> be the probability that at least one of the <strong>leaves</strong> descended from <span class="math notranslate nohighlight">\(k\)</span> receive a packet. This can be estimated by <span class="math notranslate nohighlight">\(n\)</span> observations of <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>, using the relative frequency <span class="math notranslate nohighlight">\(\hat{\gamma}(k)=(1 / n) \sum_{i=1}^{n}\left[\prod_{j \in R(k)} x_{ij}\right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma(U)=\mathbb{P}\left(\cup_{k \in U} \cup_{j \in R(k)}\left\{X_{j}=1\right\}\right)\)</span> similarly, for an arbitrary set of vertices <span class="math notranslate nohighlight">\(U\)</span>. In particular, if <span class="math notranslate nohighlight">\(U = C(k)\)</span>, then <span class="math notranslate nohighlight">\(\gamma(U) = \gamma(k)\)</span>.</p></li>
</ul>
<p>Then, it is not difficult to show that for any <span class="math notranslate nohighlight">\(U \subseteq C(k)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
1- \frac{\gamma(U)}{A(k)}=\prod_{j \in U}\left[ 1- \frac{\gamma(j)}{A(k)} \right]
\]</div>
<p>Therefore, we can first estimate <span class="math notranslate nohighlight">\(\gamma(k)\)</span> by <span class="math notranslate nohighlight">\(\hat{\gamma}(k)\)</span>, then solve the above function to obtain <span class="math notranslate nohighlight">\(\hat{A}(k)\)</span> for all <span class="math notranslate nohighlight">\(k \in V_T\)</span>. To build a agglomerative clustering tree, we can use the true shared losses rate <span class="math notranslate nohighlight">\(1 - \hat{A}(a(\{j, k\}))\)</span> as similarity measure. See [SAND 128] for consistency of the resulting estimator <span class="math notranslate nohighlight">\(T\)</span> for recovering a binary multicast tree <span class="math notranslate nohighlight">\(T\)</span>, under the model assumptions.</p>
</div>
<div class="section" id="likelihood-based">
<h3>Likelihood-based<a class="headerlink" href="#likelihood-based" title="Permalink to this headline">Â¶</a></h3>
<p>We can specify a conditional density or PMF <span class="math notranslate nohighlight">\(f(\boldsymbol{x} \vert T)\)</span> for the <span class="math notranslate nohighlight">\(N_\ell\)</span>-length vector of random variables <span class="math notranslate nohighlight">\(\boldsymbol{x} = [X_1, \ldots, X_{N_\ell}]\)</span>, given a tree-topology <span class="math notranslate nohighlight">\(T\)</span>. If we assumed independence among the <span class="math notranslate nohighlight">\(n\)</span> observations, the likelihood has the form</p>
<div class="math notranslate nohighlight">
\[
L(T) = \prod_{i=1}^n f(\boldsymbol{x} _i \vert T)
\]</div>
<p>and the MLE tree is simple <span class="math notranslate nohighlight">\(\hat{T}_{ML} = \arg \max _{T \in \mathcal{T} _{N_\ell}} L(T)\)</span>. However, it is typically that there are parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> relating to the evolution of a tree <span class="math notranslate nohighlight">\(T\)</span>. Hence, the likelihood has an integrated form</p>
<div class="math notranslate nohighlight">
\[
L(T) = \prod_{i=1}^{n} \int f\left(\mathbf{x}_i \mid T, \theta\right) f(\theta \mid T) d \theta
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(\theta \vert T)\)</span> is an appropriately defined distribution on <span class="math notranslate nohighlight">\(\theta\)</span>, given <span class="math notranslate nohighlight">\(T\)</span>. Depending on the nature of their integrands, the integrals may or may not lend themselves well to computational evaluation.</p>
<dl class="simple myst">
<dt>Definition (Profile likelihood)</dt><dd><p>An alternative that can be more computationally tractable, or that can be used
when we cannot or do not wish to specify a distribution <span class="math notranslate nohighlight">\(f(\theta \vert T)\)</span>, is to define <span class="math notranslate nohighlight">\(\hat{T}\)</span> through maximization of a profile likelihood. Let</p>
<div class="math notranslate nohighlight">
\[
  L(T, \theta) =\prod_{i=1}^{n} f\left(\mathbf{x}_{i} \mid T, \theta\right)
  \]</div>
<p>Then define</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \hat{\theta}_{T}
  &amp;= \arg \max _{\theta} L(T, \theta)\\
  \widehat{T}_{PL}&amp;=\arg \max _{T \in \mathscr{T}_{N_\ell}} L\left(T, \hat{\theta}_{T}\right) \\
  \end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(L\left(T, \hat{\theta}_{T}\right)\)</span> is called the <strong>profile likelihood</strong>, and  <span class="math notranslate nohighlight">\(\widehat{T}_{PL}\)</span> is called the <strong>maximum profile likelihood estimator</strong>.</p>
</dd>
</dl>
<p>For instance, for the multicast three model, <span class="math notranslate nohighlight">\(\theta = \left\{ \alpha_j \right\}\)</span>, the profile likelihood is</p>
<div class="math notranslate nohighlight">
\[
L(T, \alpha)=\prod_{i=1}^{n} \prod_{j \in V_{T}} \eta_{j}^{(i)}
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>For detail about the chain rule factorization, see <a class="reference internal" href="../35-graphical-models/00-graphical-models.html#graphical-models"><span class="std std-ref">graphical models</span></a>. But how to we know <span class="math notranslate nohighlight">\(x_{p a(j)}^{(i)}\)</span>??</p>
</div>
<p>where by chain rule factorization,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\eta_{j}^{(i)}=\left\{\begin{array}{ll}
1, &amp; \text { if } j=r, \\
\mathbb{P} (x_j ^{(i)} \vert x_{pa(j)}^{(i)}, \alpha_j) = \left\{\begin{array}{ll}
\alpha_{j}^{x_{j}^{(i)}}\left(1-\alpha_{j}\right)^{1-x_{j}^{(i)}}, &amp; \text { if } x_{p a(j)}^{(i)}=1 \\
1, &amp; \text { if } x_{p a(j)}^{(i)}=0
\end{array}\right.
\end{array}\right.
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(pa(j)\)</span> denotes the parent of <span class="math notranslate nohighlight">\(j\)</span> in <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>The profile MLE is <span class="math notranslate nohighlight">\(\widehat{T}_{P L}=\arg \max _{T \in \mathcal{T}_{N_{l}}} L\left(T, \hat{\alpha}_{T}\right)\)</span>, where <span class="math notranslate nohighlight">\(\hat{\alpha}_{T}=\arg \max _{\alpha} L(T, \alpha)\)</span>. [SAND 128] prove its consistency under appropriate conditions.</p>
<div class="note dropdown admonition">
<p class="admonition-title"> Computation</p>
<p>Implementation of this estimator is non-trivial. For example, consider the estimation of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>, a parameter vector of potentially quite large dimension. An efficient, recursive estimation algorithm is proposed [SAND 71] using the relation</p>
<div class="math notranslate nohighlight">
\[
\alpha_j = \mathbb{P}\left(X_{j}=1 \mid X_{pa(j)}=1\right) = \frac{A(j)}{A(pa(j))}
\]</div>
<p>And recall that <span class="math notranslate nohighlight">\(A(k)\)</span> may be obtained from <span class="math notranslate nohighlight">\(\gamma(k)\)</span> and <span class="math notranslate nohighlight">\(\gamma(k)\)</span> can be estimated by frequencies. Although the resulting estimator <span class="math notranslate nohighlight">\(\tilde{\alpha}_k\)</span> is not strictly the maximum likelihood estimate, Caceres et al. [SAND 71] show that, when <span class="math notranslate nohighlight">\(\alpha_k \in (0, 1)\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>, we nevertheless have <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\alpha}} = \hat{\boldsymbol{\alpha}}\)</span> with high probability.</p>
<p>For the optimization of the profile likelihood <span class="math notranslate nohighlight">\(L\left(T, \hat{\alpha}_{T}\right)\)</span> for <span class="math notranslate nohighlight">\(T \in \mathcal{T} _{N_\ell}\)</span>, if <span class="math notranslate nohighlight">\(N_\ell\)</span> is small (&lt;5) we can use simulation for exhaustive search of <span class="math notranslate nohighlight">\(\mathcal{T} _{N_\ell}\)</span>, else we can use greedy techniques or MCMC-based techniques.</p>
</div>
<p>In the phylogenetic tree example, we assume independence between measurements <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span> (e.g. between locations in the measured DNA sequence) and use Markov-like cascade model to capture the effects of evolution through time. By coding purines and pyrimidines as 0 and 1 respectively, we use a symmetric model of change between the two states of zero and one, in traversing from one end of a branch to the other, according to probability</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left(X_{j}=1 \mid X_{k}=0\right)=\frac{1}{2}\left(1-e^{-2 w_{j}}\right)
\]</div>
<p>parameterized by <span class="math notranslate nohighlight">\(w_j\)</span>, the length of brach leading from <span class="math notranslate nohighlight">\(k\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, where <span class="math notranslate nohighlight">\(j \in C(k)\)</span>.</p>
<p>The likelihood corresponding to this model is</p>
<div class="math notranslate nohighlight">
\[
L(T, \mathbf{w})=\prod_{i=1}^{n} \sum_{\left\{x_{j}^{(i)}\right\}_{j \in V_{T} \backslash R}} \prod_{j \in V_{T}} \eta_{j}^{(i)}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\eta_{j}^{(i)}=\left\{\begin{array}{ll}
\frac{1}{2}\left(1-e^{-2 w_{j}}\right), &amp; \text { if } x_{j}^{(i)} \neq x_{p a(j)}^{(i)} \\
\frac{1}{2}\left(1+e^{-2 w_{j}}\right), &amp; \text { if } x_{j}^{(i)}=x_{p a(j)}^{(i)}
\end{array}\right.
\end{split}\]</div>
<p>Note that the summation (i.e. marginalization) is over all possible ways that the states zero or one can be assigned to the internal vertices of the tree <span class="math notranslate nohighlight">\(T\)</span>, and is absent in the case of multicast data because of the hereditary constraints on the process <span class="math notranslate nohighlight">\(\left\{ X_j \right\}j \in V_T\)</span> (but still have multiple assignments??).</p>
<div class="note admonition">
<p class="admonition-title"> Computation</p>
<ul class="simple">
<li><p>Likelihood: in general, the presence of such product-sum combinations can be problematic from a computational perspective. However, in this particular case, the likelihood can be calculated efficiently using a dynamic programming algorithm â€“ the so-called <strong>pruning algorithm</strong> â€“ proposed by Felsenstein [SAND 142]. Working recursively from the leaves towards the root, components of the likelihood are computed on sub-trees and combined in a clever fashion
to yield the likelihoods on larger sub-trees containing them, until the likelihood for the entire tree is obtained.</p></li>
<li><p>Optimization in <span class="math notranslate nohighlight">\((\widehat{T}, \hat{\mathbf{w}})\)</span> is NP hard. In practice, a profile maximum likelihood method typically is used (??). See [SAND 143]. MCMC can also be pursued.</p></li>
</ul>
</div>
</div>
<div class="section" id="summarizing-collections-of-trees">
<h3>Summarizing Collections of Trees<a class="headerlink" href="#summarizing-collections-of-trees" title="Permalink to this headline">Â¶</a></h3>
<p>All the above inference method output a single tree <span class="math notranslate nohighlight">\(\hat{T}\)</span>, like a point estimate. Can we output a collection of trees, in the spirit of an interval estimate? In practice, such collections arise in various ways, such as</p>
<ul class="simple">
<li><p>listing a number of trees of nearly maximum likelihood, rather than simply a single maximum-likelihood tree, or</p></li>
<li><p>from bootstrap re-sampling in an effort to assess the variability in an inferred tree, output <span class="math notranslate nohighlight">\(\{\widehat{T}^{(b)}\}_{b=1}^{B}\)</span>, or</p></li>
<li><p>from MCMC sampling of an appropriate posterior distribution on <span class="math notranslate nohighlight">\(\mathcal{T} _{N_\ell}\)</span></p></li>
</ul>
<p>Given such a collection of trees, how can we usefully summarize the information
therein? How they are similar or different?</p>
<ul class="simple">
<li><p>For similarity, we can use <strong>consensus tree</strong>. A consensus tree is a single tree that aims to summarize the information in a collection of trees in a â€˜representativeâ€™ manner. There are many methods to define such trees,</p>
<ul>
<li><p>Margush and McMorris [SAND 268] defined <span class="math notranslate nohighlight">\(M_\ell\)</span>-trees indexed by <span class="math notranslate nohighlight">\(\ell \in [0.5, 1]\)</span>, which contains all groups of leaves <span class="math notranslate nohighlight">\(U \subseteq R\)</span> that occur in more than a fraction <span class="math notranslate nohighlight">\(\ell\)</span> of the trees in a collection.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\ell=1\)</span>, it is called strict consensus tree</p></li>
<li><p><span class="math notranslate nohighlight">\(\ell=0.5\)</span>, it is called majority-rule consensus tree</p></li>
</ul>
</li>
<li><p>additional information can be added to a consensus tree in the form of branch weights</p></li>
</ul>
</li>
<li><p>For differences, there are various notions of distance between pairs of trees</p>
<ul>
<li><p><strong>symmetric difference</strong> works by counting branches.</p></li>
<li><p><strong>nearest-neighbor interchange</strong> (NNI) counts the number of swaps of adjacent branches that must be made to transform one tree into the other (computationally daunting)</p></li>
<li><p>both are metrics</p></li>
</ul>
</li>
<li><p>Relationships: if we define a <strong>median tree</strong> in a collection to be a tree <span class="math notranslate nohighlight">\(T\)</span> whose total distance â€“ based on the symmetric difference â€“ to all other trees is a minimum, then this tree is equivalent to the majority-rule tree, when the number of trees <span class="math notranslate nohighlight">\(t\)</span> in the collection is odd [SAND 21].</p></li>
</ul>
</div>
</div>
<div class="section" id="more">
<h2>More<a class="headerlink" href="#more" title="Permalink to this headline">Â¶</a></h2>
<p>More examples</p>
<ul class="simple">
<li><p>link prediction / association networks</p>
<ul>
<li><p>inferring networks from so-called co-occurrence data, e.g. telecommunications and genetics [SAND 318, 319]</p></li>
<li><p>protein complexes [SAND 343]</p></li>
<li><p>inferring networks with links defined according to the dynamic interactions of elements in a system (e.g., such as of information passing between neurons firing in sequence). [SAND 355]</p></li>
</ul>
</li>
<li><p>tomographic inference</p>
<ul>
<li><p>infer coalescent trees, which have to do with the evolution of genes within a population (i.e., below the level of species). See Felsenstein [141, Ch. 26].</p></li>
</ul>
</li>
</ul>
<p>More measurements</p>
<ul class="simple">
<li><p>association networks</p>
<ul>
<li><p>Spearman rank correlation and partial correlation might be used as robust alternatives to Pearson correlation [350.sec.3.9], [110]</p></li>
<li><p>Or a measure capable of summarizing nonlinear association, such as mutual information [140]</p></li>
</ul>
</li>
<li><p>computer networks</p>
<ul>
<li><p>mean delay, see the sandwich probing in case study 7.4.5.</p></li>
</ul>
</li>
</ul>
<p>Open problems</p>
<ul class="simple">
<li><p>basic questions of consistency, consensus, robustness, beyond trees?</p></li>
<li><p>how to best characterize the merit of partially accurate estimates <span class="math notranslate nohighlight">\(\hat{G}\)</span>? e.g. focusing on the accuracy with which small subgraphs (e.g., network motifs) or paths are recovered.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./38-ml-for-graph-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="21-modeling.html" title="previous page">Modeling</a>
    <a class='right-next' id="next-link" href="41-processes.html" title="next page">Processes on Graphs</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>