
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Graphical Neural Networks &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Embeddings" href="51-embeddings.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/12-bisect.html">
     Bisection Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-model-selection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/41-penalized-regression.html">
     Penalized Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/12-pca-variants.html">
     PCA Variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/61-indep-component-analysis.html">
     Independent Component Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/81-density-fitting.html">
     Application to Density Fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Graphical Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/38-ml-for-graph-data/53-graph-neural-networks.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F38-ml-for-graph-data/53-graph-neural-networks.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structure">
   Structure
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-graphs">
     Computation Graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neurons">
     Neurons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised">
     Supervised
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#node-level">
       Node-level
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#edge-level">
       Edge-level
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-level">
       Graph-level
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised">
     Unsupervised
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch">
     Batch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-splitting">
     Dataset Splitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Node-level
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#link-level">
       Link-level
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pros">
   Pros
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variants">
   Variants
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graphsage">
     GraphSAGE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-operations">
     Matrix Operations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-attention-networks">
     Graph Attention Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gin">
     GIN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expressiveness">
       Expressiveness
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       GIN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#r-t-wl-kernel">
       R.t. WL Kernel
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#improvement">
     Improvement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-design">
   Layer Design
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modules">
     Modules
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#issue-with-deep-gnn">
     Issue with Deep GNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solution">
     Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-manipulation">
   Graph Manipulation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-augmentation">
     Feature Augmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#virtual-nodes-edges">
     Virtual Nodes/Edges
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#virtual-edges">
       Virtual Edges
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#virtual-nodes">
       Virtual Nodes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neighborhood-sampling">
     Neighborhood Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-generative-models">
   Graph Generative Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graphrnn">
     GraphRNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tractability">
     Tractability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-of-graphs">
     Similarity of Graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Variants
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#limitations">
   Limitations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#position-aware-gnns">
     Position-aware GNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identity-aware-gnn">
     Identity-aware GNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coding">
   Coding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-pyg">
     Using PyG
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepsnap">
     DeepSNAP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples">
     Examples
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="graphical-neural-networks">
<h1>Graphical Neural Networks<a class="headerlink" href="#graphical-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Recall the limitations of shallow embedding methods:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{O} (N_v)\)</span> parameters are needed</p>
<ul>
<li><p>no sharing of parameters between nodes</p></li>
</ul>
</li>
<li><p>transductive, no out-of-sample prediction.</p></li>
<li><p>do not incorporate node features</p></li>
</ul>
<p>Now we introduce deep graph encoders for node embeddings, where <span class="math notranslate nohighlight">\(\operatorname{ENC}(v)\)</span> are deep neural networks. Note that the <span class="math notranslate nohighlight">\(\operatorname{similarity}(u,v)\)</span> measures can also be incorporated into GNN. The output can be node embeddings, sub graph embeddings, or entire graph embeddings, to be used for downstream tasks.</p>
<div class="figure align-default" id="gnn-pipeline">
<a class="reference internal image-reference" href="../_images/gnn-pipeline.png"><img alt="" src="../_images/gnn-pipeline.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 234 </span><span class="caption-text">Pipeline of graphical neural networks</span><a class="headerlink" href="#gnn-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(G\)</span> be an undirected graph of order <span class="math notranslate nohighlight">\(N_v\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> be the <span class="math notranslate nohighlight">\(N_v \times N_v\)</span> adjacency matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> be the <span class="math notranslate nohighlight">\(N_v \times d\)</span> node features</p></li>
</ul>
<p>A naive idea to represent the graph with features is to use the <span class="math notranslate nohighlight">\(N_v \times (N_v + d)\)</span> matrix <span class="math notranslate nohighlight">\([\boldsymbol{A} , \boldsymbol{X}]\)</span>, and then feed into NN. However, there are some issues</p>
<ul class="simple">
<li><p>for node-level task</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_v + d\)</span> parameters &gt; <span class="math notranslate nohighlight">\(N_v\)</span> examples (nodes), easy overfit</p></li>
</ul>
</li>
<li><p>for graph-level task:</p>
<ul>
<li><p>not applicable to graphs of different sizes</p></li>
<li><p>sensitive to node ordering</p></li>
</ul>
</li>
</ul>
<div class="section" id="structure">
<h2>Structure<a class="headerlink" href="#structure" title="Permalink to this headline">¶</a></h2>
<p>To solve the above issues, GNN borrows idea of CNN filters (hence GNN is also called graphical convolutional neural networks GCN).</p>
<div class="section" id="computation-graphs">
<h3>Computation Graphs<a class="headerlink" href="#computation-graphs" title="Permalink to this headline">¶</a></h3>
<p>In CNN, a convolutional operator can be viewed as an operator over lattices. Can we generalize it to subgraphs? How to define sliding windows?</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Bipartite graph can be ‘projected’ to obtain ‘neighbors’. Useful in recommender systems.</p>
</div>
<p>Consider a 3 by 3 filter in CNN. We aggregate information in 9 cells and and then output one cell. In a graph, we can aggregate information in a neighborhood <span class="math notranslate nohighlight">\(\mathscr{N} (v)\)</span> and output one ‘node’. For instance, given messages <span class="math notranslate nohighlight">\(h_j\)</span> from neighbor <span class="math notranslate nohighlight">\(j\)</span> with weight <span class="math notranslate nohighlight">\(w_j\)</span>, we can output new message <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{N} (v)} w_j h_j\)</span>. In GNN, every node defines a <strong>computation graph</strong> based on its neighborhood</p>
<div class="figure align-default" id="gnn-cnn-filter">
<a class="reference internal image-reference" href="../_images/gnn-cnn-filter.png"><img alt="" src="../_images/gnn-cnn-filter.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 235 </span><span class="caption-text">CNN filter and GNN</span><a class="headerlink" href="#gnn-cnn-filter" title="Permalink to this image">¶</a></p>
</div>
<p>It also borrows idea from message passing. The information in GNN propagate through neighbors like those in belief networks. The number of hops determines the number of layers of GNN. For a GNN designed to find node embeddings, the information at each layer is node embeddings.</p>
<div class="figure align-default" id="gnn-aggregate">
<a class="reference internal image-reference" href="../_images/gnn-aggregate.png"><img alt="" src="../_images/gnn-aggregate.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 236 </span><span class="caption-text">Aggregation of neighbors information</span><a class="headerlink" href="#gnn-aggregate" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="neurons">
<h3>Neurons<a class="headerlink" href="#neurons" title="Permalink to this headline">¶</a></h3>
<p>The block in the previous computation graph represents an aggregation-and-transform step, where we use neighbors’ embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _u ^{(\ell)}\)</span>’s and self embedding <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> to obtain <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}\)</span>. This step work like a neuron. Different GNN models differ in this step.</p>
<div class="note admonition">
<p class="admonition-title"> Note</p>
<p>One important property of aggregation is that the aggregation operator should be permutation invariant, since neighbors have no orders.</p>
</div>
<p>A basic approach of aggregation-and-transform is to average last layer information, take linear transformation, and then non-linear activation. Consider an <span class="math notranslate nohighlight">\(L\)</span>-layer GNN to obtain <span class="math notranslate nohighlight">\(k\)</span>-dimensional embeddings</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(0)} = \boldsymbol{x} _v\)</span>: initial <span class="math notranslate nohighlight">\(0\)</span>-th layer embeddings, equal to node features</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell +1)} = \sigma \left( \boldsymbol{W} _\ell \frac{1}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)} + \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)} \right)\)</span> for <span class="math notranslate nohighlight">\(\ell = \left\{ 0, \ldots, L-1 \right\}\)</span></p>
<ul>
<li><p>average last layer (its neighbors’) hidden embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _u ^{(\ell)}\)</span>, linearly transformed by <span class="math notranslate nohighlight">\(k \times k\)</span> weight matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}_ \ell\)</span></p></li>
<li><p>also take as input its hidden embedding <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> at last layer (last updated embedding?? stored in <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span>??), linearly transformed by <span class="math notranslate nohighlight">\(k\times k\)</span> weight matrix <span class="math notranslate nohighlight">\(\boldsymbol{B}_ \ell\)</span></p></li>
<li><p>finally activated by <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{z} _v = \boldsymbol{h} _v ^{(L)}\)</span> final output embeddings.</p></li>
</ul>
<p>The weight parameters in layer <span class="math notranslate nohighlight">\(\ell\)</span> are <span class="math notranslate nohighlight">\(\boldsymbol{W} _\ell\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{B} _\ell\)</span>, which are shared across neurons in layer <span class="math notranslate nohighlight">\(\ell\)</span>. Hence, the number of model parameters is sub-linear in <span class="math notranslate nohighlight">\(N_v\)</span>.</p>
<p>If we write the hidden embeddings <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell)}\)</span> as rows in a matrix <span class="math notranslate nohighlight">\(\boldsymbol{H}^{(\ell)}\)</span>, then the aggregation step can be written as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}=\sigma\left(\boldsymbol{D} ^{-1} \boldsymbol{A} \boldsymbol{H} ^{(\ell)} \boldsymbol{W} _{\ell}^{\top}+\boldsymbol{H} ^{(\ell)} \boldsymbol{B} _{\ell}^{\top}\right)
\]</div>
<p>where we update the <span class="math notranslate nohighlight">\(v\)</span>-th row at a neuron for node <span class="math notranslate nohighlight">\(v\)</span>, or several rows corresponding to neurons in a layer.</p>
<p>In practice, <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is sparse, hence some sparse matrix multiplication can be used. But not all GNNs can be expressed in matrix form, when
aggregation function is complex.</p>
</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="supervised">
<h3>Supervised<a class="headerlink" href="#supervised" title="Permalink to this headline">¶</a></h3>
<p>After build GNN layers, to train it, we compute loss and do SGD. The pipeline is</p>
<div class="figure align-default" id="gnn-training-pipeline">
<a class="reference internal image-reference" href="../_images/gnn-training-pipeline.png"><img alt="" src="../_images/gnn-training-pipeline.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 237 </span><span class="caption-text">GNN Training Pipeline</span><a class="headerlink" href="#gnn-training-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>The prediction heads depend on whether the task is at node-level, edge-level, or graph-level.</p>
<div class="section" id="node-level">
<h4>Node-level<a class="headerlink" href="#node-level" title="Permalink to this headline">¶</a></h4>
<p>If we have some node label, we can minimize the loss</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_v, f(\boldsymbol{z} _v))
\]</div>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y\)</span> is a real number, then <span class="math notranslate nohighlight">\(f\)</span> maps embedding from <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is L2 loss</p></li>
<li><p>if <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(C\)</span>-way categorical, <span class="math notranslate nohighlight">\(f\)</span> maps embedding from <span class="math notranslate nohighlight">\(\mathbb{R} ^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R} ^C\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is cross-entropy.</p></li>
</ul>
</div>
<div class="section" id="edge-level">
<h4>Edge-level<a class="headerlink" href="#edge-level" title="Permalink to this headline">¶</a></h4>
<p>If we have some edge label <span class="math notranslate nohighlight">\(y_{uv}\)</span>, then the loss is</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_{uv}, f(\boldsymbol{z} _u, \boldsymbol{z} _v))
\]</div>
<p>To aggregate the two embedding vectors, <span class="math notranslate nohighlight">\(f\)</span> can be</p>
<ul class="simple">
<li><p>concatenation and then linear transformation</p></li>
<li><p>inner product <span class="math notranslate nohighlight">\(\boldsymbol{z} _u ^{\top} \boldsymbol{z} _v\)</span> (for 1-way prediction)</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{uv} ^{(r)} = \boldsymbol{z} _u ^{\top} \boldsymbol{W} ^{(r)} \boldsymbol{z} _v\)</span> then <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}} _{uv} = [y_{uv} ^{(1)}, \ldots , y_{uv} ^{(R)}]\)</span>. The weights <span class="math notranslate nohighlight">\(\boldsymbol{W} ^{(r)}\)</span> are trainable.</p></li>
</ul>
</div>
<div class="section" id="graph-level">
<h4>Graph-level<a class="headerlink" href="#graph-level" title="Permalink to this headline">¶</a></h4>
<p>For graph-level task, we make prediction using all the node embeddings in our graph. The loss is</p>
<div class="math notranslate nohighlight">
\[
\min\ \mathcal{L} (y_{G}, f \left( \left\{ \boldsymbol{z} _v, \forall v \in V \right\} \right))
\]</div>
<p>where <span class="math notranslate nohighlight">\(f \left( \left\{ \boldsymbol{z} _v, \forall v \in V \right\} \right)\)</span> is similar to <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> in a GNN layer</p>
<ul>
<li><p>global pooling: <span class="math notranslate nohighlight">\(f = \operatorname{max}, \operatorname{mean}, \operatorname{sum}\)</span></p></li>
<li><p>hierarchical pooling: global pooling may lose information. We can apply pooling to some subgraphs to obtain subgraph embeddings, and then pool these subgraph embeddings.</p>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/graph-hier-emb.png"><img alt="" src="../_images/graph-hier-emb.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 238 </span><span class="caption-text">Hierarchical Pooling</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>To decide subgraph assignment, standard community detection or graph partition algorithms can be used. The assignment can also be learned: we build two GNNs,</p>
<ul class="simple">
<li><p>GNN A: one computes node embeddings</p></li>
<li><p>GNN B: one computes the subgraph assignment that a node belongs to</p></li>
</ul>
<p>GNNs A and B at each level can be executed in parallel. See Ying et al. Hierarchical Graph Representation Learning with Differentiable Pooling, NeurIPS 2018.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="unsupervised">
<h3>Unsupervised<a class="headerlink" href="#unsupervised" title="Permalink to this headline">¶</a></h3>
<p>In unsupervised setting, we use information from the graph itself as labels.</p>
<ul class="simple">
<li><p>Node-level:</p>
<ul>
<li><p>some node statistics, e.g. clustering coefficient, PageRank.</p></li>
<li><p>if we know some nodes form a cluster, then we can treat cluster assignment as node label.</p></li>
</ul>
</li>
<li><p>Edge-level: <span class="math notranslate nohighlight">\(y_{u, v} = 1\)</span> when node <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are similar. Similarity can be defined by edges, random walks neighborhood, node proximity, etc.</p></li>
<li><p>Graph-level: some graph statistic, e.g. predict if two graphs are isomorphic, have similar graphlets, etc.</p></li>
</ul>
</div>
<div class="section" id="batch">
<h3>Batch<a class="headerlink" href="#batch" title="Permalink to this headline">¶</a></h3>
<p>We also use batch gradient descent. In each iteration, we train on a set of nodes, i.e., a batch of compute graphs.</p>
</div>
<div class="section" id="dataset-splitting">
<h3>Dataset Splitting<a class="headerlink" href="#dataset-splitting" title="Permalink to this headline">¶</a></h3>
<p>Given a graph input with features and labels <span class="math notranslate nohighlight">\(\boldsymbol{G} = (V, E, \boldsymbol{X} , \boldsymbol{y})\)</span>, how do we split it into train / validation / test set?</p>
<ul class="simple">
<li><p>Training set: used for optimizing GNN parameters</p></li>
<li><p>Validation set: develop model/hyperparameters</p></li>
<li><p>Test set: held out until we report final performance</p></li>
</ul>
<p>The speciality of graph is that nodes as observations are connected by edges, they are not independent due to message passing. Sometimes we cannot guarantee that the test set will really be held out. Some data leakage issue may exist.</p>
<div class="note admonition">
<p class="admonition-title"> Random split</p>
<p>In contrast to this fixed split, another way is random split: we randomly split the data set to train / validation / test. We report average performance over different random seeds.</p>
</div>
<p>The ways of splitting depends on tasks.</p>
<div class="section" id="id1">
<h4>Node-level<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>“transductive” means the entire graph can be observed in all dataset splits</p>
</div>
<p>Transductive setting: split nodes labels into <span class="math notranslate nohighlight">\(\boldsymbol{y} _{\text{train} }, \boldsymbol{y} _{\text{valid} }, \boldsymbol{y} _{\text{test} }\)</span>.</p>
<ul class="simple">
<li><p>training: use information <span class="math notranslate nohighlight">\((V, E, \boldsymbol{X} , \boldsymbol{y} _{\text{train} })\)</span> to train a GNN. Of course, the computation graphs are for those labeled nodes.</p></li>
<li><p>validation: evaluate trained GNN on validation nodes with labels <span class="math notranslate nohighlight">\(\boldsymbol{y} _{\text{valid} }\)</span>, using all remaining information <span class="math notranslate nohighlight">\((V, E, \boldsymbol{X})\)</span></p></li>
<li><p>test: evaluate developed GNN on test nodes with labels <span class="math notranslate nohighlight">\(\boldsymbol{y} _{\text{test} }\)</span>, using all remaining information <span class="math notranslate nohighlight">\((V, E, \boldsymbol{X})\)</span></p></li>
<li><p>also applicable to edge-level tasks, not applicable to graph-level tasks.</p></li>
</ul>
<p>Inductive setting: partition the graph <span class="math notranslate nohighlight">\(G\)</span> into training subgraph <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span>, validation subgraph <span class="math notranslate nohighlight">\(G_{\text{valid} }\)</span>, and test subgraph <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span>, <strong>remove</strong> across-subgraph edges. Then the three subgraphs are independent.</p>
<ul class="simple">
<li><p>training: use <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span> to train a GNN</p></li>
<li><p>valid: evaluate trained GNN on <span class="math notranslate nohighlight">\(G_{\text{valid} }\)</span></p></li>
<li><p>test: evaluate developed GNN on <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span></p></li>
<li><p>pros: applicable to node / edge / graph tasks</p></li>
<li><p>cons: not applicable to small graphs</p></li>
</ul>
<div class="figure align-default" id="gnn-train-test-split">
<a class="reference internal image-reference" href="../_images/gnn-train-test-split.png"><img alt="" src="../_images/gnn-train-test-split.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 239 </span><span class="caption-text">Splitting graph, transductive (left) and inductive (right)</span><a class="headerlink" href="#gnn-train-test-split" title="Permalink to this image">¶</a></p>
</div>
<p>In the first layer only features not labels are fed into GNN.</p>
</div>
<div class="section" id="link-level">
<h4>Link-level<a class="headerlink" href="#link-level" title="Permalink to this headline">¶</a></h4>
<p>It is worth noting that a good link prediction model need to predict both existence of an edge <span class="math notranslate nohighlight">\((a_{ij}=1)\)</span> and non-existence of an edge <span class="math notranslate nohighlight">\((a_{ij}=0)\)</span>, given two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Hence, both positive edge and negative edge (non-existence) should be treated as labels. But often the graph is sparse, i.e. #negative edges &gt;&gt; #positive edges. Hence, negative edges are <strong>sampled</strong> in training.</p>
<p>In DeepSNAP, the <code class="docutils literal notranslate"><span class="pre">edge_label</span></code> and <code class="docutils literal notranslate"><span class="pre">edge_label_index</span></code> have included the negative edges (default number of negative edges is same with the number of positive edges).</p>
<p>Inductive setting</p>
<ol class="simple">
<li><p>Partition edges <span class="math notranslate nohighlight">\(E\)</span> into</p>
<ul class="simple">
<li><p>message edges <span class="math notranslate nohighlight">\(E_m\)</span>, used for GNN message passing, and</p></li>
<li><p>supervision edges <span class="math notranslate nohighlight">\(E_s\)</span>, use for computing objective, not fed into GNN</p></li>
</ul>
</li>
<li><p>Partition graph into training subgraph <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span>, valid subgraph <span class="math notranslate nohighlight">\(G_{\text{valid} }\)</span>, and test subgraph <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span>, remove across-subgraph edges. Each subgraph will have some <span class="math notranslate nohighlight">\(E_m\)</span> and some <span class="math notranslate nohighlight">\(E_s\)</span>.</p>
<ul class="simple">
<li><p>Training on <span class="math notranslate nohighlight">\(G_{\text{train} }\)</span></p></li>
<li><p>Develop on <span class="math notranslate nohighlight">\(G_{\text{valid} }\)</span></p></li>
<li><p>Test on <span class="math notranslate nohighlight">\(G_{\text{test} }\)</span></p></li>
</ul>
</li>
</ol>
<div class="figure align-default" id="gnn-split-edge-ind">
<a class="reference internal image-reference" href="../_images/gnn-split-edge-ind.png"><img alt="" src="../_images/gnn-split-edge-ind.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 240 </span><span class="caption-text">Inductive splitting for link prediction</span><a class="headerlink" href="#gnn-split-edge-ind" title="Permalink to this image">¶</a></p>
</div>
<p>Transductive setting (common setting, <code class="docutils literal notranslate"><span class="pre">edge_train_mode</span> <span class="pre">=</span> <span class="pre">&quot;disjoint&quot;</span></code> in DeepSNAP)</p>
<ol class="simple">
<li><p>Partition the edges into</p>
<ul class="simple">
<li><p>training message edges <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span></p></li>
<li><p>training supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span></p></li>
<li><p>validation edges <span class="math notranslate nohighlight">\(E_{\text{valid}}\)</span></p></li>
<li><p>test edges <span class="math notranslate nohighlight">\(E_{\text{test}}\)</span></p></li>
</ul>
</li>
<li><p>Training: use training message edges <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span> to predict training supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span></p></li>
<li><p>Validation: Use <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span>, <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span> to predict <span class="math notranslate nohighlight">\(E_{\text{valid}}\)</span></p></li>
<li><p>Test: Use <span class="math notranslate nohighlight">\(E_{\text{train, m}}\)</span>, <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span>, and <span class="math notranslate nohighlight">\(E_{\text{valid}}\)</span> to predict <span class="math notranslate nohighlight">\(E_{\text{test}}\)</span></p></li>
</ol>
<p>Another transductive setting is <code class="docutils literal notranslate"><span class="pre">edge_train_mode</span> <span class="pre">=</span> <span class="pre">&quot;all&quot;</span></code>.</p>
<p>After training, supervision edges are <strong>known</strong> to GNN. Therefore, an ideal model should use supervision edges <span class="math notranslate nohighlight">\(E_{\text{train, s}}\)</span> in message passing at test time.</p>
<div class="figure align-default" id="gnn-split-edge-tran">
<a class="reference internal image-reference" href="../_images/gnn-split-edge-tran.png"><img alt="" src="../_images/gnn-split-edge-tran.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 241 </span><span class="caption-text">Transductive splitting for link prediction</span><a class="headerlink" href="#gnn-split-edge-tran" title="Permalink to this image">¶</a></p>
</div>
<p>Another way of train-test split for link-prediction in general ML models:</p>
<ul class="simple">
<li><p>Assume the graph have all edges labeled (no unknown edge).</p></li>
<li><p>Partition nodes into training set <span class="math notranslate nohighlight">\(V_{train}\)</span> and test set <span class="math notranslate nohighlight">\(V_{test}\)</span>, then training set of edges <span class="math notranslate nohighlight">\(E_{train}\)</span> include the edges in the induced subgraph of <span class="math notranslate nohighlight">\(V_{train}\)</span>, while test set of edges include the edges in the induced subgraph of <span class="math notranslate nohighlight">\(V_{test}\)</span>, <strong>as well as</strong> across-subgraph edges.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(E_{train} = E(V_{train})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(E_{test} = E(V_{test}) \cup E(V_{train}, V_{test})\)</span></p></li>
</ul>
</li>
<li><p>All edge sets above include both positive edges and negative edges.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="pros">
<h2>Pros<a class="headerlink" href="#pros" title="Permalink to this headline">¶</a></h2>
<p>Inductive capability</p>
<ul class="simple">
<li><p>new graph: after training GNN on one graph, we can generalize it to an unseen graph. For instance, train on protein interaction graph from model organism A and generate embeddings on newly collected data about organism B.</p></li>
<li><p>new nodes: if an unseen node is added to the graph, we can directly run forward propagation to obtain its embedding.</p></li>
</ul>
</div>
<div class="section" id="variants">
<h2>Variants<a class="headerlink" href="#variants" title="Permalink to this headline">¶</a></h2>
<p>As said, different GNN models mainly differ in the aggregation-and-transform step. Let’s write the aggregation step as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{AGG} \left( \left\{ \boldsymbol{h} _u ^{(\ell)}, \forall u \in \mathscr{N} (v)  \right\} \right)
\]</div>
<p>In basic GNN, the aggregation function is just average. And the update function is</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{h} _v ^{(\ell +1)} = \sigma \left( \boldsymbol{W} _\ell \frac{u, v}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)} + \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)} \right)\]</div>
<p>This is called graph convolutional networks [Kipf and Welling ICLR 2017].</p>
<p>There are many variants and extensions to this update function. Before aggregation, there can be some transformation of the neighbor embeddings. The aggregation-and-transform step then becomes transform-aggregation-transform.</p>
<div class="section" id="graphsage">
<h3>GraphSAGE<a class="headerlink" href="#graphsage" title="Permalink to this headline">¶</a></h3>
<p>[Hamilton et al., NIPS 2017]</p>
<p>In GraphSAGE, the update function is more general,</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{h} _v ^{(\ell + 1)} = \sigma \left( \left[ \boldsymbol{W} _\ell \operatorname{AGG} \left( \left\{ \boldsymbol{h} _u ^{(\ell)}, \forall u \in \mathscr{N} (v)  \right\} \right), \boldsymbol{B} _\ell \boldsymbol{h} _v ^{(\ell)}\right] \right)\]</div>
<p>where we concatenate two vectors instead of summation, and <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> is a flexible aggregation function. L2 normalization of <span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}\)</span> to a unit length embedding vector can also be applied. In some cases (not always), normalization of embedding results in performance improvement</p>
<p>AGG can be</p>
<ul>
<li><p>Mean</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{AGG} = \frac{u, v}{d_v}\sum_{u \in \mathscr{N} (v)} \boldsymbol{h} _u ^ {(\ell)}
  \]</div>
</li>
<li><p>Pool: Transform neighbor vectors and apply symmetric vector function <span class="math notranslate nohighlight">\(\gamma\)</span>, e.g. mean, max</p>
<div class="math notranslate nohighlight">
\[\operatorname{AGG} = \gamma \left( \left\{ \operatorname{MLP} (\boldsymbol{h} _u ^{(\ell)} ), \forall u \in \mathscr{N} (v)   \right\} \right)\]</div>
</li>
<li><p>LSTM: apply LSTM, but to make it permutation invariant, we reshuffle the neighbors (some random order)</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{AGG}  = \operatorname{LSTM} \left( \left[ \boldsymbol{h} _u ^{(\ell)} , \forall u \in \pi (\mathscr{N} (v)) \right] \right)
  \]</div>
</li>
</ul>
</div>
<div class="section" id="matrix-operations">
<h3>Matrix Operations<a class="headerlink" href="#matrix-operations" title="Permalink to this headline">¶</a></h3>
<p>If we use mean, then the aggregation step (ignore <span class="math notranslate nohighlight">\(\boldsymbol{B}_\ell\)</span>) can be written as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}  = \boldsymbol{D} ^{-1} \boldsymbol{A} \boldsymbol{H}  ^{(\ell)}
\]</div>
<p>A variant [Kipf+ 2017]</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} ^{(\ell+1)}  = \boldsymbol{D} ^{-1/2} \boldsymbol{A} \boldsymbol{D} ^{-1/2} \boldsymbol{H}  ^{(\ell)}
\]</div>
<p>!!Laplacian</p>
</div>
<div class="section" id="graph-attention-networks">
<h3>Graph Attention Networks<a class="headerlink" href="#graph-attention-networks" title="Permalink to this headline">¶</a></h3>
<p>If AGG is mean, then the message from neighbors are of equal importance with the same weight <span class="math notranslate nohighlight">\(\frac{u, v}{d_v}\)</span>. Can we specify some unequal weight/attention <span class="math notranslate nohighlight">\(\alpha_{vu}\)</span>?</p>
<div class="math notranslate nohighlight">
\[
\operatorname{AGG} = \sum_{u \in \mathscr{N} (v)} \alpha_{vu} \boldsymbol{h} _u ^ {(\ell)}
\]</div>
<ul>
<li><p>Compute <strong>attention coefficients</strong> <span class="math notranslate nohighlight">\(e_{vu}\)</span> across pairs of nodes <span class="math notranslate nohighlight">\(u, v\)</span> based on their messages, by some <strong>attention mechanism</strong> <span class="math notranslate nohighlight">\(a\)</span></p>
<div class="math notranslate nohighlight">
\[
  e_{v u}=a\left(\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{W}^{(l)} \boldsymbol{h}_{v}^{(l-1)}\right)
  \]</div>
<p>The attention coefficients <span class="math notranslate nohighlight">\(e_{vu}\)</span> indicates the importance of <span class="math notranslate nohighlight">\(u\)</span>’s message to node <span class="math notranslate nohighlight">\(v\)</span>.</p>
</li>
<li><p>Normalize <span class="math notranslate nohighlight">\(e_vu\)</span> into the final <strong>attention weight</strong> <span class="math notranslate nohighlight">\(\alpha_{vu}\)</span> by softmax function</p>
<div class="math notranslate nohighlight">
\[
  \alpha_{v u}=\frac{\exp \left(e_{v u}\right)}{\sum_{k \in N(v)} \exp \left(e_{v k}\right)}
  \]</div>
</li>
</ul>
<p>How to design attention mechanism <span class="math notranslate nohighlight">\(a\)</span>? For instance, <span class="math notranslate nohighlight">\(a\)</span> can be some MLP. The parameters in <span class="math notranslate nohighlight">\(a\)</span> are trained jointly.</p>
<p>We can generalize to use multi-head attention, which are shown to stabilizes the learning process of attention mechanism [Velickovic+ 2018]. There are <span class="math notranslate nohighlight">\(R\)</span> independent attention mechanisms are used. Each one of them, namely a ‘heard’, computes a set of attention weight <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _{v} ^{(r)}\)</span>. Finally, we aggregate the <span class="math notranslate nohighlight">\(\boldsymbol{h} _v\)</span> again, by concatenation or summation.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell+1, \color{red}{r})} = \sigma \left( \boldsymbol{W} ^{(\ell) }\sum_{u \in \mathscr{N} (v)} \alpha_{vu}^ {(\color{red}{r})} \boldsymbol{h} _u ^ {(\ell)}  \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{h} _v ^{(\ell + 1)}  = \operatorname{AGG} \left( \left\{ \boldsymbol{h} _v ^{(\ell+1, \color{red}{r})}, r = 1, \ldots, R \right\}  \right)\)</span></p></li>
</ul>
<p>Pros</p>
<ul class="simple">
<li><p>allow for implicitly specifying weights</p></li>
<li><p>computationally efficient, parallelizable</p></li>
<li><p>storage efficient, total number of parameters <span class="math notranslate nohighlight">\(\mathcal{O} (N_v + N_e)\)</span></p></li>
</ul>
<p>If edge weights <span class="math notranslate nohighlight">\(w_{vu}\)</span> are given, we can</p>
<ul class="simple">
<li><p>use it as weights <span class="math notranslate nohighlight">\(\alpha_vu\)</span>, e.g. by softmax function</p></li>
<li><p>incorporate it into the design of attention mechanism <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
<p>In many cases, attention leads to performance gains.</p>
</div>
<div class="section" id="gin">
<h3>GIN<a class="headerlink" href="#gin" title="Permalink to this headline">¶</a></h3>
<div class="section" id="expressiveness">
<h4>Expressiveness<a class="headerlink" href="#expressiveness" title="Permalink to this headline">¶</a></h4>
<p>Consider a special case that node features are the same <span class="math notranslate nohighlight">\(\boldsymbol{x} _{v_1} = \boldsymbol{x} _{v_2} = \ldots\)</span>, represented by colors in the discussion below. Then if the computational graph are exactly the same for two nodes, then they have the same embeddings.</p>
<div class="figure align-default" id="gnn-expr-same">
<a class="reference internal image-reference" href="../_images/gnn-expr-same.png"><img alt="" src="../_images/gnn-expr-same.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 242 </span><span class="caption-text">Same computational graphs of two nodes</span><a class="headerlink" href="#gnn-expr-same" title="Permalink to this image">¶</a></p>
</div>
<p>Computational graphs are identical to <strong>rooted subtree</strong> structures around each node. GNN’s node embeddings capture rooted subtree structures. Most expressive GNN maps different rooted subtrees into different node embeddings, i.e. should be like an injective function.</p>
<div class="figure align-default" id="gnn-injective">
<a class="reference internal image-reference" href="../_images/gnn-injective.png"><img alt="" src="../_images/gnn-injective.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 243 </span><span class="caption-text">Injective mapping from computational graph to embeddings</span><a class="headerlink" href="#gnn-injective" title="Permalink to this image">¶</a></p>
</div>
<p>Some of the previously seen models do not use injective function at the neighbor aggregation step. Note that neighbor aggregation is a function over multi-sets (sets with repeating elements). They are not maximally powerful GNNs in terms of expressiveness.</p>
<ul class="simple">
<li><p>GCN (mean-pool)</p></li>
<li><p>GraphSAGE aggregation function (MLP + max-pool) cannot distinguish different multi-sets with the same set of distinct colors.</p></li>
</ul>
<div class="figure align-default" id="gnn-expr-fail">
<a class="reference internal image-reference" href="../_images/gnn-expr-fail.png"><img alt="" src="../_images/gnn-expr-fail.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 244 </span><span class="caption-text">Mean and max pooling failure cases</span><a class="headerlink" href="#gnn-expr-fail" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id2">
<h4>GIN<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>How to design maximally powerful GNNs? Can we design a neural network in the aggregation step that can model injective multi-set function.</p>
<p>Theorem [Xu et al. ICLR 2019]: Any injective multi-set function can be expressed
as:</p>
<div class="math notranslate nohighlight">
\[
\Phi \left( \sum_{x \in S} f(x)  \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f, \Phi\)</span> are non-linear functions. To model them, we can use MLP which have universal approximation power.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MLP}_{\Phi}  \left( \sum_{x \in S} \operatorname{MLP}_{f} (x)  \right)
\]</div>
<p>In practice, MLP hidden dimensionality of 100 to 500 is sufficient. The model is called Graph Isomorphism Network (GIN) [Xu+ 2019].</p>
<p>GIN‘s neighbor aggregation function is injective. It is the most expressive GNN in the class of message-passing GNNs. The key is to use element-wise sum pooling, instead of mean-/max-pooling.</p>
</div>
<div class="section" id="r-t-wl-kernel">
<h4>R.t. WL Kernel<a class="headerlink" href="#r-t-wl-kernel" title="Permalink to this headline">¶</a></h4>
<p>It is a “neural network” version of the <a class="reference internal" href="11-descriptive-analysis.html#wl-kernel"><span class="std std-ref">WL graph kernel</span></a>, where the color update function is</p>
<div class="math notranslate nohighlight">
\[
c^{(k+1)}(v)=\operatorname{HASH}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)
\]</div>
<p>Xu proved that any injective function over the tuple <span class="math notranslate nohighlight">\(\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)\)</span> can be modeled as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GINConv}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right) =  \left.\mathrm{MLP}_{\Phi}\left((1+\epsilon) \cdot \mathrm{MLP}_{f}\left(c^{(k)}(v)\right)\right)+\sum_{u \in N(v)} \mathrm{MLP}_{f}\left(c^{(k)}(u)\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon\)</span> is a learnable scalar.</p>
<p>Advantages of GIN over the WL graph kernel are:</p>
<ul class="simple">
<li><p>Node embeddings are low-dimensional; hence, they can capture the fine-grained similarity of different nodes.</p></li>
<li><p>Parameters of the update function can be learned for the downstream tasks.</p></li>
</ul>
<p>Because of the relation between GIN and the WL graph kernel, their expressive is exactly the same. If two graphs can be distinguished by GIN, they can be also distinguished by the WL kernel, and vice versa. They are powerful to distinguish most of the real graphs!</p>
</div>
</div>
<div class="section" id="improvement">
<h3>Improvement<a class="headerlink" href="#improvement" title="Permalink to this headline">¶</a></h3>
<p>There are basic graph structures that existing GNN framework cannot distinguish, such as difference in cycles. One node in 3-node-cycle and the other in 4-node cycle have the same computational graph when layer. GNNs’ expressive power can be improved to resolve the above problem. [You et al. AAAI 2021, Li et al. NeurIPS 2020]</p>
</div>
</div>
<div class="section" id="layer-design">
<h2>Layer Design<a class="headerlink" href="#layer-design" title="Permalink to this headline">¶</a></h2>
<div class="section" id="modules">
<h3>Modules<a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h3>
<p>We can include modern deep learning modules that proved to be useful in many domains, including</p>
<ul class="simple">
<li><p>BatchNorm to stabilize neural network training, used to normalize embeddings.</p></li>
<li><p>Dropout to prevent overfitting, used in linear layers <span class="math notranslate nohighlight">\(\boldsymbol{W} _\ell \boldsymbol{h} _u ^{(\ell)}\)</span>.</p></li>
</ul>
<div class="figure align-default" id="gnn-layer">
<a class="reference internal image-reference" href="../_images/gnn-layer.png"><img alt="" src="../_images/gnn-layer.png" style="width: 20%;" /></a>
<p class="caption"><span class="caption-number">Fig. 245 </span><span class="caption-text">A GNN Layer</span><a class="headerlink" href="#gnn-layer" title="Permalink to this image">¶</a></p>
</div>
<p>Try design ideas in <a class="reference external" href="https://github.com/snap-stanford/GraphGym">GraphGym</a>.</p>
<p>We then introduce how to add skip connection across layers.</p>
</div>
<div class="section" id="issue-with-deep-gnn">
<h3>Issue with Deep GNN<a class="headerlink" href="#issue-with-deep-gnn" title="Permalink to this headline">¶</a></h3>
<p>Unlike some other NN models, adding more GNN layers do not always help. The issue of stacking many GNN layers is that GNN suffers from the <strong>over-smoothing</strong> problem, where all the node embeddings converge to the same value.</p>
<p>Let receptive field be the set of nodes that determine the embedding of a node of interest. In a <span class="math notranslate nohighlight">\(L\)</span>-layer GNN, this set is the union of <span class="math notranslate nohighlight">\(\ell\)</span>-hop neighborhood for <span class="math notranslate nohighlight">\(\ell=1, 2, \ldots, L\)</span>. As the number of layers <span class="math notranslate nohighlight">\(L\)</span> goes large, the <span class="math notranslate nohighlight">\(L\)</span>-hop neighborhood increases exponentially. The receptive filed quickly covers all nodes in the graph.</p>
<div class="figure align-default" id="gnn-receptive-filed">
<a class="reference internal image-reference" href="../_images/gnn-receptive-filed.png"><img alt="" src="../_images/gnn-receptive-filed.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 246 </span><span class="caption-text">Receptive field for different layers of GNN</span><a class="headerlink" href="#gnn-receptive-filed" title="Permalink to this image">¶</a></p>
</div>
<p>For different nodes, the <strong>shared</strong> neighbors quickly grow when we
increase the number of hops (num of GNN layers). Their computation graphs are similar, hence similar embeddings.</p>
</div>
<div class="section" id="solution">
<h3>Solution<a class="headerlink" href="#solution" title="Permalink to this headline">¶</a></h3>
<p>How to enhance the expressive power of a shallow GNN?</p>
<ul class="simple">
<li><p>Increase the expressive power within each GNN layer</p>
<ul>
<li><p>make <span class="math notranslate nohighlight">\(\operatorname{AGG}\)</span> a neural network</p></li>
</ul>
</li>
<li><p>Add layers that do not pass messages</p>
<ul>
<li><p>MLP before and after GNN layers, as pre-process layers (image, text) and post-process layers for downstream tasks.</p></li>
</ul>
</li>
</ul>
<p>We can also add shortcut connections (aka skip connections) in GNN. Then we automatically get a mixture of shallow GNNs and deep GNNs. We can even add shortcuts from each layer to the final layer, then final layer directly aggregates from the all the node embeddings in the previous layers</p>
<div class="figure align-default" id="gnn-skip-connection">
<a class="reference internal image-reference" href="../_images/gnn-skip-connection.png"><img alt="" src="../_images/gnn-skip-connection.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 247 </span><span class="caption-text">Skip connection in GNN</span><a class="headerlink" href="#gnn-skip-connection" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="graph-manipulation">
<h2>Graph Manipulation<a class="headerlink" href="#graph-manipulation" title="Permalink to this headline">¶</a></h2>
<p>It is unlikely that the input graph happens to be the optimal computation graph for embeddings.</p>
<p>Issues and solutions:</p>
<ul class="simple">
<li><p>Graph Feature manipulation</p>
<ul>
<li><p>prob: the input graph lacks features.</p></li>
<li><p>sol: feature augmentation</p></li>
</ul>
</li>
<li><p>Graph Structure manipulation if the graph is</p>
<ul>
<li><p>too sparse</p>
<ul>
<li><p>prob: inefficient message passing</p></li>
<li><p>sol: add virtual nodes / edges</p></li>
</ul>
</li>
<li><p>too dense</p>
<ul>
<li><p>prob: message passing is too costly</p></li>
<li><p>sol: sample neighbors when doing message passing</p></li>
</ul>
</li>
<li><p>too large</p>
<ul>
<li><p>prob: cannot fit the computational graph into a GPU</p></li>
<li><p>sol: sample subgraphs to compute embeddings</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="section" id="feature-augmentation">
<h3>Feature Augmentation<a class="headerlink" href="#feature-augmentation" title="Permalink to this headline">¶</a></h3>
<p>Sometimes input graph does not have node features, e.g. we only have the adjacency matrix.</p>
<p>Standard approaches to augment node features</p>
<ul class="simple">
<li><p>assign constant values to nodes</p></li>
<li><p>assign unique IDs to nodes, in the form of <span class="math notranslate nohighlight">\(N_v\)</span>-dimensional one-hot vectors</p></li>
<li><p>assign other node-level descriptive features, that are hard to learn by GNN</p>
<ul>
<li><p>centrality</p></li>
<li><p>clustering coefficients</p></li>
<li><p>PageRank</p></li>
<li><p>cycle count (as one-hot encoding vector)</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="gnn-feature-aug">
<a class="reference internal image-reference" href="../_images/gnn-feature-aug.png"><img alt="" src="../_images/gnn-feature-aug.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 248 </span><span class="caption-text">Comparison of feature augmentation methods</span><a class="headerlink" href="#gnn-feature-aug" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="virtual-nodes-edges">
<h3>Virtual Nodes/Edges<a class="headerlink" href="#virtual-nodes-edges" title="Permalink to this headline">¶</a></h3>
<p>If the graph is too sparse, then the receptive field of a node covers small number of nodes. The message passing is then inefficient. We can add virtual nodes or virtual edges to augment sparse graphs.</p>
<div class="section" id="virtual-edges">
<h4>Virtual Edges<a class="headerlink" href="#virtual-edges" title="Permalink to this headline">¶</a></h4>
<p>We can connect 2-hop neighbors via virtual edges. Rather than using adjacency matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, we use <span class="math notranslate nohighlight">\(\boldsymbol{A} + \boldsymbol{A} ^2\)</span>.</p>
<p>For instance, in bipartite graphs, after introducing virtual edges to connect 2-hop neighbors, we obtained a folded bipartite graphs.</p>
</div>
<div class="section" id="virtual-nodes">
<h4>Virtual Nodes<a class="headerlink" href="#virtual-nodes" title="Permalink to this headline">¶</a></h4>
<p>We can add a virtual node, and connected it to all <span class="math notranslate nohighlight">\(N_v\)</span> nodes in the graph. Hence all nodes will have a distance at most 2. (too dense?? if <span class="math notranslate nohighlight">\(L=2\)</span> then the input layer covers all nodes??)</p>
<div class="figure align-default" id="gnn-virtual-node">
<a class="reference internal image-reference" href="../_images/gnn-virtual-node.png"><img alt="" src="../_images/gnn-virtual-node.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 249 </span><span class="caption-text">A virtual node</span><a class="headerlink" href="#gnn-virtual-node" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="neighborhood-sampling">
<h3>Neighborhood Sampling<a class="headerlink" href="#neighborhood-sampling" title="Permalink to this headline">¶</a></h3>
<p>In the standard setting, for a node <span class="math notranslate nohighlight">\(v\)</span>, all the nodes in <span class="math notranslate nohighlight">\(\mathscr{N} _(v)\)</span> are used for message passing. If <span class="math notranslate nohighlight">\(\left\vert \mathscr{N} _(v) \right\vert\)</span> is large, we can actually randomly sample a <strong>subset</strong> of a node’s neighborhood for message passing.</p>
<p>Next time when we compute the embeddings, we can sample <strong>different</strong> neighbors. In expectation, we will still use all neighbors vectors.</p>
<p>Benefits:</p>
<ul class="simple">
<li><p>greatly reduce computational cost</p></li>
<li><p>allow for scaling to large graphs.</p></li>
</ul>
</div>
</div>
<div class="section" id="graph-generative-models">
<h2>Graph Generative Models<a class="headerlink" href="#graph-generative-models" title="Permalink to this headline">¶</a></h2>
<p>Types</p>
<ul class="simple">
<li><p>Realistic graph generation: generate graphs that are similar to a given set of graphs. (our focus)</p></li>
<li><p>goal-directed graph generation: generate graphs that optimize given objectives/constraints, e.g. molecules chemical properties</p></li>
</ul>
<p>Given a set of graph, we want to</p>
<ul class="simple">
<li><p><strong>Density estimation</strong>: model the distribution of these graphs by <span class="math notranslate nohighlight">\(p_{\text{model} }\)</span>, hope it is close to <span class="math notranslate nohighlight">\(p_{\text{data} }\)</span>, e.g. maximum likelihood</p></li>
<li><p><strong>Sampling</strong>: sample from <span class="math notranslate nohighlight">\(p_{\text{model} }\)</span>. A common approach is</p>
<ul>
<li><p>sample  from noise <span class="math notranslate nohighlight">\(z_i \sim \mathcal{N} (0, 1)\)</span></p></li>
<li><p>transform the noise <span class="math notranslate nohighlight">\(z_i\)</span> via <span class="math notranslate nohighlight">\(f\)</span> to obtain <span class="math notranslate nohighlight">\(x_i\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> can be trained NN.</p></li>
</ul>
</li>
</ul>
<p>Challenge</p>
<ul class="simple">
<li><p>large output space: <span class="math notranslate nohighlight">\(N^2\)</span> bits of adjacency matrix?</p></li>
<li><p>variable output space: how to generate graph of different sizes?</p></li>
<li><p>non-unique representation: for a fixed graph, its adjacency matrix has <span class="math notranslate nohighlight">\(N_v !\)</span> ways of representation.</p></li>
<li><p>dependency: existence of an edge may depend on the entire graph</p></li>
</ul>
<div class="section" id="graphrnn">
<h3>GraphRNN<a class="headerlink" href="#graphrnn" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf">You+ 2018</a></p>
<p>Recall that by chain rule, a joint distribution can be factorized as</p>
<div class="math notranslate nohighlight">
\[
p_{\text {model }}(\boldsymbol{x} ; \theta)=\prod_{t=1}^{n} p_{\text {model }}\left(x_{t} \mid x_{u, v}, \ldots, x_{t-1} ; \theta\right)
\]</div>
<p>In our case, <span class="math notranslate nohighlight">\(x_t\)</span> will be the <span class="math notranslate nohighlight">\(t\)</span>-th action (add node, add edge). The ordering of nodes is a random ordering <span class="math notranslate nohighlight">\(\pi\)</span>. For a fixed ordering <span class="math notranslate nohighlight">\(\pi\)</span> of nodes, for each node, we add a sequence of edges.</p>
<div class="figure align-default" id="gnn-gen">
<a class="reference internal image-reference" href="../_images/gnn-gen.png"><img alt="" src="../_images/gnn-gen.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 250 </span><span class="caption-text">Generation process of a graph</span><a class="headerlink" href="#gnn-gen" title="Permalink to this image">¶</a></p>
</div>
<p>It is like propagating along columns of upper-triangularized adjacency matrix corresponding to the ordering <span class="math notranslate nohighlight">\(\pi\)</span>. Essentially, we have transformed graph generation problem into a sequence generation problem. We need to model two processes</p>
<ol class="simple">
<li><p>Generate a state for a new node (Node-level sequence)</p></li>
<li><p>Generate edges for the new node based on its state (Edge-level sequence)</p></li>
</ol>
<p>To model these two sequences, we can use <a class="reference internal" href="../37-neural-networks/31-sequential-models.html#rnn"><span class="std std-ref">RNNs</span></a>.</p>
<p>GraphRNN has a node-level RNN and an edge-level RNN.</p>
<ul class="simple">
<li><p>Node-level RNN generates the initial state for edge-level RNN</p></li>
<li><p>Edge-level RNN sequentially predict the probability that the new node will connect to each of the previous node. Then the last hidden state is used to run node RNN for another step.</p></li>
</ul>
<div class="figure align-default" id="graph-rnn-pipeline">
<a class="reference internal image-reference" href="../_images/graph-rnn-pipeline.png"><img alt="" src="../_images/graph-rnn-pipeline.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 251 </span><span class="caption-text">GraphRNN Pipeline (node RNN + edge RNN)</span><a class="headerlink" href="#graph-rnn-pipeline" title="Permalink to this image">¶</a></p>
</div>
<p>The training and test pipeline is:</p>
<div class="figure align-default" id="graph-rnn-train-test">
<a class="reference internal image-reference" href="../_images/graph-rnn-train-test.png"><img alt="" src="../_images/graph-rnn-train-test.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 252 </span><span class="caption-text">GraphRNN training and test example</span><a class="headerlink" href="#graph-rnn-train-test" title="Permalink to this image">¶</a></p>
</div>
<ul>
<li><p>SOS means start of sequence, EOS means end of sequence.</p></li>
<li><p>If Edge RNN outputs EOS at step 1, we know no edges are connected to the new node. We stop the graph generation. At test time, this means node sequence length is not fixed.</p></li>
<li><p>In training of edge RNN, teacher enforcing of edge existence is applied. The loss is binary cross entropy</p>
<div class="math notranslate nohighlight">
\[L=- \sum _{u, v} \left[y_{u, v} \log \left(\hat{y}_{u, v}\right)+\left(1-y_{u, v}\right) \log \left(1-\hat{y}_{u, v}\right)\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_{u, v}\)</span> is predicted probability of existence of edge <span class="math notranslate nohighlight">\((u, v)\)</span> and <span class="math notranslate nohighlight">\(y_{u,v}\)</span> is ground truth 1 or 0.</p>
</li>
</ul>
</div>
<div class="section" id="tractability">
<h3>Tractability<a class="headerlink" href="#tractability" title="Permalink to this headline">¶</a></h3>
<p>In the structure introduced above, each edge RNN can have at most <span class="math notranslate nohighlight">\(n-1\)</span> step. How to limit this?</p>
<p>The answer is to use Breadth-First search node ordering rather than random ordering of nodes.</p>
<div class="figure align-default" id="graph-rnn-ordering">
<a class="reference internal image-reference" href="../_images/graph-rnn-ordering.png"><img alt="" src="../_images/graph-rnn-ordering.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 253 </span><span class="caption-text">Random ordering and BFS ordering</span><a class="headerlink" href="#graph-rnn-ordering" title="Permalink to this image">¶</a></p>
</div>
<p>Illustrated in adjacency matrix, we only look at connectivity with nodes in the BFS frontier, rather than all previous nodes.</p>
<div class="figure align-default" id="graph-rnn-ordering-2">
<a class="reference internal image-reference" href="../_images/graph-rnn-ordering-2.png"><img alt="" src="../_images/graph-rnn-ordering-2.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 254 </span><span class="caption-text">Random ordering and BFS ordering in adjacency matrices</span><a class="headerlink" href="#graph-rnn-ordering-2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="similarity-of-graphs">
<h3>Similarity of Graphs<a class="headerlink" href="#similarity-of-graphs" title="Permalink to this headline">¶</a></h3>
<p>How to compare two graphs? Qualitatively, we can compare visual similarity. Quantitatively, we can compare graph statistics distribution such as</p>
<ul class="simple">
<li><p>degree distribution</p></li>
<li><p>clustering coefficient distribution</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Graphlets#Graphlet-based_network_properties">Graphlet-based</a> distribution
The distance between two distributions can be measured by <a class="reference external" href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">earth mover distance</a> (aka Wasserstein metric in math), which measure the minimum effort that move earth from one pile to the other.</p></li>
</ul>
<div class="figure align-default" id="emd">
<a class="reference internal image-reference" href="../_images/emd.png"><img alt="" src="../_images/emd.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 255 </span><span class="caption-text">Earth mover distance illustration</span><a class="headerlink" href="#emd" title="Permalink to this image">¶</a></p>
</div>
<p>How two compare two sets of graphs by some set distance? Each graph element may have different <span class="math notranslate nohighlight">\(N_v, N_e\)</span>. First we compute a statistic for each graph element, then compute Maximum Mean Discrepancy (MDD). If <span class="math notranslate nohighlight">\(\mathcal{X} = \mathcal{H} = \mathbb{R} ^d\)</span> and we choose <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> to be <span class="math notranslate nohighlight">\(\phi(x)=x\)</span>, then it becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{MMD}(P, Q) &amp;=\left\|\mathbb{E}_{X \sim P}[\varphi(X)]-\mathbb{E}_{Y \sim Q}[\varphi(Y)]\right\|_{\mathcal{H}} \\
&amp;=\left\|\mathbb{E}_{X \sim P}[X]-\mathbb{E}_{Y \sim Q}[Y]\right\|_{\mathbb{R}^{d}} \\
&amp;=\left\|\mu_{P}-\mu_{Q}\right\|_{\mathbb{R}^{d}}
\end{aligned}
\end{split}\]</div>
<p>Given a set of input graphs, we can generate a set of graphs using some algorithms. Then compare the set distance. Many algorithms are particularly designed to generate certain graphs, but GraphRNN can learn from the input and generate any types of graphs (e.g. grid).</p>
</div>
<div class="section" id="id3">
<h3>Variants<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Graph convolutional policy network</p>
<ul class="simple">
<li><p>use GNN to predict the generation action</p></li>
<li><p>further uses RL to direct graph generation to certain goals</p></li>
</ul>
<p>Hierarchical generation: generate subgraphs at each step</p>
</div>
</div>
<div class="section" id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h2>
<p>For a perfect GNN:</p>
<ol class="simple">
<li><p>If two nodes have the same neighborhood structure, they must have the same embedding</p></li>
<li><p>If two nodes have different neighborhood structure, they must have different embeddings</p></li>
</ol>
<p>However,</p>
<ul class="simple">
<li><p>point 1 may not be practical, sometimes we want to assign different embeddings to two nodes with the same neighborhood structure. Solution: position-aware GNNs.</p></li>
<li><p>point 2 often cannot be satisfied. Nodes on two rings have the same computational graphs. Sol: idendity-aware GNNs</p></li>
</ul>
<div class="section" id="position-aware-gnns">
<h3>Position-aware GNNs<a class="headerlink" href="#position-aware-gnns" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/1906.04817">J. You, R. Ying, J. Leskovec. Position-aware Graph Neural Networks, ICML 2019</a></p>
<ul class="simple">
<li><p>structure-aware task: nodes are labeled by their structural roles in the graph</p></li>
<li><p>position-aware task: nodes are labeled by their positions in the graph</p></li>
</ul>
<div class="figure align-default" id="gnn-struc-posi-aware">
<a class="reference internal image-reference" href="../_images/gnn-struc-posi-aware.png"><img alt="" src="../_images/gnn-struc-posi-aware.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 256 </span><span class="caption-text">Two types of labels.</span><a class="headerlink" href="#gnn-struc-posi-aware" title="Permalink to this image">¶</a></p>
</div>
<p>GNNs differentiate nodes by their computational graphs. Thus, they often work well for structure-aware task, but fail for position-aware tasks (but node features are different??)</p>
<p>To solve this, we introduce anchors. Randomly pick some nodes or some set of nodes in the graph as <strong>anchor-sets</strong>. Then we compute the relative distances from every nodes to these anchor-sets.</p>
<div class="figure align-default" id="gnn-anchor-set">
<a class="reference internal image-reference" href="../_images/gnn-anchor-set.png"><img alt="" src="../_images/gnn-anchor-set.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 257 </span><span class="caption-text">Anchors</span><a class="headerlink" href="#gnn-anchor-set" title="Permalink to this image">¶</a></p>
</div>
<p>The distance can then be used as <strong>position encoding</strong>, which represent a node’s position by its distance to randomly selected anchor-set.</p>
<p>Note that each dimension of the position encoding is tied to an anchor-set. Permutation of the order does not change the meaning of the encoding. Thus, we cannot directly use this encoding as augmented feature.</p>
<p>We require a special NN that can maintain the permutation invariant property of position encoding. Permuting the input feature dimension will only result in the permutation of the output dimension, the <strong>value</strong> in each dimension won’t change.</p>
</div>
<div class="section" id="identity-aware-gnn">
<h3>Identity-aware GNN<a class="headerlink" href="#identity-aware-gnn" title="Permalink to this headline">¶</a></h3>
<p>[J. You, J. Gomes-Selman, R. Ying, J. Leskovec. Identity-aware Graph Neural Networks, AAAI 2021]</p>
<p>Heterogenous: different types of message passing is applied to different nodes. Suppose two nodes <span class="math notranslate nohighlight">\(v_1, v_2\)</span> have the same computational graph structure, but have different node colorings. Since we will apply different neural network for embedding computation, their embeddings will be different.</p>
<div class="figure align-default" id="gnn-idgnn">
<a class="reference internal image-reference" href="../_images/gnn-idgnn.png"><img alt="" src="../_images/gnn-idgnn.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 258 </span><span class="caption-text">Identity-aware GNN</span><a class="headerlink" href="#gnn-idgnn" title="Permalink to this image">¶</a></p>
</div>
<p>ID-GNN can count cycles originating from a given node, but GNN cannot.</p>
<p>Rather than to heterogenous message passing, we can include identity information as an augmented node feature (no need to do heterogenous message passing). Use cycle counts in each layer as an augmented node feature.</p>
<div class="figure align-default" id="gnn-idgnn-cycle">
<a class="reference internal image-reference" href="../_images/gnn-idgnn-cycle.png"><img alt="" src="../_images/gnn-idgnn-cycle.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 259 </span><span class="caption-text">Cycle count at each level forms a vector</span><a class="headerlink" href="#gnn-idgnn-cycle" title="Permalink to this image">¶</a></p>
</div>
<p>ID-GNN is more expressive than their GNN counterparts. ID-GNN is the first message passing GNN that is more expressive than 1-WL test.</p>
</div>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Tutorials and overviews:</p>
<ul>
<li><p>Relational inductive biases and graph networks (Battaglia et al., 2018)</p></li>
<li><p>Representation learning on graphs: Methods and applications (Hamilton et al., 2017)</p></li>
</ul>
</li>
<li><p>Attention-based neighborhood aggregation:</p>
<ul>
<li><p>Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)</p></li>
</ul>
</li>
<li><p>Embedding entire graphs:</p>
<ul>
<li><p>Graph neural nets with edge embeddings (Battaglia et al., 2016; Gilmer et. al., 2017)</p></li>
<li><p>Embedding entire graphs (Duvenaud et al., 2015; Dai et al., 2016; Li et al., 2018) and graph pooling (Ying et al., 2018, Zhang et al., 2018)</p></li>
<li><p>Graph generation and relational inference (You et al., 2018; Kipf et al., 2018)</p></li>
<li><p>How powerful are graph neural networks(Xu et al., 2017)</p></li>
</ul>
</li>
<li><p>Embedding nodes:</p>
<ul>
<li><p>Varying neighborhood: Jumping knowledge networks (Xu et al., 2018), GeniePath (Liu et al., 2018)</p></li>
<li><p>Position-aware GNN (You et al. 2019)</p></li>
</ul>
</li>
<li><p>Spectral approaches to graph neural networks:</p>
<ul>
<li><p>Spectral graph CNN &amp; ChebNet (Bruna et al., 2015; Defferrard et al., 2016)</p></li>
<li><p>Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)</p></li>
</ul>
</li>
<li><p>Other GNN techniques:</p>
<ul>
<li><p>Pre-training Graph Neural Networks (Hu et al., 2019)</p></li>
<li><p>GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="coding">
<h2>Coding<a class="headerlink" href="#coding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="using-pyg">
<h3>Using PyG<a class="headerlink" href="#using-pyg" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In forward, GCNConv take as input both node features <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">edge_list</span></code></p></li>
<li><p>In training, for convenience purpose, we feed all data, but the loss only consists of <code class="docutils literal notranslate"><span class="pre">training_idx</span></code>, i.e. only use training nodes to construct computation graphs, and update parameters</p></li>
<li><p>In validation, save the ‘best’ model with highest validation accuracy</p></li>
<li><p>model.train(), model.eval() flag</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">binary</span> <span class="n">label</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>    <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>  <span class="n">Binary</span> <span class="n">Cross</span> <span class="n">Entropy</span>
<span class="n">los</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">multiple</span> <span class="n">classes</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span>  <span class="n">negative</span> <span class="n">log</span> <span class="n">likelihood</span> <span class="n">loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">multiple</span> <span class="n">classes</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>  <span class="n">raw</span><span class="p">,</span> <span class="n">unnormalized</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">each</span> <span class="k">class</span>
<span class="nc">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="n">combines</span> <span class="n">LogSoftmax</span> <span class="ow">and</span> <span class="n">NLLLoss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="deepsnap">
<h3>DeepSNAP<a class="headerlink" href="#deepsnap" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>dataset split</p></li>
<li><p>transformation, feature computation</p></li>
</ul>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Colab</p></th>
<th class="head"><p>dataset</p></th>
<th class="head"><p>dataset info</p></th>
<th class="head"><p>task</p></th>
<th class="head"><p>output</p></th>
<th class="head"><p>loss</p></th>
<th class="head"><p>remark</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Colab 0</p></td>
<td><p>pyg.datasets.KarateClub</p></td>
<td><p>34 nodes, 34 features, 4 classes</p></td>
<td><p>node classification</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">self.classifier(h)</span></code>, and <code class="docutils literal notranslate"><span class="pre">h</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></p></td>
<td><p>only uses training set</p></td>
</tr>
<tr class="row-odd"><td><p>Colab 1</p></td>
<td><p>nx.karate_club_graph()</p></td>
<td><p>embedding_dim=16, label=1 (edge) or 0 (neg edge)</p></td>
<td><p>node embeddings</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">sigmoid()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BCELoss</span></code></p></td>
<td><p>convert nx graph to tensor</p></td>
</tr>
<tr class="row-even"><td><p>Colab 2</p></td>
<td><p>Open Graph Benchmark ogbn-arxiv</p></td>
<td><p>169343 nodes, 128 f</p></td>
<td><p>node classification</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">nn.LogSoftmax(dim=-1)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.nll_loss</span></code></p></td>
<td><p>keep best model, no batch</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Open Graph Benchmark ogbg-molhiv</p></td>
<td><p>41,127 graphs, &lt;25&gt; nodes, 2 classes</p></td>
<td><p>graph classification</p></td>
<td><p>pool</p></td>
<td><p>BCEWithLogitsLoss</p></td>
<td><p>AtomEncoder</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>ENZYMES</p></td>
<td><p>600 graphs, 6 classes, 3 features</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Colab 3</p></td>
<td><p>Planetoid CORA citation networks</p></td>
<td><p>2708 nodes, 1433 features are elements of a bag-or-words representation of a document, 7 classes</p></td>
<td><p>link prediction</p></td>
<td><p>inner product</p></td>
<td><p>nn.BCEWithLogitsLoss()</p></td>
<td><p>Implement GraphSAGE, GAT. Loss plot. DeepSNAP</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>TUDataset COX2</p></td>
<td><p>many graphs</p></td>
<td><p>graph classification</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>Qs</p>
<ul class="simple">
<li><p>Colab 0, what are the features?</p></li>
<li><p>Colab 2, batch.y == batch.y?
.</p></li>
</ul>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./38-ml-for-graph-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="51-embeddings.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Embeddings</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>