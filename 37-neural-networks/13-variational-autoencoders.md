# Variational Autoencoders

More or less simultaneously proposed by Kingma & Welling (2013) and Rezende et al. (2014)

Popular, fast, and relatively easy to train.

VAE is a type of generative model for a vector of random variables $\boldsymbol{x}$ assumed to be generated from a set of latent variables $\boldsymbol{z}$. The unconditional distribution is

$$
p(\mathbf{x})=\int_{\mathbf{z}} p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z}) d \mathbf{z}
$$

For the discussion below, assume $\boldsymbol{x} ,\boldsymbol{z}$ are both continuous.

VAE is initially used for generation, but has also been successful for representation learning.


:::{admonition,note} VAE vs AE

- “Autoencoders” because they compute a density of z given x via an encoder and a density of x given z via a decoder

- “Variational” because they involve approximating a density via optimization

:::

## Assumptions


$$
\begin{aligned}
&\boldsymbol{z}  \sim p(\boldsymbol{z}) \quad \text {often multivariate Gaussian }\\
&\boldsymbol{x}  \mid \boldsymbol{z} \sim p_{\theta}(\boldsymbol{x}  \mid \boldsymbol{z} )
\end{aligned}
$$

Can be viewed as a decoder.


Theorem (Validation of VAE)
: Any $d$-dimensional distribution can be generated by taking d normally distributed variables and mapping them through some appropriate (possibly very complicated) function

Of course, the function can be a neural network.

## Objective

Assume w.l.o.g. that latent variable $\boldsymbol{z}$ is 1-dimensional, $\boldsymbol{x}$  is 2-dimensional
For now, assume that $\boldsymbol{z}$ is Gaussian, $\boldsymbol{x}$ is diagonal Gaussian

[img31]

But $p(\mathbf{x} \mid \mathbf{z})$ can be very costly to estimate
We will approximate it with another neural function $p(\mathbf{x} \mid \mathbf{z})$ That function $q(\mathbf{x} \mid \mathbf{z})$ is the encoder.

[img34 (some typos)]

The learned representation are described by the learned parameters of the distribution $\boldsymbol{\mu} , \sigma^2$. To produce a single value, we can use mean or mode.

To generate $\boldsymbol{x}$, we can draw from the prior and generate $\boldsymbol{x} \mid \boldsymbol{z}$ from the decoder.

## Training

Maximizing the likelihood can be challenging. Instead, we maximize the lower bound of the likelihood.

By some formula from conditional probability and information theory, we have


$$
\begin{aligned}
L &=\log p(x) \\
\text { multiply by 1... } &=\sum_{z} q(z \mid x) \log p(x) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{p(z \mid x)}\right) \\
\text { multiply by 1... } &=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)} \frac{q(z \mid x)}{p(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right)+\sum_{z} q(z \mid x) \log \left(\frac{q(z \mid x)}{p(z \mid x)}\right) \\
&=L^{v}+D_{K L}(q(z \mid x) \| p(z \mid x)) \\
& \geq L^{v}
\end{aligned}
$$

```{margin}
The lower abound $L^v$ is also known as evidence lower bound, or elbow.
```
The lower bound $L^v$ can be further arranged to

$$
\begin{aligned}
L^{v} &=\sum_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(x \mid z) p(z)}{q(z \mid x)}\right) \\
&=\sum_{z} q(z \mid x) \log \left(\frac{p(z)}{q(z \mid x)}\right)+\sum_{z} q(z \mid x) \log p(x \mid z\\
&=-D_{K L}(q(z \mid x)|| p(z))+E_{q(z \mid x)}(\log p(x \mid z)) \\
\text { for } x_{i} \ldots &=-D_{K L}\left(q\left(z \mid x_{i}\right) \| p(z)\right)+E_{q\left(z \mid x_{i}\right)}\left(\log p\left(x_{i} \mid z\right)\right)
\end{aligned}
$$

We then maximizing the last line.

- The first term is the negative KL divergence between the posterior and the prior (often Gaussian). We want to minimize the distance. It can be viewed as a regularizer.

    The KL divergence has a closed form when $p(z) = N (0, 1)$ and $q(z|x)$ is also Gaussian:

    $$
    -D_{K L}\left(q\left(z \mid x_{i}\right) \| p(z)\right)=\frac{1}{2} \sum_{j=1}^{J} 1+\log \left(\sigma_{z_{i, j}}^{2}\right)-\mu_{z_{i, j}}^{2}-\sigma_{z_{i, j}}^{2}
    $$

- The second term can be seen as a reconstruction loss; equals log(1) if $\boldsymbol{x}_i$ is perfectly reconstructed from $\boldsymbol{z}$. In training, the expectation is estimated by sampling $B$ samples from $q(z\mid x_i)$

    $$
    E_{q(z \mid x)}(\log p(x \mid z)) = \frac{1}{B} \sum_{l=1}^{B}\left(\log p\left(x_{i} \mid z_{i, l}\right)\right)
    $$

    If $B=1$, then it is just a least squares loss

    $$
    \left(\log p\left(x_{i} \mid z_{i}\right)\right)=\sum_{j=1}^{D} \frac{1}{2} \log \sigma_{x_{j}}^{2}+\frac{\left(x_{i, j}-\mu_{x_{j}}\right)^{2}}{2 \sigma_{x_{j}}^{2}}
    $$
