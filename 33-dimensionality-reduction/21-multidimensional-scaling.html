
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Multidimensional Scaling &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Graph-based Spectral Methods" href="23-graph-based-spectral-methods.html" />
    <link rel="prev" title="Canonical Correlation Analysis" href="13-canonical-correlation-analysis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/71-streaming.html">
     Streaming Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-model-selection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/41-penalized-regression.html">
     Penalized Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-normal.html">
     For Gaussian Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/13-linear-discriminant-analysis.html">
     Linear Discriminant Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/31-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-pca-variants.html">
     PCA Variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="61-indep-component-analysis.html">
     Independent Component Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/91-computation.html">
     Computation Issues
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/81-density-fitting.html">
     Application to Density Fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/51-embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/53-graph-neural-networks.html">
     Graphical Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/33-dimensionality-reduction/21-multidimensional-scaling.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F33-dimensionality-reduction/21-multidimensional-scaling.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning">
   Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     Spectral Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practical-issues">
   Practical Issues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goodness-of-fit">
     Goodness of Fit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning">
     Tuning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relation-to-pca">
   Relation to PCA
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="multidimensional-scaling">
<h1>Multidimensional Scaling<a class="headerlink" href="#multidimensional-scaling" title="Permalink to this headline">¶</a></h1>
<p>In this section we introduce (metric) multidimensional scaling (MDS). It finds some representation of input data, which can be used for dimension reduction or visualization. In addition to a <span class="math notranslate nohighlight">\(n \times d\)</span> data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, MDS can also take pairwise relations, aka <strong>proximity</strong> data, as input, such as</p>
<ul>
<li><p>pairwise dissimilarity measures of data points as input, denoted <span class="math notranslate nohighlight">\(\boldsymbol{D} \in \mathbb{R} ^{n \times n}\)</span></p>
<ul class="simple">
<li><p>a matrix of Euclidean distances between data points</p></li>
<li><p>a matrix of city-city airline distance (not necessarily Euclidean)</p></li>
<li><p>survey results of customers’ perception of dissimilarity between products</p></li>
</ul>
<p>Note that the input dissimilarity matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span> may not be exactly euclidean distance, hence it does not enjoy all properties that a distance matrix has.</p>
</li>
<li><p>pairwise similarity measures of data points as input, denoted <span class="math notranslate nohighlight">\(\boldsymbol{S} \in \mathbb{R} ^{n \times n}\)</span></p>
<ul class="simple">
<li><p>co-occurrence counts of words between documents</p></li>
<li><p>an adjacency matrix of web pages (MDS as a graph layout technique)</p></li>
<li><p>survey results of customers’ perception of similarity between products</p></li>
</ul>
</li>
<li><p>Usually the diagonal entires of a dissimilarity matrix are <span class="math notranslate nohighlight">\(0\)</span>, and those of a similarity matrix are 1. A dissimilarity measure can be created based on the given similarity measure,
and vice versa.</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title"> On similarity, dissimilarity, proximity, distance</p>
<ul class="simple">
<li><p>Similarity measure reflects how close two objects are. The closer the two
objects are to each other, the larger is their similarity value.</p></li>
<li><p>Dissimilarity measure indicates how different two objects are. The farther
the two objects are to each other, the larger is their dissimilarity value.</p></li>
<li><p>Proximity can be either similarity or dissimilarity.</p></li>
<li><p>Distance sometime loosely refers to a dissimilarity measure. On the other hand, distance as a <a class="reference internal" href="../11-math/20-vector-spaces.html#metric"><span class="std std-ref">metric</span></a> is a more strictly defined mathematical concept.</p></li>
</ul>
</div>
<p>In total, there are four types of MDS: {metric, non-metric} <span class="math notranslate nohighlight">\(\times\)</span> {distance, classical}.</p>
<ul class="simple">
<li><p><strong>distance scaling</strong>: fit dissimilarity by Euclidean distance <span class="math notranslate nohighlight">\(d_{ij} \approx \left\| \boldsymbol{z} _i - \boldsymbol{z} _j \right\|\)</span></p></li>
<li><p><strong>classical scaling</strong>: transform dissimilarity to some form of ‘similarity’ and then fit by inner product  <span class="math notranslate nohighlight">\(s_{ij} \approx \langle \boldsymbol{z}_i , \boldsymbol{z} _j \rangle\)</span>. Note the identity <span class="math notranslate nohighlight">\(\left\| \boldsymbol{z}_i - \boldsymbol{z}_j  \right\| ^2 = \left\| \boldsymbol{z} _i \right\|^2 + \left\| z
_j \right\| ^2 - 2 \langle \boldsymbol{z}_i , \boldsymbol{z}_j  \rangle\)</span>.</p></li>
<li><p><strong>metric scaling</strong> uses actual numerical values of dissimilarities</p></li>
<li><p><strong>non-metric scaling</strong> applies ranks, KL divergences, etc. rather than numerical dissimilarities.</p></li>
</ul>
<p>Here, we introduce metric classical scaling. For others, see this <a class="reference external" href="http://www.stat.yale.edu/~lc436/papers/JCGS-mds.pdf">paper</a>.</p>
<p>Many non-linear dimensionality reduction methods are extension to MDS. MDS is a clue to link linear and non-linear dimensionality reduction.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Given a distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\in \mathbb{R} ^{n \times n}\)</span>, can we find a <span class="math notranslate nohighlight">\(2\)</span>-dimensional representation of it <span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R} ^{n \times k}\)</span>, to visualize them in 2-D plane?</p>
<p>More generally, suppose <span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R} ^{n \times k}\)</span> are the underlying embeddings, MDS finds <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span> such that the distance is nearly preserved: <span class="math notranslate nohighlight">\(\left\| \boldsymbol{z} _i - \boldsymbol{z} _j \right\| \approx d_{ij}\)</span>. Hence the objective is to find embeddings <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span> that minimizes <strong>stress</strong></p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Stress}_{D}\left(\boldsymbol{z}_{1}, \ldots, \boldsymbol{z}_{N}\right)=\left(\sum_{i \neq j=1}^n\left(d_{ij}-\left\|\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right\|\right)^{2}\right)^{1 / 2}
  \]</div>
</li>
<li><p>Given a distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>, find coordinates for the <span class="math notranslate nohighlight">\(n\)</span> points in a low dimensional space such that the corresponding distances maintain the ordering of pairwise distance in <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>.</p></li>
</ul>
<!--
This can be solved by gradient descent. However, we can also transform $\boldsymbol{D}$ to a form $\boldsymbol{B}$ that is naturally fitted by inner product. The transformation satisfies $d_{ij} = b_{ii} - 2b_{ij} + b_{ij}$, thereby mimicking the corresponding identities for $\left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\|$ and $\langle \boldsymbol{x} _i, \boldsymbol{x} _j \rangle$.


If the input is similarity matrix $\boldsymbol{S}$, for the conversion of similarities $s_{ij}$ to dissimilarities $d_{ij}$, one could in principle use any monotone decreasing transformation, but the following conversion is preferred.

$$
d_{ij} = s_{ii} - 2 s_{i, j} + s_{jj}
$$

It interprets the similarities as inner product data, and guarantee $d_{ii} = 0$. -->
<ul>
<li><p>Given data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X} \in \mathbb{R} ^{n \times d}\)</span> with zero column means, as a dimensionality reduction method, MDS seeks a <span class="math notranslate nohighlight">\(k\)</span>-dimensional representation <span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R} ^{n \times k}\)</span> that preserves inner products (a similarity measure) between pairs of data points <span class="math notranslate nohighlight">\((\boldsymbol{x_i}, \boldsymbol{x}_j)\)</span></p>
<div class="math notranslate nohighlight">
\[
  \min \sum_{i, j}\left(\boldsymbol{x}_{i} ^\top  \boldsymbol{x}_{j}-\boldsymbol{z}_{i} ^\top  \boldsymbol{z}_{j}\right)^{2}
  \]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[
  \min \left\Vert \boldsymbol{X} \boldsymbol{X} ^\top  - \boldsymbol{Z} \boldsymbol{Z} ^\top    \right\Vert _F^2
  \]</div>
<p>The lower-dimensional embeddings can be then used for downstream tasks, including clustering, classification, etc. It worths mentioning that the embeddings are exactly the same as PCA’s. See the last section for proof.</p>
</li>
</ul>
</div>
<div class="section" id="learning">
<h2>Learning<a class="headerlink" href="#learning" title="Permalink to this headline">¶</a></h2>
<p>Depending on how we formulate the objective, there are mainly two methods.</p>
<div class="section" id="spectral-decomposition">
<h3>Spectral Decomposition<a class="headerlink" href="#spectral-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{C}  = \boldsymbol{I}-\frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}\)</span> be a centering matrix.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note the inner product matrix <span class="math notranslate nohighlight">\(\boldsymbol{G}_{n\times n} = \boldsymbol{X} \boldsymbol{X} ^\top\)</span> is different from the data covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}_{d\times d} = \boldsymbol{X} ^\top \boldsymbol{X}\)</span>.</p>
</div>
<ul>
<li><p>If the input matrix is data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, the solution can be obtained from the <span class="math notranslate nohighlight">\(n\times n\)</span> Gram matrix <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> of inner products of centered data <span class="math notranslate nohighlight">\(\bar{\boldsymbol{X} }= \boldsymbol{C} \boldsymbol{X}\)</span> with zero column means, where <span class="math notranslate nohighlight">\(\boldsymbol{C} = (\boldsymbol{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1} ^{\top})\)</span>.</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{G} := \bar{\boldsymbol{X}} \bar{\boldsymbol{X}} ^\top
  \]</div>
<p>where <span class="math notranslate nohighlight">\(g_{i j}=\bar{\boldsymbol{x}}_{i} \bar{\boldsymbol{x}}_{j} ^{\top}\)</span>. Recall our objective is preserve inner product</p>
<div class="math notranslate nohighlight">
\[
  \min \left\Vert \bar{\boldsymbol{X}} \bar{\boldsymbol{X}} ^\top  - \boldsymbol{Z} \boldsymbol{Z} ^\top    \right\Vert _F^2
  \]</div>
<p>Hence it is natural to approximate <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> by spectral method. Suppose the spectral decomposition of <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{G} = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V} ^\top\)</span>, then the <span class="math notranslate nohighlight">\(k\)</span>-dimensional representation is given by</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{Z}_{n \times k} = \left[  \boldsymbol{V}  \boldsymbol{\Lambda} ^{1/2} \right]_{[:k]} = \boldsymbol{V}_{[: k]} \boldsymbol{\Lambda}_{[: k,: k]}^{1 / 2}
  \]</div>
<p>For derivation see <a class="reference external" href="http://www.math.uwaterloo.ca/~aghodsib/courses/f10stat946/notes/lec10-11.pdf">here</a>. The reconstruction of <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> from <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span> is then <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{G}} = \boldsymbol{Z} \boldsymbol{Z} ^\top\)</span>.</p>
</li>
<li><p>If the input is a similarity matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, we can run the above algorithm by treating <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span>. Some other methods first convert a similarity measure to a distance measure and then analyze it. There are many ways for such conversion, e.g. <span class="math notranslate nohighlight">\(d = 10 \sqrt{2 (1-s)}\)</span>, which guarantees that the obtained dissimilarity order is exactly the inversion of the similarly order.</p></li>
<li><p>If the input is an <strong>Euclidean</strong> distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>, suppose the true data is <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, then by definition we have</p>
<div class="math notranslate nohighlight">
\[
  d_{i j}=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|^{2}=\left\|\boldsymbol{x}_{i}\right\|^{2}-2 \boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{j}+\left\|\boldsymbol{x}_{j}\right\|^{2}
  \]</div>
<p>We can convert the Euclidean distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span> to the Gram matrix <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> for centered <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> by left- and right-multiplying <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span></p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{G} = - \frac{1}{2} \boldsymbol{C}  (\boldsymbol{D} * \boldsymbol{D} )\boldsymbol{C} ^{\top}
  \]</div>
<p>where <span class="math notranslate nohighlight">\([\boldsymbol{D} * \boldsymbol{D}]_{ij} = d_{ij}^2\)</span>. Then we can run MDS over <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> to obtain <span class="math notranslate nohighlight">\(k\)</span>-dimensional representation. Note that since <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> is row-centered, it has an eigenvalue 0. Hence, the maximal possible value of <span class="math notranslate nohighlight">\(k\)</span> is <span class="math notranslate nohighlight">\(d-1\)</span>, rather than <span class="math notranslate nohighlight">\(d\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  d_{ij}^2
  &amp;= \left\| x_i \right\|^2  + \left\| x_j \right\|^2  - 2 \boldsymbol{x}_i ^{\top} \boldsymbol{x}_j  \\
  \Rightarrow \boldsymbol{D} * \boldsymbol{D} &amp;= \boldsymbol{v} \boldsymbol{1} ^{\top} + \boldsymbol{1} \boldsymbol{v} ^{\top} - 2 \boldsymbol{X}  \boldsymbol{X} ^{\top}\text{ where }  v_i = \left\| \boldsymbol{x}_i  \right\| ^2  \\
  \Rightarrow \boldsymbol{C} (\boldsymbol{D} * \boldsymbol{D}) \boldsymbol{C} &amp;= -2 \boldsymbol{C} \boldsymbol{X}  \boldsymbol{X} ^{\top} \boldsymbol{C} \quad \because \boldsymbol{C} (\boldsymbol{v} \boldsymbol{1} ^{\top}) \boldsymbol{C} = 0\\
  \Rightarrow -\frac{1}{2} \boldsymbol{C} (\boldsymbol{D} * \boldsymbol{D}) \boldsymbol{C} &amp;= \bar{\boldsymbol{X}} \bar{\boldsymbol{X}} ^{\top}  \\
  &amp;= \boldsymbol{G}
  \end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</li>
<li><p>Motivated by this, if the input matrix is dissimilarity (not necessarily Euclidean distance) matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>, we can run the above algorithm over <span class="math notranslate nohighlight">\(\boldsymbol{B} = - \frac{1}{2} \boldsymbol{C} \boldsymbol{D} \boldsymbol{C}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> approximates <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span>.</p>
<p>Hence can see, this spectral method is particularly appropriate when the dissimilarities are actually or at least approximately Euclidean distances.</p>
<div class="warning admonition">
<p class="admonition-title"> Warning</p>
<p>Since the dissimilarity measure may not be Euclidean, <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> may have some negative eigenvalues. The maximal possible value of <span class="math notranslate nohighlight">\(k\)</span> is then the number of of positive eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span>.</p>
</div>
</li>
</ul>
<p>This spectral method also uses principal components (eigenvectors) like PCA. Therefore sometimes classical MDS is also called “principal coordinates analysis”.</p>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>If we have some non-trivial objective function, say some measure of the goodness-of-fit of the lower dimensional representation <span class="math notranslate nohighlight">\(\boldsymbol{Z} \in \mathbb{R}^{n \times k}\)</span>, then the measure can be viewed as a loss function on <span class="math notranslate nohighlight">\(\mathbb{R} ^{n \times k}\)</span>. The optimization algorithm can be carried out by numerical methods such as gradient descent.</p>
</div>
</div>
<div class="section" id="practical-issues">
<h2>Practical Issues<a class="headerlink" href="#practical-issues" title="Permalink to this headline">¶</a></h2>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>In R, if the input matrix <code class="docutils literal notranslate"><span class="pre">L</span></code> is in lower triangle format, use <code class="docutils literal notranslate"><span class="pre">as.dist(L)</span></code> before calling <code class="docutils literal notranslate"><span class="pre">cmdscale()</span></code> function.</p>
</div>
<div class="section" id="goodness-of-fit">
<h3>Goodness of Fit<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">¶</a></h3>
<p>We can use (scaled) stress [Kruskal 1964] to measure the goodness-of-fit of the lower dimensional representation, whose value is in range [0, 1].</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Stress}_ \boldsymbol{D} (k) = \left\{ \frac{\sum_{i \ne j} (d_{ij} - \hat{d}_{ij})^2}{\sum_{i \ne j} d_{ij}^2}  \right\}  ^{-1/2}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d_{ij}\)</span> are the entries in distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>, presumably the distance of the two objects in a higher dimensional space</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{d}_{ij} = \left\| \boldsymbol{z} _i - \boldsymbol{z} _j \right\|_2\)</span> is the Euclidean distance in the <span class="math notranslate nohighlight">\(k\)</span>-dimensional space (or some other distance measure).</p></li>
</ul>
<p>An alternative measure SStress developed by Takane 1997 is</p>
<div class="math notranslate nohighlight">
\[
\operatorname{SStress}_{\boldsymbol{D} }(k) = \left\{ \frac{\sum_{i \ne j} (d_{ij}^2 - \hat{d}_{ij}^2)^2}{\sum_{i \ne j} d_{ij}^4}  \right\}  ^{1/2}
\]</div>
<p>Typically the value of Stress or SStress less than 0.1 is considered a good representation of the objects by the points in the given low dimension configuration. In practice, the goodness of fit of Stress is based on the range of the values:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c||c|c|c|c|c}
&amp; \text { Perfect } &amp; \text { Excellent } &amp; \text { Good } &amp; \text { Fair } &amp; \text { Poor } \\
\hline \text { Stress value } &amp; 0 \% &amp; 2.5 \% &amp; 5 \% &amp; 10 \% &amp; 20 \%
\end{array}
\end{split}\]</div>
<p>Besides a scalar measure, we can also plot the fitted distance <span class="math notranslate nohighlight">\(\hat{d}_ij\)</span> over the input distance <span class="math notranslate nohighlight">\(d_{ij}\)</span>. If the points on the <span class="math notranslate nohighlight">\(45^\circ\)</span> <span class="math notranslate nohighlight">\(y=x\)</span> line, then the distances are preserved. See the plots in next section.</p>
</div>
<div class="section" id="tuning">
<h3>Tuning<a class="headerlink" href="#tuning" title="Permalink to this headline">¶</a></h3>
<p>How to choose optimal <span class="math notranslate nohighlight">\(k\)</span>? We can see how the goodness-of-fit mentioned above varies with <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="figure align-default" id="mds-tuning">
<a class="reference internal image-reference" href="../_images/mds-tuning.png"><img alt="" src="../_images/mds-tuning.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 99 </span><span class="caption-text">Stress plots (upper row) and fitted vs real distance plots (lower row). .</span><a class="headerlink" href="#mds-tuning" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>Both Stress and SStress drop when <span class="math notranslate nohighlight">\(k=1\)</span> changes to <span class="math notranslate nohighlight">\(k=2\)</span>, but both increase afterwards, i.e. the loss is not monotonically decreasing as <span class="math notranslate nohighlight">\(k\)</span> becomes large!</p></li>
<li><p>We plot fitted distance for <span class="math notranslate nohighlight">\(k=1\)</span> (blue), <span class="math notranslate nohighlight">\(k=2\)</span> (red), <span class="math notranslate nohighlight">\(k=8\)</span> (black). When <span class="math notranslate nohighlight">\(k=1\)</span>, the fitting is poor. When <span class="math notranslate nohighlight">\(k=2\)</span>, the fitting is already quite good. As <span class="math notranslate nohighlight">\(k\)</span> increases, the points roughly stay unchanged, i.e. no obvious improvement. The results are consistent with the stress results.</p></li>
</ul>
<p>Note that the maximal possible <span class="math notranslate nohighlight">\(k\)</span> is also related to the rank of the input distance matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span>.</p>
</div>
</div>
<div class="section" id="relation-to-pca">
<h2>Relation to PCA<a class="headerlink" href="#relation-to-pca" title="Permalink to this headline">¶</a></h2>
<p>We compare PCA and MDS in terms of finding representation <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span> given <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>,</p>
<ul>
<li><p>Difference: unlike PCA which gives a projection equation <span class="math notranslate nohighlight">\(\boldsymbol{z} = \boldsymbol{U} ^\top \boldsymbol{x}\)</span>, MDS only gives a projected result for the training set. It does not give us a way to project a new data point.</p></li>
<li><p>Connections</p>
<ul>
<li><p>the two representation are exactly the <strong>same</strong>. Suppose the data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is centered. Let <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{PCA}\)</span> be the <span class="math notranslate nohighlight">\(n\times k\)</span> projected matrix by PCA and <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{MDS}\)</span> be that by MDS. Then it can be shown that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \boldsymbol{Z} _{PCA} = \boldsymbol{Z} _{MDS}\\
    \end{split}\]</div>
<p>This also implies that, to obtain PCA projections, we can use the covariance matrix, the Gram matrix <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span>, or the Euclidean distances matrix <span class="math notranslate nohighlight">\(\boldsymbol{F}\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Consider the SVD of the <strong>centered</strong> data matrix</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{X}_{n\times d} = \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{U} ^\top\]</div>
<ul>
<li><p>In PCA, the <span class="math notranslate nohighlight">\(n\times d\)</span> embedding matrix is</p>
<div class="math notranslate nohighlight">
\[
        \boldsymbol{Z}_{PCA} = \boldsymbol{X} \boldsymbol{U}
        \]</div>
</li>
<li><p>In MDS, the EVD of the inner product matrix <span class="math notranslate nohighlight">\(\boldsymbol{G}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
        \boldsymbol{G}_{n \times n} = \boldsymbol{X} \boldsymbol{X} ^\top = \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{\Sigma} ^\top \boldsymbol{V} ^\top = \boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V} ^\top = \boldsymbol{V} _{[:d]} \boldsymbol{D} \boldsymbol{V} _{[:d]} ^\top
        \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \boldsymbol{\Lambda} _{n \times n} = \left[\begin{array}{cc}
        \boldsymbol{D} _{d \times d} &amp; \boldsymbol{0}  \\
        \boldsymbol{0}  &amp; \boldsymbol{0}_{(n-d) \times (n-d)}
        \end{array}\right]
        \end{split}\]</div>
<p>The embedding matrix is</p>
<div class="math notranslate nohighlight">
\[
        \boldsymbol{Z} _{MDS} = \boldsymbol{V}_{[:d]} \boldsymbol{\Lambda} ^{1/2}_{[:d, :d]} = \boldsymbol{V}_{[:d]} \boldsymbol{D} ^{1/2}
        \]</div>
</li>
</ul>
<p>Substituting the SVD of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> to the <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{PCA}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \boldsymbol{Z} _{PCA}
    &amp;= \boldsymbol{X} \boldsymbol{U}\\
    &amp;= \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{U} ^{\top} \boldsymbol{U} \\
    &amp;= \boldsymbol{V} \boldsymbol{\Sigma}_{n \times d}\\
    &amp;= \boldsymbol{V}_{[:d]} \boldsymbol{\Sigma}_{[:d, :d]}\\
    &amp;= \boldsymbol{V}_{[:d]} \boldsymbol{D}^ {1/2} \quad \because \text{singular value } \sigma_j \ge 0$\\
    &amp;= \boldsymbol{Z} _{MDS} \\
    \end{aligned}\end{split}\]</div>
</div>
</li>
<li><p>PCA finds basis <span class="math notranslate nohighlight">\(\boldsymbol{u} \in \mathbb{R} ^n\)</span> (principle directions) for spanning <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, and MDS finds the coordinates <span class="math notranslate nohighlight">\(\boldsymbol{z} \in \mathbb{R} ^d\)</span> of the embeddings associated with the PCA basis.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \boldsymbol{X} ^{\top}
    &amp;= \boldsymbol{U} (\boldsymbol{V} \boldsymbol{\Sigma} ) ^{\top} \\
    &amp;= \boldsymbol{U} (\boldsymbol{V}_{[:d]} \boldsymbol{\Sigma}_{[:d]} ) ^{\top} \\
    &amp;= \boldsymbol{U} \boldsymbol{Z} _{MDS} ^{\top} \\
    [\boldsymbol{x} _i \ \ldots \ \boldsymbol{x} _n]&amp;= [\boldsymbol{u}_1 \ \ldots \ \boldsymbol{u} _d ] [\boldsymbol{z} _1 \ \ldots \ \boldsymbol{z} _n] \\
    \boldsymbol{x} _i&amp;= \sum_{j=1}^d
    z^{MDS}_{ij} \boldsymbol{u} _j\\
    \end{aligned}\end{split}\]</div>
</li>
</ul>
</li>
</ul>
<p>Reference: Davis-kahan theorem</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./33-dimensionality-reduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="13-canonical-correlation-analysis.html" title="previous page">Canonical Correlation Analysis</a>
    <a class='right-next' id="next-link" href="23-graph-based-spectral-methods.html" title="next page">Graph-based Spectral Methods</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>