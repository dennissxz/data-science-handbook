
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Principal Component Analysis &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Canonical Correlation Analysis" href="13-canonical-correlation-analysis.html" />
    <link rel="prev" title="Dimensionality Reduction" href="00-dimensionality-reduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/test.html">
     Interactive data visualizations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/33-dimensionality-reduction/11-principal-component-analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/33-dimensionality-reduction/11-principal-component-analysis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F33-dimensionality-reduction/11-principal-component-analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/dennissxz/data-science-handbook/master?urlpath=tree/33-dimensionality-reduction/11-principal-component-analysis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tasks">
     Tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formulation-based-on-distribution">
     Formulation based on Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formulation-based-on-data">
     Formulation based on Data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning">
   Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequential-maximization">
     Sequential Maximization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     Spectral Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#properties">
     Properties
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practical-issues">
   Practical Issues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning">
     Tuning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation">
     Interpretation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#geometric-meaning-direction-of-variation">
       Geometric Meaning: Direction of Variation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#proportion-explained">
       Proportion Explained
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#score-of-an-observation-in-sample-data">
       Score of an Observation in Sample Data
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#special-cases">
     Special Cases
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variables-are-uncorrelated">
       Variables are Uncorrelated
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variables-are-perfectly-correlated">
       Variables are Perfectly Correlated
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#few-variables-have-extremely-large-variances">
       Few Variables Have Extremely Large Variances
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cons">
     Cons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relation-to">
   Relation to
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svd">
     SVD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compression">
     Compression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussians">
     Gaussians
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoencoders">
     Autoencoders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extension">
   Extension
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-mle">
     Learning (MLE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Properties
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation">
     Representation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advanced-topics">
   Advanced Topics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#large-sample-inference">
     Large Sample Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identifiability">
     Identifiability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#phase-transition">
       Phase Transition
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#comparison-to-davis-kahan-theorem">
       Comparison to Davis-Kahan Theorem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h1>
<p>Proposed by Pearson in 1901 and further developed by Hotelling in 1993.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tasks">
<h3>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span>, we want to extract the most useful information of <span class="math notranslate nohighlight">\(p\)</span> measurements such that</p>
<ol class="simple">
<li><p><strong>explore underlying dimension</strong> behind the <span class="math notranslate nohighlight">\(p\)</span> original measurements to explain the variation of <span class="math notranslate nohighlight">\(p\)</span> original measurements, which may have interesting or revealing interpretations, such as size, shape and contrasts in natural science;</p></li>
<li><p><strong>estimate latent variables</strong> (i.e. variables that cannot be measured or observed.) which can explain the variation of the <span class="math notranslate nohighlight">\(p\)</span> original measurements, especially in
social behavioral sciences.</p></li>
<li><p><strong>simplify the dimension</strong> of the observed data set. Lower dimension can be chosen from the data set such that the variations of measurements can be captured with an acceptable level. For example, <span class="math notranslate nohighlight">\(k \ll d\)</span> latent variables are chosen to capture 90% of variation of <span class="math notranslate nohighlight">\(p\)</span> original measurements. Indeed, this can be regarded as the data reduction or dimension reduction.</p></li>
</ol>
</div>
<div class="section" id="formulation-based-on-distribution">
<h3>Formulation based on Distribution<a class="headerlink" href="#formulation-based-on-distribution" title="Permalink to this headline">¶</a></h3>
<p>Consider a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector <span class="math notranslate nohighlight">\(\boldsymbol{x} = \left( X_1, X_2, \ldots, X_d \right)^\top\)</span> with mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \left( \mu_1, \ldots, \mu_d \right)^\top\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. PCA aims to obtain the variables <span class="math notranslate nohighlight">\(Z_1, Z_2, \ldots, Z_k\)</span> which are the <strong>linear combinations</strong> of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span> and <span class="math notranslate nohighlight">\(k \le d\)</span>, such that</p>
<ul>
<li><p>The sum of the new individual variances</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( Z_1 \right) + \operatorname{Var}\left( Z_2 \right) + \ldots + \operatorname{Var}\left( Z_k \right)
  \]</div>
<p>is <strong>close</strong> to the sum of the original individual variances</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( X_1 \right) + \operatorname{Var}\left( X_2 \right) + \ldots + \operatorname{Var}\left( X_d \right)
  \]</div>
</li>
<li><p>The linear combinations <span class="math notranslate nohighlight">\(Z_i\)</span> and <span class="math notranslate nohighlight">\(Z_j\)</span> are <strong>uncorrelated</strong> for <span class="math notranslate nohighlight">\(i\ne j\)</span>. This imply that each variable in <span class="math notranslate nohighlight">\(\boldsymbol{z} = \left( Z_1, Z_2, \ldots, Z_k \right)^\top\)</span> can be analyzed by using <strong>univariate</strong> techniques.</p></li>
</ul>
</div>
<div class="section" id="formulation-based-on-data">
<h3>Formulation based on Data<a class="headerlink" href="#formulation-based-on-data" title="Permalink to this headline">¶</a></h3>
<p>Other formulations of PCA based on sample data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> aim to find a linear mapping <span class="math notranslate nohighlight">\(\mathbb{R} ^d \rightarrow \mathbb{R} ^k\)</span> (assume <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is centered) to project the data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}_{n \times d}\)</span> to a lower dimensional embedding matrix <span class="math notranslate nohighlight">\(\boldsymbol{Z}_{n \times k}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{z}_i &amp;= \boldsymbol{W}_{d \times k} ^\top \boldsymbol{x}_i \\
\boldsymbol{Z}_{n \times k} &amp;= \boldsymbol{X}_{n \times d}  \boldsymbol{W} _{d \times k} \\
\end{aligned}\end{split}\]</div>
<p>The mapping <span class="math notranslate nohighlight">\(\boldsymbol{W} _{d \times k}\)</span> are also called <strong>loadings</strong>, and the embeddings <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{n \times k}\)</span> are called <strong>scores</strong>. The loadings can be used to visualize how the original variables <span class="math notranslate nohighlight">\(X_j\)</span> contributes to each principal component <span class="math notranslate nohighlight">\(Z_j\)</span>. The scores <span class="math notranslate nohighlight">\(\boldsymbol{Z} _{n \times k}\)</span> can then be used for downstream tasks, e.g. visualization.</p>
<p>There are various equivalent formulations of the loadings <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>:</p>
<ul>
<li><p>Maximize the total variances <span class="math notranslate nohighlight">\(\sum_i \operatorname{Var}\left( Z_i \right)\)</span> of the projected data <span class="math notranslate nohighlight">\(\boldsymbol{Z} =  \boldsymbol{X}  \boldsymbol{W}\)</span> (similar to the population formulation above)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmax}} \, &amp; \operatorname{tr}\left( \boldsymbol{Z} ^\top \boldsymbol{Z}  \right)   \\
     = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmax}} \, &amp; \operatorname{tr}\left( (\boldsymbol{X}\boldsymbol{W}) ^\top \boldsymbol{X} \boldsymbol{W} \right)   \\
     \text{s.t.}  &amp; \ \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  \\
       &amp;\ \boldsymbol{W} \in \mathbb{R} _{d \times k}
    \end{align}\end{split}\]</div>
</li>
<li><p>Minimize total reconstruction loss, where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x} }_i = \boldsymbol{W} \boldsymbol{z} _i = \boldsymbol{W} \boldsymbol{W} ^{\top} \boldsymbol{x} _i\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmin}} \, &amp; \sum_i^n \left\Vert \boldsymbol{x}_i - \hat{\boldsymbol{x} }_i \right\Vert ^2    \\
     \text{s.t.}  &amp; \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  \\
       &amp;\ \boldsymbol{W} \in \mathbb{R} _{d \times k}
    \end{align}\end{split}\]</div>
</li>
<li><p>Low-dimensional Hyperplane fitting</p>
<p>Fit a low-dimensional hyperplane such that, when we project our data <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> onto the hyperplane and obtain <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span>, the variance of our data is changed as little as possible. The low-dimensional hyperplane is defined by <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, which is the matrix of basis vectors that span it. Minimizing the change in variance between the original data <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and its reconstruction <span class="math notranslate nohighlight">\(\boldsymbol{Z} \boldsymbol{W}^{\top}\)</span> is equivalent to minimizing the sum of squared error loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmin}} \, &amp; \sum_i^n \left\Vert \boldsymbol{x}_i - \boldsymbol{W} \boldsymbol{z} _i \right\Vert ^2    \\
   \text{s.t.}  &amp; \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  \\
     &amp;\ \boldsymbol{W} \in \mathbb{R} _{d \times k}
  \end{align}\end{split}\]</div>
</li>
</ul>
</div>
</div>
<div class="section" id="learning">
<h2>Learning<a class="headerlink" href="#learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sequential-maximization">
<h3>Sequential Maximization<a class="headerlink" href="#sequential-maximization" title="Permalink to this headline">¶</a></h3>
<p>The first variable in <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>, i.e. <span class="math notranslate nohighlight">\(Z_1 = \boldsymbol{u} \boldsymbol{x}\)</span> is obtained to maximize its variance, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Var}\left(Z_{1}\right)=\max _{\left\Vert \boldsymbol{u}  \right\Vert _2^2 = 1 } \boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}
\]</div>
<p>where the constraint is to removes scaling or <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>. Suppose the maximum is achieved at <span class="math notranslate nohighlight">\(\boldsymbol{u} = \boldsymbol{u} _1\)</span> and we call the variable <span class="math notranslate nohighlight">\(Z_1\)</span> given below the <strong>first population principal component</strong></p>
<div class="math notranslate nohighlight">
\[
Z_1 = \boldsymbol{u} _1^{\top} \boldsymbol{x}
\]</div>
<p>Successively for <span class="math notranslate nohighlight">\(k=2, \ldots, d\)</span> the variance of <span class="math notranslate nohighlight">\(Z_i\)</span> can be obtained by the following maximization</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\max _{\boldsymbol{u} _k} &amp;&amp; \boldsymbol{u}_k^{\top} \boldsymbol{\Sigma} \boldsymbol{u}_k &amp; \\
\mathrm{s.t.}
&amp;&amp; \quad \boldsymbol{u}_k^{\top} \boldsymbol{u}_k&amp;=1 \\
&amp;&amp; \boldsymbol{u}_k ^{\top} \boldsymbol{u} _j &amp;= 0  \text{  for }  j=1, \ldots, k-1 \\
\end{aligned}\end{split}\]</div>
<p>where the second constraint <span class="math notranslate nohighlight">\(\boldsymbol{u}_k ^{\top} \boldsymbol{u} _j = 0\)</span> comes from the identity</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{Cov}\left(Z_{k}, Z_{j}\right)  &amp;=\operatorname{Cov}\left(\boldsymbol{u} _{k}^{\top} \boldsymbol{x}, \boldsymbol{u}_{j}^{\top} \boldsymbol{x}\right) \\
&amp;=\boldsymbol{u}_{k}^{\top} \operatorname{Cov}(\boldsymbol{x}, \boldsymbol{x}) \boldsymbol{u}_{j} \\
&amp;=\boldsymbol{u}_{k}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}_{j} \\
&amp;=\boldsymbol{u}_{k}^{\top} \lambda_{j} \boldsymbol{u}_{j} \\
&amp;=\lambda_{j} \boldsymbol{u}_{k}^{\top} \boldsymbol{u}_{j}
\end{aligned}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\lambda_j &gt; 0\)</span> and we want uncorrelated principal components <span class="math notranslate nohighlight">\(\operatorname{Cov}\left(Z_k, Z_{j}\right) = 0\)</span>, we obtain <span class="math notranslate nohighlight">\(\boldsymbol{u} _k ^{\top} \boldsymbol{u} _j = 0\)</span>.</p>
<p>After we solve <span class="math notranslate nohighlight">\(\boldsymbol{u} _k\)</span>, the <span class="math notranslate nohighlight">\(k\)</span>-th population principal component is then</p>
<div class="math notranslate nohighlight">
\[
Z_k = \boldsymbol{u} _k^\top \boldsymbol{x}
\]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> Derivation</p>
<p>The problem to find <span class="math notranslate nohighlight">\(\boldsymbol{u} _1\)</span> is an <a class="reference internal" href="../11-math/test.html#rayleigh-quotient"><span class="std std-ref">Rayleigh quotient</span></a> problem which has been solved there. We show how to solve the subsequent <span class="math notranslate nohighlight">\(\boldsymbol{u} _k\)</span> for <span class="math notranslate nohighlight">\(k \ge 2\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(k=2\)</span>, the problem is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\max _{\boldsymbol{u} _2} &amp;&amp; \boldsymbol{u}_2^{\top} \boldsymbol{\Sigma} \boldsymbol{u}_2 &amp; \\
\mathrm{s.t.}
&amp;&amp; \quad \boldsymbol{u}_2^{\top} \boldsymbol{u}_2&amp;=1 \\
&amp;&amp; \boldsymbol{u}_2 ^{\top} \boldsymbol{u} _1 &amp;= 0\\
\end{aligned}\end{split}\]</div>
<p>The Lagrangean is</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{2}=\boldsymbol{u}_{2}^{\top} \Sigma \boldsymbol{u}_{2}-\lambda\left(\boldsymbol{u}_{2}^{\top} \boldsymbol{u}_{2}-1\right)-\delta\left(\boldsymbol{u}_{2}^{\top} \boldsymbol{u}_{1}-0\right)
\]</div>
<p>Taking derivative w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{u} _2\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{2}}{\partial \boldsymbol{u}_{2}}=2 \boldsymbol{\Sigma} \boldsymbol{u}_{2}-2 \lambda \boldsymbol{u}_{2}-\delta \boldsymbol{u}_{1}=0
\]</div>
<p>Left-multiply <span class="math notranslate nohighlight">\(\boldsymbol{u} _1 ^{\top}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
RHS &amp;= \boldsymbol{u}_{1}^{\top} \boldsymbol{0} =0 \\
LHS &amp;=2\left(\boldsymbol{u}_{1}^{\top} \Sigma \boldsymbol{u}_{2}-\boldsymbol{u}_{1}^{\top} \lambda \boldsymbol{u}_{2}\right)-\boldsymbol{u}_{1}^{\top} \delta \boldsymbol{u}_{1} \\
&amp;=2\left(\left(\boldsymbol{\Sigma} \boldsymbol{u}_{1}\right)^{\top} \boldsymbol{u}_{2}-\lambda \boldsymbol{u}_{1}^{\top} \boldsymbol{u}_{2}\right)-\delta \boldsymbol{u}_{1}^{\top} \boldsymbol{u}_{1} \\
&amp;=2\left(\lambda \boldsymbol{u}_{1}^{\top} \boldsymbol{u}_{2}-\lambda \boldsymbol{u}_{1}^{\top} \boldsymbol{u}_{2}\right)-\delta \\
&amp;=- \delta
\end{aligned}
\end{split}\]</div>
<p>Hence <span class="math notranslate nohighlight">\(\delta = 0\)</span>. Substituting this back gives <span class="math notranslate nohighlight">\(\Sigma \boldsymbol{u} _2= \lambda\boldsymbol{u} _{2}\)</span>. Thus, <span class="math notranslate nohighlight">\(\boldsymbol{u} _2\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. The objective is then <span class="math notranslate nohighlight">\(\boldsymbol{u} _2 \boldsymbol{\Sigma} \boldsymbol{u} _2 = \lambda\)</span>. Hence, we choose <span class="math notranslate nohighlight">\(\lambda_2\)</span>, and choose <span class="math notranslate nohighlight">\(\boldsymbol{u} _2\)</span> to be the eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> associated with <span class="math notranslate nohighlight">\(\lambda_2\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(k=3, \ldots, n\)</span>, the Lagrangean is</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{k}=\boldsymbol{u}_{k}^{\top} \Sigma \boldsymbol{u}_{k}-\lambda\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{u}_{k}-1\right)-\delta_{1}\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{u}_{1}\right)-\cdots-\delta_{k-1}\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{u}_{k-1}\right)
\]</div>
<p>Taking derivative gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L_{k}}{\partial \boldsymbol{u}_{k}} &amp;=2(\Sigma-\lambda I) \boldsymbol{u}_{k}-\delta_{1} \boldsymbol{u}_{1}-\cdots-\delta_{k-1} \boldsymbol{u}_{k-1} \\
&amp;=2 \boldsymbol{\Sigma} \boldsymbol{u}_{k}-2 \lambda \boldsymbol{u}_{k}-\delta_{1} \boldsymbol{u}_{1}-\cdots-\delta_{k-1} \boldsymbol{u}_{k-1} \\
&amp;=0
\end{aligned}
\end{split}\]</div>
<p>Similarly, left multiplying <span class="math notranslate nohighlight">\(\boldsymbol{u} _j ^{\top}\)</span> gives <span class="math notranslate nohighlight">\(\delta_j=0\)</span> for <span class="math notranslate nohighlight">\(j=1, \ldots, k-1\)</span>. Substituting <span class="math notranslate nohighlight">\(\delta_j=0\)</span> back gives <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} \boldsymbol{u} _k = \lambda \boldsymbol{u} _k\)</span>. We can then choose the <span class="math notranslate nohighlight">\(k\)</span>-th largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_k\)</span> and the associated eigenvector to be <span class="math notranslate nohighlight">\(\boldsymbol{u}_k\)</span>.</p>
</div>
</div>
<div class="section" id="spectral-decomposition">
<h3>Spectral Decomposition<a class="headerlink" href="#spectral-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Rather than obtaining the principal components sequentially, the principal components and their variances can be obtained simultaneously by solving for the eigenvectors and eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. Its <a class="reference internal" href="../11-math/21-linear-algebra.html#eigen-decomposition"><span class="std std-ref">spectral decomposition</span></a> is,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma}  = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top = \sum_i^d \lambda_i \boldsymbol{u} _i \boldsymbol{u} _i ^\top
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_2 &gt; \dots&gt; \lambda_d \ge 0\)</span> are ordered eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} =  \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{u} _1, \ldots, \boldsymbol{u} _d\)</span> are their corresponding normalized eigenvectors forming the column vectors of the orthogonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{U} = \left( \boldsymbol{u} _1\  \boldsymbol{u} _2 \ \ldots \  \boldsymbol{u} _d \right)\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{U}  ^\top \boldsymbol{U}   = \boldsymbol{I}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{u} _i ^\top \boldsymbol{u} _j = 1\)</span> if <span class="math notranslate nohighlight">\(i=j\)</span> and 0 otherwise.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-th population principal component is defined as</p>
<div class="math notranslate nohighlight">
\[
Z_{k}=\boldsymbol{u}_{k}^{\top} \boldsymbol{x}=u_{1 k} X_{1}+u_{2 k} X_{2}+\cdots+u_{d k} X_{k}, \quad k=1, \ldots, d
\]</div>
<p>The principal component transform using the first <span class="math notranslate nohighlight">\(k\)</span> principal directions is then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z} = \boldsymbol{U}_{[:k]} ^\top \boldsymbol{x}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{Z} = \boldsymbol{X} \boldsymbol{U} _{[:k]}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{U} _{[:k]}\)</span> means the matrix consisting of the first <span class="math notranslate nohighlight">\(k\)</span> columns of <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span>.</p>
</div>
<div class="section" id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>All principal components are uncorrelated, i.e., <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Z_i, Z_j \right) = 0\)</span> for <span class="math notranslate nohighlight">\(i \ne j\)</span></p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \operatorname{Cov}\left(Z_{i}, Z_{j}\right) &amp;=\operatorname{Cov}\left(\boldsymbol{u}_{i}^{\top} \boldsymbol{x}, \boldsymbol{u}_{j}^{\top} \boldsymbol{x}\right) \\
    &amp;=\mathrm{E}\left(\boldsymbol{u}_{i}^{\top}(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{u}_{j}\right) \\
    &amp;=\boldsymbol{u}_{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}_{j} \\
    &amp;=\boldsymbol{u}_{i}^{\top} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^{\top} \boldsymbol{u}_{j} \\
    &amp;=\left(\boldsymbol{u}_{i}^{\top}\right)\left(\boldsymbol{u}_{1} \boldsymbol{u}_{2} \cdots \boldsymbol{u}_{d}\right) \boldsymbol{\Lambda}\left(\begin{array}{c}
    \boldsymbol{u}_{1}^{\top} \\
    \boldsymbol{u}_{2}^{\top} \\
    \vdots \\
    \boldsymbol{u}_{d}^{\top}
    \end{array}\right) \boldsymbol{u}_{j} \\
    &amp;=\boldsymbol{e}_{i}^{\top} \boldsymbol{\Lambda} \boldsymbol{e}_{j} \\
    &amp;=0
    \end{aligned}
    \end{split}\]</div>
</div>
</li>
<li><p>The variance of the <span class="math notranslate nohighlight">\(i\)</span>-th principal component is <span class="math notranslate nohighlight">\(\lambda_i\)</span>, i.e. <span class="math notranslate nohighlight">\(\operatorname{Var}\left( Z_i \right) = \lambda_i\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \operatorname{Var}\left( Z_i \right)
    &amp;= \operatorname{Var}\left( \boldsymbol{u} _i ^\top \boldsymbol{x}  \right) \\
    &amp;= \boldsymbol{u} _i ^\top \boldsymbol{\Sigma} \boldsymbol{u} _i \\
    &amp;= \boldsymbol{e}_i ^\top \boldsymbol{\Lambda} \boldsymbol{e}_i  \\
    &amp;= \lambda_i
    \end{aligned}\end{split}\]</div>
</div>
</li>
<li><p>The first principal component <span class="math notranslate nohighlight">\(Z_1 = \boldsymbol{u} _1 ^\top \boldsymbol{x}\)</span> has the largest variance among all linear combinations of <span class="math notranslate nohighlight">\(X_i\)</span>’s. Then for <span class="math notranslate nohighlight">\(i=2, \ldots, p\)</span>, the <span class="math notranslate nohighlight">\(i\)</span>-th principal component has the largest variance among all linear combinations of <span class="math notranslate nohighlight">\(X_i\)</span>’s, which are uncorrelated with the first <span class="math notranslate nohighlight">\((i-1)\)</span> principal components.</p></li>
<li><p>The principal component preserve the total variance</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^{d} \operatorname{Var}\left(Z_{i}\right)=\sum_{i=1}^{d} \operatorname{Var}\left(X_{i}\right)
    \]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^{d} \lambda_{i}=\sum_{i=1}^{d} \sigma_{i i}
    \]</div>
<p>Hence, the PCA procedure, the total variance is preserved by re-allocated to <span class="math notranslate nohighlight">\(\operatorname{Var}\left( Z_1 \right) \ge \operatorname{Var}\left( Z_2 \right) \ge \ldots \operatorname{Var}\left( Z_d \right)\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \sum_{i=1}^{d} \sigma_{i i} &amp;=\operatorname{tr}(\boldsymbol{\Sigma}) \\
    &amp;=\operatorname{tr}\left(\sum_{i=1}^{d} \lambda_{i} \boldsymbol{u}_{i} \boldsymbol{u}_{i}^{\top}\right) \\
    &amp;=\sum_{i=1}^{d} \lambda_{i} \operatorname{tr}\left(\boldsymbol{u}_{i} \boldsymbol{u}_{i}^{\top}\right) \\
    &amp;=\sum_{i=1}^{d} \lambda_{i} \operatorname{tr}\left(\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{i}\right) \\
    &amp;=\sum_{i=1}^{d} \lambda_{i}
    \end{aligned}
    \end{split}\]</div>
</div>
</li>
<li><p>If the correlation matrix <span class="math notranslate nohighlight">\(\boldsymbol{\rho} = \boldsymbol{D}^{-1}\boldsymbol{\Sigma} \boldsymbol{D}^{-1}\)</span> instead of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is used, i.e. variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span> are standardized, then</p>
<div class="math notranslate nohighlight">
\[
   \sum_i^d \lambda_i = \sum_i^d \sigma_{ii} = \sum_i^d 1 =  d
   \]</div>
</li>
<li><p>The correlation between a principal component <span class="math notranslate nohighlight">\(Z_j\)</span> and an original variable <span class="math notranslate nohighlight">\(X_i\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Corr}\left( X_i, Z_j \right) = \frac{\sqrt{\lambda_j}a_{ij}}{\sqrt{\sigma_{ii}}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(u_{ij}\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>-th element of <span class="math notranslate nohighlight">\(\boldsymbol{u} _j\)</span>.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \operatorname{Cov}\left(X_{i}, Z_{j}\right) &amp;=\operatorname{Cov}\left(X_{i}, \boldsymbol{u}_{j}^{\top} \boldsymbol{x}\right) \\
    &amp;=\operatorname{Cov}\left(\boldsymbol{e}_{i}^{\top} \boldsymbol{x}, \boldsymbol{u}_{j}^{\top} \boldsymbol{x}\right) \\
    &amp;=\boldsymbol{e}_{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}_{j} \\
    &amp;=\boldsymbol{e}_{i}^{\top} \sum_{k=1}^{d} \lambda_{k} \boldsymbol{u}_{k} \boldsymbol{u}_{k}^{\top} \boldsymbol{u}_{j} \\
    &amp;=\lambda_{j} \boldsymbol{e}_{i}^{\top} \boldsymbol{u}_{j} \boldsymbol{u}_{j}^{\top} \boldsymbol{u}_{j} \\
    &amp;=\lambda_{j} \boldsymbol{e}_{i}^{\top} \boldsymbol{u}_{j} \\
    &amp;=\lambda_{j} u_{i j}
    \end{aligned}
    \end{split}\]</div>
<p>and then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \operatorname{Corr}\left(X_{i}, Z_{j}\right)
    &amp;=\frac{\operatorname{Cov}\left(X_{i}, Z_{j}\right)}{\sqrt{\operatorname{Var}\left(X_{i}\right) \operatorname{Var}\left(Z_{j}\right)}} \\
    &amp;=\frac{\lambda_{j} u_{i j}}{\sqrt{\sigma_{i i} \lambda_{j}}} \\
    &amp;=\frac{\sqrt{\lambda_{j}} u_{i j}}{\sqrt{\sigma_{i i}}}
    \end{align}\end{split}\]</div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="practical-issues">
<h2>Practical Issues<a class="headerlink" href="#practical-issues" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tuning">
<h3>Tuning<a class="headerlink" href="#tuning" title="Permalink to this headline">¶</a></h3>
<p>There are several ways to choose the number of principal components to retain.</p>
<ol>
<li><p><strong>Cumulative proportion cutoff</strong>:</p>
<p>Include the components such that the cumulative proportion of the total variance explained is just more than a threshold value, say 80%, i.e., if</p>
<div class="math notranslate nohighlight">
\[
    \begin{equation}
    \frac{\sum_{i=1}^{k} \operatorname{Var}\left( Z_i \right)}{\sum_{i=1}^{d} \operatorname{Var}\left( X_i \right)} &gt;0.8
    \end{equation}
    \]</div>
<p>This method keeps <span class="math notranslate nohighlight">\(m\)</span> principal components.</p>
</li>
<li><p><strong>Proportion cutoff</strong></p>
<p>Select the components whose eigenvalues are greater than a threshold value, say average of eigenvalues; for correlation matrix input, this average is <span class="math notranslate nohighlight">\(d^{-1} \sum_{i=1}^{d} \operatorname{Var}\left( Z_i \right)=d^{-1} d=1\)</span> if we use the correlation matrix <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span>.</p>
</li>
<li><p><strong>Scree plot</strong></p>
<p>Construct the so-called scree plot of the eigenvalue <span class="math notranslate nohighlight">\(\ell_i\)</span> on the vertical axis versus <span class="math notranslate nohighlight">\(i\)</span> on horizontal axis with equal intervals for <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, d\)</span>, and join the points into a decreasing polygon. Try to find a “clean-cut” where the polygon “levels off” so that the first few eigenvalues seem to be far apart from the others.</p>
<div class="myclass figure align-default" id="pca-scree-plot">
<a class="reference internal image-reference" href="../_images/pca-scree-plot.png"><img alt="" src="../_images/pca-scree-plot.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 73 </span><span class="caption-text">Scree plot of <span class="math notranslate nohighlight">\(\lambda\)</span>. [Fung 2021]</span><a class="headerlink" href="#pca-scree-plot" title="Permalink to this image">¶</a></p>
</div>
</li>
<li><p><strong>Hypothesis testing</strong></p>
<p>Perform formal significance tests to determine the larger an unequal eigenvalues and retain the principal components to these eigenvalues.</p>
</li>
<li><p><strong>Reconstruction loss</strong></p>
<p>Recall the principal component transform <span class="math notranslate nohighlight">\(\boldsymbol{z} = \boldsymbol{U}_{[:k]} ^\top \boldsymbol{x}\)</span>. Hence, to reconstruct <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x} }\)</span> by the first <span class="math notranslate nohighlight">\(k\)</span> components <span class="math notranslate nohighlight">\(\boldsymbol{u} _1, \ldots, \boldsymbol{u} _k\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> , we can use the expansion</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{x} }= \boldsymbol{U}_{[:k]} \boldsymbol{z} =\sum_{j=1}^{k}z_j \boldsymbol{u} _{j} = \sum_{j=1}^{k}\left(\boldsymbol{u}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{u} _{j}
    \]</div>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> was centered before PCA, we add the mean back</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{x} }=\boldsymbol{\mu} _{\boldsymbol{x}} +\sum_{j=1}^{k}\left(\boldsymbol{u}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{u} _{j}
    \]</div>
<p>To choose an optimal number of principal components <span class="math notranslate nohighlight">\(k\)</span>, we can examine the magnitude of the residual <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{x} - \hat{\boldsymbol{x} } \right\Vert ^2\)</span>. The expected residual corresponds to variance in the <strong>remaining</strong> subspace.</p>
<div class="myclass figure align-default" id="pca-reconstruction">
<a class="reference internal image-reference" href="../_images/pca-reconstruction.png"><img alt="" src="../_images/pca-reconstruction.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 74 </span><span class="caption-text">Reconstruction of digits with mean and principal components [Livescu 2021]</span><a class="headerlink" href="#pca-reconstruction" title="Permalink to this image">¶</a></p>
</div>
</li>
<li><p><strong>Downstream task performance</strong></p>
<p>Use the performance of the downstream task to choose an optimal number of principal components.</p>
</li>
</ol>
</div>
<div class="section" id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="geometric-meaning-direction-of-variation">
<h4>Geometric Meaning: Direction of Variation<a class="headerlink" href="#geometric-meaning-direction-of-variation" title="Permalink to this headline">¶</a></h4>
<p>For the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, thelcenter location is determined by <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ X\)</span> and the variation is captured by each principal direction <span class="math notranslate nohighlight">\(\boldsymbol{u} _i\)</span></p>
<p>For the multinormal distribution, the family of <strong>contours</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> (on each of which the pdf is a constant) is a family of ellipsoids in the original coordinate system <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> satisfying the following equation for a
constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})=c^{2}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> serves as an index of the family. This family of ellipsoids have orthogonal principal axes</p>
<div class="math notranslate nohighlight">
\[
\pm c\lambda_i^{1/2}\boldsymbol{u}_i, i= 1, 2, \ldots, p
\]</div>
<p>with length</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(2c\lambda_i^{1/2}\)</span></p></li>
<li><p>directional cosines as coefficients given in <span class="math notranslate nohighlight">\(\boldsymbol{u} _i\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>-th axis.</p></li>
</ul>
<div class="figure align-default" id="pca-gausian-ellipsoids">
<a class="reference internal image-reference" href="../_images/pca-pc-ellipsoids.png"><img alt="" src="../_images/pca-pc-ellipsoids.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 75 </span><span class="caption-text">PCA and Ellipsoids of Gaussian [Fung 2018]</span><a class="headerlink" href="#pca-gausian-ellipsoids" title="Permalink to this image">¶</a></p>
</div>
<p>Another example is hand written digits. Suppose <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ \boldsymbol{x}\)</span> is the sample mean that determines the “mean” appearance of the digit <span class="math notranslate nohighlight">\(2\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_j\)</span> is a principal direction which determines the location of variation of the black/white pixels.</p>
<div class="myclass figure align-default" id="pca-reconstruction-scale">
<a class="reference internal image-reference" href="../_images/pca-pc-digits.png"><img alt="" src="../_images/pca-pc-digits.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 76 </span><span class="caption-text">Reconstruction of digits with mean and scaled principal components [Livescu 2021]</span><a class="headerlink" href="#pca-reconstruction-scale" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="proportion-explained">
<h4>Proportion Explained<a class="headerlink" href="#proportion-explained" title="Permalink to this headline">¶</a></h4>
<p>The proportion of total variance explained by <span class="math notranslate nohighlight">\(Z_i\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda_i}{\sum_{j=1}^d \lambda_j}\]</div>
<p>is considered as a measure of <strong>importance</strong> of <span class="math notranslate nohighlight">\(Z_i\)</span> in a more parsimonious description of the system.</p>
</div>
<div class="section" id="score-of-an-observation-in-sample-data">
<h4>Score of an Observation in Sample Data<a class="headerlink" href="#score-of-an-observation-in-sample-data" title="Permalink to this headline">¶</a></h4>
<p>For a data set of <span class="math notranslate nohighlight">\(n\)</span> observations, we decompose the sample covariance matrix <span class="math notranslate nohighlight">\(S\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\boldsymbol{S} =\sum_{i=1}^{d} \ell_{i} \boldsymbol{u} _{i} \boldsymbol{u} _{i}^{\top}=\boldsymbol{U} \boldsymbol{L}  \boldsymbol{U} ^\top
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell_i\)</span> are eigencalues of <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{u} _i\)</span>’s are their corresponding normalized eigenvectors.</p>
<p>The <span class="math notranslate nohighlight">\(j\)</span>-th <strong>sample</strong> principal component is defined as</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
Z_{j}=\boldsymbol{u}_{j}^{\top} \boldsymbol{x}=u_{1 j} X_{1}+u_{2 j} X_{2}+\cdots+u_{d j} X_{d}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\begin{equation}
\boldsymbol{u}_{j}^{\top}=\left(\begin{array}{llll}
u_{1 j} &amp; u_{2 j} &amp; \cdots &amp; u_{d j}
\end{array}\right)
\end{equation}\)</span>.</p>
<p>The data layout is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{array}{cccccccc}
&amp;&amp; \text{Data} \ \ \boldsymbol{X}  &amp;&amp;&amp;&amp;\text{PC} \ \ \boldsymbol{Z} &amp;\\
\hline X_{1} &amp; X_{2} &amp; \cdots &amp; X_{d} &amp; \quad \quad Z_{1} &amp; Z_{2} &amp; \cdots &amp; Z_{k} \\
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 d} &amp; \quad \quad z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1 k} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 d} &amp; \quad \quad z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2 k} \\
&amp; &amp; \vdots &amp; &amp; &amp; &amp; \vdots &amp; \\
x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n d} &amp; \quad \quad z_{n 1} &amp; z_{n 2} &amp; \cdots &amp; z_{n k}
\end{array}
\end{equation}
\end{split}\]</div>
<p>where the corresponding row vectors on the data matrices are related as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{z} _i = \boldsymbol{U}^\top  \boldsymbol{x} _i , i= 1, 2, \ldots, n\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{z} _i\)</span> can be interpreted as a vector of principal component scores for the <span class="math notranslate nohighlight">\(i\)</span>-th observation.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Properties of the population principal components are all valid in the sample context, by replacing</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}, \boldsymbol{\Sigma} , \boldsymbol{\rho}, \lambda_i, \boldsymbol{u} _i\]</div>
<p>by</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{x}}, \boldsymbol{S} , \boldsymbol{R} , \ell_i, \boldsymbol{a} _i\]</div>
</div>
</div>
</div>
<div class="section" id="special-cases">
<h3>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">¶</a></h3>
<div class="section" id="variables-are-uncorrelated">
<h4>Variables are Uncorrelated<a class="headerlink" href="#variables-are-uncorrelated" title="Permalink to this headline">¶</a></h4>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{X_i}\)</span> are uncorrelated, then <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \operatorname{diag}\left( \sigma_{11}, \sigma_{22}, \ldots, \sigma_{dd} \right)\)</span>. Without loss of generality, assume <span class="math notranslate nohighlight">\(\sigma_{11} &gt; \sigma_{22} &gt; \ldots &gt; \sigma_{dd}\)</span>, then from its spectral decomposition <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \boldsymbol{U} ^\top \boldsymbol{\Lambda} \boldsymbol{U}\)</span>, we have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} = \boldsymbol{I}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \operatorname{diag}\left( \sigma_{ii} \right)\)</span>, or <span class="math notranslate nohighlight">\(\lambda_i = \sigma_{ii}\)</span>.</p></li>
</ul>
<p>Hence, the principal component is</p>
<div class="math notranslate nohighlight">
\[
Z_i = X_i
\]</div>
<p>Clearly, it is <strong>not</strong> necessary to perform PCA in this case.</p>
</div>
<div class="section" id="variables-are-perfectly-correlated">
<h4>Variables are Perfectly Correlated<a class="headerlink" href="#variables-are-perfectly-correlated" title="Permalink to this headline">¶</a></h4>
<p>In this case, the covariance matrix is not of full rank, i.e., <span class="math notranslate nohighlight">\(\left\vert \boldsymbol{\Sigma}  \right\vert = 0\)</span>. Then, some eiganvalues equal zero. In other words,</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 &gt; \lambda_2 &gt; \ldots &gt; \lambda_m &gt; \lambda_{m+1} = \ldots = \lambda_d = 0
\]</div>
<p>Only <span class="math notranslate nohighlight">\(m\)</span> eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{u} _i\)</span> can be obtained with <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{u}_i  \right\Vert _2 ^2 =  1\)</span> .</p>
</div>
<div class="section" id="few-variables-have-extremely-large-variances">
<h4>Few Variables Have Extremely Large Variances<a class="headerlink" href="#few-variables-have-extremely-large-variances" title="Permalink to this headline">¶</a></h4>
<p>If a few variables have extremely large variances in comparison with other variables, they will dominate the first few principal components and give the foregone conclusion that a few principal components is sufficient in summarizing information. That conclusion may even be spurious, as the measurement scales, which affect the variances, are quite arbitrary in a lot of applications.</p>
<p>For example, <span class="math notranslate nohighlight">\(X_1\)</span> is measured in meters while <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span> are measured in kilometers. The first PC should have particularly large variance (<span class="math notranslate nohighlight">\(\lambda_1\)</span> is particularly large relative to <span class="math notranslate nohighlight">\(\lambda_2\)</span> and <span class="math notranslate nohighlight">\(\lambda_3\)</span>). This property suggests that if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>  are on different, or non-commensurable, measurement units, we should <strong>standardize</strong> them,</p>
<div class="math notranslate nohighlight">
\[
Z_i = \frac{X_i - \mu_i}{\sigma_i}
\]</div>
<p>before performing PCA.</p>
</div>
</div>
<div class="section" id="cons">
<h3>Cons<a class="headerlink" href="#cons" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Sensitive to variable transformation</p>
<ul class="simple">
<li><p>The results of PCA are not invariant under a linear transformation and, even worse, there is no easy correspondence between the two sets of results <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z} ^\top\)</span>, before and after the linear transformation. For example, the PCA using <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is not the same as the PCA using <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span> and we cannot use the PCA from <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span> to get the PCA results from the original variables.</p></li>
<li><p>If the two sets of results are consistent to each other, the PCA based on <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>  may be preferred in some situation. If they are very different, or even contradictory, subject-matter knowledge and/or wisdom are needed to make a choice.</p></li>
<li><p>The PCA based on covariance matrix is preferred when the original measurements units are very important, like in many applications in natural sciences. However, when the units of measurement are of artificial nature, like scores in some questions as frequently used in social sciences, the PCA based on correlation matrix is preferred.</p></li>
</ul>
</li>
<li><p>Direction of variance may not be discriminative</p>
<p>But note that the direction of largest variance need not to be the most discriminative direction. See the example below.</p>
<div class="myclass figure align-default" id="pca-not-discriminative">
<a class="reference internal image-reference" href="../_images/pca-classification.png"><img alt="" src="../_images/pca-classification.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 77 </span><span class="caption-text">PCA may not be discriminative [Livescu 2021]</span><a class="headerlink" href="#pca-not-discriminative" title="Permalink to this image">¶</a></p>
</div>
<p>If we knew the labels, we could use a supervised dimensionality reduction, e.g. linear discriminant analysis.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="relation-to">
<h2>Relation to<a class="headerlink" href="#relation-to" title="Permalink to this headline">¶</a></h2>
<div class="section" id="svd">
<h3>SVD<a class="headerlink" href="#svd" title="Permalink to this headline">¶</a></h3>
<p>Recall the SVD of the data matrix</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}  = \boldsymbol{U} \boldsymbol{D} \boldsymbol{V} ^\top
\]</div>
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span> is centered, then <span class="math notranslate nohighlight">\(n\)</span> times the sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
n\boldsymbol{S} = \boldsymbol{X} ^\top \boldsymbol{X} = \boldsymbol{V} \boldsymbol{D} ^\top \boldsymbol{D} \boldsymbol{V} ^\top
\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\boldsymbol{S} = \boldsymbol{V} \left( \frac{1}{n} \boldsymbol{D} ^{\top} \boldsymbol{D}  \right) \boldsymbol{V} ^{\top}\)</span>,</p>
<ul class="simple">
<li><p>the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> are the right singular vectors <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>,</p></li>
<li><p>the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> are proportional to the squared singular values of <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p></li>
</ul>
<p>So we can compute the PCA solutions via an SVD of data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
</div>
<div class="section" id="compression">
<h3>Compression<a class="headerlink" href="#compression" title="Permalink to this headline">¶</a></h3>
<p>Instead of storing the <span class="math notranslate nohighlight">\(n \times d\)</span> data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, not we need to store the <span class="math notranslate nohighlight">\(d \times 1\)</span> mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ \boldsymbol{x}\)</span> and the <span class="math notranslate nohighlight">\(k\times d\)</span> projection matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, and the <span class="math notranslate nohighlight">\(n \times k\)</span> projected data matrix <span class="math notranslate nohighlight">\(\boldsymbol{Z}\)</span>.</p>
<p>To transmit <span class="math notranslate nohighlight">\(n\)</span> examples, we need <span class="math notranslate nohighlight">\(d+dk+nk\)</span> numbers instead of <span class="math notranslate nohighlight">\(nd\)</span>.</p>
</div>
<div class="section" id="gaussians">
<h3>Gaussians<a class="headerlink" href="#gaussians" title="Permalink to this headline">¶</a></h3>
<p>PCA essentially models variance in the data. What distribution is characterized by variance? Gaussian. The covariance matrix parameter in <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma} )\)</span> has EVD</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}=\boldsymbol{R}\left[\begin{array}{lll}
\lambda_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \lambda_{d}
\end{array}\right] \boldsymbol{R}^{\top}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> determines the orientation of the ellipsoid, and the eigenvalues specifies the scaling along the principal directions. The PCA solution <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> from the sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> should be close to <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>.</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>For a classification task, we can perform PCA on the features before fitting the data to a classifier. The classifier might be more accurate since PCA reduces noise.</p>
</div>
<div class="section" id="autoencoders">
<span id="pca-autoencoder"></span><h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<p>PCA can be viewed as an <a class="reference internal" href="../37-neural-networks/11-autoencoders.html"><span class="doc std std-doc">autoencoder</span></a> of one single layer with certain constraints.</p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k(\boldsymbol{x}, \boldsymbol{w} )\)</span> be a kernel node parameterized by <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> and output the kernel value.</p></li>
<li><p><span class="math notranslate nohighlight">\(w_{ij}\)</span> be the weight of the edge from the <span class="math notranslate nohighlight">\(i\)</span>-th input node to the <span class="math notranslate nohighlight">\(j\)</span>-th hidden node</p></li>
<li><p><span class="math notranslate nohighlight">\(v_{ij}\)</span> be the weight of the edge from the <span class="math notranslate nohighlight">\(i\)</span>-th hidden node to the <span class="math notranslate nohighlight">\(j\)</span>-th output node</p></li>
</ul>
<p>Recall that the projected vector in PCA is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z} = \boldsymbol{W} ^\top \boldsymbol{x}
\]</div>
<p>and the reconstruction is</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{x}} = \boldsymbol{W} \boldsymbol{z}
\]</div>
<p>The objective is to minimize the total reconstruction loss</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{W}^{*}= \underset{\boldsymbol{W}}{\operatorname{argmin}}  &amp;\ \sum_{i}^{n}\left\|\boldsymbol{x}_{i}-\hat{\boldsymbol{x}}_{i}\right\|^{2} \\
=\underset{\boldsymbol{W}}{\operatorname{argmin}} &amp;\ \sum_{i}^{n}\left\|\boldsymbol{x}_{i}-\boldsymbol{W} \boldsymbol{z}_{i}\right\|^{2} \\
\operatorname{s.t.} &amp;\boldsymbol{W}^{\top} \boldsymbol{W}=\boldsymbol{I} \\
&amp;\ \boldsymbol{W} \in \mathbb{R}_{d \times k}
\end{aligned}
\end{split}\]</div>
<p>Hence we can construct a neural network as follows</p>
<ul class="simple">
<li><p>Input layer:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d\)</span> nodes, which represent <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R} ^d\)</span></p></li>
</ul>
</li>
<li><p>Hidden layer:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(k\)</span> nodes, which represent <span class="math notranslate nohighlight">\(\boldsymbol{z} \in \mathbb{R} ^k\)</span></p></li>
<li><p>The weights <span class="math notranslate nohighlight">\(w_{ij}\)</span> are the entires in the matrix <span class="math notranslate nohighlight">\(\boldsymbol{W} \in \mathbb{R}_{d \times k}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}\)</span></p></li>
<li><p>The activation function is simply the identity function</p></li>
</ul>
</li>
<li><p>Output layer:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d\)</span> nodes, which represent <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x}} \in \mathbb{R} ^d\)</span></p></li>
<li><p>The weights <span class="math notranslate nohighlight">\(v_{ij} = w_{ji}\)</span></p></li>
<li><p>The activation function is simply the identity function</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="extension">
<h2>Extension<a class="headerlink" href="#extension" title="Permalink to this headline">¶</a></h2>
<p>We introduce an extension of PCA: probabilistic PCA. For another extension Kernel PCA, see <a class="reference internal" href="31-kernel-pca.html#kernel-pca"><span class="std std-ref">here</span></a>.</p>
<p><em>Independently proposed by [Tipping &amp; Bishop 1997, 1999] and [Roweis 1998]</em></p>
<p>Probabilistic PCA adds a probabilistic component (interpretation) to the PCA model. It provides</p>
<ul class="simple">
<li><p>a way of approximating a Gaussian using fewer parameters (e.g. common noise variance).</p></li>
<li><p>a way of sampling from the data distribution as a probabilistic model (thus aka sensible PCA).</p></li>
</ul>
<div class="section" id="id1">
<h3>Objective<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>In a PPCA model, we first draw low dimensional <span class="math notranslate nohighlight">\(\boldsymbol{z} \in \mathbb{R}^{k}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{z}) =\mathcal{N}( \boldsymbol{0}, \boldsymbol{I})
\]</div>
<p>and draw <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^{d}, k \leq d\)</span> by</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{x} \mid \boldsymbol{z}) =\mathcal{N}\left( \boldsymbol{W} \boldsymbol{z}+\boldsymbol{\mu} , \sigma^{2} \boldsymbol{I}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{W} \in \mathbb{R} ^{d \times k}\)</span></p>
<p>Or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\boldsymbol{x}=\boldsymbol{W} \boldsymbol{z}+\boldsymbol{\mu} + \boldsymbol{\epsilon} , \text { where } \boldsymbol{\epsilon}  \sim \mathcal{N}\left(0, \sigma^{2} \boldsymbol{I}\right)
\end{equation}
\]</div>
<p>If <span class="math notranslate nohighlight">\(\sigma = 0\)</span> then we get standard PCA.</p>
<p>By the property of multivariate Gaussian, we have</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{\mu} , \boldsymbol{W} \boldsymbol{W}^{\top} +\sigma^{2} \boldsymbol{I}\right)
\]</div>
<p>The goal is to estimate the parameter <span class="math notranslate nohighlight">\(\boldsymbol{W} , \boldsymbol{\mu} , \sigma\)</span> that maximize the log likelihood <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} \log p\left(\boldsymbol{x}_{i} \mid \boldsymbol{W} , \boldsymbol{\mu} , \sigma\right)\)</span>.</p>
</div>
<div class="section" id="learning-mle">
<h3>Learning (MLE)<a class="headerlink" href="#learning-mle" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">MLE not unique</p>
<p>Before seeking the ML solution, notice that the solution is not unique: if <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is an orthogonal matrix, then <span class="math notranslate nohighlight">\(\widetilde{\boldsymbol{W}} = \boldsymbol{W} \boldsymbol{\boldsymbol{R}}\)</span> is indistinguishable from <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span></p>
<div class="math notranslate nohighlight">
\[
\widetilde{\boldsymbol{W}} \widetilde{\boldsymbol{W}} ^{\top}=\boldsymbol{W} (\boldsymbol{R} \boldsymbol{R} ^{\top}) \boldsymbol{W} ^{\top}  =\boldsymbol{W} \boldsymbol{W} ^{\top}
\]</div>
<p>So we will find a solution <span class="math notranslate nohighlight">\(W _{ML}\)</span> up to a rotation <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>.</p>
</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{C}  = \boldsymbol{W} \boldsymbol{W} ^\top + \sigma^2 \boldsymbol{I}_d\)</span>. The log likelihood function is</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\sum_{i=1}^{n} \log p\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, \mu, \sigma^{2}\right) =
-\frac{n d}{2} \log (2 \pi)-\frac{n}{2} \log |\boldsymbol{C}|-\frac{1}{2} \sum_{i=1}^{n}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu} \right) ^{\top}  \boldsymbol{C}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu} \right)
\end{equation}
\]</div>
<p>Setting the derivative w.r.t. <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> to <span class="math notranslate nohighlight">\(\boldsymbol{0} \)</span> we have</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu} _{ML} = \bar{\boldsymbol{x}}\]</div>
<p>i.e. the sample mean. The solution for <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is more complicated, but closed form.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{aligned}
\boldsymbol{W}_{M L} &amp;=\boldsymbol{U}_{d \times k}\left(\boldsymbol{\Lambda} _{k}-\sigma^{2} \boldsymbol{I}_k\right)^{1 / 2} \boldsymbol{R}_k \\
\sigma_{M L}^{2} &amp;=\frac{1}{d-k} \sum_{j=k+1}^{d} \lambda_{j}
\end{aligned}
\end{equation}
\end{split}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">EM also works</p>
<p>It is also possible to find the PPCA solution iteratively, visa the EM algorithm. This is useful if doing the eigenvalue decomposition is too computationally demanding.</p>
</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} _{d \times k}\)</span> is the first <span class="math notranslate nohighlight">\(k\)</span> eigenvectors of the sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_k\)</span> is the diagonal matrix of eigenvalues</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{R}_k\)</span> is an arbitrary orthogonal matrix</p></li>
</ul>
</div>
<div class="section" id="id2">
<h3>Properties<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>For <span class="math notranslate nohighlight">\(\boldsymbol{R}_k = \boldsymbol{I}_k\)</span> , the solution for <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> is just a scaled version (by the diagonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} _k - \sigma^2 \boldsymbol{I} _k\)</span>) of that of standard PCA <span class="math notranslate nohighlight">\(U_{d\times k}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2_{ML}\)</span> is the average variance of the discarded dimensions in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. We view the remaining dimensions as accounting for noise. Their average variance defines the common variance of the noise. The covariance is viewed as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \boldsymbol{\Sigma}=\boldsymbol{U}\left[\begin{array}{cccccccc}
    \lambda_{1} &amp; \ldots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \\
    &amp; \ddots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \\
    0 &amp; \ldots &amp; \lambda_{k} &amp; \ldots &amp; \ldots &amp; \ldots \\
    0 &amp; \ldots &amp; 0 &amp; \sigma^{2} &amp; 0 &amp; \ldots \\
    &amp; &amp; &amp; &amp; \ddots &amp; \\
    0 &amp; \ldots &amp; \ldots &amp; \ldots &amp; 0 &amp; \sigma^{2}
    \end{array}\right] \boldsymbol{U} ^\top
    \end{split}\]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(k = d\)</span>, i.e., no dimension reduction, then the MLE for the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is equal to <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>, which is just the standard ML solution for a Gaussian distribution.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C}_{ML} = \boldsymbol{W} _{ML} \boldsymbol{W} _{ML} ^\top + \sigma^2 \boldsymbol{I}  = \boldsymbol{U} (\boldsymbol{\Lambda} - \sigma^2 I) \boldsymbol{U} ^\top  + \sigma^2 \boldsymbol{I}   = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top  = \boldsymbol{S}.
\]</div>
</div>
<div class="section" id="representation">
<h3>Representation<a class="headerlink" href="#representation" title="Permalink to this headline">¶</a></h3>
<p>The conditional distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> given <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{z} \mid \boldsymbol{x})=\mathcal{N}\left(\boldsymbol{M}^{-1} \boldsymbol{W} ^{\top} (\boldsymbol{x}- \boldsymbol{\mu} ), \sigma^{2} \boldsymbol{M}^{-1}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{M} = \boldsymbol{W} ^\top \boldsymbol{W}  + \sigma^2 \boldsymbol{I}_k\)</span>.</p>
<p>A reduced-dimensionality representation of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is given by the estimated conditional mean</p>
<div class="math notranslate nohighlight">
\[
\widehat{\operatorname{E}}\left( \boldsymbol{z} \mid \boldsymbol{x}   \right) = \boldsymbol{M}  ^{-1} _{ML} \boldsymbol{W} ^\top _{ML}(\boldsymbol{x} - \bar{\boldsymbol{x}})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{M} _{ML} = \boldsymbol{W} _{ML} ^\top \boldsymbol{W} _{ML}  + \sigma^2 _{ML} \boldsymbol{I}_k\)</span>.</p>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\(\sigma^2 _{ML} \rightarrow 0\)</span>, the posterior mean approaches the standard PCA projection <span class="math notranslate nohighlight">\(\boldsymbol{ z } =  \boldsymbol{U}  ^\top (\boldsymbol{x}  - \bar{\boldsymbol{x} })\)</span></p></li>
<li><p>As <span class="math notranslate nohighlight">\(\sigma^2 _{ML}&gt; 0\)</span>, the posterior mean “shrinks” the solution in magnitude from standard PCA. Since we are less certain about the representation when the noise is large.</p></li>
</ul>
</div>
</div>
<div class="section" id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="large-sample-inference">
<h3>Large Sample Inference<a class="headerlink" href="#large-sample-inference" title="Permalink to this headline">¶</a></h3>
<p>Assume that <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \overset{\text{iid}}{\sim}\mathcal{N} _p(\boldsymbol{\mu} , \boldsymbol{\Sigma} )\)</span>, let <span class="math notranslate nohighlight">\((\lambda_i, \boldsymbol{u} _i)\)</span> and <span class="math notranslate nohighlight">\((\hat{\lambda}_i, \hat{\boldsymbol{u}} _i)\)</span> be respectively the eigen pair of population covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> and of sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>. Let <span class="math notranslate nohighlight">\(d_i = \sqrt{n-1} (\hat{\lambda}_i - \lambda_i)\)</span> be a difference measure between the two eigenvalues, and <span class="math notranslate nohighlight">\(\boldsymbol{z} _i = \sqrt{n-1}(\hat{\boldsymbol{u}} _i - \boldsymbol{u} _i)\)</span> be a difference measure between the two eigenvectors, then we have the following asymptotic results as <span class="math notranslate nohighlight">\((n - 1 - p) \rightarrow \infty\)</span>,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\([d_1, \ldots, d_p]\)</span> is independent of <span class="math notranslate nohighlight">\([\boldsymbol{z} _1, \ldots, \boldsymbol{z} _p]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_i\)</span>’s are independently <span class="math notranslate nohighlight">\(\mathcal{N} (0, 2\lambda_i^2)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{z} _i \sim \mathcal{N} _p(\boldsymbol{0} , \sum_{k=1 \atop k \neq i}^{p} \frac{\lambda_{i} \lambda_{k}}{\left(\lambda_{i}-\lambda_{k}\right)^{2}} \boldsymbol{u}_{k} \boldsymbol{u}_{k}^{\top})\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Cov}\left(\boldsymbol{z}_{i}, \boldsymbol{z}_{j}\right)=-\frac{\lambda_{i} \lambda_{j}}{\left(\lambda_{i}-\lambda_{j}\right)^{2}} \boldsymbol{u}_{j} \boldsymbol{u}_{i}^{\top}\)</span></p></li>
</ul>
</div>
<div class="section" id="identifiability">
<h3>Identifiability<a class="headerlink" href="#identifiability" title="Permalink to this headline">¶</a></h3>
<p>PCA use EVD of sample covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>. As <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>, by the Law of Large numbers, <span class="math notranslate nohighlight">\(\boldsymbol{S} \rightarrow \boldsymbol{\Sigma}\)</span> hence its eigenvalues also converges to those of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, i.e. we are able to recover the signal subspace. However, in high dimensional setting where <span class="math notranslate nohighlight">\(p\)</span> is large, is this still true?</p>
<p>Let <span class="math notranslate nohighlight">\(\gamma = \lim_{n ,p \rightarrow \infty} \frac{p}{n}\)</span>. If <span class="math notranslate nohighlight">\(\gamma &gt; 0\)</span>, the top eigenvectors of sample covariance matrices might not reflect the subspace of signals. In fact, there is a threshold of signal-noise ratio.</p>
<ul class="simple">
<li><p>below a threshold of SNR, PCA fails w.h.p.</p></li>
<li><p>above SNR, PCA approximate the signal subspace w.h.p.</p></li>
</ul>
<p>We illustrate this using a simplest rank-1 (spike) signal model. Suppose a <span class="math notranslate nohighlight">\(p\)</span>-variate random vector consists of signal and noise</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x} = g_0 \boldsymbol{u} + \boldsymbol{g}  
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{u} \in \mathbb{R} ^p\)</span> is some fixed signal <strong>direction</strong>, w.l.o.g. set <span class="math notranslate nohighlight">\(\left\| \boldsymbol{u}  \right\| = 1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(g_0 \sim \mathcal{N} (0, \beta)\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> control randomness and variation (<strong>energy</strong>) of signal</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{g} \sim \mathcal{N} (\boldsymbol{0} , \sigma^2 _{\epsilon}\boldsymbol{I} _p)\)</span> is white noise, independent of <span class="math notranslate nohighlight">\(g_0\)</span>.</p></li>
</ul>
<p>Can we recover signal direction <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> from principal component analysis on noisy measurements <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>? First note the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is still Gaussian, with mean and variance</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E} [\boldsymbol{x}] &amp;= \boldsymbol{0} \\
\boldsymbol{\Sigma}
&amp;= \beta \boldsymbol{u} \boldsymbol{u} ^{\top} + \sigma^2 _{\epsilon}\boldsymbol{I} _p \\
\end{aligned}\end{split}\]</div>
<p>It is easy to see that the first eigen pair of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is <span class="math notranslate nohighlight">\((\beta +\sigma^2 _\epsilon, \boldsymbol{u})\)</span> and the other eigenvalues are all 1.</p>
<p>How the size of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 _\epsilon\)</span> affect the identifiability of the signal direction <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>? Define the signal-noise ratio</p>
<div class="math notranslate nohighlight">
\[
SNR = \frac{\beta}{\sigma^2 _{\epsilon}}
\]</div>
<p>For simplicity we assume that <span class="math notranslate nohighlight">\(\sigma^2 _{\epsilon}=1\)</span> w.l.o.g. Then <span class="math notranslate nohighlight">\(SNR = \beta\)</span>. Intuitively, if <span class="math notranslate nohighlight">\(\beta\)</span> is small, then the sample variance matrix is like meaningless <span class="math notranslate nohighlight">\(\boldsymbol{I} _p\)</span>. PCA fail to distinguish <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> from noise <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>. Pictorially, when <span class="math notranslate nohighlight">\(p=2\)</span>, the signal and data points are shown below. If <span class="math notranslate nohighlight">\(\beta\)</span> is too small, then it is hard to distinguish the signal direction from the data points. As <span class="math notranslate nohighlight">\(\beta\)</span> increases, the overall data points suggests a direction aligned with the signal direction.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">u1</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">=</span> <span class="n">u3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">g0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">signal1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">b1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g0</span> <span class="o">*</span> <span class="n">u1</span>
<span class="n">signal2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">b2</span><span class="p">)</span> <span class="o">*</span> <span class="n">g0</span> <span class="o">*</span> <span class="n">u2</span>
<span class="n">signal3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">b3</span><span class="p">)</span> <span class="o">*</span> <span class="n">g0</span> <span class="o">*</span> <span class="n">u3</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">signal1</span> <span class="o">+</span> <span class="n">g</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">signal2</span> <span class="o">+</span> <span class="n">g</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">signal3</span> <span class="o">+</span> <span class="n">g</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta=0.25$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">signal1</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">signal1</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;signal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x1</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;observed&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">bottom</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">right</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">top</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta=1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">signal2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">signal2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">bottom</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">right</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">top</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta=4$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">signal3</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">signal3</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x3</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x3</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">left</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">bottom</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">right</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">top</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11-principal-component-analysis_1_0.png" src="../_images/11-principal-component-analysis_1_0.png" />
</div>
</div>
<p>We are interested in high-dimensional setting <span class="math notranslate nohighlight">\(n, p \rightarrow \infty\)</span>, under what conditions of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span> can we detect <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> via PCA from the observed data <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. Let <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\Sigma}}_{n} = \frac{1}{n} \boldsymbol{X} ^{\top} \boldsymbol{X}\)</span> and <span class="math notranslate nohighlight">\((\hat{\lambda}, \hat{\boldsymbol{u}})\)</span> be its top eigen pair, here detection means</p>
<ul class="simple">
<li><p>the top eigenvalue <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> due to signal <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> is distinguishable from those due to noise <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span></p></li>
<li><p>the estimated signal direction <span class="math notranslate nohighlight">\(\hat{\boldsymbol{u}}\)</span> is close to the true signal direction <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>, measured by <span class="math notranslate nohighlight">\(\langle \hat{\boldsymbol{u}} , \boldsymbol{u} \rangle \ne 0\)</span> w.h.p.</p></li>
</ul>
<div class="section" id="phase-transition">
<h4>Phase Transition<a class="headerlink" href="#phase-transition" title="Permalink to this headline">¶</a></h4>
<p>With random matrix theory, using the <a class="reference internal" href="../11-math/21-linear-algebra.html#marchenko-pastur-distribution"><span class="std std-ref">Marchenko-pastur Distribution</span></a>, we have the following conclusions, as <span class="math notranslate nohighlight">\(n, p \rightarrow \infty\)</span>:</p>
<ul>
<li><p>The largest eigenvalue <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \lambda_{\max }\left(\widehat{\boldsymbol{\Sigma}}_{n}\right) \rightarrow\left\{\begin{array}{ll}
  (1+\sqrt{\gamma})^{2}=\gamma_{+}, &amp; \beta \leq \sqrt{\gamma} \\
  \left(1+\beta\right)\left(1+\frac{\gamma}{\beta}\right) &gt; \gamma_{+}, &amp; \beta&gt;\sqrt{\gamma}
  \end{array}\right.
  \end{split}\]</div>
<ul class="simple">
<li><p>if signal energy (or SNR) <span class="math notranslate nohighlight">\(\beta\)</span> is smaller than <span class="math notranslate nohighlight">\(\sqrt{\gamma}\)</span>, the top eigenvalue of sample covariance matrix never ‘pops up’ from those of noise random matrix <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \boldsymbol{g}  \right)\)</span>, which follows M-P distribution in range <span class="math notranslate nohighlight">\([\gamma_{-}, \gamma_{+}]\)</span>. That is, we don’t know whether it is due to signal <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> or due to noise <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>.</p></li>
<li><p>only if the signal energy <span class="math notranslate nohighlight">\(\beta\)</span> is beyond the phase transition threshold <span class="math notranslate nohighlight">\(\sqrt{\gamma}\)</span>, the top eigenvalue can be separated from noise random matrix eigenvalues. However, even in the latter case it is a <strong>biased</strong> estimation of the top eigenvalue <span class="math notranslate nohighlight">\((1 + \beta)\)</span>.</p></li>
</ul>
</li>
<li><p>The estimated signal direction <span class="math notranslate nohighlight">\(\hat{\boldsymbol{u} }\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \left|\left\langle \boldsymbol{u} , \hat{\boldsymbol{u} }\right\rangle\right|^{2} \rightarrow\left\{\begin{array}{ll}
  0 &amp; \beta \leq \sqrt{\gamma} \\
  \frac{1-\gamma/\beta^2}{1+\gamma/\beta^2}  &amp; \beta&gt;\sqrt{\gamma}
  \end{array}\right.
  \end{split}\]</div>
<ul class="simple">
<li><p>if signal is of low energy <span class="math notranslate nohighlight">\((\beta \le \sqrt{\gamma})\)</span>, the estimated top eigenvector is <strong>orthogonal</strong> to the true direction <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>. PCA will tell us nothing about the true signal. In the extreme case <span class="math notranslate nohighlight">\(\beta = 0\)</span>, the largest eigenvector returned by PCA is just that from <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( \boldsymbol{g}  \right) = \boldsymbol{I} _p\)</span>, which is purely a random direction.</p></li>
<li><p>if the signal is of high energy <span class="math notranslate nohighlight">\((\beta \ge \sqrt{\gamma})\)</span>, PCA will return a <strong>biased</strong> estimation which lies over a lateral surface of a <strong>cone</strong> whose angle with the true signal is <span class="math notranslate nohighlight">\(\arccos \left( \sqrt{\frac{1-\gamma/\beta^2}{1+\gamma/\beta^2} } \right)\)</span>.</p></li>
</ul>
</li>
</ul>
<p>For derivation when <span class="math notranslate nohighlight">\(\beta &gt; \sqrt{\gamma}\)</span> case, see Yao’s <a class="reference external" href="https://github.com/yao-lab/yao-lab.github.io/blob/master/book_datasci.pdf">notes</a> (there are some typos). For the limiting distribution of <span class="math notranslate nohighlight">\(\lambda_{max}\)</span>, see <a class="reference external" href="https://arxiv.org/pdf/math/0611589.pdf">Johnstone</a> p.16-17.</p>
<p>Key techniques in Yao’s notes:</p>
<ul class="simple">
<li><p>Use ‘whitening’ <span class="math notranslate nohighlight">\(\boldsymbol{Z} = \boldsymbol{\Sigma} ^{-1/2} \boldsymbol{X} \sim \mathcal{N} (\boldsymbol{0} , \boldsymbol{I} _p)\)</span> and then <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma} }_{n}=\frac{1}{n} \boldsymbol{X} \boldsymbol{X} ^{T}=\boldsymbol{\Sigma} ^{1 / 2}\left(\frac{1}{n} \boldsymbol{Z} \boldsymbol{Z} ^{T}\right) \boldsymbol{\Sigma} ^{1 / 2}\)</span> to relate the eigenvalue <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma} }_n\)</span> with M-P distribution for eigenvalues of <span class="math notranslate nohighlight">\(\frac{1}{n} \boldsymbol{Z} \boldsymbol{Z} ^{T}\)</span>.</p></li>
<li><p>Use integration to approximate infinite summation, if</p>
<ul>
<li><p>The summation can be expressed as the expectation of some continuous random variable with known distribution function</p></li>
<li><p>The number of terms in the summation, denoted <span class="math notranslate nohighlight">\(p\)</span>, is large enough, <span class="math notranslate nohighlight">\(p \rightarrow \infty\)</span></p></li>
<li><p>No term ‘explode’ to <span class="math notranslate nohighlight">\(\infty\)</span>. For instance, for the summation <span class="math notranslate nohighlight">\(\sum_{i=1}^p \frac{c}{\lambda - \lambda_j}\)</span> where <span class="math notranslate nohighlight">\(\lambda_j \sim f_{MP}\)</span> over <span class="math notranslate nohighlight">\([\gamma_-, \gamma_+]\)</span>, if <span class="math notranslate nohighlight">\(\lambda \in [\gamma_-, \gamma_+]\)</span>, then as <span class="math notranslate nohighlight">\(p \rightarrow \infty\)</span>, some denominator <span class="math notranslate nohighlight">\(\lambda - \lambda_j\)</span> will be infinitely small, and that term explode.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="comparison-to-davis-kahan-theorem">
<h4>Comparison to Davis-Kahan Theorem<a class="headerlink" href="#comparison-to-davis-kahan-theorem" title="Permalink to this headline">¶</a></h4>
<p>Recall that <span class="math notranslate nohighlight">\(\boldsymbol{x}_i = g_0 \boldsymbol{u} + \boldsymbol{g} _i\)</span>. If we use <a class="reference internal" href="../11-math/21-linear-algebra.html#davis-kahan"><span class="std std-ref">Davis-Kahan theorem</span></a>, by viewing</p>
<ul class="simple">
<li><p>truth: <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \beta \boldsymbol{u} \boldsymbol{u} ^{\top} + \boldsymbol{I} _p\)</span></p></li>
<li><p>noise: <span class="math notranslate nohighlight">\(\boldsymbol{H} = \frac{1}{n} \sum_{i=1}^n  \boldsymbol{g}_i \boldsymbol{g}_i ^{\top}\)</span></p></li>
<li><p>observed: <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\Sigma}} _n = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}_i \boldsymbol{x}_i ^{\top}\)</span></p></li>
</ul>
<p>The distance between the first eigenvector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{u}}\)</span> of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\Sigma}} _n\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> of the truth <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\operatorname{dist}(\hat{\boldsymbol{u}}, \boldsymbol{u})=\left\|\hat{\boldsymbol{u}} \hat{\boldsymbol{u}}^{\top}-\boldsymbol{u} \boldsymbol{u}^{\top}\right\|_{2} \leq \frac{\|\boldsymbol{H}\|}{\lambda_1(\boldsymbol{\Sigma}) - \lambda_2(\boldsymbol{\Sigma} )-\|\boldsymbol{H} \|} = \frac{\gamma_{+}}{\lambda_1 - \lambda_2 - \gamma_{+}}
\]</div>
<p>where the spectral norm <span class="math notranslate nohighlight">\(\|\frac{1}{n} \boldsymbol{g}_i \boldsymbol{g}_i ^{\top}\| = \gamma_{+}\)</span> since the upper bound of the M-P distribution is <span class="math notranslate nohighlight">\(\gamma_{+}\)</span>.</p>
<p>If we want <span class="math notranslate nohighlight">\(|\langle \boldsymbol{u} ,  \hat{\boldsymbol{u}}\rangle| ^2 &gt; c\)</span>, then it is equivalent to <span class="math notranslate nohighlight">\(\left\|\hat{\boldsymbol{u}} \hat{\boldsymbol{u}}^{\top}-\boldsymbol{u} \boldsymbol{u}^{\top}\right\|_{2}^2 &lt; 1-c\)</span> since they <a class="reference internal" href="../11-math/21-linear-algebra.html#norm"><span class="std std-ref">sum up to</span></a> 1. That is, the denominator has some lower bound, i.e. <span class="math notranslate nohighlight">\(\lambda_1 - \lambda_2 - \gamma_{+}&gt;  b\)</span>. In the spike model, <span class="math notranslate nohighlight">\(\lambda_1 - \lambda_2 = (1+\beta) - 1 = \beta\)</span>. Hence, the condition for <span class="math notranslate nohighlight">\(\beta\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\beta &gt; (1 + \sqrt{\gamma})^2 + b
\]</div>
<p>This condition is stronger than the above result: <span class="math notranslate nohighlight">\(\beta &gt; \sqrt{\gamma}\)</span> is ok.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./33-dimensionality-reduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="00-dimensionality-reduction.html" title="previous page">Dimensionality Reduction</a>
    <a class='right-next' id="next-link" href="13-canonical-correlation-analysis.html" title="next page">Canonical Correlation Analysis</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
    
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-150740237-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>