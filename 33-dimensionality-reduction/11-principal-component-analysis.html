
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Principal Component Analysis &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Canonical Corerlation Analysis" href="13-canonical-correlation-analysis.html" />
    <link rel="prev" title="Dimensionality Reduction" href="00-dimensionality-reduction.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/51-optimization.html">
     Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/25-two-sample-tests.html">
     Two Sample Mean Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Regression - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-extension.html">
     Linear Regression - Extension
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="25-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-t-SNE.html">
     SNE and $t$-SNE
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     K-means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/33-dimensionality-reduction/11-principal-component-analysis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F33-dimensionality-reduction/11-principal-component-analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning">
   Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequential-maximization">
     Sequential Maximization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     Spectral Decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties">
   Properties
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning">
   Tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#geometric-meaning-direction-of-variation">
     Geometric Meaning: Direction of Variation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proportion-explained">
     Proportion Explained
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#score-of-an-observation-in-sample-data">
     Score of an Observation in Sample Data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-cases">
   Special Cases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-are-uncorrelated">
     Variables are Uncorrelated
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-are-perfectly-correlated">
     Variables are Perfectly Correlated
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#few-variables-have-extremely-large-variances">
     Few Variables Have Extremely Large Variances
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cons">
   Cons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relation-to">
   Relation to
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svd">
     SVD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compression">
     Compression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussians">
     Gaussians
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extension-probabilistic-pca">
   Extension: Probabilistic PCA
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-mle">
     Learning (MLE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Properties
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation">
     Representation
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h1>
<p>Proposed by Pearson in 1901 and further developed by Hotelling in 1993.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Given $X_1, X_2, \ldots, X_d$, we want to extract the most useful information of $p$ measurements such that</p>
<ol class="simple">
<li><p><strong>explore underlying dimension</strong> behind the $p$ original measurements to explain the variation of $p$ original measurements, which may have interesting or revealing interpretations, such as size, shape and contrasts in natural science;</p></li>
<li><p><strong>estimate latent variables</strong> (i.e. variables that cannot be measured or observed.) which can explain the variation of the $p$ original measurements, especially in
social behavioral sciences.</p></li>
<li><p><strong>simplify the dimension</strong> of the observed data set. Lower dimension can be chosen from the data set such that the variations of measurements can be captured with an acceptable level. For example, $k \ll d$ latent variables are chosen to capture 90% of variation of $p$ original measurements. Indeed, this can be regarded as the data reduction or dimension reduction.</p></li>
</ol>
<p>Consider a $d$-dimensional random vector $\boldsymbol{x} = \left( X_1, X_2, \ldots, X_d \right)^\top$ with mean vector $\boldsymbol{\mu} = \left( \mu_1, \ldots, \mu_d \right)^\top$ and covariance matrix $\boldsymbol{\Sigma}$. PCA aimes to obtain the variables $Z_1, Z_2, \ldots, Z_k$ which are the <strong>linear combinations</strong> of $X_1, X_2, \ldots, X_d$ and $k \le d$, such that</p>
<ul>
<li><p>The sum of the new individual variances</p>
<p>$$
\operatorname{Var}\left( Z_1 \right) + \operatorname{Var}\left( Z_2 \right) + \ldots + \operatorname{Var}\left( Z_k \right)
$$</p>
<p>is <strong>close</strong> to the sum of the original individual variances</p>
<p>$$
\operatorname{Var}\left( X_1 \right) + \operatorname{Var}\left( X_2 \right) + \ldots + \operatorname{Var}\left( X_d \right)
$$</p>
</li>
<li><p>The linear combinations $Z_i$ and $Z_j$ are <strong>uncorrelated</strong> for $i\ne j$. This imply that each variable in $\boldsymbol{z} = \left( Z_1, Z_2, \ldots, Z_k \right)^\top$ can be analyzed by using <strong>univariate</strong> techniques.</p></li>
</ul>
<p>Other formulations: Find a linear mapping $\mathbb{R} ^d \rightarrow \mathbb{R} ^k$ (assume $\boldsymbol{X}$  is centered) from to project the data matrix $\boldsymbol{X}$ to a lower dimensional embedding matrix $\boldsymbol{Z}$.</p>
<p>$$\begin{aligned}
\boldsymbol{z}<em>i &amp;= \boldsymbol{W}</em>{d \times k} ^\top \boldsymbol{x}<em>i \
\boldsymbol{Z}</em>{n \times k} &amp;= \boldsymbol{X}_{n \times d}  \boldsymbol{W} _{d \times k} \
\end{aligned}$$</p>
<ul>
<li><p>Minimize total reconstruction loss</p>
<p>$$\begin{align}
\boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmin}} , &amp; \sum_i^n \left\Vert \boldsymbol{x}_i - \hat{\boldsymbol{x} }_i \right\Vert ^2    \
= \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmin}} , &amp; \sum_i^n \left\Vert \boldsymbol{x}_i - \boldsymbol{W} \boldsymbol{z}  _i \right\Vert ^2    \
\text{s.t.}  &amp; \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  \
&amp;\ \boldsymbol{W} \in \mathbb{R} _{d \times k}
\end{align}$$</p>
</li>
<li><p>Maximize the total variances $\sum_i \operatorname{Var}\left( Z_i \right)$ of the projected data $\boldsymbol{Z} =  \boldsymbol{X}  \boldsymbol{W}$</p>
<p>$$\begin{align}
\boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmax}} , &amp; \operatorname{tr}\left( \boldsymbol{Z} ^\top \boldsymbol{Z}  \right)   \
= \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmax}} , &amp; \operatorname{tr}\left( \boldsymbol{W} ^\top \boldsymbol{X} ^\top \boldsymbol{X} \boldsymbol{W} \right)   \
\text{s.t.}  &amp; \ \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  \
&amp;\ \boldsymbol{W} \in \mathbb{R} _{d \times k}
\end{align}$$</p>
</li>
</ul>
</div>
<div class="section" id="learning">
<h2>Learning<a class="headerlink" href="#learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sequential-maximization">
<h3>Sequential Maximization<a class="headerlink" href="#sequential-maximization" title="Permalink to this headline">¶</a></h3>
<p>The first variable in $\boldsymbol{z}$, i.e. $Z_1 = \boldsymbol{u} \boldsymbol{x}$ is obtained to maximize its variance, i.e.,</p>
<p>$$
\lambda_{1} \equiv \operatorname{Var}\left(Z_{1}\right)=\max _{\left\Vert \boldsymbol{u}  \right\Vert _2^2 = 1 } \boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}
$$</p>
<p>Suppose the maximum is achieved at $\boldsymbol{u} = \boldsymbol{u} _1$ and we call $Z_1$ given below the first population principal component</p>
<p>$$
Z_1 = \boldsymbol{u} _1^T \boldsymbol{x}
$$</p>
<p>Successively for $i=2, \ldots, m$ the variance of $Z_i$ can be obtained by the following maximization</p>
<p>$$
\begin{aligned}
&amp;&amp;&amp;\lambda_{i} \equiv \operatorname{Var}\left(Z_{i}\right)=\max <em>{u} \boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}\
&amp; &amp;\mathrm{s.t.}  \quad &amp;\boldsymbol{u}^{\top} \boldsymbol{u}=1 \
&amp; &amp; &amp; \ \boldsymbol{u}^{\top} \boldsymbol{x} \text { being uncorrelated with } Z</em>{1}, \ldots, Z_{i-1}<br />
\end{aligned}
$$</p>
<p>The maximum is achieved at $\boldsymbol{u} = \boldsymbol{u} _i$ and the $i$-th population principal component is</p>
<p>$$
Z_i = \boldsymbol{u} _i^\top \boldsymbol{x}
$$</p>
<p>:::{admonition,dropdown,seealso} Derivation
We consider the maximization problem:</p>
<p>$$\begin{align}
\max _{\boldsymbol{u}} \quad &amp; \boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}  \
\text {s.t.} \quad &amp; \boldsymbol{u}^{\top} \boldsymbol{u}=1
\end{align}$$</p>
<p>$$
\quad
$$</p>
<p>The Lagrangean is</p>
<p>$$
\begin{equation}
L(\boldsymbol{u}, \theta)=\boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}-\lambda\left(\boldsymbol{u}^{\top} \boldsymbol{u}-1\right)
\end{equation}
$$</p>
<p>The first order conditions are</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{u}}
&amp;= 2 \boldsymbol{\Sigma} \boldsymbol{u}-2 \lambda \boldsymbol{u} \
&amp;=\boldsymbol{0} \
\Rightarrow \quad \quad \boldsymbol{\Sigma} \boldsymbol{u} &amp;=\lambda \boldsymbol{u}  \quad \quad \quad \quad (1)
\end{aligned}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \lambda}
&amp;= 1-\boldsymbol{u}^{\top} \boldsymbol{u} \
&amp;= 0 \
\Rightarrow \quad \quad  \boldsymbol{u}^{\top} \boldsymbol{u}
&amp;=1 \quad \quad \quad \quad (2)
\end{aligned}
$$</p>
<p>Premultiply $(1)$ by $\boldsymbol{u} ^\top$ we have
$$
\boldsymbol{u}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}=\lambda \boldsymbol{u}^{\top} \boldsymbol{u}
$$</p>
<p>Hence,</p>
<p>$$
\lambda = \boldsymbol{u} ^\top \boldsymbol{\Sigma} \boldsymbol{u}
$$</p>
<p>Note that $(1)$ also gives</p>
<p>$$
(\boldsymbol{\Sigma}-\lambda \boldsymbol{I}) \boldsymbol{u} =\boldsymbol{0}
$$</p>
<p>which implies $\lambda$ is the eigenvalue of $\boldsymbol{\Sigma}$.</p>
<p>Therefore, the maximized variance $\boldsymbol{u} ^\top \boldsymbol{\Sigma} \boldsymbol{u}$ equals to the largest eigenvalue of $\boldsymbol{\Sigma}$.
:::</p>
</div>
<div class="section" id="spectral-decomposition">
<h3>Spectral Decomposition<a class="headerlink" href="#spectral-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Rather than obtaining the principal components sequentially, the principal components and their variances can be obtained simultaneously by solving for the eigenvectors and eigenvalues of $\boldsymbol{\Sigma}$. Using the Spectral Decomposition Theorem,</p>
<p>$$
\boldsymbol{\Sigma}  = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top = \sum_i^p \lambda_i \boldsymbol{u} _i \boldsymbol{u} _i ^\top
$$</p>
<p>where</p>
<ul class="simple">
<li><p>$\lambda_1 &gt; \lambda_2 &gt; \dots&gt; \lambda_d \ge 0$ are ordered eigenvalues of $\boldsymbol{\Sigma}$: $\boldsymbol{\Lambda} =  \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d)$</p></li>
<li><p>$\boldsymbol{u} _1, \ldots, \boldsymbol{u} _d$ are their corresponding normalized eigenvectors forming the column vectors of the orthogonal matrix $\boldsymbol{U} = \left( \boldsymbol{u} _1\  \boldsymbol{u} _2 \ \ldots \  \boldsymbol{u} _d \right)$, where $\boldsymbol{U}  ^\top \boldsymbol{U}   = \boldsymbol{I}$ or $\boldsymbol{u} _i ^\top \boldsymbol{u} _j = 1$ if $i=j$ and 0 otherwise.</p></li>
</ul>
<p>The $k$-th population principal component is defined as</p>
<p>$$
Z_{k}=\boldsymbol{u}<em>{k}^{\top} \boldsymbol{x}=u</em>{1 k} X_{1}+u_{2 k} X_{2}+\cdots+u_{d k} X_{k}, \quad k=1, \ldots, d
$$</p>
<p>The principal component transform using the first $k$ principal directions is then</p>
<p>$$
\boldsymbol{z} = \boldsymbol{U}_{[:k]} ^\top \boldsymbol{x}
$$</p>
<p>or</p>
<p>$$
\boldsymbol{Z} = \boldsymbol{X} \boldsymbol{U} _{[:k]}
$$</p>
<p>where $\boldsymbol{U} _{[:k]}$ means the matrix consisting of the first $k$ columns of $\boldsymbol{U}$.</p>
</div>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>All principal components are uncorrelated, i.e., $\operatorname{Cov}\left( Z_i, Z_j \right) = 0$ for $i \ne j$</p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em>
$$
\begin{aligned}
\operatorname{Cov}\left(Z_{i}, Z_{j}\right) &amp;=\operatorname{Cov}\left(\boldsymbol{u}<em>{i}^{\top} \boldsymbol{x}, \boldsymbol{u}</em>{j}^{\top} \boldsymbol{x}\right) \
&amp;=\mathrm{E}\left(\boldsymbol{u}<em>{i}^{\top}(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{u}</em>{j}\right) \
&amp;=\boldsymbol{u}<em>{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}</em>{j} \
&amp;=\boldsymbol{u}<em>{i}^{\top} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^{\top} \boldsymbol{u}</em>{j} \
&amp;=\left(\boldsymbol{u}<em>{i}^{\top}\right)\left(\boldsymbol{u}</em>{1} \boldsymbol{u}<em>{2} \cdots \boldsymbol{u}</em>{p}\right) \boldsymbol{\Lambda}\left(\begin{array}{c}
\boldsymbol{u}<em>{1}^{\top} \
\boldsymbol{u}</em>{2}^{\top} \
\vdots \
\boldsymbol{u}<em>{p}^{\top}
\end{array}\right) \boldsymbol{u}</em>{j} \
&amp;=\boldsymbol{e}<em>{i}^{\top} \boldsymbol{\Lambda} \boldsymbol{e}</em>{j} \
&amp;=0
\end{aligned}
$$
:::</p>
</li>
<li><p>The variance of the $i$-th principal component is $\lambda_i$, i.e. $\operatorname{Var}\left( Z_i \right) = \lambda_i$.</p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em></p>
<p>$$\begin{aligned}
\operatorname{Var}\left( Z_i \right)
&amp;= \operatorname{Var}\left( \boldsymbol{u} _i ^\top \boldsymbol{x}  \right) \
&amp;= \boldsymbol{u} _i ^\top \boldsymbol{\Sigma} \boldsymbol{u} _i \
&amp;= \boldsymbol{e}_i ^\top \boldsymbol{\Lambda} \boldsymbol{e}_i  \
&amp;= \lambda_i
\end{aligned}$$
:::</p>
</li>
<li><p>The first principal component $Z_1 = \boldsymbol{u} _1 ^\top \boldsymbol{x}$ has the largest variance among all linear combinations of $X_i$’s. The $i=2, \ldots, p$, the $i$-th principal component has the largest variance among all linear combinations of $X_i$’s, which are uncorrelated with the first $(i-1)$ principal components.</p></li>
<li><p>The principal component preserve the total variance</p>
<p>$$
\sum_{i=1}^{p} \operatorname{Var}\left(Z_{i}\right)=\sum_{i=1}^{p} \operatorname{Var}\left(X_{i}\right)
$$</p>
<p>or</p>
<p>$$
\sum_{i=1}^{p} \lambda_{i}=\sum_{i=1}^{p} \sigma_{i i}
$$</p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em>
$$
\begin{aligned}
\sum_{i=1}^{p} \sigma_{i i} &amp;=\operatorname{tr}(\boldsymbol{\Sigma}) \
&amp;=\operatorname{tr}\left(\sum_{i=1}^{p} \lambda_{i} \boldsymbol{u}<em>{i} \boldsymbol{u}</em>{i}^{\top}\right) \
&amp;=\sum_{i=1}^{p} \lambda_{i} \operatorname{tr}\left(\boldsymbol{u}<em>{i} \boldsymbol{u}</em>{i}^{\top}\right) \
&amp;=\sum_{i=1}^{p} \lambda_{i} \operatorname{tr}\left(\boldsymbol{u}<em>{i}^{\top} \boldsymbol{u}</em>{i}\right) \
&amp;=\sum_{i=1}^{p} \lambda_{i}
\end{aligned}
$$
:::</p>
</li>
<li><p>The correlation between a principal component $Z_j$ and an original variable $X_i$ is given by</p>
<p>$$
\operatorname{Corr}\left( X_i, Z_j \right) = \frac{\sqrt{\lambda_j}a_{ij}}{\sqrt{\sigma_{ii}}}
$$</p>
<p>where $u_{ij}$ denotes the $i$-th element of $\boldsymbol{u} _j$.</p>
<p>:::{admonition,dropdown,seealso} <em>Proof</em>
$$
\begin{aligned}
\operatorname{Cov}\left(X_{i}, Z_{j}\right) &amp;=\operatorname{Cov}\left(X_{i}, \boldsymbol{u}<em>{j}^{\top} \boldsymbol{x}\right) \
&amp;=\operatorname{Cov}\left(\boldsymbol{e}</em>{i}^{\top} \boldsymbol{x}, \boldsymbol{u}<em>{j}^{\top} \boldsymbol{x}\right) \
&amp;=\boldsymbol{e}</em>{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{u}<em>{j} \
&amp;=\boldsymbol{e}</em>{i}^{\top} \sum_{k=1}^{p} \lambda_{k} \boldsymbol{u}<em>{k} \boldsymbol{u}</em>{k}^{\top} \boldsymbol{u}<em>{j} \
&amp;=\lambda</em>{j} \boldsymbol{e}<em>{i}^{\top} \boldsymbol{u}</em>{j} \boldsymbol{u}<em>{j}^{\top} \boldsymbol{u}</em>{j} \
&amp;=\lambda_{j} \boldsymbol{e}<em>{i}^{\top} \boldsymbol{u}</em>{j} \
&amp;=\lambda_{j} u_{i j}
\end{aligned}
$$</p>
<p>and then</p>
<p>$$\begin{align}
\operatorname{Corr}\left(X_{i}, Z_{j}\right)
&amp;=\frac{\operatorname{Cov}\left(X_{i}, Z_{j}\right)}{\sqrt{\operatorname{Var}\left(X_{i}\right) \operatorname{Var}\left(Z_{j}\right)}} \
&amp;=\frac{\lambda_{j} u_{i j}}{\sqrt{\sigma_{i i} \lambda_{j}}} \
&amp;=\frac{\sqrt{\lambda_{j}} u_{i j}}{\sqrt{\sigma_{i i}}}
\end{align}$$
:::</p>
</li>
<li><p>If the correlation matrix $\boldsymbol{\rho} = \boldsymbol{D}^{-1}\boldsymbol{\Sigma} \boldsymbol{D}^{-1}$ instead of the covariance matrix $\boldsymbol{\Sigma}$ is used, i.e. variables $X_1, X_2, \ldots, X_d$ are standardized, then</p>
<p>$$
\sum_i^p \lambda_i = \sum_i^p \sigma_{ii} = d
$$</p>
</li>
</ol>
</div>
<div class="section" id="tuning">
<h2>Tuning<a class="headerlink" href="#tuning" title="Permalink to this headline">¶</a></h2>
<p>There are several ways to choose the number of principal components to retain.</p>
<ol>
<li><p><strong>Cumulative proportion cutoff</strong>:</p>
<p>Include the components such that the cumulative proportion of the total variance explained is just more than a threshold value, say 80%, i.e., if</p>
<p>$$
\begin{equation}
\frac{\sum_{i=1}^{m} \ell_{i}}{\sum_{i=1}^{d} \ell_{i}} &gt;0.8
\end{equation}
$$</p>
<p>This method keeps $m$ principal components.</p>
</li>
<li><p><strong>Proportion cutoff</strong></p>
<p>Select the components whose eigenvalues are greater than a threshold value, say average of eigenvalues; for correlation matrix input, this average is $d^{-1} \sum_{i=1}^{d} \ell_{i}=d^{-1} d=1$ if we use the correlation matrix $\boldsymbol{\rho}$.</p>
</li>
<li><p><strong>Scree plot</strong></p>
<p>Construct the so-called scree plot of the eigenvalue $\ell_i$ on the vertical axis versus $i$ on horizontal axis with equal intervals for $i = 1, 2, \ldots, d$, and join the points into a decreasing polygon. Try to find a “clean-cut” where the polygon “levels off” so that the first few eigenvalues seem to be far apart from the others.</p>
<p>:::{figure,myclass} pca-scree-plot
<img src="../imgs/pca-scree-plot.png" width = "50%" alt=""/></p>
<p>Scree plot of $\lambda$. [Fung 2021]
:::</p>
</li>
<li><p><strong>Hypothesis testing</strong></p>
<p>Perform formal significance tests to determine the larger an unequal eigenvalues and retain the principal components to these eigenvalues.</p>
</li>
<li><p><strong>Reconstruction loss</strong></p>
<p>Recall the principal component transform $\boldsymbol{z} = \boldsymbol{U}_{[:k]} ^\top \boldsymbol{x}$. Hence, to reconstruct $\hat{\boldsymbol{x} }$ by the first $k$ components $\boldsymbol{u} _1, \ldots, \boldsymbol{u} _k$ in $\boldsymbol{U}$ , we can use the expansion</p>
<p>$$
\hat{\boldsymbol{x} }= \boldsymbol{U}<em>{[:k]} \boldsymbol{z} =\sum</em>{j=1}^{k}z_j \boldsymbol{u} <em>{j} = \sum</em>{j=1}^{k}\left(\boldsymbol{u}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{u} _{j}
$$</p>
<p>If $\boldsymbol{x}$ was centered before PCA, we add the mean back
$$
\hat{\boldsymbol{x} }=\boldsymbol{\mu} <em>{\boldsymbol{x}} +\sum</em>{j=1}^{k}\left(\boldsymbol{u}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{u} _{j}
$$</p>
<p>To choose an optimal number of principal components $k$, we can examine the magnitude of the residual $\left\Vert \boldsymbol{x} - \hat{\boldsymbol{x} } \right\Vert ^2$. The expected residual corresponds to variance in the <strong>remaining</strong> subspace.</p>
<p>:::{figure,myclass} pca-reconstruction
<img src="../imgs/pca-reconstruction.png" width = "80%" alt=""/></p>
<p>Reconstruction of digits with mean and principal components [Livescu 2021]
:::</p>
</li>
<li><p><strong>Downstream task performance</strong></p>
<p>Use the performance of the downstream task to choose an optimal number of principal components.</p>
</li>
</ol>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="geometric-meaning-direction-of-variation">
<h3>Geometric Meaning: Direction of Variation<a class="headerlink" href="#geometric-meaning-direction-of-variation" title="Permalink to this headline">¶</a></h3>
<p>For the distribution of $\boldsymbol{x}$, thelcenter location is determined by $\boldsymbol{\mu} _ \boldsymbol{x}$ and the variation is captured by each principal direction $\boldsymbol{u} _i$</p>
<p>For the multinormal distribution, the family of <strong>contours</strong> of $\boldsymbol{x}$ (on each of which the pdf is a constant) is a family of ellipsoids in the original coordinate system $\boldsymbol{x}$ satisfying the following equation for a
constant $c$,</p>
<p>$$
\begin{equation}
(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})=c^{2}
\end{equation}
$$</p>
<p>where $c$ serves as an index of the family. This family of ellipsoids have orthogonal principal axes</p>
<p>$$
\pm c\lambda_i^{1/2}\boldsymbol{u}_i, i= 1, 2, \ldots, p
$$</p>
<p>with length</p>
<ul class="simple">
<li><p>$2c\lambda_i^{1/2}$</p></li>
<li><p>directional cosines as coefficients given in $\boldsymbol{u} _i$ for the $i$-th axis.</p></li>
</ul>
<p>:::{figure} pca-gausian-ellipsoids
<img src="../imgs/pca-pc-ellipsoids.png" width = "80%" alt=""/></p>
<p>PCA and Ellipsoids of Gaussian [Fung 2018]
:::</p>
<p>Another example is hand written digits. Suppose $\boldsymbol{\mu} _ \boldsymbol{x}$ is the sample mean that determines the “mean” appearance of the digit $2$, then $\boldsymbol{\phi}_j$ is a principal direction which determines the location of variation of the black/white pixels.</p>
<p>:::{figure,myclass} pca-reconstruction-scale
<img src="../imgs/pca-pc-digits.png" width = "50%" alt=""/></p>
<p>Reconstruction of digits with mean and scaled principal components [Livescu 2021]
:::</p>
</div>
<div class="section" id="proportion-explained">
<h3>Proportion Explained<a class="headerlink" href="#proportion-explained" title="Permalink to this headline">¶</a></h3>
<p>The proportion of total variance explained by $Z_i$, which is</p>
<p>$$\frac{\lambda_i}{\sum_{j=1}^p \lambda_j}$$</p>
<p>is considered as a measure of <strong>importance</strong> of $Z_i$ in a more parsimonious description of the system.</p>
</div>
<div class="section" id="score-of-an-observation-in-sample-data">
<h3>Score of an Observation in Sample Data<a class="headerlink" href="#score-of-an-observation-in-sample-data" title="Permalink to this headline">¶</a></h3>
<p>For a data set of $n$ observations, we decompose the sample covariance matrix $S$ as</p>
<p>$$
\begin{equation}
\boldsymbol{S} =\sum_{i=1}^{d} \ell_{i} \boldsymbol{u} _{i} \boldsymbol{u} _{i}^{\top}=\boldsymbol{U} \boldsymbol{L}  \boldsymbol{U} ^\top
\end{equation}
$$</p>
<p>where $\ell_i$ are eigencalues of $\boldsymbol{S}$ and $\boldsymbol{u} _i$’s are their corresponding normalized eigenvectors.</p>
<p>The $j$-th <strong>sample</strong> principal component is defined as</p>
<p>$$
\begin{equation}
Z_{j}=\boldsymbol{u}<em>{j}^{\top} \boldsymbol{x}=u</em>{1 j} X_{1}+u_{2 j} X_{2}+\cdots+u_{d j} X_{d}
\end{equation}
$$</p>
<p>where $\begin{equation}
\boldsymbol{u}<em>{j}^{\top}=\left(\begin{array}{llll}
u</em>{1 j} &amp; u_{2 j} &amp; \cdots &amp; u_{d j}
\end{array}\right)
\end{equation}$.</p>
<p>The data layout is</p>
<p>$$
\begin{equation}
\begin{array}{cccccccc}
&amp;&amp; \text{Data} \ \ \boldsymbol{X}  &amp;&amp;&amp;&amp;\text{PC} \ \ \boldsymbol{Z} &amp;\
\hline X_{1} &amp; X_{2} &amp; \cdots &amp; X_{d} &amp; \quad \quad Z_{1} &amp; Z_{2} &amp; \cdots &amp; Z_{k} \
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 d} &amp; \quad \quad z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1 k} \
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 d} &amp; \quad \quad z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2 k} \
&amp; &amp; \vdots &amp; &amp; &amp; &amp; \vdots &amp; \
x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n d} &amp; \quad \quad z_{n 1} &amp; z_{n 2} &amp; \cdots &amp; z_{n k}
\end{array}
\end{equation}
$$</p>
<p>where the corresponding row vectors on the data matrices are related as</p>
<p>$$\boldsymbol{z} _i = \boldsymbol{U}^\top  \boldsymbol{x} _i , i= 1, 2, \ldots, n$$</p>
<p>where $\boldsymbol{z} _i$ can be interpreted as a vector of principal component scores for the $i$-th observation.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Properties of the population principal components are all valid in the sample context, by replacing</p>
<p>$$\boldsymbol{\mu}, \boldsymbol{\Sigma} , \boldsymbol{\rho}, \lambda_i, \boldsymbol{u} _i$$</p>
<p>by</p>
<p>$$\bar{\boldsymbol{x}}, \boldsymbol{S} , \boldsymbol{R} , \ell_i, \boldsymbol{a} _i$$</p>
</div>
</div>
</div>
<div class="section" id="special-cases">
<h2>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">¶</a></h2>
<div class="section" id="variables-are-uncorrelated">
<h3>Variables are Uncorrelated<a class="headerlink" href="#variables-are-uncorrelated" title="Permalink to this headline">¶</a></h3>
<p>If $\boldsymbol{X_i}$ are uncorrelated, then $\boldsymbol{\Sigma}$ is a diagonal matrix, i.e., $\boldsymbol{\Sigma} = \operatorname{diag}\left( \sigma_{11}, \sigma_{22}, \ldots, \sigma_{dd} \right)$. Without loss of generality, assume $\sigma_{11} &gt; \sigma_{22} &gt; \ldots &gt; \sigma_{dd}$, then from its spectral decomposition $\boldsymbol{\Sigma} = \boldsymbol{U} ^\top \boldsymbol{\Lambda} \boldsymbol{U}$, we have</p>
<ul class="simple">
<li><p>$\boldsymbol{U} = \boldsymbol{I}$</p></li>
<li><p>$\boldsymbol{\Lambda} = \operatorname{diag}\left( \sigma_{ii} \right)$, or $\lambda_i = \sigma_{ii}$.</p></li>
</ul>
<p>Hence, the principal component is</p>
<p>$$
Z_i = X_i
$$</p>
<p>Clearly, it is <strong>not</strong> necessary to perform PCA in this case.</p>
</div>
<div class="section" id="variables-are-perfectly-correlated">
<h3>Variables are Perfectly Correlated<a class="headerlink" href="#variables-are-perfectly-correlated" title="Permalink to this headline">¶</a></h3>
<p>In this case, the covariance matrix is not of full rank, i.e., $\left\vert \boldsymbol{\Sigma}  \right\vert = 0$. Then, some eiganvalues equal zero. In other words,</p>
<p>$$
\lambda_1 &gt; \lambda_2 &gt; \ldots &gt; \lambda_m &gt; \lambda_{m+1} = \ldots = \lambda_d = 0
$$</p>
<p>Only $m$ eigenvectors $\boldsymbol{u} _i$ can be obtained with $\left\Vert \boldsymbol{u}_i  \right\Vert _2 ^2 =  1$ .</p>
</div>
<div class="section" id="few-variables-have-extremely-large-variances">
<h3>Few Variables Have Extremely Large Variances<a class="headerlink" href="#few-variables-have-extremely-large-variances" title="Permalink to this headline">¶</a></h3>
<p>If a few variables have extremely large variances in comparison with other variables, they will dominate the first few principal components and give the foregone conclusion that a few principal components is sufficient in summarizing information. That conclusion may even be spurious, as the measurement scales, which affect the variances, are quite arbitrary in a lot of applications.</p>
<p>For example, $X_1$ is measured in meters while $X_2$ and $X_3$ are measured in kilometers. The first PC should have particularly large variance ($\lambda_1$ is particularly large relative to $\lambda_2$ and $\lambda_3$). This property suggests that if $\boldsymbol{x}$  are on different, or non-commensurable, measurement units, we should <strong>standardize</strong> them,</p>
<p>$$
Z_i = \frac{X_i - \mu_i}{\sigma_i}
$$</p>
<p>before performing PCA.</p>
</div>
</div>
<div class="section" id="cons">
<h2>Cons<a class="headerlink" href="#cons" title="Permalink to this headline">¶</a></h2>
<p><strong>Sensitive to Variable Transformation</strong></p>
<p>The results of PCA are not invariant under a linear transformation and, even worse, there is no easy correspondence between the two sets of results $\boldsymbol{z}$ and $\boldsymbol{z} ^\prime$, before and after the linear transformation. For example, the PCA using $\boldsymbol{\Sigma}$ is not the same as the PCA using $\boldsymbol{\rho}$ and we cannot use the PCA from $\boldsymbol{\rho}$ to get the PCA results from the original variables.</p>
<p>If the two sets of results are consistent to each other, the PCA based on $\boldsymbol{\Sigma}$  may be preferred in some situation. If they are very different, or even contradictory, subject-matter knowledge and/or wisdom are needed to make a choice.</p>
<p>The PCA based on covariance matrix is preferred when the original measurements units are very important, like in many applications in
natural sciences. However, when the units of measurement are of artificial nature, like scores in some questions as frequently used in social sciences, the PCA based on correlation matrix is preferred.</p>
<p><strong>Direction of Variance may not be Discriminative</strong></p>
<p>But note that the direction of largest variance need not to be the most discriminative direction. See the example below.</p>
<p>:::{figure,myclass} pca-not-discriminative
<img src="../imgs/pca-classification.png" width = "80%" alt=""/></p>
<p>PCA may not be discriminative [Livescu 2021]
:::</p>
<p>If we knew the labels, we could use a supervised dimensionality reduction, e.g. linear discriminant analysis.</p>
</div>
<div class="section" id="relation-to">
<h2>Relation to<a class="headerlink" href="#relation-to" title="Permalink to this headline">¶</a></h2>
<div class="section" id="svd">
<h3>SVD<a class="headerlink" href="#svd" title="Permalink to this headline">¶</a></h3>
<p>Recall the SVD of the data matrix</p>
<p>$$
X = \boldsymbol{U} \boldsymbol{S} \boldsymbol{V} ^\top
$$</p>
<p>Suppose $X$ is centered, then</p>
<p>$$
\boldsymbol{\Sigma} = \boldsymbol{X} ^\top \boldsymbol{X} = \boldsymbol{V} \boldsymbol{S} ^\top \boldsymbol{S} \boldsymbol{V} ^\top
$$</p>
<p>Thus,</p>
<ul class="simple">
<li><p>the right singular vectors $\boldsymbol{V}$ are the eigenvectors of $\boldsymbol{X} ^\top \boldsymbol{X}$,</p></li>
<li><p>the eigenvalues of $\boldsymbol{X} ^\top \boldsymbol{X}$ are proportional to the squared singular values of $\sigma_i$.</p></li>
</ul>
<p>So we can compute the PCA solutions via an SVD.</p>
</div>
<div class="section" id="compression">
<h3>Compression<a class="headerlink" href="#compression" title="Permalink to this headline">¶</a></h3>
<p>Instead of storing the $n \times d$ data matrix $\boldsymbol{X}$, not we need to store the $d \times 1$ mean vector $\boldsymbol{\mu} _ \boldsymbol{x}$ and the $k\times d$ projection matrix $\boldsymbol{W}$, and the $n \times k$ projected data matrix $\boldsymbol{Z}$.</p>
<p>To transmit $n$ examples, we need $d+dk+nk$ numbers instead of $nd$.</p>
</div>
<div class="section" id="gaussians">
<h3>Gaussians<a class="headerlink" href="#gaussians" title="Permalink to this headline">¶</a></h3>
<p>PCA essentially models variance in the data. What distribution is characterized by variance? Gaussian. The covariance matrix parameter in $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma} )$ has EVD</p>
<p>$$
\boldsymbol{\Sigma}=\boldsymbol{R}\left[\begin{array}{lll}
\lambda_{1} &amp; &amp; \
&amp; \ddots &amp; \
&amp; &amp; \lambda_{d}
\end{array}\right] \boldsymbol{R}^{T}
$$</p>
<p>where $\boldsymbol{R}$ determines the orientation of the ellipsoid, and the eigenvalues specifies the scaling along the principal directions. The PCA solution $\boldsymbol{U}$ from the sample covariance matrix $\boldsymbol{S}$ should be close to $\boldsymbol{R}$.</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>For a classification task, we can perform PCA on the features before fitting the data to a classifier. The classifier might be more accurate since PCA reduces noise.</p>
</div>
</div>
<div class="section" id="extension-probabilistic-pca">
<h2>Extension: Probabilistic PCA<a class="headerlink" href="#extension-probabilistic-pca" title="Permalink to this headline">¶</a></h2>
<p><em>Independently proposed by [Tipping &amp; Bishop 1997, 1999] and [Roweis 1998]</em></p>
<p>Probabilistic PCA adds a probabilistic component (interpretation) to the PCA model. It provides</p>
<ul class="simple">
<li><p>a way of approximating a Gaussian using fewer parameters (e.g. common noise variance).</p></li>
<li><p>a way of sampling from the data distribution as a probabilistic model (thus aka sensible PCA).</p></li>
</ul>
<div class="section" id="id1">
<h3>Objective<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>In a PPCA model, we first draw low dimensional $\boldsymbol{z} \in \mathbb{R}^{k}$,</p>
<p>$$
p(\boldsymbol{z}) =\mathcal{N}( \boldsymbol{0}, \boldsymbol{I})
$$</p>
<p>and draw $\boldsymbol{x} \in \mathbb{R}^{d}, k \leq d$ by</p>
<p>$$
p(\boldsymbol{x} \mid \boldsymbol{z}) =\mathcal{N}\left( \boldsymbol{W} \boldsymbol{z}+\boldsymbol{\mu} , \sigma^{2} \boldsymbol{I}\right)
$$</p>
<p>where $\boldsymbol{W} \in \mathbb{R} ^{d \times k}$</p>
<p>Or equivalently,
$$
\begin{equation}
\boldsymbol{x}=\boldsymbol{W} \boldsymbol{z}+\boldsymbol{\mu} + \boldsymbol{\epsilon} , \text { where } \boldsymbol{\epsilon}  \sim \mathcal{N}\left(0, \sigma^{2} \boldsymbol{I}\right)
\end{equation}
$$</p>
<p>If $\sigma = 0$ then we get standard PCA.</p>
<p>By the property of multivariate Gaussian, we have</p>
<p>$$
p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{\mu} , \boldsymbol{W} \boldsymbol{W}^{T}+\sigma^{2} \boldsymbol{I}\right)
$$</p>
<p>The goal is to estimate the parameter $\boldsymbol{W} , \boldsymbol{\mu} , \sigma$ that maximize the log likelihood $\sum_{i=1}^{n} \log p\left(\boldsymbol{x}_{i} \mid \boldsymbol{W} , \boldsymbol{\mu} , \sigma\right)$.</p>
</div>
<div class="section" id="learning-mle">
<h3>Learning (MLE)<a class="headerlink" href="#learning-mle" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">MLE not unique</p>
<p>Before seeking the ML solution, notice that the solution is not unique: if $\boldsymbol{R}$ is an orthogonal matrix, then $\widetilde{\boldsymbol{W}} = \boldsymbol{W} \boldsymbol{\boldsymbol{R}}$ is indistinguishable from $\boldsymbol{W}$</p>
<p>$$
\widetilde{\boldsymbol{W}} \widetilde{\boldsymbol{W}}^{T}=\boldsymbol{W} \boldsymbol{R} \boldsymbol{R}^{T} \boldsymbol{W}^{T}=\boldsymbol{W} \boldsymbol{W}^{T}
$$</p>
<p>So we will find a solution $W _{ML}$ up to a rotation $\boldsymbol{R}$.</p>
</div>
<p>Let $\boldsymbol{C}  = \boldsymbol{W} \boldsymbol{W} ^\top + \sigma^2 \boldsymbol{I}_d$. The log likelihood function is</p>
<p>$$
\begin{equation}
\sum_{i=1}^{n} \log p\left(\boldsymbol{x}<em>{i} ; \boldsymbol{W}, \mu, \sigma^{2}\right) =
-\frac{n d}{2} \log (2 \pi)-\frac{n}{2} \log |\boldsymbol{C}|-\frac{1}{2} \sum</em>{i=1}^{n}\left(\boldsymbol{x}<em>{i}-\boldsymbol{\mu} \right)^{T} \boldsymbol{C}^{-1}\left(\boldsymbol{x}</em>{i}-\boldsymbol{\mu} \right)
\end{equation}
$$</p>
<p>Setting the derivative w.r.t. $\boldsymbol{\mu}$ to $\boldsymbol{0} $ we have</p>
<p>$$\boldsymbol{\mu} _{ML} = \bar{\boldsymbol{x}}$$</p>
<p>i.e. the sample mean. The solution for $\boldsymbol{W}$ and $\sigma^2$ is more complicated, but closed form.</p>
<p>$$
\begin{equation}
\begin{aligned}
\boldsymbol{W}<em>{M L} &amp;=\boldsymbol{U}</em>{d \times k}\left(\boldsymbol{\Lambda} <em>{k}-\sigma^{2} \boldsymbol{I}<em>k\right)^{1 / 2} \boldsymbol{R}<em>k \
\sigma</em>{M L}^{2} &amp;=\frac{1}{d-k} \sum</em>{j=k+1}^{d} \lambda</em>{j}
\end{aligned}
\end{equation}
$$</p>
<div class="margin sidebar">
<p class="sidebar-title">EM also works</p>
<p>It is also possible to find the PPCA solution iteratively, visa the EM algorithm. This is useful if doing the eigenvalue decomposition is too computationally demanding.</p>
</div>
<p>where</p>
<ul class="simple">
<li><p>$\boldsymbol{U} _{d \times k}$ is the first $k$ eigenvectors of the sample covariance matrix $\boldsymbol{S}$</p></li>
<li><p>$\boldsymbol{\Lambda}_k$ is the diagonal matrix of eigenvalues</p></li>
<li><p>$\boldsymbol{R}_k$ is an arbitrary orthogonal matrix</p></li>
</ul>
</div>
<div class="section" id="id2">
<h3>Properties<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>For $\boldsymbol{R}_k = \boldsymbol{I}_k$ , the solution for $\boldsymbol{W}$ is just a scaled version (by the diagonal matrix $\boldsymbol{\Lambda} _k - \sigma^2 \boldsymbol{I} <em>k$) of that of standard PCA $U</em>{d\times k}$.</p></li>
<li><p>$\sigma^2_{ML}$ is the average variance of the discarded dimensions in $\mathcal{X}$. We view the remaining dimensions as accounting for noise. Their average variance defines the common variance of the noise. The covariance is viewed as</p>
<p>$$
\boldsymbol{\Sigma}=\boldsymbol{U}\left[\begin{array}{cccccccc}
\lambda_{1} &amp; \ldots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \
&amp; \ddots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \
0 &amp; \ldots &amp; \lambda_{k} &amp; \ldots &amp; \ldots &amp; \ldots \
0 &amp; \ldots &amp; 0 &amp; \sigma^{2} &amp; 0 &amp; \ldots \
&amp; &amp; &amp; &amp; \ddots &amp; \
0 &amp; \ldots &amp; \ldots &amp; \ldots &amp; 0 &amp; \sigma^{2}
\end{array}\right] \boldsymbol{U} ^\top
$$</p>
</li>
<li><p>If $k = d$, i.e., no dimension reduction, then the MLE for the covariance matrix $\boldsymbol{C}$ of $\boldsymbol{x}$ is equal to $\boldsymbol{S}$, which is just the standard ML solution for a Gaussian distribution.</p></li>
</ul>
<p>$$
\boldsymbol{C}_{ML} = \boldsymbol{W} _{ML} \boldsymbol{W} _{ML} ^\top + \sigma^2 \boldsymbol{I}  = \boldsymbol{U} (\boldsymbol{\Lambda} - \sigma^2 I) \boldsymbol{U} ^\top  + \sigma^2 \boldsymbol{I}   = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top  = \boldsymbol{S}.
$$</p>
</div>
<div class="section" id="representation">
<h3>Representation<a class="headerlink" href="#representation" title="Permalink to this headline">¶</a></h3>
<p>The conditional distribution of $\boldsymbol{z}$ given $\boldsymbol{x}$ is</p>
<p>$$
p(\boldsymbol{z} \mid \boldsymbol{x})=\mathcal{N}\left(\boldsymbol{M}^{-1} \boldsymbol{W}^{T}(\boldsymbol{x}- \boldsymbol{\mu} ), \sigma^{2} \boldsymbol{M}^{-1}\right)
$$</p>
<p>where $\boldsymbol{M} = \boldsymbol{W} ^\top \boldsymbol{W}  + \sigma^2 \boldsymbol{I}_k$.</p>
<p>A reduced-dimansionality representation of $\boldsymbol{x}$ is given by the estimated conditional mean</p>
<p>$$
\widehat{\operatorname{E}}\left( \boldsymbol{z} \mid \boldsymbol{x}   \right) = \boldsymbol{M}  ^{-1} _{ML} \boldsymbol{W} ^\top _{ML}(\boldsymbol{x} - \bar{\boldsymbol{x}})
$$</p>
<p>where $\boldsymbol{M} _{ML} = \boldsymbol{W} _{ML} ^\top \boldsymbol{W} _{ML}  + \sigma^2 _{ML} \boldsymbol{I}_k$.</p>
<ul class="simple">
<li><p>As $\sigma^2 _{ML} \rightarrow 0$, the posterior mean approaches the standard PCA projection $\boldsymbol{ z } =  \boldsymbol{U}  ^\top (\boldsymbol{x}  - \bar{\boldsymbol{x} })$</p></li>
<li><p>As $\sigma^2 _{ML}&gt; 0$, the posterior mean “shrinks” the solution in magnitude from standard PCA. Since we are less certain about the representation when the noise is large.</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./33-dimensionality-reduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-dimensionality-reduction.html" title="previous page">Dimensionality Reduction</a>
    <a class='right-next' id="next-link" href="13-canonical-correlation-analysis.html" title="next page">Canonical Corerlation Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>