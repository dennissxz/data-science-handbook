
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Principal Component Analysis &#8212; Data Science Handbook</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Canonical Corerlation Analysis" href="13-canonical-correlation-analysis.html" />
    <link rel="prev" title="Dimensionality Reduction" href="00-dimensionality-reduction.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11-math/00-math.html">
   Math
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/12-derangement.html">
     Derangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/21-linear-algebra.html">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-math/90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-stat-sampling.html">
     Statistical Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Causality and Randomized Trial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14-python/00-python.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/11-programming-tools.html">
     Programmer tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/12-syntax.html">
     Syntax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/13-data-structure.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-python/15-functional-programming.html">
     Functional Programming
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17-sql/00-sql.html">
   SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/11-database.html">
     Database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/12-relational-structure.html">
     Relational Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/31-data-query.html">
     Data Query
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-sql/33-data-manipulation.html">
     Data Manipulation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19-miscellaneous/00-miscellaneous.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/11-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19-miscellaneous/13-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20-algorithms-basics/00-algorithms-basics.html">
   Algorithms Basics
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/13-denominations.html">
     Denominations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/01-information-theory.html">
     Information Theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-linear-models.html">
     Linear Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-canonical-correlation-analysis.html">
     Canonical Corerlation Analysis
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/33-dimensionality-reduction/11-principal-component-analysis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F33-dimensionality-reduction/11-principal-component-analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning">
   Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequential-maximization">
     Sequential Maximization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalue-decomposition">
     Eigenvalue Decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-cases">
   Special Cases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-are-uncorrelated">
     Variables are Uncorrelated
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-are-perfectly-correlated">
     Variables are Perfectly Correlated
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#few-variables-have-extremely-large-variances">
     Few Variables Have Extremely Large Variances
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties">
   Properties
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning">
   Tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#geometric-meaning-direction-of-variation">
     Geometric Meaning: Direction of Variation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proportion-explained">
     Proportion Explained
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#score-of-an-observation-in-sample-data">
     Score of an Observation in Sample Data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cons">
   Cons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relation-to">
   Relation to
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svd">
     SVD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compression">
     Compression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussians">
     Gaussians
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extension">
   Extension
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h1>
<p>Proposed by Pearson in 1901 and further deveoped by Hotelling in 1993.</p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Given <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span>, we want to extract the most useful information of <span class="math notranslate nohighlight">\(p\)</span> measurements such that</p>
<ol class="simple">
<li><p><strong>explore underlying dimension</strong> behind the <span class="math notranslate nohighlight">\(p\)</span> original measurements to explain the variation of <span class="math notranslate nohighlight">\(p\)</span> original measurements, which may have interesting or revealing interpretations, such as size, shape and contrasts in natural science;</p></li>
<li><p><strong>estimate latent variables</strong> (i.e. variables that cannot be measured or observed.) which can explain the variation of the <span class="math notranslate nohighlight">\(p\)</span> original measurements, especially in
social behavioral sciences.</p></li>
<li><p><strong>simplify the dimension</strong> of the observed data set. Lower dimension can be chosen from the data set such that the variations of measurements can be captured with an acceptable level. For example, <span class="math notranslate nohighlight">\(m \ll p\)</span> latent variables are chosen to capture 90% of variation of <span class="math notranslate nohighlight">\(p\)</span> original measurements. Indeed, this can be regarded as the data reduction or dimension reduction.</p></li>
</ol>
<p>Consider a <span class="math notranslate nohighlight">\(p\)</span>-dimensional random vector <span class="math notranslate nohighlight">\(\boldsymbol{x} = \left( X_1, X_2, \ldots, X_p \right)^\top\)</span> with mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \left( \mu_1, \ldots, \mu_p \right)^\top\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. PCA aimes to obtain the variables <span class="math notranslate nohighlight">\(Y_1, Y_2, \ldots, Y_m\)</span> which are the <strong>linear combinations</strong> of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span> and <span class="math notranslate nohighlight">\(m \le p\)</span>, such that</p>
<ul>
<li><p>The sum of the new individual variances</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( Y_1 \right) + \operatorname{Var}\left( Y_2 \right) + \ldots + \operatorname{Var}\left( Y_m \right)
  \]</div>
<p>is <strong>close</strong> to the sum of the original individual variances</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{Var}\left( X_1 \right) + \operatorname{Var}\left( X_2 \right) + \ldots + \operatorname{Var}\left( X_m \right)
  \]</div>
</li>
<li><p>The linear combinations <span class="math notranslate nohighlight">\(Y_i\)</span> and <span class="math notranslate nohighlight">\(Y_j\)</span> are <strong>uncorrelated</strong> for <span class="math notranslate nohighlight">\(i\ne j\)</span>. This imply that each variable in <span class="math notranslate nohighlight">\(\boldsymbol{y} = \left( Y_1, Y_2, \ldots, Y_m \right)^\top\)</span> can be analyzed by using <strong>univariate</strong> techniques.</p></li>
</ul>
<p><img alt="" src="../_images/pca_illustration.png" /></p>
<p>Other formulations: Find a linear mapping <span class="math notranslate nohighlight">\(\boldsymbol{W}: \mathbb{R} ^p \rightarrow \mathbb{R} ^m\)</span> (assume <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>  is centered) to</p>
<ul>
<li><p>Minimize reconstruction residuals</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmin}} \, &amp; \sum_i^n \left\Vert \boldsymbol{x}_i - \hat{\boldsymbol{x} }_i \right\Vert ^2    \\
     \text{s.t.}  &amp; \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  
    \end{align}\end{split}\]</div>
</li>
<li><p>Maximize the total variance <span class="math notranslate nohighlight">\(\sum_i \operatorname{Var}\left( Y_i \right)\)</span> of projected data <span class="math notranslate nohighlight">\(\boldsymbol{Y} =  \boldsymbol{W} ^\top \boldsymbol{X}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \boldsymbol{W}^*  = \underset{\boldsymbol{\boldsymbol{W} } }{\operatorname{argmax}} \, &amp; \operatorname{tr}\left( \boldsymbol{W} ^\top \boldsymbol{X} \boldsymbol{X} ^\top \boldsymbol{W} \right)   \\
     \text{s.t.}  &amp; \ \boldsymbol{W} ^\top \boldsymbol{W} = \boldsymbol{I}  
    \end{align}\end{split}\]</div>
</li>
</ul>
</div>
<div class="section" id="learning">
<h2>Learning<a class="headerlink" href="#learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sequential-maximization">
<h3>Sequential Maximization<a class="headerlink" href="#sequential-maximization" title="Permalink to this headline">¶</a></h3>
<p>The first variable in <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, i.e. <span class="math notranslate nohighlight">\(Y_1 = \boldsymbol{\alpha} \boldsymbol{x}\)</span> is obtained to maximize its variance, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{1} \equiv \operatorname{Var}\left(Y_{1}\right)=\max _{\left\Vert \boldsymbol{\alpha}  \right\Vert _2^2 = 1 } \boldsymbol{\alpha}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}
\]</div>
<p>Suppose the maximum is achieved at <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = \boldsymbol{\alpha} _1\)</span> and we call <span class="math notranslate nohighlight">\(Y_1\)</span> given below the first population principal component</p>
<div class="math notranslate nohighlight">
\[
Y_1 = \boldsymbol{\alpha} _1^T \boldsymbol{x}
\]</div>
<p>Successively for <span class="math notranslate nohighlight">\(i=2, \ldots, m\)</span> the variance of <span class="math notranslate nohighlight">\(Y_i\)</span> can be obtained by the following maximization</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;&amp;&amp;\lambda_{i} \equiv \operatorname{Var}\left(Y_{i}\right)=\max _{\alpha} \boldsymbol{\alpha}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}\\
&amp; &amp;\mathrm{s.t.}  \quad &amp;\boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}=1 \\
&amp; &amp; &amp; \ \boldsymbol{\alpha}^{\top} \boldsymbol{x} \text { being uncorrelated with } Y_{1}, \ldots, Y_{i-1}  
\end{aligned}
\end{split}\]</div>
<p>The maximum is achieved at <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = \boldsymbol{\alpha} _i\)</span> and the <span class="math notranslate nohighlight">\(i\)</span>-th population principal component is</p>
<div class="math notranslate nohighlight">
\[
Y_i = \boldsymbol{\alpha} _i^\top \boldsymbol{x}
\]</div>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Derivation<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">We consider the maximization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\max _{\boldsymbol{\alpha}} \quad &amp; \boldsymbol{\alpha}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}  \\
\text {s.t.} \quad &amp; \boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}=1
\end{align}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\quad
\]</div>
<p class="card-text">The Lagrangean is</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
L(\boldsymbol{\alpha}, \theta)=\boldsymbol{\alpha}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}-\lambda\left(\boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}-1\right)
\end{equation}
\]</div>
<p class="card-text">The first order conditions are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{\alpha}}
&amp;= 2 \boldsymbol{\Sigma} \boldsymbol{\alpha}-2 \lambda \boldsymbol{\alpha} \\
&amp;=\mathbf{0} \\
\Rightarrow \quad \quad \boldsymbol{\Sigma} \boldsymbol{\alpha} &amp;=\lambda \boldsymbol{\alpha}  \quad \quad \quad \quad (1)
\end{aligned}
\end{split}\]</div>
<p class="card-text">and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L}{\partial \lambda}
&amp;= 1-\boldsymbol{\alpha}^{\top} \boldsymbol{\alpha} \\
&amp;= 0 \\
\Rightarrow \quad \quad  \boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}
&amp;=1 \quad \quad \quad \quad (2)
\end{aligned}
\end{split}\]</div>
<p class="card-text">Premultiply <span class="math notranslate nohighlight">\((1)\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} ^\top\)</span> we have
$<span class="math notranslate nohighlight">\(
\boldsymbol{\alpha}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}=\lambda \boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}
\)</span>$</p>
<p class="card-text">Hence,</p>
<div class="math notranslate nohighlight">
\[
\lambda = \boldsymbol{\alpha} ^\top \boldsymbol{\Sigma} \boldsymbol{\alpha}
\]</div>
<p class="card-text">Note that <span class="math notranslate nohighlight">\((1)\)</span> also gives</p>
<div class="math notranslate nohighlight">
\[
(\boldsymbol{\Sigma}-\lambda \boldsymbol{I}) \boldsymbol{\alpha} =\mathbf{0}
\]</div>
<p class="card-text">which implies <span class="math notranslate nohighlight">\(\lambda\)</span> is the eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
<p class="card-text">Therefore, the maximized variance <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} ^\top \boldsymbol{\Sigma} \boldsymbol{\alpha}\)</span> equals to the largest eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
</div>
</details></div>
<div class="section" id="eigenvalue-decomposition">
<h3>Eigenvalue Decomposition<a class="headerlink" href="#eigenvalue-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Rather than obtaining the principal components sequentially, the principal components and their variances can be obtained simultaneously by solving for the eigenvectors and eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. Using the Spectral Decomposition Theorem,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma} = \sum_i^p \lambda_i \boldsymbol{\alpha} _i \boldsymbol{\alpha} _i ^\top = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^\top
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_2 &gt; \dots&gt; \lambda_p \ge 0\)</span> are ordered eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} =  \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_p)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _1, \ldots, \boldsymbol{\alpha} _p\)</span> are their corresponding normalized eigenvectors forming the column vectors of the orthogonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{U} = \left( \boldsymbol{\alpha} _1\  \boldsymbol{\alpha} _2 \ \ldots \  \boldsymbol{\alpha} _p \right)\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{U}  ^\top \boldsymbol{U}   = \boldsymbol{I}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _i ^\top \boldsymbol{\alpha} _j = 1\)</span> if <span class="math notranslate nohighlight">\(i=j\)</span> and 0 otherwise.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-th population principal component is defined as</p>
<div class="math notranslate nohighlight">
\[
Y_{k}=\boldsymbol{\alpha}_{k}^{\top} \boldsymbol{x}=\alpha_{1 k} X_{1}+\alpha_{2 k} X_{2}+\cdots+\alpha_{p k} X_{p}, \quad k=1, \ldots, p
\]</div>
<p>The principal component transform is then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{U} ^\top \boldsymbol{x}
\]</div>
</div>
</div>
<div class="section" id="special-cases">
<h2>Special Cases<a class="headerlink" href="#special-cases" title="Permalink to this headline">¶</a></h2>
<div class="section" id="variables-are-uncorrelated">
<h3>Variables are Uncorrelated<a class="headerlink" href="#variables-are-uncorrelated" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{X_i}\)</span> are uncorrelated, then <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix, i.e., <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \operatorname{diag}\left( \sigma_{11}, \sigma_{22}, \sigma_{pp} \right)\)</span>. Without loss of generality, assume <span class="math notranslate nohighlight">\(\sigma_{11} &gt; \sigma_{22} &gt; \sigma_{pp}\)</span>, then from its spectral decomposition <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \boldsymbol{U} ^\top \boldsymbol{\Lambda} \boldsymbol{U}\)</span>, we have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} = \boldsymbol{I}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \operatorname{diag}\left( \sigma_{ii} \right)\)</span>, or <span class="math notranslate nohighlight">\(\lambda_i = \sigma_{ii}\)</span>.</p></li>
</ul>
<p>Hence, the principal component is</p>
<div class="math notranslate nohighlight">
\[
Y_i = X_i
\]</div>
<p>Clearly, it is <strong>not</strong> necessary to perform PCA in this case.</p>
</div>
<div class="section" id="variables-are-perfectly-correlated">
<h3>Variables are Perfectly Correlated<a class="headerlink" href="#variables-are-perfectly-correlated" title="Permalink to this headline">¶</a></h3>
<p>In this case, the covariance matrix is not of full rank, i.e., <span class="math notranslate nohighlight">\(\left\vert \boldsymbol{\Sigma}  \right\vert = 0\)</span>. Then, some eiganvalues equal zero. In other words,</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 &gt; \lambda_2 &gt; \ldots, &gt; \lambda_m &gt; \lambda_{m+1} = \ldots = \lambda_p = 0
\]</div>
<p>Only <span class="math notranslate nohighlight">\(m\)</span> eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _i\)</span> can be obtained with <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{\alpha}_i  \right\Vert _2 ^2 =  1\)</span> .</p>
</div>
<div class="section" id="few-variables-have-extremely-large-variances">
<h3>Few Variables Have Extremely Large Variances<a class="headerlink" href="#few-variables-have-extremely-large-variances" title="Permalink to this headline">¶</a></h3>
<p>If a few variables have extremely large variances in comparison with other variables, they will dominate the first few principal components and give the foregone conclusion that a few principal components is sufficient in summarizing information. That conclusion may even be spurious, as the measurement scales, which affect the variances, are quite arbitrary in a lot of applications.</p>
<p>For example, <span class="math notranslate nohighlight">\(X_1\)</span> is measured in meters while <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span> are measured in kilometers. The first PC should have particularly large variance (<span class="math notranslate nohighlight">\(\lambda_1\)</span> is particularly large relative to <span class="math notranslate nohighlight">\(\lambda_2\)</span> and <span class="math notranslate nohighlight">\(\lambda_3\)</span>). This property suggests that if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>  are on different, or non-commensurable, measurement units, we should standardize them,</p>
<div class="math notranslate nohighlight">
\[
Z_i = \frac{X_i - \mu_i}{\sigma_i}
\]</div>
<p>before performing PCA.</p>
</div>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>All principal components are uncorrelated, i.e., <span class="math notranslate nohighlight">\(\operatorname{Cov}\left( Y_i, Y_j \right) = 0\)</span> for <span class="math notranslate nohighlight">\(i \ne j\)</span></p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Proof<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{Cov}\left(Y_{i}, Y_{j}\right) &amp;=\operatorname{Cov}\left(\boldsymbol{\alpha}_{i}^{\top} \boldsymbol{x}, \boldsymbol{\alpha}_{j}^{\top} \boldsymbol{x}\right) \\
&amp;=\mathrm{E}\left(\boldsymbol{\alpha}_{i}^{\top}(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\alpha}_{j}\right) \\
&amp;=\boldsymbol{\alpha}_{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}_{j} \\
&amp;=\boldsymbol{\alpha}_{i}^{\top} \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^{\top} \boldsymbol{\alpha}_{j} \\
&amp;=\left(\boldsymbol{\alpha}_{i}^{\top}\right)\left(\boldsymbol{\alpha}_{1} \boldsymbol{\alpha}_{2} \cdots \boldsymbol{\alpha}_{p}\right) \boldsymbol{\Lambda}\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{\top} \\
\boldsymbol{\alpha}_{2}^{\top} \\
\vdots \\
\boldsymbol{\alpha}_{p}^{\top}
\end{array}\right) \boldsymbol{\alpha}_{j} \\
&amp;=\boldsymbol{e}_{i}^{\top} \boldsymbol{\Lambda} \boldsymbol{e}_{j} \\
&amp;=0
\end{aligned}
\end{split}\]</div>
</div>
</details></li>
<li><p>The variance of the <span class="math notranslate nohighlight">\(i\)</span>-th principal component is <span class="math notranslate nohighlight">\(\lambda_i\)</span>, i.e. <span class="math notranslate nohighlight">\(\operatorname{Var}\left( Y_i \right) = \lambda_i\)</span>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Proof<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Var}\left( Y_i \right)
&amp;= \operatorname{Var}\left( \boldsymbol{\alpha} _i ^\top \boldsymbol{x}  \right) \\
&amp;= \boldsymbol{\alpha} _i ^\top \boldsymbol{\Sigma} \boldsymbol{\alpha} _i \\
&amp;= \boldsymbol{e}_i ^\top \boldsymbol{\Lambda} \boldsymbol{e}_i  \\
&amp;= \lambda_i
\end{align}\end{split}\]</div>
</div>
</details></li>
<li><p>The first principal component <span class="math notranslate nohighlight">\(Y_1 = \boldsymbol{\alpha} _1 ^\top \boldsymbol{x}\)</span> has the largest variance among all linear combinations of <span class="math notranslate nohighlight">\(X_i\)</span>’s. The <span class="math notranslate nohighlight">\(i=2, \ldots, p\)</span>, the <span class="math notranslate nohighlight">\(i\)</span>-th principal component has the largest variance among all linear combinations of <span class="math notranslate nohighlight">\(X_i\)</span>’s, which are uncorrelated with the first <span class="math notranslate nohighlight">\((i-1)\)</span> principal components.</p></li>
<li><p>The principal component preserve the total variance</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^{p} \operatorname{Var}\left(Y_{i}\right)=\sum_{i=1}^{p} \operatorname{Var}\left(X_{i}\right)
    \]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^{p} \lambda_{i}=\sum_{i=1}^{p} \sigma_{i i}
    \]</div>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Proof<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sum_{i=1}^{p} \sigma_{i i} &amp;=\operatorname{tr}(\boldsymbol{\Sigma}) \\
&amp;=\operatorname{tr}\left(\sum_{i=1}^{p} \lambda_{i} \boldsymbol{\alpha}_{i} \boldsymbol{\alpha}_{i}^{\top}\right) \\
&amp;=\sum_{i=1}^{p} \lambda_{i} \operatorname{tr}\left(\boldsymbol{\alpha}_{i} \boldsymbol{\alpha}_{i}^{\top}\right) \\
&amp;=\sum_{i=1}^{p} \lambda_{i} \operatorname{tr}\left(\boldsymbol{\alpha}_{i}^{\top} \boldsymbol{\alpha}_{i}\right) \\
&amp;=\sum_{i=1}^{p} \lambda_{i}
\end{aligned}
\end{split}\]</div>
</div>
</details></li>
<li><p>The correlation between a principal component <span class="math notranslate nohighlight">\(Y_j\)</span> and an original variable <span class="math notranslate nohighlight">\(X_i\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
    \operatorname{Corr}\left( X_i, Y_j \right) = \frac{\sqrt{\lambda_j}a_{ij}}{\sqrt{\sigma_{ii}}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_{ij}\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>-th element of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _j\)</span>.</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Proof<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{Cov}\left(X_{i}, Y_{j}\right) &amp;=\operatorname{Cov}\left(X_{i}, \boldsymbol{\alpha}_{j}^{\top} \boldsymbol{x}\right) \\
&amp;=\operatorname{Cov}\left(\boldsymbol{e}_{i}^{\top} \boldsymbol{x}, \boldsymbol{\alpha}_{j}^{\top} \boldsymbol{x}\right) \\
&amp;=\boldsymbol{e}_{i}^{\top} \boldsymbol{\Sigma} \boldsymbol{\alpha}_{j} \\
&amp;=\boldsymbol{e}_{i}^{\top} \sum_{k=1}^{p} \lambda_{k} \boldsymbol{\alpha}_{k} \boldsymbol{\alpha}_{k}^{\top} \boldsymbol{\alpha}_{j} \\
&amp;=\lambda_{j} \boldsymbol{e}_{i}^{\top} \boldsymbol{\alpha}_{j} \boldsymbol{\alpha}_{j}^{\top} \boldsymbol{\alpha}_{j} \\
&amp;=\lambda_{j} \boldsymbol{e}_{i}^{\top} \boldsymbol{\alpha}_{j} \\
&amp;=\lambda_{j} \alpha_{i j}
\end{aligned}
\end{split}\]</div>
<p class="card-text">and then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\operatorname{Corr}\left(X_{i}, Y_{j}\right)
&amp;=\frac{\operatorname{Cov}\left(X_{i}, Y_{j}\right)}{\sqrt{\operatorname{Var}\left(X_{i}\right) \operatorname{Var}\left(Y_{j}\right)}} \\
&amp;=\frac{\lambda_{j} \alpha_{i j}}{\sqrt{\sigma_{i i} \lambda_{j}}} \\
&amp;=\frac{\sqrt{\lambda_{j}} \alpha_{i j}}{\sqrt{\sigma_{i i}}}
\end{align}\end{split}\]</div>
</div>
</details></li>
<li><p>If the correlation matrix <span class="math notranslate nohighlight">\(\boldsymbol{\rho} = \boldsymbol{D}^{-1}\boldsymbol{\Sigma} \boldsymbol{D}^{-1}\)</span> instead of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is used, i.e. variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span> are standardized, then</p>
<div class="math notranslate nohighlight">
\[
   \sum_i^p \lambda_i = \sum_i^p \sigma_{ii} = p
   \]</div>
</li>
</ol>
</div>
<div class="section" id="tuning">
<h2>Tuning<a class="headerlink" href="#tuning" title="Permalink to this headline">¶</a></h2>
<p>There are several ways to choose the number of principal components to retain.</p>
<ol>
<li><p><strong>Cumulative proportion cutoff</strong>:</p>
<p>Include the components such that the cumulative proportion of the total variance explained is just more than a threshold value, say 80%, i.e., if</p>
<div class="math notranslate nohighlight">
\[
    \begin{equation}
    \frac{\sum_{i=1}^{m} \ell_{i}}{\sum_{i=1}^{p} \ell_{i}} &gt;0.8
    \end{equation}
    \]</div>
<p>This method keeps <span class="math notranslate nohighlight">\(m\)</span> principal components.</p>
</li>
<li><p><strong>Proportion cutoff</strong></p>
<p>Select the components whose eigenvalues are greater than a threshold value, say average of eigenvalues; for correlation matrix input, this average is <span class="math notranslate nohighlight">\(p^{-1} \sum_{i=1}^{p} \ell_{i}=p^{-1} p=1\)</span> if we use the correlation matrix <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span>.</p>
</li>
<li><p><strong>Scree plot</strong></p>
<p>Construct the so-called scree plot of the eigenvalue <span class="math notranslate nohighlight">\(\ell_i\)</span> on the vertical axis versus <span class="math notranslate nohighlight">\(i\)</span> on horizontal axis with equal intervals for <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, p\)</span>, and join the points into a decreasing polygon. Try to find a “clean-cut” where the polygon “levels off” so that the first few eigenvalues seem to be far apart from the others.</p>
 <div align="center">
 <img src="../imgs/pca_scree_plot.png" width = "40%" alt="scree plot" align=center />
 </div>
</li>
<li><p><strong>Hypothesis testing</strong></p>
<p>Perform formal significance tests to determine the larger an unequal eigenvalues and retain the principal components to these eigenvalues.</p>
</li>
<li><p><strong>Reconstruction loss</strong></p>
<p>Recall the principal component transform <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{U} ^\top \boldsymbol{x}\)</span>. Hence, <span class="math notranslate nohighlight">\(\boldsymbol{U} \boldsymbol{y} = \boldsymbol{x}\)</span>. To reconstruct <span class="math notranslate nohighlight">\(\hat{\boldsymbol{x} }\)</span> by the first <span class="math notranslate nohighlight">\(k\)</span> components <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _1, \ldots, \boldsymbol{\alpha} _k\)</span> in <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> , we can use the expansion</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{x} }=\sum_{j=1}^{k}y_j \boldsymbol{\alpha} _{j} = \sum_{j=1}^{k}\left(\boldsymbol{\alpha}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{\alpha} _{j}
    \]</div>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> was centered before PCA, we add the mean back
$<span class="math notranslate nohighlight">\(
 \hat{\boldsymbol{x} }=\boldsymbol{\mu} _{\boldsymbol{x}} +\sum_{j=1}^{k}\left(\boldsymbol{\alpha}_{j}^{\top} \boldsymbol{x} \right) \boldsymbol{\alpha} _{j}
 \)</span>$</p>
<p>To choose an optimal number of principal components <span class="math notranslate nohighlight">\(k\)</span>, we can examine the magnitude of the residual <span class="math notranslate nohighlight">\(\left\Vert \boldsymbol{x} - \hat{\boldsymbol{x} } \right\Vert ^2\)</span>. The expected residual corresponds to variance in the <strong>remaining</strong> subspace.</p>
 <div align="center">
 <img src="../imgs/pca_reconstruction.png" width = "90%" alt="" align=center />
 </div>
</li>
<li><p><strong>Downstream task performance</strong></p>
<p>Use the performance of the downstream task to choose an optimal number of principal components.</p>
</li>
</ol>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="geometric-meaning-direction-of-variation">
<h3>Geometric Meaning: Direction of Variation<a class="headerlink" href="#geometric-meaning-direction-of-variation" title="Permalink to this headline">¶</a></h3>
<p>For the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, thelcenter location is determined by <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ \boldsymbol{x}\)</span> and the variation is captured by each principal direction <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _i\)</span></p>
<p>For the multinormal distribution, the family of <strong>contours</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> (on each of which the pdf is a constant) is a family of ellipsoids in the original coordinate system <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> satisfying the following equation for a
constant <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})=c^{2}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> serves as an index of the family. This family of ellipsoids have orthogonal principal axes</p>
<div class="math notranslate nohighlight">
\[
\pm c\lambda_i^{1/2}\boldsymbol{\alpha}_i, i= 1, 2, \ldots, p
\]</div>
<p>with length</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(2c\lambda_i^{1/2}\)</span></p></li>
<li><p>directional cosines as coefficients given in <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} _i\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>-th axis.</p></li>
</ul>
<div align="center">
<img src="../imgs/pca_pc_ellipsoids.png" width = "80%" alt="" align=center />
</div>
<p>Another example is hand written digits. Suppose <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ \boldsymbol{x}\)</span> is the sample mean that determines the “mean” appearance of the digit <span class="math notranslate nohighlight">\(2\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_j\)</span> is a principal direction which determines the location of variation of the black/white pixels.</p>
<div align="center">
<img src="../imgs/pca_pc_digits.png" width = "50%" alt="" align=center />
</div>
</div>
<div class="section" id="proportion-explained">
<h3>Proportion Explained<a class="headerlink" href="#proportion-explained" title="Permalink to this headline">¶</a></h3>
<p>The proportion of total variance explained by <span class="math notranslate nohighlight">\(Y_i\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda_i}{\sum_{j=1}^p \lambda_j}\]</div>
<p>is considered as a measure of <strong>importance</strong> of <span class="math notranslate nohighlight">\(Y_i\)</span> in a more parsimonious description of the system.</p>
</div>
<div class="section" id="score-of-an-observation-in-sample-data">
<h3>Score of an Observation in Sample Data<a class="headerlink" href="#score-of-an-observation-in-sample-data" title="Permalink to this headline">¶</a></h3>
<p>For a data set of <span class="math notranslate nohighlight">\(n\)</span> observations, we decompose the sample covariance matrix <span class="math notranslate nohighlight">\(S\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\boldsymbol{S} =\sum_{i=1}^{p} \ell_{i} \boldsymbol{a} _{i} \boldsymbol{a} _{i}^{\top}=\boldsymbol{A} \boldsymbol{L}  \boldsymbol{A} ^\top
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_i\)</span> are eigencalues of <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{a} _i\)</span>’s are their corresponding normalized eigenvectors.</p>
<p>The <span class="math notranslate nohighlight">\(i\)</span>-th <strong>sample</strong> principal component is defined as</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
Y_{i}=\boldsymbol{a}_{i}^{\top} \boldsymbol{x}=a_{1 i} X_{1}+a_{2 i} X_{2}+\cdots+\alpha_{p i} X_{p}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\begin{equation}
\boldsymbol{a}_{i}^{\top}=\left(\begin{array}{llll}
a_{1 i} &amp; a_{2 i} &amp; \cdots &amp; a_{p i}
\end{array}\right)
\end{equation}\)</span>.</p>
<p>The data layout is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{array}{cccccccc}
&amp;&amp; \text{Data} \ \ \boldsymbol{X}  &amp;&amp;&amp;&amp;\text{PC} \ \ \boldsymbol{Y} &amp;\\
\hline X_{1} &amp; X_{2} &amp; \cdots &amp; X_{p} &amp; \quad \quad Y_{1} &amp; Y_{2} &amp; \cdots &amp; Y_{p} \\
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 p} &amp; \quad \quad y_{11} &amp; y_{12} &amp; \cdots &amp; y_{1 p} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 p} &amp; \quad \quad y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2 p} \\
&amp; &amp; \vdots &amp; &amp; &amp; &amp; \vdots &amp; \\
x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n p} &amp; \quad \quad y_{n 1} &amp; y_{n 2} &amp; \cdots &amp; y_{n p}
\end{array}
\end{equation}
\end{split}\]</div>
<p>where the corresponding row vectors on the data matrices are related as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{y} _i ^\top = \boldsymbol{x} _i ^\top \boldsymbol{A} , i= 1, 2, \ldots, n\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{y} _i\)</span> can be interpreted as a vector of principal component scores for the <span class="math notranslate nohighlight">\(i\)</span>-th observation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Properties of the population principal components are all valid in the sample context, by replaceing</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}, \boldsymbol{\Sigma} , \boldsymbol{\rho}, \lambda_i, \boldsymbol{\alpha} _i\]</div>
<p>by</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{x}}, \boldsymbol{S} , \boldsymbol{R} , \ell_i, \boldsymbol{a} _i\]</div>
</div>
</div>
</div>
<div class="section" id="cons">
<h2>Cons<a class="headerlink" href="#cons" title="Permalink to this headline">¶</a></h2>
<p><strong>Sensitive to Variable Transformation</strong></p>
<p>The results of PCA are not invariant under a linear transformation and, even worse, there is no easy correspondence between the two sets of results <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y} ^\prime\)</span>, before and after the linear transformation. For example, the PCA using <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is not the same as the PCA using <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span> and we cannot use the PCA from <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span> to get the PCA results from the original variables.</p>
<p>If the two sets of results are consistent to each other, the PCA based on <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>  may be preferred in some situation. If they are very different, or even contradictory, subject-matter knowledge and/or wisdom are needed to make a choice.</p>
<p>The PCA based on covariance matrix is preferred when the original measurements units are very important, like in many applications in
natural sciences. However, when the units of measurement are of artificial nature, like scores in some questions as frequently used in social sciences, the PCA based on correlation matrix is preferred.</p>
<p><strong>Direction of Variance may not be Discriminative</strong></p>
<p>But note that the direction of largest variance need not to be the most discriminative direction. See the example below.</p>
<div align="center">
<img src="../imgs/pca_classification.png" width = "90%" alt="" align=center />
</div>
<p>If we knew the labels, we could use a supervised dimensionality reduction, e.g. linear discriminant analysis.</p>
</div>
<div class="section" id="relation-to">
<h2>Relation to<a class="headerlink" href="#relation-to" title="Permalink to this headline">¶</a></h2>
<div class="section" id="svd">
<h3>SVD<a class="headerlink" href="#svd" title="Permalink to this headline">¶</a></h3>
<p>Recall the SVD of the data matrix</p>
<div class="math notranslate nohighlight">
\[
X = \boldsymbol{U} \boldsymbol{S} \boldsymbol{V} ^\top
\]</div>
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span> is centered, then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma} = \boldsymbol{X} ^\top \boldsymbol{X} = \boldsymbol{V} \boldsymbol{S} ^\top \boldsymbol{S} \boldsymbol{V} ^\top
\]</div>
<p>So the right singular vectors <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> are the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span>, the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{X} ^\top \boldsymbol{X}\)</span> are proportional to the squared singular values of <span class="math notranslate nohighlight">\(\sigma_i\)</span></p>
<p>So we can compute the PCS via an SVD.</p>
</div>
<div class="section" id="compression">
<h3>Compression<a class="headerlink" href="#compression" title="Permalink to this headline">¶</a></h3>
<p>Instead of storing the <span class="math notranslate nohighlight">\(n \times p\)</span> data matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, not we need to store the <span class="math notranslate nohighlight">\(p \times 1\)</span> mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} _ \boldsymbol{x}\)</span> and the <span class="math notranslate nohighlight">\(m\times p\)</span> projection matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, and the <span class="math notranslate nohighlight">\(n \times m\)</span> projected data matrix <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span>.</p>
<p>To transmit <span class="math notranslate nohighlight">\(N\)</span> examples, we need <span class="math notranslate nohighlight">\(p+pm+nm\)</span> numbers instead of <span class="math notranslate nohighlight">\(np\)</span>.</p>
</div>
<div class="section" id="gaussians">
<h3>Gaussians<a class="headerlink" href="#gaussians" title="Permalink to this headline">¶</a></h3>
<p>PCA essentially models variance in the data. What distribution is characterized by variance? Gaussian.
can be described by Gaussians</p>
<p>Probabilistic PCA</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>For a classification task, we can perform PCA on the features before fitting the data to a classifier. The classifier might be more accurate since PCA reduces noise.</p>
</div>
</div>
<div class="section" id="extension">
<h2>Extension<a class="headerlink" href="#extension" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic PCA  is a method of fitting a constrained Gaussian, where some variances are equal.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\boldsymbol{\Sigma}=\boldsymbol{U}\left[\begin{array}{ccccccc}
\lambda_{1} &amp; \ldots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \\
&amp; \ddots &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots \\
0 &amp; \ldots &amp; \lambda_{k} &amp; \ldots &amp; \ldots &amp; \ldots \\
0 &amp; \ldots &amp; 0 &amp; \sigma^{2} &amp; 0 &amp; \ldots \\
&amp; &amp; &amp; &amp; \ddots &amp; \\
0 &amp; \ldots &amp; \ldots &amp; \ldots &amp; 0 &amp; \sigma^{2}
\end{array}\right] \boldsymbol{U}^{T}
\end{equation}
\end{split}\]</div>
<p>Estimate for the noise variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\sigma^{2}=\frac{1}{d-k} \sum_{j=k+1}^{d} \lambda_{j}
\end{equation}
\]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./33-dimensionality-reduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00-dimensionality-reduction.html" title="previous page">Dimensionality Reduction</a>
    <a class='right-next' id="next-link" href="13-canonical-correlation-analysis.html" title="next page">Canonical Corerlation Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>