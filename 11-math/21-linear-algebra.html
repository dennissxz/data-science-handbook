
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Algebra &#8212; Data Science Handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Programming" href="51-linear-programming.html" />
    <link rel="prev" title="Geometry" href="31-geometry.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Data Science Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Data Science Handbook
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00-math.html">
   Math
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="11-combinatorics.html">
     Combinatorics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="20-vector-spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="31-geometry.html">
     Geometry
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="51-linear-programming.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="52-non-linear-programming.html">
     Non-linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="90-puzzles.html">
     Puzzles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-probabilities/00-probabilities.html">
   Probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/11-expectation-and-variance.html">
     Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/13-correlation-and-dependence.html">
     Correlation and Dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/31-bayesian-theorem.html">
     Bayesian’s Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/51-markov-chain.html">
     Markov Chain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/71-sampling.html">
     Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/90-multivariate-notations.html">
     Multivariate Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-exponential-families.html">
     Exponential Families
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-probabilities/91-large-sample-theory.html">
     Large Sample Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-statistics/00-statistics.html">
   Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/11-sample-survey.html">
     Sample Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/13-randomized-trial.html">
     Randomized Controlled Trials
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/21-hypothesis-testing.html">
     Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/23-common-tests.html">
     Common Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/33-confusion-matrix.html">
     Confusion Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/41-maximum-likelihood-estimation.html">
     Maximum Likelihood Estimator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-statistics/43-estimators-evaluation.html">
     Estimators Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-tools/00-tools.html">
   Tools
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/11-python.html">
     Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/21-r.html">
     R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/31-sql.html">
     SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/41-latex.html">
     LaTeX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-tools/51-myst.html">
     MyST Markdown
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20-algorithms-concepts/00-algorithms-concepts.html">
   Algorithms Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/51-polynomial-reduction.html">
     Polynomial Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/53-P-and-NP.html">
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(NP\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20-algorithms-concepts/61-randomized-algo.html">
     Randomized Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21-greedy-algorithms/00-greedy-algorithms.html">
   Greedy Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/11-interval-scheduling.html">
     Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21-greedy-algorithms/31-huffman-coding.html">
     Huffman Coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../23-dynamic-programming/00-dynamic-programming.html">
   Dynamic Programming
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/11-weighted-interval-scheduling.html">
     Weighted Interval Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/13-longest-common-subsequence.html">
     Longest Common Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/15-longest-increasing-subsequence.html">
     Longest Increasing Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/17-largest-sum-subsequence.html">
     Largest Sum Subsequence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/31-knapsack.html">
     Minimum Knapsack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23-dynamic-programming/51-chain-matrix-multiplication.html">
     Chain Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../25-graph-related/00-graph-related.html">
   Graph Related
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/13-shortest-path.html">
     Shortest Path
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/21-minimum-spanning-tree.html">
     Minimum Spanning Tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/31-maximum-flow.html">
     Maximum Flow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/32-matching.html">
     Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/42-maximum-independent-set.html">
     Maximum Independent Set in Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25-graph-related/91-LP-max-flow-min-cut.html">
     LP on Max-flow and Min-cut
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../26-algo-for-big-data/00-algo-for-big-data.html">
   For Big Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../26-algo-for-big-data/10-streaming.html">
     Streaming Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../30-ml-basics/00-ml-basics.html">
   Machine Learning Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/02-taxonomy.html">
     Taxonomy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/03-information-theory.html">
     Information Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/05-kernels.html">
     Kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/11-data-issues.html">
     Data Issues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/51-semi-supervised.html">
     Semi-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/53-self-supervised.html">
     Self-supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../30-ml-basics/61-fourier-transform.html">
     Fourier Transform-based Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../31-regression/00-regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/11-lm-estimation.html">
     Linear Models - Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/12-lm-inference.html">
     Linear Models - Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/13-lm-diagnosis.html">
     Linear Models - Diagnosis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/14-lm-advanced.html">
     Linear Models - Advanced Topics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/21-generalized-linear-models.html">
     Generalized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/22-logistic-regression.html">
     Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/23-multinomial-logitsitc.html">
     Multinomial Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/24-ordinal-logistic.html">
     Ordinal Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/25-poisson-regression.html">
     Poisson Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../31-regression/31-multivariate-regression.html">
     Multivariate Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../32-classification/00-classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/09-k-nearest-neighbors.html">
     K-nearest neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/11-support-vector-machine.html">
     Support Vector Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../32-classification/21-decision-tree.html">
     Decision Tree
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../33-dimensionality-reduction/00-dimensionality-reduction.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/11-principal-component-analysis.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/13-canonical-correlation-analysis.html">
     Canonical Correlation Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/21-multidimensional-scaling.html">
     Multidimensional Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/23-graph-based-spectral-methods.html">
     Graph-based Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/25-t-SNE.html">
     SNE and
     <span class="math notranslate nohighlight">
      \(t\)
     </span>
     -SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/31-kernel-pca.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/32-kernel-cca.html">
     Kernel CCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/41-factor-analysis.html">
     Factor Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../33-dimensionality-reduction/51-correspondence-analysis.html">
     Correspondence Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../34-clustering/00-clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/11-k-means.html">
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/13-agglomerative-methods.html">
     Agglomerative Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/31-spectral-clustering.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../34-clustering/41-gaussian-mixtures.html">
     Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../35-graphical-models/00-graphical-models.html">
   Graphical Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/03-random-walks.html">
     Random Walks in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/11-hidden-markov-models.html">
     Hidden Markov Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/31-topic-models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../35-graphical-models/33-language-models.html">
     Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../37-neural-networks/00-neural-networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/01-stochastic-gradient-descent.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/03-trainability.html">
     Trainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/05-regularization.html">
     Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/11-autoencoders.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/13-variational-autoencoders.html">
     Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/31-sequential-models.html">
     Sequential Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../37-neural-networks/41-GAN.html">
     Generative Adversarial Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../38-ml-for-graph-data/00-ml-for-graph-data.html">
   For Graph-structured Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/01-graph-basics.html">
     Graph Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/11-descriptive-analysis.html">
     Descriptive Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/13-sampling-and-estimation.html">
     Sampling and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/21-modeling.html">
     Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/31-topology-inference.html">
     Topology Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../38-ml-for-graph-data/41-processes.html">
     Processes on Graphs
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/11-math/21-linear-algebra.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dennissxz/data-science-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dennissxz/data-science-handbook/issues/new?title=Issue%20on%20page%20%2F11-math/21-linear-algebra.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-operations">
   Matrix Operations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transpose">
     Transpose
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#determinant">
     Determinant
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse">
     Inverse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trace">
     Trace
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">
     Eigenvalues and Eigenvectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-matrices">
   Special Matrices
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetric-matrices">
     Symmetric Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#orthogonal-matrices">
     Orthogonal Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#idempotent-matrices">
     Idempotent Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reflection-matrices">
     Reflection Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projection-matrices">
     Projection Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similar-matrices">
     Similar Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#positive-semi-definite">
     Positive (Semi-)Definite
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-negative-definite">
     Conditional Negative Definite
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-decomposition">
   Matrix Decomposition
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     Spectral Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cholesky-decomposition">
     Cholesky Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#canonical-decomposition">
     Canonical Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular Value Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qr-decomposition">
     QR Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lu-decomposition">
     LU Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#schur-decomposition">
     Schur Decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-topics">
   More Topics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-differentiation">
     Matrix Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-norms">
     Matrix Norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#johnson-lindenstrauss-lemma">
     Johnson-Lindenstrauss Lemma
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#low-rank-approximation">
     Low-rank Approximation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perturbation-and-davis-kahan-theorem">
     Perturbation and Davis-Kahan Theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marchenkopastur-distribution">
     Marchenko–Pastur Distribution
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-algebra">
<h1>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a></h1>
<p>reference:</p>
<ul class="simple">
<li><p>Numerical Linear Algebra, Volker Mehrmann <a class="reference external" href="http://www.hamilton.ie/ollie/Downloads/NLA10.pdf">link</a></p></li>
</ul>
<div class="section" id="matrix-operations">
<h2>Matrix Operations<a class="headerlink" href="#matrix-operations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="transpose">
<h3>Transpose<a class="headerlink" href="#transpose" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p><span class="math notranslate nohighlight">\(\boldsymbol{A} ^\top =\left( a_{ji} \right)\)</span></p>
</dd>
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \left(\boldsymbol{A}^{\top}\right)^{\top} &amp;=\boldsymbol{A} \\
  (\boldsymbol{A}+\boldsymbol{B})^{\top} &amp;=\boldsymbol{A}^{\top}+\boldsymbol{B}^{\top} \\
  (\boldsymbol{A} \boldsymbol{B})^{\top} &amp;=\boldsymbol{B}^{\top} \boldsymbol{A}^{\top} \\
  \left(\begin{array}{ll}
  \boldsymbol{A} &amp; \boldsymbol{B} \\
  \boldsymbol{C} &amp; \boldsymbol{D}
  \end{array}\right)^{\top} &amp;=\left(\begin{array}{l}
  \boldsymbol{A}^{\top} &amp; \boldsymbol{C}^{\top} \\
  \boldsymbol{B}^{\top} &amp; \boldsymbol{D}^{\top}
  \end{array}\right)
  \end{aligned}
  \end{split}\]</div>
</dd>
</dl>
</div>
<div class="section" id="determinant">
<h3>Determinant<a class="headerlink" href="#determinant" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The determinant is a scalar value function of a square matrix. The Leibniz formula is</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{det}(A)=\sum_{\sigma \in S_{n}}\left(\operatorname{sgn}(\sigma) \prod_{i=1}^{n} a_{i, \sigma_{i}}\right)
  \]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is a permutation of set <span class="math notranslate nohighlight">\({1, 2, \ldots, n}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(S_n\)</span> is the set of all such permutations.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_i\)</span> is the value in the <span class="math notranslate nohighlight">\(i\)</span>-th position after the reordering <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{sgn}{\sigma}\)</span> is the signature of <span class="math notranslate nohighlight">\(\sigma\)</span>, which is <span class="math notranslate nohighlight">\(1\)</span> if the reordering given by <span class="math notranslate nohighlight">\(\sigma\)</span> can be achieved by successively interchanging two entries an even number of times, and <span class="math notranslate nohighlight">\(-1\)</span> otherwise.</p></li>
</ul>
</dd>
</dl>
<p>The absolute value of the determinant of a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} =[\boldsymbol{a} \ \boldsymbol{b}] \in \mathbb{R} ^{2 \times 2}\)</span> can be interpreted as the area of parallelogram spanned by the vectors <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>. The absolute value of the determinant of a <span class="math notranslate nohighlight">\(3 \times 3\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} = [\boldsymbol{a} \ \boldsymbol{b} \ \boldsymbol{c}] \in \mathbb{R} ^{3 \times 3}\)</span> equals the volume of a parallelepiped spanned by the vectors <span class="math notranslate nohighlight">\(\boldsymbol{a} ,\boldsymbol{b} ,\boldsymbol{c}\)</span>. This extends to <span class="math notranslate nohighlight">\(n\)</span>-dimensional parallelotope <span class="math notranslate nohighlight">\(P\)</span>: <span class="math notranslate nohighlight">\(\operatorname{det}(\boldsymbol{A}) = \pm \operatorname{vol} (P)\)</span>.</p>
<dl class="simple myst">
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align}
  |\boldsymbol{A} \boldsymbol{B}|&amp;=|\boldsymbol{A}||\boldsymbol{B}| \\
  \left\vert\begin{array}{l}
  \boldsymbol{A} \quad C \\
  0 \quad \boldsymbol{B}
  \end{array}\right\vert&amp;=| \boldsymbol{A}|| \boldsymbol{B} \mid \\
  \left|\boldsymbol{I}_{p}+\boldsymbol{A} \boldsymbol{B}\right|&amp;=\left|\boldsymbol{I}_{q}+\boldsymbol{B} \boldsymbol{A}\right|
  \end{align}
  \end{split}\]</div>
</dd>
</dl>
</div>
<div class="section" id="inverse">
<span id="matrix-inverse"></span><h3>Inverse<a class="headerlink" href="#inverse" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>The inverse of a square matrix is <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{-1}\)</span> such that <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{A} ^{-1} = \boldsymbol{I}\)</span>.</p>
</dd>
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align}
  \left(\boldsymbol{A}^{\top}\right)^{-1}&amp;=\left(\boldsymbol{A}^{-1}\right)^{\top} \\
  (\boldsymbol{A} \boldsymbol{B})^{-1}&amp;=\boldsymbol{B}^{-1} \boldsymbol{A}^{-1} \\
  \left|\boldsymbol{A}^{-1}\right|&amp;=|\boldsymbol{A}|^{-1} \\
  (\boldsymbol{A}+\boldsymbol{C B D})^{-1}
  &amp;=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1} \boldsymbol{C B}\left(\boldsymbol{B}+\boldsymbol{B D A}^{-1} \boldsymbol{C B}\right)^{-1} \boldsymbol{B D A}^{-1} \\
  \left(\boldsymbol{A}+\boldsymbol{c} \boldsymbol{d}^{\top}\right)^{-1}&amp;=\boldsymbol{A}^{-1}-\frac{\boldsymbol{A}^{-1} \boldsymbol{c} \boldsymbol{d}^{\top} \boldsymbol{A}^{-1}}{1+\boldsymbol{d}^{\top} \boldsymbol{A}^{-1} \boldsymbol{c}} \\
  \left|\boldsymbol{A}+\boldsymbol{c} \boldsymbol{d}^{\top}\right|&amp;=|\boldsymbol{A}|\left(1+\boldsymbol{d}^{\top} \boldsymbol{A}^{-1} \boldsymbol{c}\right)
  \end{align}
  \end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(
\boldsymbol{M}=\left[\begin{array}{cc}
\boldsymbol{A} &amp; \boldsymbol{b} \\
\boldsymbol{b}^{\top} &amp; c
\end{array}\right]
\)</span> Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \boldsymbol{M}^{-1}=\left[\begin{array}{cc}
  \left(\boldsymbol{A}-\frac{1}{c} \boldsymbol{b} \boldsymbol{b}^{\top}\right)^{-1} &amp; -\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \\
  -\frac{1}{k} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; \frac{1}{k}
  \end{array}\right]=\left[\begin{array}{cc}
  \boldsymbol{A}^{-1}+\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; -\frac{1}{k} \boldsymbol{A}^{-1} \boldsymbol{b} \\
  -\frac{1}{k} \boldsymbol{b}^{\top} \boldsymbol{A}^{-1} &amp; \frac{1}{k}
  \end{array}\right]
  \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k = c- \boldsymbol{b} ^\top \boldsymbol{A} ^{-1} \boldsymbol{b}\)</span>.</p>
</dd>
</dl>
</div>
<div class="section" id="trace">
<h3>Trace<a class="headerlink" href="#trace" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>For a square matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\operatorname{tr}\left( \boldsymbol{A}  \right)\)</span> is the sum of the diagonal elements</p>
<div class="math notranslate nohighlight">
\[
  \operatorname{tr}\left( \boldsymbol{A}  \right) = \sum_i a_{ii}
  \]</div>
</dd>
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \operatorname{tr}(\boldsymbol{A}+\boldsymbol{B})
  &amp;=\operatorname{tr}(\boldsymbol{A})+\operatorname{tr}(\boldsymbol{B}) \\
  \operatorname{tr}(\boldsymbol{A B}) &amp;=\operatorname{tr}(\boldsymbol{B} \boldsymbol{A}) \\
  \operatorname{tr}(\alpha \boldsymbol{A}) &amp;=\alpha \operatorname{tr}(\boldsymbol{A}) \\
  \end{align}\end{split}\]</div>
</dd>
</dl>
</div>
<div class="section" id="eigenvalues-and-eigenvectors">
<span id="eigenvalue-eigenvector"></span><h3>Eigenvalues and Eigenvectors<a class="headerlink" href="#eigenvalues-and-eigenvectors" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>If we restricted to <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span> then some matrices do not have real eigenvalues.</p>
</div>
<dl class="simple myst">
<dt>Definitions</dt><dd><p>Let <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> square matrix and let <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> be an <span class="math notranslate nohighlight">\(n\times 1\)</span> nonzero vector that <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{x} = \lambda \boldsymbol{x}\)</span>. Then, <span class="math notranslate nohighlight">\(\lambda \in \mathbb{C}\)</span> is called an eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is called an eigenvector corresponding to eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. The eigenvalues are the solutions of the <strong>characteristic function</strong></p>
<div class="math notranslate nohighlight">
\[
  \left\vert \boldsymbol{A} - \lambda \boldsymbol{I}  \right\vert = 0
  \]</div>
</dd>
</dl>
<p>For a fixed <span class="math notranslate nohighlight">\(\lambda\)</span>, the non-zero solution <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> to <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v}\)</span> is called an <strong>eigenvector</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> corresponding to <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<dl class="simple myst">
<dt>Properties</dt><dd><ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(\boldsymbol{A}_{n\times n}\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2, \ldots, \lambda_n\)</span>, we have</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\operatorname{tr}(\boldsymbol{A}) =\sum_{i=1}^{n} \lambda_{i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|\boldsymbol{A}|=\prod_{i=1}^{n} \lambda_{i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\left|\boldsymbol{I}_{n} \pm \boldsymbol{A}\right|=\prod_{i=1}^{n}\left(1 \pm \lambda_{i}\right)\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\lambda_1\ge \ldots, \ge \lambda_n\)</span>, we say <span class="math notranslate nohighlight">\(\lambda_1\)</span> is the <strong>largest</strong> (first, top) eigenvalue, and <span class="math notranslate nohighlight">\(\lambda_n\)</span> is the <strong>smallest</strong> (last, bottom) eigenvalue. Same for corresponding eigenvectors.</p></li>
</ul>
</li>
<li><p>The nonzero eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{B}\)</span> are the same as those of <span class="math notranslate nohighlight">\(\boldsymbol{B} \boldsymbol{A}\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}  + \boldsymbol{B} = \boldsymbol{I}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{A}\boldsymbol{v} = \lambda \boldsymbol{v}\)</span>, then we can see <span class="math notranslate nohighlight">\(\boldsymbol{B} \boldsymbol{v} = (\boldsymbol{I} - \boldsymbol{A} )\boldsymbol{v} = (1-\lambda)\boldsymbol{v}\)</span>.</p>
<ul>
<li><p>if <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> has eigen pairs <span class="math notranslate nohighlight">\((\lambda_i, \boldsymbol{v} _i)\)</span> then <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> has eigen pairs <span class="math notranslate nohighlight">\((1-\lambda_i, \boldsymbol{v} _i)\)</span></p></li>
<li><p>the sequence of pairs is reversed, e.g. the largest eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is the smallest eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span>.</p></li>
</ul>
</li>
</ul>
</dd>
</dl>
<div class="note admonition">
<p class="admonition-title"> Uniqueness of eigenvalues and eigenvectors</p>
<p>For an <span class="math notranslate nohighlight">\(N \times N\)</span> square matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>,</p>
<ul>
<li><p>Eigenvalues <strong>may not</strong> be unique. The characteristic function <span class="math notranslate nohighlight">\(p(\lambda)=\operatorname{det}(\boldsymbol{A}-\lambda \boldsymbol{I})=0\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[
  p(\lambda)=\left(\lambda-\lambda_{1}\right)^{n_{1}}\left(\lambda-\lambda_{2}\right)^{n_{2}} \cdots\left(\lambda-\lambda_{N_{\lambda}}\right)^{n_{N_{\lambda}}}=0
  \]</div>
<p>where <span class="math notranslate nohighlight">\(N_\lambda \le N\)</span> is the number of distinct solutions.</p>
<ul class="simple">
<li><p>The integer <span class="math notranslate nohighlight">\(n_i\)</span> is called the <strong>algebraic multiplicity</strong> of <span class="math notranslate nohighlight">\(\lambda_i\)</span>.</p></li>
<li><p>If the field of scalars is algebraically closed, the algebraic multiplicities sum to <span class="math notranslate nohighlight">\(N\)</span>: <span class="math notranslate nohighlight">\(\sum_{i=1}^{N_{\lambda}} n_{i}=N\)</span>.</p></li>
</ul>
</li>
<li><p>For a fixed eigenvalue <span class="math notranslate nohighlight">\(\lambda_i\)</span>, the solution to <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> to the eigenvalue equation <span class="math notranslate nohighlight">\((\boldsymbol{A} - \lambda_i \boldsymbol{I} )\boldsymbol{v} = \boldsymbol{0}\)</span> is not unique.</p>
<ul class="simple">
<li><p>Any scaling <span class="math notranslate nohighlight">\(c \boldsymbol{v}\)</span> is also an solution.</p></li>
<li><p>There may be <span class="math notranslate nohighlight">\(m_i\)</span> linearly independent solutions. A linear combinations of the <span class="math notranslate nohighlight">\(m_i\)</span> solutions is also an solution. The number <span class="math notranslate nohighlight">\(m_i\)</span> is called the <strong>geometric multiplicity</strong> of <span class="math notranslate nohighlight">\(\lambda_i\)</span>. We have <span class="math notranslate nohighlight">\(m_i \le n_i\)</span>.</p></li>
<li><p>The total number of linearly independent eigenvectors can be calculated by summing the geometric multiplicities <span class="math notranslate nohighlight">\(\sum_{i=1}^{N_{\lambda}} m_{i}=N_{\boldsymbol{v}} \le N\)</span> with equality iff <span class="math notranslate nohighlight">\(m_i = n_i\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(E=\{\boldsymbol{v}:(\boldsymbol{A} -\lambda_i \boldsymbol{I} ) \boldsymbol{v}=\boldsymbol{0}\}\)</span> is called the <strong>eigenspace</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> associated with <span class="math notranslate nohighlight">\(\lambda_i\)</span>. We have <span class="math notranslate nohighlight">\(\operatorname{dim}(E)=m_i\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Claims</p>
<ul class="simple">
<li><p>eigenvectors corresponds to distinct eigenvalues are independent.</p></li>
<li><p>Perron-Frobenius theorem: If <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R} ^{n \times n}\)</span> has all entries positive <span class="math notranslate nohighlight">\(a_{ij} &gt; 0\)</span>, then it has a unique largest eigenvalue, and the corresponding eigenvector can be chosen to have all entires positive.</p></li>
</ul>
<p>For more details see <a class="reference external" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenvalues_and_eigenvectors_of_matrices">Wikipedia</a>.</p>
</div>
</div>
</div>
<div class="section" id="special-matrices">
<h2>Special Matrices<a class="headerlink" href="#special-matrices" title="Permalink to this headline">¶</a></h2>
<div class="section" id="symmetric-matrices">
<h3>Symmetric Matrices<a class="headerlink" href="#symmetric-matrices" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>A matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is <strong>symmetric</strong> if <span class="math notranslate nohighlight">\(\boldsymbol{A} ^\top =\boldsymbol{A}\)</span>. This is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathrm{Sym}\)</span>.</p>
</dd>
</dl>
</div>
<div class="section" id="orthogonal-matrices">
<h3>Orthogonal Matrices<a class="headerlink" href="#orthogonal-matrices" title="Permalink to this headline">¶</a></h3>
<p>Aka rotation matrices.</p>
<dl class="simple myst">
<dt>Definition</dt><dd><p>A real square matrix <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is <strong>orthogonal</strong> if <span class="math notranslate nohighlight">\(\boldsymbol{U} ^{-1} = \boldsymbol{U} ^\top\)</span>. Equivalently, if its columns and rows are orthonormal: <span class="math notranslate nohighlight">\(\boldsymbol{U} ^{\top} \boldsymbol{U} = \boldsymbol{U} \boldsymbol{U} ^{\top} = \boldsymbol{I}\)</span>.</p>
</dd>
</dl>
<p>For instance, <span class="math notranslate nohighlight">\(\boldsymbol{R} =\left[\begin{array}{cc}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{array}\right]\)</span> is a rotation matrix rotates points in the <span class="math notranslate nohighlight">\(xy\)</span>-plane counterclockwise through an angle <span class="math notranslate nohighlight">\(\theta\)</span> with respect to the <span class="math notranslate nohighlight">\(x\)</span> axis about the origin of a two-dimensional Cartesian coordinate system. Given a point <span class="math notranslate nohighlight">\(\boldsymbol{v} = (x, y)\)</span>, after rotation its coordinates becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R \boldsymbol{v} =\left[\begin{array}{cc}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{l}
x \cos \theta-y \sin \theta \\
x \sin \theta+y \cos \theta
\end{array}\right]
\end{split}\]</div>
<dl class="simple myst">
<dt>Properties</dt><dd><p>Transformation by <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> preserves the length of a vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, the angle between two vectors <span class="math notranslate nohighlight">\(\boldsymbol{x} , \boldsymbol{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \|\boldsymbol{U}\boldsymbol{x} \| &amp;=\|\boldsymbol{x}\| \\
  (\boldsymbol{U x})^{\top} \boldsymbol{U} \boldsymbol{y} &amp;=\boldsymbol{x}^{\top} \boldsymbol{y}
  \end{aligned}\end{split}\]</div>
<p>For a distribution <span class="math notranslate nohighlight">\(f(\boldsymbol{x}; \boldsymbol{\theta})\)</span>, rotation <span class="math notranslate nohighlight">\(\boldsymbol{U} ^{\top} \boldsymbol{x}\)</span> preserves its shape, i.e. we can find <span class="math notranslate nohighlight">\(\boldsymbol{\theta} ^\prime\)</span> such that <span class="math notranslate nohighlight">\(f(\boldsymbol{x}; \boldsymbol{\theta}) = f(\boldsymbol{U} ^{\top} \boldsymbol{x}; \boldsymbol{\theta} ^\prime)\)</span>, for all <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>.</p>
</dd>
</dl>
</div>
<div class="section" id="idempotent-matrices">
<h3>Idempotent Matrices<a class="headerlink" href="#idempotent-matrices" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>A matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is idempotent if <span class="math notranslate nohighlight">\(\boldsymbol{A} ^2 = \boldsymbol{A}\)</span>.</p>
</dd>
<dt>Properties</dt><dd><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is idempotent, then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{A}\)</span> is also idempotent</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} ^\top \boldsymbol{A} \boldsymbol{U}\)</span> is idempotent if <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is orthogonal</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A} ^n = \boldsymbol{A}\)</span> for all positive integer <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>it is non-singular iff <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{I}\)</span>.</p></li>
<li><p>has eigenvalues 0 or 1 since <span class="math notranslate nohighlight">\(\lambda \boldsymbol{v} = \boldsymbol{A} \boldsymbol{v}  = \boldsymbol{A} ^2 \boldsymbol{v} = \lambda \boldsymbol{A} \boldsymbol{v} = \lambda^2 \boldsymbol{v}\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is also symmetric, then</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\operatorname{rank}\left( \boldsymbol{A} \right) = \operatorname{tr}\left( \boldsymbol{A}  \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{rank}\left( \boldsymbol{A}  \right) = r \Rightarrow \boldsymbol{A}\)</span> has <span class="math notranslate nohighlight">\(r\)</span> eigenvalues equal to 1 and <span class="math notranslate nohighlight">\(n-r\)</span> equal to <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{rank}\left( \boldsymbol{A}  \right) = n \Rightarrow \boldsymbol{A} = \boldsymbol{I} _n\)</span></p></li>
</ul>
</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="reflection-matrices">
<h3>Reflection Matrices<a class="headerlink" href="#reflection-matrices" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Householder reflection)</dt><dd><p>A Householder transformation (aka Householder reflection) is a linear transformation that describe a reflection about a plane or hyperplane containing the origin. The reflection of a point <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> about a hyperplane defined by <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is the linear transformation</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{x} - 2 \langle \boldsymbol{x}, \boldsymbol{v}  \rangle \boldsymbol{v}
  \]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is the unit vector that is orthogonal to the hyperplane.</p>
</dd>
<dt>Definition (Householder matrices)</dt><dd><p>The matrix constructed from this transformation can be expressed in terms of an outer product as</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{H} = \boldsymbol{I} - 2 \boldsymbol{v} \boldsymbol{v} ^{\top}
  \]</div>
</dd>
</dl>
<p>Properties</p>
<ul class="simple">
<li><p>symmetric: <span class="math notranslate nohighlight">\(\boldsymbol{H} = \boldsymbol{H} ^{\top}\)</span></p></li>
<li><p>unitary: <span class="math notranslate nohighlight">\(\boldsymbol{H}^{-1} = \boldsymbol{H} ^{\top}\)</span></p></li>
<li><p>involutory: <span class="math notranslate nohighlight">\(\boldsymbol{H}^{-1} = \boldsymbol{H}\)</span></p></li>
<li><p>has eigenvalues</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(-1\)</span>, since <span class="math notranslate nohighlight">\(\boldsymbol{H} \boldsymbol{v} = - \boldsymbol{v}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(1\)</span> of multiplicity <span class="math notranslate nohighlight">\(n-1\)</span>, since <span class="math notranslate nohighlight">\(\boldsymbol{H} \boldsymbol{u} = \boldsymbol{u}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{u} \perp \boldsymbol{v}\)</span>, and there are <span class="math notranslate nohighlight">\(n-1\)</span> independent vectors orthogonal to <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span></p></li>
</ul>
</li>
<li><p>has determinant <span class="math notranslate nohighlight">\(-1\)</span>.</p></li>
</ul>
</div>
<div class="section" id="projection-matrices">
<h3>Projection Matrices<a class="headerlink" href="#projection-matrices" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition(Projection matrices)</dt><dd><p>A square matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> is called a projection matrix if <span class="math notranslate nohighlight">\(\boldsymbol{P}^2 = \boldsymbol{P}\)</span>. By definition, a projection <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> is idempotent.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(P\)</span> is further symmetric, then it is called an orthogonal projection matrix.</p></li>
<li><p>otherwise it called an oblique projection matrix.</p></li>
</ul>
</dd>
</dl>
<p>(Orthogonal) Projection</p>
<ul class="simple">
<li><p>onto a line for which <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> is a unit vector: <span class="math notranslate nohighlight">\(\boldsymbol{P}_u = \boldsymbol{u} \boldsymbol{u} ^{\top}\)</span></p></li>
<li><p>onto a subspace <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> with orthonormal basis <span class="math notranslate nohighlight">\(\boldsymbol{u} _1, \ldots, \boldsymbol{u} _k\)</span> forming matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{P}_A = \boldsymbol{A} \boldsymbol{A} ^{\top} = \sum_i \langle \boldsymbol{u} _i, \cdot \rangle \boldsymbol{u} _i\)</span></p></li>
<li><p>onto subspace <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> with (not necessarily orthonormal) basis <span class="math notranslate nohighlight">\(\boldsymbol{u} _1, \ldots, \boldsymbol{u} _k\)</span> forming matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{P} _{A}= \boldsymbol{A} \left( \boldsymbol{A} ^{\top} \boldsymbol{A}  \right) ^{-1} \boldsymbol{A} ^{\top}\)</span>. Such as that in linear regression.</p></li>
</ul>
</div>
<div class="section" id="similar-matrices">
<span id="similar-matrix"></span><h3>Similar Matrices<a class="headerlink" href="#similar-matrices" title="Permalink to this headline">¶</a></h3>
<p>Two square matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> are called similar if there exists an invertible matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{B}  = \boldsymbol{P} ^{-1} \boldsymbol{A} \boldsymbol{P}  
\]</div>
<p>Similar matrices represent the same linear operator under two (possibly) different bases, with P being the change of basis matrix. As a result, similar matrices share all properties of their shared underlying operator:</p>
<ul class="simple">
<li><p>Rank</p></li>
<li><p>Characteristic polynomial, and attributes that can be derived from it:</p>
<ul>
<li><p>Eigenvalues, and their algebraic multiplicities</p></li>
<li><p>Determinant</p></li>
<li><p>Trace</p></li>
</ul>
</li>
<li><p>Geometric multiplicities of eigenvalues (but not the eigenspaces, which are transformed according to the base change matrix P used).</p></li>
<li><p>Frobenius normal form</p></li>
<li><p>Jordan normal form, up to a permutation of the Jordan blocks</p></li>
</ul>
<p>Besides, <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is called <strong>diagonalizable</strong> if it is similar to a diagonal matrix.</p>
</div>
<div class="section" id="positive-semi-definite">
<span id="pd-matrix"></span><h3>Positive (Semi-)Definite<a class="headerlink" href="#positive-semi-definite" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definitions</dt><dd><ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is <strong>positive semi-defiinite</strong> (p.s.d.) if <span class="math notranslate nohighlight">\(\boldsymbol{c}^\top \boldsymbol{A} \boldsymbol{c} \ge 0\)</span> for all <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>. This is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{A} \succ \boldsymbol{0}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathrm{PD}\)</span>.</p></li>
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is <strong>positive definite</strong> (p.d.) if <span class="math notranslate nohighlight">\(\boldsymbol{c}^\top \boldsymbol{A} \boldsymbol{c} \ge 0\)</span> for all <span class="math notranslate nohighlight">\(\boldsymbol{c}\ne \boldsymbol{0}\)</span>. This is denoted by <span class="math notranslate nohighlight">\(\boldsymbol{A} \succeq \boldsymbol{0}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathrm{PSD}\)</span>.</p></li>
</ul>
</dd>
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \boldsymbol{A} \in \mathrm{PD} &amp;\Leftrightarrow \lambda_i(\boldsymbol{A}) &gt; 0 \\
  &amp;\Leftrightarrow \exists \text{ non-singular } \boldsymbol{R}: \boldsymbol{A} = \boldsymbol{R} \boldsymbol{R} ^\top ( \text{Cholesky decomposition} )\\
  &amp;\Rightarrow \boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Lambda}  \boldsymbol{U} ^{\top} \quad \text{EVD = SVD}\\
  &amp;\Rightarrow \boldsymbol{A} ^{-1} = \boldsymbol{U} \boldsymbol{\Lambda} ^{-1} \boldsymbol{U} ^{\top} \text{ where } \operatorname{diag}(\boldsymbol{\Lambda} ^{-1} )=\frac{1}{\lambda_i}   \\
  &amp;\Rightarrow \exists \boldsymbol{B} = \boldsymbol{U}
  \boldsymbol{\Lambda}^{1/2} \boldsymbol{U} ^{\top} \in \mathrm{PD}: \boldsymbol{B} ^2 = \boldsymbol{A}, \text{denoted } \boldsymbol{B} = \boldsymbol{A} ^{1/2}  = \sqrt{\boldsymbol{A}}\\
  &amp;\Rightarrow \sqrt{\boldsymbol{A} ^{-1} } = (\sqrt{\boldsymbol{A} })^{-1},\text{denoted } \boldsymbol{A} ^{-1/2} \\
  \boldsymbol{A} \in \mathrm{PSD} &amp;\Leftrightarrow \lambda_i(\boldsymbol{A}) \ge 0 \\
  &amp;\Leftrightarrow \exists \text{ square } \boldsymbol{R}, \operatorname{rank}\left( R \right) = \operatorname{rank}\left( \boldsymbol{A}  \right): \boldsymbol{A} = \boldsymbol{R} \boldsymbol{R} ^\top\\
  &amp;\Rightarrow \exists \boldsymbol{B} = \boldsymbol{U}
  \boldsymbol{\Lambda}^{1/2} \boldsymbol{U} ^{\top} \in \mathrm{PSD}: \boldsymbol{B} ^2 = \boldsymbol{A} \\
  \text{square } \boldsymbol{B} &amp;\Rightarrow \boldsymbol{B} ^\top \boldsymbol{B} \in \mathrm{PSD}\\
  \text{any } \boldsymbol{M} \in \mathbb{R} ^{m \times n} &amp;\Rightarrow \boldsymbol{M} \boldsymbol{M}  ^\top, \boldsymbol{M} ^{\top} \boldsymbol{M} \in \mathrm{PSD}\\
  \end{align}\end{split}\]</div>
</dd>
</dl>
<p>Inequalities</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is p.d., then</p>
<div class="math notranslate nohighlight">
\[
    \max _{\boldsymbol{a}} \frac{\left(\boldsymbol{a}^{\top} \boldsymbol{b} \right)^{2}}{\boldsymbol{a}^{\top} \boldsymbol{A} \boldsymbol{a}} \leq \boldsymbol{b} ^{\top} \boldsymbol{A}^{-1} \boldsymbol{b}
    \]</div>
<p>The equality holds when <span class="math notranslate nohighlight">\(\boldsymbol{a} \propto \boldsymbol{R} ^{-1} \boldsymbol{b}\)</span>. This inequality can be proved by Cauchy-Schwarz inequality <span class="math notranslate nohighlight">\(\left(\boldsymbol{v}^{\top} \boldsymbol{w}\right)^{2} \leq\|\boldsymbol{v}\|^{2}\|\boldsymbol{w}\|^{2}=\left(\boldsymbol{v}^{\top} \boldsymbol{v}\right)\left(\boldsymbol{w}^{\top} \boldsymbol{w}\right)\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{v} = \boldsymbol{A} ^{1/2} \boldsymbol{a} , \boldsymbol{w} = \boldsymbol{A} ^{-1/2} \boldsymbol{b}\)</span>.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is symmetric and <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> is p.d., both of size <span class="math notranslate nohighlight">\(n \times n\)</span>, then for all <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span> ,</p>
<div class="math notranslate nohighlight">
\[
    \lambda_{\min}(\boldsymbol{B} ^{-1} \boldsymbol{A} )
    \le
    \frac{\boldsymbol{a} ^\top \boldsymbol{A} \boldsymbol{a} }{\boldsymbol{a} ^\top \boldsymbol{B} \boldsymbol{a} }  
    \le
    \lambda_{\max}(\boldsymbol{B} ^{-1} \boldsymbol{A} )
    \]</div>
<p>The equality on either side holds when <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span> is proportional to the corresponding eigenvector.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> are p.d.,</p>
<div class="math notranslate nohighlight">
\[
    \max _{\boldsymbol{a} , \boldsymbol{b}} \frac{\left(\boldsymbol{a}^{\top} \boldsymbol{D} \boldsymbol{b}\right)^{2}}{\boldsymbol{a}^{\top} \boldsymbol{A} \boldsymbol{a} \cdot \boldsymbol{b}^{\top} \boldsymbol{B} \boldsymbol{b}}=\theta
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the largest eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{-1} \boldsymbol{D} \boldsymbol{B} ^{-1} \boldsymbol{D} ^\top\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{B} ^{-1} \boldsymbol{D} ^\top \boldsymbol{A} ^{-1} \boldsymbol{D}\)</span>.</p>
<p>The maximum is obtained when <span class="math notranslate nohighlight">\(\boldsymbol{a}\)</span> is proportional to an eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{-1} \boldsymbol{D} \boldsymbol{B} ^{-1} \boldsymbol{D} ^\top\)</span> corresponding to <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> is proportional to an eigenvector of <span class="math notranslate nohighlight">\(\boldsymbol{B} ^{-1} \boldsymbol{D} ^\top \boldsymbol{A} ^{-1} \boldsymbol{D}\)</span> corresponding to <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\boldsymbol{A} , \boldsymbol{\Sigma}\)</span> are p.d., then the function</p>
<div class="math notranslate nohighlight">
\[
    f(\boldsymbol{\Sigma} ) = \log \left\vert \boldsymbol{\Sigma}  \right\vert + \operatorname{tr}\left( \boldsymbol{\Sigma} ^{-1} \boldsymbol{A}  \right)
    \]</div>
<p>is minimized uniquely at <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} =\boldsymbol{A}\)</span>. This is used in the derivation of MLE for multivariate Gaussian.</p>
</li>
</ul>
</div>
<div class="section" id="conditional-negative-definite">
<h3>Conditional Negative Definite<a class="headerlink" href="#conditional-negative-definite" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition (Conditionally negative definite)</dt><dd><p>A symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is called conditionally negative definite (c.n.d.) if <span class="math notranslate nohighlight">\(\boldsymbol{c}^{\top} \boldsymbol{A}  \boldsymbol{c} \le 0\)</span> for all <span class="math notranslate nohighlight">\(\boldsymbol{c}:\boldsymbol{1} ^{\top} \boldsymbol{c} = 0\)</span>.</p>
</dd>
<dt>Theorem (Schoenberg)</dt><dd><p>A symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> with zero diagonal entires is c.n.d. if and only if it can be realized as the square of the mutual Euclidean distance between points: <span class="math notranslate nohighlight">\(a_{ij} = \left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\|\)</span> for <span class="math notranslate nohighlight">\(i, j= 1, \ldots, n\)</span> and some <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \in \mathbb{R} ^d\)</span>.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="matrix-decomposition">
<h2>Matrix Decomposition<a class="headerlink" href="#matrix-decomposition" title="Permalink to this headline">¶</a></h2>
<p>Summary table</p>
<div class="section" id="spectral-decomposition">
<span id="eigen-decomposition"></span><h3>Spectral Decomposition<a class="headerlink" href="#spectral-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Aka eigendecomposition.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>For discussion of uniqueness and independence of eigenvectors, see previous <a class="reference internal" href="#eigenvalue-eigenvector"><span class="std std-ref">section</span></a>.</p>
</div>
<p>If a square matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> has <span class="math notranslate nohighlight">\(n\)</span> independent eigenvectors, then it can be written as <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{U} \boldsymbol{A} \boldsymbol{U} ^{-1}\)</span>. The columns in the matrix <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> are the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix of eigenvalues. We say <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> has eigendecomposition (EVD), and is diagonalizable.</p>
<p>In particular, if <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is symmetric, then <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is a orthogonal matrix, and hence <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U} ^{\top}\)</span>. Moreover, <span class="math notranslate nohighlight">\(\lambda\)</span> are all real.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Suppose that <span class="math notranslate nohighlight">\((\lambda, \boldsymbol{v})\)</span> is a (possibly complex) eigenvalue and eigenvector pair of the a symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. Using the fact that <span class="math notranslate nohighlight">\(\overline{\boldsymbol{A}  \boldsymbol{v}} = \overline{\lambda \boldsymbol{v}} \Rightarrow \boldsymbol{A}  \overline{\boldsymbol{v}} = \overline{\lambda} \overline{\boldsymbol{v}}\)</span> and  <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{\top} = \boldsymbol{A}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\overline{\boldsymbol{v}}^{\top} \boldsymbol{A} \boldsymbol{v}=\overline{\boldsymbol{v}}^{\top}(\boldsymbol{A} \boldsymbol{v})=\overline{\boldsymbol{v}}^{\top}(\lambda \boldsymbol{v})=\lambda(\overline{\boldsymbol{v}} \cdot \boldsymbol{v}) \\
\overline{\boldsymbol{v}}^{\top} \boldsymbol{A} \boldsymbol{v}=(\boldsymbol{A} \overline{\boldsymbol{v}})^{\top} \boldsymbol{v}=(\bar{\lambda} \overline{\boldsymbol{v}})^{\top} \boldsymbol{v}=\bar{\lambda}(\overline{\boldsymbol{v}} \cdot \boldsymbol{v})
\end{aligned}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\boldsymbol{v} \neq \boldsymbol{0}\)</span>, we have <span class="math notranslate nohighlight">\(\overline{\boldsymbol{v}} \cdot \boldsymbol{v} \neq 0\)</span>. Thus <span class="math notranslate nohighlight">\(\lambda=\bar{\lambda}\)</span>, which means <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span>.</p>
</div>
<p>We can write</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A} = \sum_{i=1}^n \lambda_i \boldsymbol{u} _i \boldsymbol{u} _i ^{\top}
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{P}_i = \boldsymbol{u} _i \boldsymbol{u} _i ^{\top}\)</span>, then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{P}_i\)</span> is an orthogonal projection matrix (or projector) to the <span class="math notranslate nohighlight">\(1\)</span>-dimensional eigenspace <span class="math notranslate nohighlight">\(\left\{ c \boldsymbol{u} _i \right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{P} _i\)</span> is idempotent: <span class="math notranslate nohighlight">\(\boldsymbol{P}_i \boldsymbol{P}  _i = \boldsymbol{P} _i\)</span></p></li>
<li><p>its complementary <span class="math notranslate nohighlight">\(\boldsymbol{I} - \boldsymbol{P}_i\)</span> is a projector to <span class="math notranslate nohighlight">\(\left\{ c \boldsymbol{u} _i \right\} ^\bot\)</span>.</p></li>
</ul>
</div>
<div class="section" id="cholesky-decomposition">
<h3>Cholesky Decomposition<a class="headerlink" href="#cholesky-decomposition" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is p.s.d. (p.d.), there exists a (unique) upper triangular matrix <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> with non-negative (positive) diagonal elements such that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A} = \boldsymbol{U} ^\top \boldsymbol{U}
\]</div>
<p>Cholesky decomposition is a special case of LU decomposition.</p>
</div>
<div class="section" id="canonical-decomposition">
<h3>Canonical Decomposition<a class="headerlink" href="#canonical-decomposition" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is symmetric and <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span> is p.d., then there exists a non-singular matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{P} ^\top \boldsymbol{A} \boldsymbol{P} = \boldsymbol{\Lambda} \text{ and } \boldsymbol{P} ^\top \boldsymbol{B} \boldsymbol{P} = \boldsymbol{I} _n\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} =\operatorname{diag}\left( \lambda_1, \lambda_2, \ldots, \lambda_n \right)\)</span> and the <span class="math notranslate nohighlight">\(\lambda_i\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{B} ^{-1} \boldsymbol{A}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{B} ^{-1}\)</span>.</p>
</div>
<div class="section" id="singular-value-decomposition">
<h3>Singular Value Decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definition</dt><dd><p>For any matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R} ^{n \times p}\)</span>, we can write <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V} ^\top\)</span>. where</p>
</dd>
</dl>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{U} \in \mathbb{R} ^{n \times n}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V} \in \mathbb{R} ^{p\times p}\)</span> are orthogonal matrices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix.</p></li>
</ul>
<dl class="simple myst">
<dt>Properties</dt><dd><p>We can also write SVD as</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{A}=\sigma_{1} \boldsymbol{u}_{1} \boldsymbol{v}_{1}^{\top}+\sigma_{2} \boldsymbol{u}_{2} \boldsymbol{v}_{2}^{\top}+\ldots+\sigma_{r} \boldsymbol{u}_{r} \boldsymbol{v}_{r}^{\top}
  \]</div>
<p>where <span class="math notranslate nohighlight">\(r = \operatorname{rank}\left( \boldsymbol{A}  \right)\)</span>.</p>
</dd>
<dd><p>As a result, <span class="math notranslate nohighlight">\(\boldsymbol{A} \boldsymbol{v}=\sigma \boldsymbol{u}, \boldsymbol{A}^{\top} \boldsymbol{u}=\sigma \boldsymbol{v}\)</span>.</p>
</dd>
<dt>Theorem</dt><dd><p>Every matrix has SVD.</p>
</dd>
</dl>
</div>
<div class="section" id="qr-decomposition">
<h3>QR Decomposition<a class="headerlink" href="#qr-decomposition" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="lu-decomposition">
<h3>LU Decomposition<a class="headerlink" href="#lu-decomposition" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="schur-decomposition">
<h3>Schur Decomposition<a class="headerlink" href="#schur-decomposition" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="more-topics">
<h2>More Topics<a class="headerlink" href="#more-topics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="matrix-differentiation">
<h3>Matrix Differentiation<a class="headerlink" href="#matrix-differentiation" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Definitions</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{array}{l}
  \frac{\partial y}{\partial \boldsymbol{x}}=\left(\begin{array}{c}
  \frac{\partial y}{\partial x_{1}} \\
  \vdots \\
  \frac{\partial y}{\partial x_{n}}
  \end{array}\right) \\
  \frac{\partial y}{\partial \boldsymbol{X}}=\left(\begin{array}{ccc}
  \frac{\partial y}{\partial x_{11}} &amp; \cdots &amp; \frac{\partial y}{\partial x_{1 n}} \\
  \vdots &amp; &amp; \vdots \\
  \frac{\partial y}{\partial x_{n 1}} &amp; \cdots &amp; \frac{\partial y}{\partial x_{n n}}
  \end{array}\right)
  \end{array}
  \end{split}\]</div>
</dd>
<dt>Properties</dt><dd><p><span class="math notranslate nohighlight">\(\ \)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \frac{\partial \boldsymbol{a}^{\top} \boldsymbol{x}}{\partial \boldsymbol{x}}&amp;=\boldsymbol{a}\\
  \frac{\partial \boldsymbol{A} \boldsymbol{x}}{\partial \boldsymbol{x}}&amp;=\boldsymbol{A}\\
  \frac{\partial \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}}{\partial \boldsymbol{x}}&amp;=\boldsymbol{A} ^{\top} \boldsymbol{x} + \boldsymbol{A} \boldsymbol{x} \\
  &amp;=2 \boldsymbol{A} \boldsymbol{x} \text { if } \boldsymbol{A} \text { is symmetric. }\\
  \frac{\partial \operatorname{tr}(\boldsymbol{X})}{\partial \boldsymbol{X}}&amp;=\boldsymbol{I}\\
  \frac{\partial \operatorname{tr}(\boldsymbol{A} \boldsymbol{X})}{\partial \boldsymbol{X}}&amp;=\left\{\begin{array}{ll}
  \boldsymbol{A}^{\top}  &amp;\text { if all elements of } \boldsymbol{X} \text { are distinct } \\
  \boldsymbol{A}+\boldsymbol{A}^{\top}-\operatorname{diag}(\boldsymbol{A})  &amp;\text { if } \boldsymbol{X} \text { is symmetric. }
  \end{array}\right.\\
  \frac{\partial|\boldsymbol{X}|}{\partial \boldsymbol{X}}&amp;=\left\{\begin{array}{ll}
  |\boldsymbol{X}|\left(\boldsymbol{X}^{-1}\right)^{\top}  &amp;\text { if all elements of } \boldsymbol{X} \text { are distinct } \\
  |\boldsymbol{X}|\left(2 \boldsymbol{X}^{-1}-\operatorname{diag}\left(\boldsymbol{X}^{-1}\right)\right)^{\top}  &amp;\text { if } \boldsymbol{X} \text { is symmetric. }
  \end{array}\right.
  \end{aligned}
  \end{split}\]</div>
</dd>
</dl>
</div>
<div class="section" id="matrix-norms">
<span id="norm"></span><h3>Matrix Norms<a class="headerlink" href="#matrix-norms" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Matrix Norm <a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/nedlagte-emner/INF-MAT4350/h09/undervisningsmateriale/lecture7.pdf">link</a></p></li>
<li><p>Frobenius Norm <a class="reference external" href="http://mlwiki.org/index.php/Frobenius_Norm">link</a></p></li>
<li><p>Spectral norm: <span class="math notranslate nohighlight">\(\left\| \boldsymbol{A} \right\| _2\)</span> is the largest singular value of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>.</p>
<ul class="simple">
<li><p>Equals the largest eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{\top} \boldsymbol{A}\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\left\| \boldsymbol{u} \right\| =1, \left\| \boldsymbol{v} \right\| =1\)</span>, let <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{u} \boldsymbol{u} ^{\top}  - \boldsymbol{v} \boldsymbol{v} ^{\top}\)</span>, then <span class="math notranslate nohighlight">\(\left\| \boldsymbol{A} \right\| _2 = \sin \theta\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the angle between <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>.</p>
<ul>
<li><p>To prove this, let <span class="math notranslate nohighlight">\(\alpha = \boldsymbol{u} ^{\top} \boldsymbol{v}\)</span>. It is easy to verify that <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> are two eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{A} ^{\top} \boldsymbol{A}\)</span> with the same eigenvalue <span class="math notranslate nohighlight">\(1 - \alpha^2\)</span>. Hence <span class="math notranslate nohighlight">\(\left\| \boldsymbol{A}  \right\| _2 ^2 = 1 - \alpha^2 = 1 - \cos^2\theta\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>For <span class="math notranslate nohighlight">\(\boldsymbol{a} , \boldsymbol{b} \in [m]^d\)</span></p>
<div class="math notranslate nohighlight">
\[
  \|\boldsymbol{a}-\boldsymbol{b}\|_{\infty} \leq\|\boldsymbol{a}-\boldsymbol{b}\|_{2} \leq\|\boldsymbol{a}-\boldsymbol{b}\|_{1} \leq \sqrt{d}\|\boldsymbol{a}-\boldsymbol{b}\|_{2} \leq d\|\boldsymbol{a}-\boldsymbol{b}\|_{\infty}
  \]</div>
</li>
</ul>
</div>
<div class="section" id="johnson-lindenstrauss-lemma">
<h3>Johnson-Lindenstrauss Lemma<a class="headerlink" href="#johnson-lindenstrauss-lemma" title="Permalink to this headline">¶</a></h3>
<dl class="simple myst">
<dt>Lemma (Johnson-Lindenstrauss)</dt><dd><p>For data vectors be <span class="math notranslate nohighlight">\(\boldsymbol{x} _1, \boldsymbol{x} _2, \ldots, \boldsymbol{x} _n \in \mathbb{R} ^d\)</span> and  tolerance <span class="math notranslate nohighlight">\(\epsilon \in (0, \frac{1}{2} )\)</span>, there exists a Lipschitz mapping <span class="math notranslate nohighlight">\(f: \mathbb{R} ^d \rightarrow \mathbb{R} ^k\)</span>, where <span class="math notranslate nohighlight">\(k = \lfloor \frac{24 \log n}{\epsilon^2} \rfloor\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
  (1 - \epsilon) \left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\| ^2 \le \left\| f(\boldsymbol{x}_i ) - f(\boldsymbol{x}_j )\right\| \le (1 + \epsilon) \left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\| ^2
  \]</div>
</dd>
</dl>
<p>How do we construct <span class="math notranslate nohighlight">\(f\)</span>? Consider a random linear mapping: <span class="math notranslate nohighlight">\(f(\boldsymbol{u}) = \frac{1}{\sqrt{k}} \boldsymbol{A} \boldsymbol{u}\)</span> for some <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R} ^{k \times d}\)</span> where <span class="math notranslate nohighlight">\(k &lt; d\)</span> and <span class="math notranslate nohighlight">\(a_{ij} \overset{\text{iid}}{\sim} \mathcal{N} (0, 1)\)</span>. The intuition: the columns of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> are orthogonal to each other in expectation. If indeed orthogonal, then <span class="math notranslate nohighlight">\(\left\| \frac{1}{\sqrt{k}} \boldsymbol{A} \boldsymbol{u}  \right\| = \left\| \boldsymbol{u}  \right\|\)</span>.</p>
<p>To prove it, we need the following lemma.</p>
<dl class="simple myst">
<dt>Lemma (Norm preserving)</dt><dd><p>Fix a vector <span class="math notranslate nohighlight">\(\boldsymbol{u} \in \mathbb{R} ^d\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> preserves its norm in expectation.</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{E} \left[ \left\| \frac{1}{\sqrt{k}} \boldsymbol{A} \boldsymbol{u}   \right\|^2 \right]  = \mathbb{E} [\left\| \boldsymbol{u}  \right\|^2]
  \]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \frac{1}{k} \mathbb{E} [\left\| \boldsymbol{A} \boldsymbol{u}  \right\| ^2]  
  &amp;= \frac{1}{k} \boldsymbol{u} ^{\top} \mathbb{E} [\boldsymbol{A} ^{\top} \boldsymbol{A} ] \boldsymbol{u}    \\
  &amp;= \frac{1}{k} \boldsymbol{u} ^{\top} k \boldsymbol{I}_{n \times n} \boldsymbol{u}     \\
  &amp;= \left\| \boldsymbol{u}  \right\|   ^2 \\
  \end{aligned}\end{split}\]</div>
<p>The second equality holds since</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathbb{E} [\boldsymbol{a} _i ^{\top} \boldsymbol{a} _j] = \left\{\begin{array}{ll}
  \sum_{p=1}^k \mathbb{E} [a_{ik}^2] = \sum_{p=1}^k 1 =k , &amp; \text { if } i=j \\
  0, &amp; \text { otherwise }
  \end{array}\right.
  \end{split}\]</div>
</div>
</dd>
<dt>Lemma (Concentration)</dt><dd><p>Blessing of high dimensionality: things concentrate around mean. The probability of deviation is bounded. We first prove one-side deviation probability. The proof for the other side is similar.</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P} \left( \left\| \frac{1}{\sqrt{k}} \boldsymbol{A} \boldsymbol{u}   \right\| ^2 &gt; (1 + \epsilon) \left\| \boldsymbol{u}  \right\|  ^2 \right)  \le \exp \left( \frac{k}{2} \left( \frac{\epsilon^2}{2} - \frac{\epsilon^3}{2}  \right) \right)
  \]</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{v} = \frac{\boldsymbol{A} \boldsymbol{u} }{\left\| \boldsymbol{u}  \right\| } \in \mathbb{R} ^k\)</span>, it is easy to see <span class="math notranslate nohighlight">\(V_i \sim \mathcal{N} (0, 1)\)</span>. In this case,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \mathbb{P}\left( \left\| \boldsymbol{v} \right\| ^2 &gt; (1 + \epsilon) k\right)
  &amp;= \mathbb{P}\left( \exp (\lambda \left\| \boldsymbol{v}  \right\| ^2) &gt; \exp (1+ \epsilon) k \lambda \right)  \\
  &amp;\le \frac{\mathbb{E} [\exp (\lambda \left\| \boldsymbol{v}  \right\| ^2)] }{\exp [ (1+ \epsilon) k\lambda]}  \quad \because \text{Markov inequality} \\
  &amp;\le \frac{[\mathbb{E} [\exp (\lambda V_i^2)]]^k }{\exp [ (1+ \epsilon) k\lambda]}  \quad \because V_i \text{ are i.i.d.}  \\
  &amp;=  \exp [-(1 + \epsilon) k \lambda] \left( \frac{1}{1-2\lambda}  \right)^{k/2} \\
  \end{aligned}\end{split}\]</div>
<p>The last equality holds since by moment generating function <span class="math notranslate nohighlight">\(\mathbb{E} [e^{tX}] = \frac{1}{\sqrt{1- 2t} }\)</span> for <span class="math notranslate nohighlight">\(X \sim \chi ^2 _1\)</span>.</p>
<p>If we choose <span class="math notranslate nohighlight">\(\lambda = \frac{\epsilon}{2(1+\epsilon)} &lt; \frac{1}{2}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P} (\left\| \boldsymbol{v}  \right\| &gt; (1 + \epsilon)k)  \le \left[ (1+\epsilon)e^{- \epsilon} \right]^{k/2}.
  \]</div>
<p>Then it remains to show <span class="math notranslate nohighlight">\(1+\epsilon \le \exp(\epsilon - \frac{\epsilon^2}{2} +  \frac{\epsilon^3}{2})\)</span> for <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, which is true by derivative test. Plug in this inequality we get the required inequality.</p>
<p>Then by union bound,</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P}\left( \left\| \boldsymbol{v}  \right\| &gt; (1 + \epsilon) k \text{ or }  \left\| \boldsymbol{v}  \right\| &lt; (1 - \epsilon) k \right) \le 2 \exp \left(\frac{k}{2}\left(\frac{\epsilon^{2}}{2}-\frac{\epsilon^{3}}{2}\right)\right)
  \]</div>
</div>
</dd>
</dl>
<p>Now we prove the JL lemma.</p>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof of JL</em></p>
<p>The probability we fail to find an <span class="math notranslate nohighlight">\(\epsilon\)</span>-distortion map for any <span class="math notranslate nohighlight">\((i, j)\)</span> pair is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;= \mathbb{P} \left( \exists i, j: \left\| \boldsymbol{A} \boldsymbol{x}_i - \boldsymbol{A} \boldsymbol{x}_j  \right\|^2 &gt; (1 + \epsilon) \left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\|  ^2  \text{ or } &lt; (1 - \epsilon) \left\| \boldsymbol{x}_i - \boldsymbol{x}_j  \right\|  ^2 \right)   \\
&amp;= \mathbb{P} \left( \cup_{(i,j)} \right)  \\
&amp;\le \binom{n}{2} 2  \exp \left(\frac{k}{2}\left(\frac{\epsilon^{2}}{2}-\frac{\epsilon^{3}}{2}\right)\right)\quad \because \text{union bound} \\
&amp;\le 2 n^2 \exp \left(\frac{k}{2}\left(\frac{\epsilon^{2}}{2}-\frac{\epsilon^{3}}{2}\right)\right)\\
\end{aligned}\end{split}\]</div>
<p>With some choice of <span class="math notranslate nohighlight">\(k\)</span>, this upper bound is <span class="math notranslate nohighlight">\(1 - \frac{1}{n}\)</span>, i.e. there is an <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> chance we get a map with <span class="math notranslate nohighlight">\(\epsilon\)</span> distortion. What if we want a higher probability?</p>
<p>For some <span class="math notranslate nohighlight">\(\alpha\)</span>, if we set, <span class="math notranslate nohighlight">\(k \ge (4 + 2\alpha) \left( \frac{\epsilon^{2}}{2}-\frac{\epsilon^{3}}{2} \right) ^{-1} \log(n)\)</span>, then the embedding <span class="math notranslate nohighlight">\(f(\boldsymbol{x} ) = \frac{1}{\sqrt{k}} \boldsymbol{A} \boldsymbol{x}\)</span> succeeds with probability at least <span class="math notranslate nohighlight">\(1 - \frac{1}{n^\alpha}\)</span>.</p>
</div>
<!-- #### Randomized SVD


SVD for $\boldsymbol{A} \in \mathbb{R} ^{n \times n}$ takes $\mathcal{O} (n^3)$, can we use this intuition for doing faster? References: Gu & Eisenstat, Tygert & Rokhlin, Martin Sison, Halto. Survey of randomized SVD: https://arxiv.org/pdf/0909.4061.pdf

A vanilla algorithm,

- create $\boldsymbol{\Omega}\in \mathbb{R} ^{n \times k}$ with $\Omega_{ij} \overset{\text{iid}}{\sim} \mathcal{N} (0, 1)$
- (find range) compute $\boldsymbol{Y} = \boldsymbol{A} \boldsymbol{\Omega} \in \mathbb{R} ^{n \times k}$, which takes $\mathcal{O} (n^2 k)$. That is, we randomly project $\boldsymbol{A}$ onto $\mathbb{R} ^{ n\times k}$, roughly preserve ranges of $\boldsymbol{A}$, and hence $\operatorname{rank}(\boldsymbol{A})$.
- (store ranges) compute QR decomposition $\boldsymbol{Y} = \boldsymbol{Q} \boldsymbol{R}$, which takes $\mathcal{O} (nk ^2)$. We want $\operatorname{range}(\boldsymbol{Q} ) = \operatorname{range} (\boldsymbol{A} )$
- compute $\tilde{\boldsymbol{A}}  = \boldsymbol{Q} (\boldsymbol{Q} ^{\top} \boldsymbol{A} ) = \boldsymbol{Q} \boldsymbol{B}$ which takes $\mathcal{O} (n^2k )$. If indeed the ranges of $\boldsymbol{Q}$ and $\boldsymbol{A}$ are the same, then $\tilde{\boldsymbol{A}} = \boldsymbol{Q}  \boldsymbol{Q} ^{\top} \boldsymbol{A}$
- SVD of $\boldsymbol{B} = \tilde{\boldsymbol{U} } \boldsymbol{\Sigma} \boldsymbol{V} ^{\top}$, which takes $\mathcal{O} (nk^2 + k^3)$
- return $\tilde{\boldsymbol{A} } = (\boldsymbol{Q} \tilde{\boldsymbol{U} }) \boldsymbol{\Sigma} \boldsymbol{V} ^{\top} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V} ^{\top}$.

If $\tilde{\boldsymbol{A}} \approx \boldsymbol{A}$ then we have the total time is $\mathcal{O} (n^2 k)$. How to improve this?

There can by other choices of $\boldsymbol{\Omega}$. For instance, in fast JL algorithm, $\boldsymbol{\Omega} = \boldsymbol{S} \boldsymbol{F} \boldsymbol{D}$, where
- $\boldsymbol{S}$ is a $k \times n$ sampling matrix having on non-zero entry in each row at random
- $\boldsymbol{F}$ is an $n \times n$ Fourier matrix
- $\boldsymbol{D}$ is an $n \times n$ diagonal matrix $d_i \pm 1$ entires with equal probability

The total complexity if $\mathcal{O} (k + n\log n + n)$.

#### Analysis and Speed Up

Lemma 1

Let
- $\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V} ^{\top}$
- usually $l \ge k$
- assume $\boldsymbol{\Omega}_1$ has full row rank
- $\boldsymbol{\Sigma} _1 = \boldsymbol{\Sigma} _{[:k, :k]}, \boldsymbol{\Sigma} _2 = \boldsymbol{\Sigma} _{[k+1:, k+1:]}$
- $V^{*} \Omega=\left[\begin{array}{l}
V_{1}^{*} \\
V_{2}^{*}
\end{array}\right] \Omega=\left[\begin{array}{l}
\Omega_{1} \\
\Omega_{2}
\end{array}\right]$
- $\boldsymbol{\Omega} _1 = \boldsymbol{\Omega} _{[:k, :]}, \boldsymbol{\Omega} _2 = \boldsymbol{\Omega} _{[k+1:, :]}$
- $\boldsymbol{P} _Y = \boldsymbol{Q} \boldsymbol{Q} ^{\top}$

Error after projection

$$
\left\| \boldsymbol{A} - \boldsymbol{P}_Y \boldsymbol{A} \right\| ^2 \le \left\| \boldsymbol{\Sigma} _2 \right\| ^2 + \left\| \boldsymbol{\Sigma} _2 \boldsymbol{\Omega}_2 \boldsymbol{\Omega}_1 ^{\dagger}  \right\| ^2
$$

where $\left\| \cdot \right\|$ is $\left\| \cdot \right\| _2$ or $\left\| \cdot \right\| _F$.

Implication:
Suppose $\boldsymbol{A} _v$ is indeed rank $k$, then $\boldsymbol{\Sigma} _2 = 0$, as long as ... Error = 0.

In $\boldsymbol{A} \boldsymbol{\Omega}$ We are forming random linear combinations of columns of $\boldsymbol{A}$.

Long proof!


Lemma 2
: Let $\boldsymbol{G}$ of size $k \times (k + p)$ be a standard Gaussian random matrix, with $p \ge 2$. Then $\mathbb{E} [\left\| \boldsymbol{G} ^ \dagger  \right\|_F^2 ]^{1/2} = \sqrt{\frac{k}{p-1} }$.

Lemma 3
: Let $\boldsymbol{Z}$ be $n \times n$ a random matrix with independent standard normal entires, $\boldsymbol{A}$ and $\boldsymbol{B}$ be constant matrices of appropriate sizes, then since Frobenius norm is consistent


$$\begin{aligned}
\mathbb{E} \left[ \left\| \boldsymbol{A} \boldsymbol{Z} \boldsymbol{B}  \right\| _F^2  \right] &\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[  \left\|\boldsymbol{Z} \right\| _F^2 \right]\left\| \boldsymbol{B}  \right\| _F^2  \\
&\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[ \operatorname{tr}\left( \boldsymbol{Z} ^{\top} \boldsymbol{Z} \right) \right]\left\| \boldsymbol{B}  \right\| _F^2 \\
&\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[ \operatorname{tr}\left( \boldsymbol{Z} ^{\top} \boldsymbol{Z} \right) \right]\left\| \boldsymbol{B}  \right\| _F^2
\end{aligned}$$


Theorem
: By Lemma 1 and Lemma 2

$$
\mathbb{E} [\left\| (\boldsymbol{I} - \boldsymbol{P}_Y ) \boldsymbol{A} \right\|_F ]  \le \left( 1 + \frac{k}{p-1}  \right)^{1/2} \left( \sum_{j > k} \sigma^2 _j  \right)^{1/2}
$$


:::{admonition,dropdown,seealso} *Proof*

Since $\boldsymbol{V} ^*$ is orthogonal, $\boldsymbol{V} ^* \boldsymbol{\Omega}$ has independent $\mathcal{N} (0, 1)$ entires. Hence, $\boldsymbol{\Omega}$ and $\boldsymbol{\Omega}$ are independent,

$$\begin{aligned}
\mathbb{E} \left[ \left\| \boldsymbol{A} \boldsymbol{Z} \boldsymbol{B}  \right\| _F^2  \right] &\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[  \left\|\boldsymbol{Z} \right\| _F^2 \right]\left\| \boldsymbol{B}  \right\| _F^2  \\
&\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[ \operatorname{tr}\left( \boldsymbol{Z} ^{\top} \boldsymbol{Z} \right) \right]\left\| \boldsymbol{B}  \right\| _F^2 \\
&\le \left\| \boldsymbol{A} \right\| _F^2\mathbb{E} \left[ \operatorname{tr}\left( \boldsymbol{Z} ^{\top} \boldsymbol{Z} \right) \right]\left\| \boldsymbol{B}  \right\| _F^2
\end{aligned}$$

:::

Conclusion: not bad as first $k$ approximation.

### Interpolative Decomposition

Less accurate.

Fourier matrix + sparse vector = dense vector with a spike. -->
</div>
<div class="section" id="low-rank-approximation">
<h3>Low-rank Approximation<a class="headerlink" href="#low-rank-approximation" title="Permalink to this headline">¶</a></h3>
<p>Problem:</p>
<div class="math notranslate nohighlight">
\[
\min _{\widehat{\boldsymbol{A}}} \quad\|\boldsymbol{A}-\widehat{\boldsymbol{A}}\|_{F} \quad \text { s.t. } \quad \operatorname{rank}(\widehat{\boldsymbol{A}}) \leq r
\]</div>
<p>Eckart–Young–Mirsky theorem: We claim that the best rank <span class="math notranslate nohighlight">\(r\)</span> approximation to <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> in the Frobenius norm, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{A}_{r}=\sum_{i=1}^{k} \sigma_{i} u_{i} v_{i}^{\top}\)</span>. The minimum equals</p>
<div class="math notranslate nohighlight">
\[
\left\|\boldsymbol{A}-\boldsymbol{A}_{r}\right\|_{F}^{2}=\left\|\sum_{i=r+1}^{n} \sigma_{i} u_{i} v_{i}^{\top}\right\|_{F}^{2}=\sum_{i=r+1}^{n} \sigma_{i}^{2}
\]</div>
</div>
<div class="section" id="perturbation-and-davis-kahan-theorem">
<span id="davis-kahan"></span><h3>Perturbation and Davis-Kahan Theorem<a class="headerlink" href="#perturbation-and-davis-kahan-theorem" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://www.cs.columbia.edu/~djhsu/coms4772-f16/lectures/davis-kahan.pdf">Reference</a></p>
<p>Suppose we want to recover the top <span class="math notranslate nohighlight">\(r\)</span> subspace of <strong>symmetric</strong> matrix <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> via <span class="math notranslate nohighlight">\(\hat{\boldsymbol{M}} = \boldsymbol{M} + \boldsymbol{H}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> is some small perturbation. Let their spectral decomposition be</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{M} &amp;= [\boldsymbol{U} _0 \ \boldsymbol{U} _1] \left[\begin{array}{cc}
\boldsymbol{\Lambda} _0 &amp; 0 \\
0 &amp; \boldsymbol{\Lambda} _1
\end{array}\right] \left[\begin{array}{c}
\boldsymbol{U} _0  ^{\top} \\
\boldsymbol{U} _1 ^{\top}
\end{array}\right] \\
\widehat{\boldsymbol{M}} &amp;= [\widehat{\boldsymbol{U}} _0 \ \widehat{\boldsymbol{U}} _1] \left[\begin{array}{cc}
\widehat{\boldsymbol{\Lambda}} _0 &amp; 0 \\
0 &amp; \widehat{\boldsymbol{\Lambda}} _1
\end{array}\right] \left[\begin{array}{c}
\widehat{\boldsymbol{U}} _0  ^{\top} \\
\widehat{\boldsymbol{U}} _1 ^{\top}
\end{array}\right]
\end{aligned}\end{split}\]</div>
<p>where the partition is at <span class="math notranslate nohighlight">\(r\)</span>.</p>
<p>We need a distance measure between subspaces <span class="math notranslate nohighlight">\(\boldsymbol{U} _0\)</span> and <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{U} }_0\)</span>, or in general, distance measure between two orthogonal matrices <span class="math notranslate nohighlight">\(\boldsymbol{X} = [\boldsymbol{X} _0 \ \boldsymbol{X} _1]\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Z} = [\boldsymbol{Z} _0 \ \boldsymbol{Z} _1]\)</span>. All norms below are spectral norms (maximum singular value of a matrix).</p>
<ul class="simple">
<li><p>bad idea: <span class="math notranslate nohighlight">\(\operatorname{dist}(\boldsymbol{X} _0, \boldsymbol{Z} _0) = \left\| \boldsymbol{X} _0 - \boldsymbol{Z} _0 \right\|_2\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{Z} _0\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Z} _0 \boldsymbol{Q}\)</span> spans the same column space (rotation of bases) for all orthogonal transformation <span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span></p></li>
<li><p>after rotation, distance changes, not a good measure.</p></li>
</ul>
</li>
<li><p>good idea: <span class="math notranslate nohighlight">\(\operatorname{dist}(\boldsymbol{X} _0, \boldsymbol{Z} _0) = \left\| \boldsymbol{X} _0 \boldsymbol{X} _0 ^{\top}  - \boldsymbol{Z} _0 \boldsymbol{Z} _0 ^{\top} \right\|_2\)</span></p>
<ul>
<li><p>invariant to rotation transformation</p></li>
<li><p>essentially compare projection</p></li>
</ul>
</li>
</ul>
<p>Lemmas (of the good idea)</p>
<ol class="simple">
<li><p>The SVD of <span class="math notranslate nohighlight">\(\boldsymbol{X} _0 ^{\top} \boldsymbol{Z} _0\)</span> can be expressed as <span class="math notranslate nohighlight">\(\boldsymbol{U} \cos (\Theta) \boldsymbol{V} ^{\top}\)</span> where <span class="math notranslate nohighlight">\(\Theta = \operatorname{diag} (\theta_1, \ldots, \theta_r)\)</span> is called the principal angles between two subspaces. (think about <span class="math notranslate nohighlight">\(r=1\)</span> case). We have</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\left\| X_0 ^{\top} \boldsymbol{Z} _1 \right\| = \left\| \sin \Theta \right\| = \max \left\{ |\sin \theta_1|, \ldots, |\sin \theta_r |\right\}\]</div>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{dist}(\boldsymbol{X} _0, \boldsymbol{Z} _0) = \left\| \boldsymbol{X} ^{\top} _0 \boldsymbol{Z} _1\right\| = \left\| \boldsymbol{Z} _0 ^{\top} \boldsymbol{X} _1 \right\|\)</span></p>
<ul class="simple">
<li><p>in particular, if <span class="math notranslate nohighlight">\(\boldsymbol{X} _0 = \boldsymbol{Z} _0\)</span>, then the distance measure should be <span class="math notranslate nohighlight">\(0\)</span>, and the RHS is indeed 0.</p></li>
</ul>
</li>
</ol>
<div class="note dropdown admonition">
<p class="admonition-title"> Example of Lemma 1</p>
<p>Think about a simple case in <span class="math notranslate nohighlight">\(2\)</span>-d: <span class="math notranslate nohighlight">\(\boldsymbol{X} = \frac{1}{\sqrt{2}}  \left[\begin{array}{cc}
1 &amp; -1 \\
1 &amp; 1
\end{array}\right]\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Z} = \left[\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1
\end{array}\right]\)</span>, then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\cos \theta_{( \boldsymbol{x} _0, \boldsymbol{z} _0 )} = \cos(\theta_1 = \frac{\pi}{4} ) = \boldsymbol{x} _0 ^{\top} \boldsymbol{z} _0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\cos \theta_{(\boldsymbol{x} _0, \boldsymbol{z} _1 )} = \cos(\frac{\pi}{2} - \theta_1 ) = \sin(\theta_1) = \boldsymbol{x} _0 ^{\top} \boldsymbol{z} _1\)</span>.</p></li>
</ul>
</div>
<div class="dropdown seealso admonition">
<p class="admonition-title"> <em>Proof</em></p>
<p>Lemma 1</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\| \boldsymbol{X} _0 ^{\top} \boldsymbol{Z} _1 \right\|
&amp;= \left\| \boldsymbol{X} _0 ^{\top} \boldsymbol{Z} _1 \boldsymbol{Z} _1 ^{\top} \boldsymbol{X} _0 \right\|^{1/2}  \quad \because \text{SVD} \\
&amp;= \left\| \boldsymbol{X} _0 (\boldsymbol{I} - \boldsymbol{Z} _0 \boldsymbol{Z} _0 ^{\top} ) \boldsymbol{X} _0 \right\|^{1/2} \\
&amp;= \left\| \boldsymbol{I} _{r} - \boldsymbol{X} _0 ^{\top}  \boldsymbol{Z} _0 \boldsymbol{Z} _0 ^{\top} \boldsymbol{X} _0 \right\|^{1/2} \\
&amp;= \left\| \boldsymbol{I} _{r} - \boldsymbol{U} [\cos \boldsymbol{\Theta}]^2 \boldsymbol{U} ^{\top} \right\|^{1/2} \\
&amp;= \left\| [\sin \boldsymbol{\Theta}]^2  \right\|^{1/2} \\
&amp;= \left\| \sin \boldsymbol{\Theta}  \right\|\\
\end{aligned}\end{split}\]</div>
<p>Lemma 2</p>
<p>By Lemma 1, it remains to show <span class="math notranslate nohighlight">\(\left\| \boldsymbol{X} _0 \boldsymbol{X} _0 ^{\top}  - \boldsymbol{Z} _0 \boldsymbol{Z} _0 ^{\top} \right\| = \left\| \sin \boldsymbol{\Theta} \right\|\)</span>. Write</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{Z} _0 = \boldsymbol{X} \boldsymbol{X} ^{\top} \boldsymbol{Z} _0 = \boldsymbol{X} \left[\begin{array}{cc}
\boldsymbol{X} _0 ^{\top}  \\
\boldsymbol{X} _1 ^{\top}
\end{array}\right] \boldsymbol{Z} _0 = \boldsymbol{X} \left[\begin{array}{cc}
\boldsymbol{U} \cos(\boldsymbol{\Theta} ) \boldsymbol{V}  ^{\top}  \\
\boldsymbol{\tilde{U}} \sin(\boldsymbol{\Theta} ) \boldsymbol{V}  ^{\top}
\end{array}\right] \end{split}\]</div>
<p>…</p>
</div>
<dl class="simple myst">
<dt>Theorem (Davis-Kahan <span class="math notranslate nohighlight">\(\sin(\boldsymbol{\Theta} )\)</span>)</dt><dd><p>If there exists <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(\Delta &gt;0\)</span> such that <span class="math notranslate nohighlight">\(\lambda_r(\boldsymbol{M}) \ge a\)</span> and <span class="math notranslate nohighlight">\(a - \Delta \ge \lambda_{r+1} (\widehat{\boldsymbol{M}})\)</span>, i.e. the two eigenvalues are at least <span class="math notranslate nohighlight">\(\Delta\)</span> apart, then the perturbation error to subspaces is bounded by</p>
</dd>
</dl>
<div class="math notranslate nohighlight">
\[
\left\| \boldsymbol{U}_0 \boldsymbol{U} _0 ^{\top} - \boldsymbol{\widehat{U}}_0 \boldsymbol{\widehat{U}} _0 ^{\top}  \right\| = \left\| \sin \boldsymbol{\Theta} \right\| \le \frac{\left\| \boldsymbol{H} \boldsymbol{U} _0\right\| }{\Delta} \le \frac{\left\| \boldsymbol{H} \right\| }{\Delta}
\]</div>
<p>The <span class="math notranslate nohighlight">\(\Delta\)</span> term in the bound involves eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{M}}\)</span>, can we simplify that? By Weyl’s inequality <span class="math notranslate nohighlight">\(\left\vert \lambda_i(\boldsymbol{M} )  - \lambda_i (\widehat{\boldsymbol{M}})\right\vert \le \left\| \boldsymbol{H}  \right\|\)</span>. Therefore, we can let</p>
<div class="math notranslate nohighlight">
\[\Delta = \lambda_r(\boldsymbol{M} ) - \lambda_{r+1} (\widehat{\boldsymbol{M}}) \ge \lambda_r(\boldsymbol{M} ) - \lambda_{r+1} (\boldsymbol{M} ) - \left\| \boldsymbol{H}  \right\| \]</div>
<p>and hence the bound becomes</p>
<div class="math notranslate nohighlight">
\[
\operatorname{dist}(\boldsymbol{U} _0, \boldsymbol{\widehat{\boldsymbol{U}}} _0) \le \frac{\left\| H \right\| }{\lambda_r(\boldsymbol{M} ) - \lambda_{r+1} (\boldsymbol{M} ) - \left\| \boldsymbol{H}  \right\|}  
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_r(\boldsymbol{M} ) - \lambda_{r+1} (\boldsymbol{M} )\)</span> is the spectral gap between the <span class="math notranslate nohighlight">\(r\)</span>-th and the <span class="math notranslate nohighlight">\((r+1)\)</span>-th eigenvalues. We can see that the bound is smaller if there is a sharp gap, and increases as noise <span class="math notranslate nohighlight">\(\left\| \boldsymbol{H} \right\|\)</span> increases.</p>
<p>More topics</p>
<ul class="simple">
<li><p>Wigner semicircle distribution of eigenvalues of a random Gaussian matrix <a class="reference external" href="https://en.wikipedia.org/wiki/Wigner_semicircle_distribution">Wikipedia</a></p></li>
</ul>
</div>
<div class="section" id="marchenkopastur-distribution">
<span id="marchenko-pastur-distribution"></span><h3>Marchenko–Pastur Distribution<a class="headerlink" href="#marchenkopastur-distribution" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The distribution describe the spectrum of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}\)</span>, i.e. the <strong>limiting histogram</strong> of its <span class="math notranslate nohighlight">\(p\)</span> eigenvalues as <span class="math notranslate nohighlight">\(p,n \rightarrow \infty\)</span>.</p>
</div>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \sim \mathcal{N} (\boldsymbol{0} , \boldsymbol{I} _p)\)</span>, then the eigenvalues of the (biased) sample covariance matrix <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Sigma}}= \frac{1}{n} \boldsymbol{X} ^{\top} \boldsymbol{X}\)</span>, as <span class="math notranslate nohighlight">\(p, n \rightarrow \infty\)</span>, follows Marchenko–Pastur distribution parameterized by <span class="math notranslate nohighlight">\(\gamma = \lim _{n, p \rightarrow \infty} \frac{p}{n}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu^{M P}(t)=\left(1-\frac{1}{\gamma}\right) \delta(x) \mathbb{I}  \left\{ \gamma&gt;1 \right\}+\left\{\begin{array}{ll}
0 &amp; t \notin[\gamma_{-}, \gamma_{+}], \\
\frac{\sqrt{(\gamma_{+}-t)(t-\gamma_{-})}}{2 \pi \gamma t} \mathrm{~d} t &amp; t \in[\gamma_{-}, \gamma_{+}],
\end{array}\right.
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\gamma_{\pm} = (1 \pm \sqrt{\gamma})^2\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\gamma \le 1\)</span>, the distribution has a support on <span class="math notranslate nohighlight">\([\gamma_{-}, \gamma_{+}]\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\gamma &gt; 1\)</span>, it has an additional point mass <span class="math notranslate nohighlight">\(1 - \gamma ^{-1}\)</span> at the origin.</p></li>
</ul>
<div class="figure align-default" id="mp-distribution">
<a class="reference internal image-reference" href="../_images/mp-distribution.png"><img alt="" src="../_images/mp-distribution.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Marchenko-Pastur distribution with <span class="math notranslate nohighlight">\(\gamma=2\)</span> (left, with a point mass at origin) and <span class="math notranslate nohighlight">\(\gamma = 0.5\)</span> (rigt). [Yao]</span><a class="headerlink" href="#mp-distribution" title="Permalink to this image">¶</a></p>
</div>
<p>Note that when <span class="math notranslate nohighlight">\(p\)</span> is fixed and sample size <span class="math notranslate nohighlight">\(n\)</span> increases, <span class="math notranslate nohighlight">\(\gamma \rightarrow 0\)</span>, the interval <span class="math notranslate nohighlight">\(\left[\gamma_{-}, \gamma_{+}\right]\)</span> is tighter, i.e. more concentrated.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./11-math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="31-geometry.html" title="previous page">Geometry</a>
    <a class='right-next' id="next-link" href="51-linear-programming.html" title="next page">Linear Programming</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dennis Zheng<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-150740237-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>